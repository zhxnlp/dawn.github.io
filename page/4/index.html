<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="nlp"><meta name="keywords" content=""><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>You got a dream, you gotta protect it | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><nav id="nav" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="site-info"><div id="site-title">zhxnlpのBlog</div><div id="site-sub-title">You got a dream, you gotta protect it</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/zhxnlp/nlp-transformers" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fa"></i></a><a class="social-icon" href="https://gitee.com/zhxscut" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-google fa"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_56591814" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-weibo fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/18/huggingface/transformers%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94GET%20STARTED/">Hugging Face官方文档——GET STARTED</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/HuggingFace/">HuggingFace</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></span><div class="content"><p>@[toc]</p>
<h2 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h2><p>🤗 Transformers为自然语言理解（NLU）和自然语言生成（NLG）提供通用架构（BERT、GPT-2、RoBERTa、XLM、DistilBert、XLNet…）  具有 100 多种语言的 32 多种预训练模型以及 Jax、PyTorch 和 TensorFlow 之间的深度互操作性。<br><a target="_blank" rel="noopener" href="https://huggingface.co/models">所有模型检查点</a>都从 Huggingface.co <a target="_blank" rel="noopener" href="https://huggingface.co/">模型中心</a>无缝集成，用户和组织直接上传。当前检查点数量22752。</p>
<h2 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h2><p>文档分为五个部分：</p>
<ul>
<li>GET STARTED：包含快速浏览、安装说明和一些关于我们的理念和词汇表的有用信息。</li>
<li>USING 🤗 TRANSFORMERS ：包含有关如何使用库的一般教程。</li>
<li>ADVANCED GUIDES ：包含特定于给定脚本或库的一部分的更高级指南。</li>
<li>RESEARCH 侧重于与如何使用库关系不大的教程，但更多地涉及transformers的一般研究</li>
<li>最后三个部分包含每个公共类和函数的文档，分为：<ul>
<li>MAIN CLASSES ：transformers库的重要 API 的main classes。</li>
<li>models：transformers库实现的每个模型相关的类和函数。</li>
<li>INTERNAL HELPERS ：内部使用的类和函数的内部帮助程序。</div><a class="more" href="/2021/09/18/huggingface/transformers%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94GET%20STARTED/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/15/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/TF-IDF%E5%92%8Cword2vec%E5%8E%9F%E7%90%86/">文本挖掘、Word2vec和gensim实现</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Word2vec/">Word2vec</a></span><div class="content"><p>@(NLP)</p>
<h2 id="文本挖掘"><a href="#文本挖掘" class="headerlink" title="文本挖掘"></a>文本挖掘</h2><p>@[toc]</p>
<h3 id="一-文本挖掘的分词原理"><a href="#一-文本挖掘的分词原理" class="headerlink" title="(一) 文本挖掘的分词原理"></a>(一) 文本挖掘的分词原理</h3><p>　　　　在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的分词原理做一个总结。</p>
<h4 id="1-分词的基本原理"><a href="#1-分词的基本原理" class="headerlink" title="1. 分词的基本原理"></a>1. 分词的基本原理</h4><p>　　　　现代分词都是基于统计的分词，而统计的样本内容来自于一些标准的语料库。假如有一个句子：“小明来到荔湾区”，我们期望语料库统计后分词的结果是：”小明/来到/荔湾/区”，而不是“小明/来到/荔/湾区”。那么如何做到这一点呢？<br></div><a class="more" href="/2021/09/15/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/TF-IDF%E5%92%8Cword2vec%E5%8E%9F%E7%90%86/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/14/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%EF%BC%9A%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">学习笔记1：统计学习方法：概述</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></span><div class="content"><h2 id="一、统计学习方法的概述"><a href="#一、统计学习方法的概述" class="headerlink" title="一、统计学习方法的概述"></a>一、统计学习方法的概述</h2><blockquote>
<p>参考《统计学习方法》第一章</p>
<ol>
<li>统计学习研究对象:数据</li>
<li>统计学习基本假设:同类数据有一定的统计规律性，所以可以用概率统计方法处理它们。用随机变量描述数据中的特征，用概率分布描述数据统计规律</li>
<li>统计学习目的/方法：基于数据通过构建概率统计模型，实现对数据的分析和预测</li>
<li>统计学习三要素：模型、策略（选取最优模型准则）和算法（如何选取最优模型）</div><a class="more" href="/2021/09/14/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%EF%BC%9A%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/13/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB2%EF%BC%9A%E5%93%88%E4%BD%9B%E3%80%8AThe%20Annotated%20Transformer%E3%80%8B%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E6%B3%A8%E9%87%8A%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E3%80%8AAttention%20Is%20All%20You%20Need%E3%80%8B/">选读2：《The Annotated Transformer》翻译</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><p>@[toc]<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">Image(filename=<span class="string">&#x27;images/aiayn.png&#x27;</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20738c10d0fe48f7854387d840d3946f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="Attention is All You Need"></p>
<blockquote>
<p>&#8195;&#8195;本文翻译自<a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">《The Annotated Transformer》</a>。 本文主要由Harvard NLP的学者在2018年初撰写，以逐行实现的形式呈现了论文的“注释”版本,对原始论文进行了重排，并在整个过程中添加了评论和注释。本文的note book可以在<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99Transformer">篇章2</a>下载。</p>
</blockquote>
<p>&#8195;&#8195;<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">“Attention is All You Need”</a> 的 Transformer 在过去的一年里一直在很多人的脑海中出现。 Transformer 在机器翻译质量上有重大改进,它还为许多其它NLP 任务提供了一种新的体系结构。论文本身写得很清楚,但传统的看法是论文很难准确的去实现。  </p>
<p>&#8195;&#8195;接下来你首先需要安装<a target="_blank" rel="noopener" href="http://pytorch.org/">PyTorch</a>. 完整的 notebook 也可以在<a target="_blank" rel="noopener" href="https://github.com/harvardnlp/annotated-transformer">github</a> 或者 Google <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view?usp=sharing">Colab</a>上找到。</p>
<p>&#8195;&#8195;这里的代码主要基于 Harvard NLP的<a target="_blank" rel="noopener" href="http://opennmt.net">OpenNMT</a> 包。 (If helpful feel free to <a href="#conclusion">cite</a>.) 对于模型的其他完整服务实现,请查看 <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> (tensorflow) 和 <a target="_blank" rel="noopener" href="https://github.com/awslabs/sockeye">Sockeye</a> (mxnet).</p>
<ul>
<li>Alexander Rush (<a target="_blank" rel="noopener" href="https://twitter.com/harvardnlp">@harvardnlp</a> or srush@seas.harvard.edu)</div><a class="more" href="/2021/09/13/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB2%EF%BC%9A%E5%93%88%E4%BD%9B%E3%80%8AThe%20Annotated%20Transformer%E3%80%8B%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E6%B3%A8%E9%87%8A%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E3%80%8AAttention%20Is%20All%20You%20Need%E3%80%8B/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/10/huggingface/%E5%9F%BA%E4%BA%8EHugging%20Face%20-Transformers%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">Hugging Face官方文档 ——Transformers预训练模型微调</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/HuggingFace/">HuggingFace</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></span><div class="content"><p>本文参考资料是<a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face主页</a>Resources下的课程,节选部分内容并注释（加粗斜体），也加了Trainer和args的主要参数介绍。感兴趣的同学可以去查看<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1">原文</a>。</p>
<hr>
<p>本章节主要内容包含两部分内容：</p>
<ul>
<li>pipeline工具演示NLP任务处理</li>
<li>构建Trainer微调模型<br></li>
</ul>
<p>@[toc]</p>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.  简介"></a>1.  简介</h2><p>本章节将使用 <a target="_blank" rel="noopener" href="https://github.com/huggingface">Hugging Face 生态系统中的库</a>——🤗 Transformers来进行自然语言处理工作(NLP)。</p>
<h3 id="Transformers的历史"><a href="#Transformers的历史" class="headerlink" title="Transformers的历史"></a>Transformers的历史</h3><p>以下是 Transformer 模型（简短）历史中的一些参考点：<br><img src="https://img-blog.csdnimg.cn/3ba51fe4f21d4d528ca7b0f2fd78aee4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="transformers_chrono"><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer 架构</a>于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括：<br></div><a class="more" href="/2021/09/10/huggingface/%E5%9F%BA%E4%BA%8EHugging%20Face%20-Transformers%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/07/huggingface/Hugging%20Face%E4%B8%BB%E9%A1%B5%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%89%E7%AF%87%E3%80%8AFine-tuning%20a%20pretrained%20model%E3%80%8B/">Hugging Face官方文档——Fine-tuning a pretrained model</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/HuggingFace/">HuggingFace</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></span><div class="content"><h1 id="微调预训练模型"><a href="#微调预训练模型" class="headerlink" title="微调预训练模型"></a>微调预训练模型</h1><p>@[toc]<br>本文翻译自 <a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face主页</a>Resources下的 <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3/1?fw=pt">course</a></p>
<p>说明：有的文章将token、Tokenizer、Tokenization翻译为令牌、令牌器和令牌化。虽然从意义上来说更加准确，但是笔者感觉还是不够简单直接，不够形象。所以文中有些地方会翻译成分词、分词器和分词，有些地方又保留英文（有可能google翻译成标记、标记化没注意到）。有其它疑问可以留言或者查看原文。</p>
<h2 id="1-本章简介"><a href="#1-本章简介" class="headerlink" title="1. 本章简介"></a>1. 本章简介</h2><p>在第 2 章中，我们探讨了如何使用分词器和预训练模型进行预测。 但是，如果您想为自己的数据集微调预训练模型怎么办？ 这就是本章的主题！ 你将学习：</p>
<ul>
<li>如何从Model Hub 准备大型数据集</li>
<li>如何使用high-level Trainer API来微调模型</li>
<li>如何使用自定义训练循环custom training loop</li>
<li>如何利用 🤗 Accelerate 库在任何分布式设置上轻松运行该custom training loop</li>
</ul>
<p>要将经过训练的checkpoint上传到 Hugging Face Hub，您需要一个 Huggingface.co 帐户：<a target="_blank" rel="noopener" href="https://huggingface.co/join">创建一个帐户</a>。<br></div><a class="more" href="/2021/09/07/huggingface/Hugging%20Face%E4%B8%BB%E9%A1%B5%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%89%E7%AF%87%E3%80%8AFine-tuning%20a%20pretrained%20model%E3%80%8B/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/27/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/%E8%8B%8F%E7%A5%9E%E6%96%87%E7%AB%A0%E8%A7%A3%E6%9E%90%EF%BC%886%E7%AF%87%EF%BC%89/">苏神文章解析</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h1 id="苏神文章解析"><a href="#苏神文章解析" class="headerlink" title="苏神文章解析"></a>苏神文章解析</h1><h2 id="1-浅谈Transformer的初始化、参数化与标准化"><a href="#1-浅谈Transformer的初始化、参数化与标准化" class="headerlink" title="1.浅谈Transformer的初始化、参数化与标准化"></a>1.浅谈Transformer的初始化、参数化与标准化</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/400925524?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《浅谈Transformer的初始化、参数化与标准化》</a></p>
<h3 id="1-1采样分布：截尾正态分布"><a href="#1-1采样分布：截尾正态分布" class="headerlink" title="1.1采样分布：截尾正态分布"></a>1.1采样分布：截尾正态分布</h3><p>&#8195;&#8195;一般情况下，我们都是从指定均值和方差的随机分布中进行采样来初始化。其中常用的随机分布有三个：正态分布（Normal）、均匀分布（Uniform）和截尾正态分布（Truncated Normal）<br><img src="https://img-blog.csdnimg.cn/00cc661bee994d4db236d0c07c6bc431.png#pic_center" alt="采样分布"><br>&#8195;&#8195;一般来说，==正态分布的采样结果更多样化一些，但它理论上是无界的，如果采样到绝对值过大的结果可能不利于优化；相反均匀分布是有界的，但采样结果通常更单一。于是就出现了结合两者优点的“截尾正态分布”==。<br><img src="https://img-blog.csdnimg.cn/e84f1003aac0410dbe56ebd289bc8f57.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="截尾正态分布"></p>
<h3 id="1-2-正交初始化：Xavier初始化"><a href="#1-2-正交初始化：Xavier初始化" class="headerlink" title="1.2 正交初始化：Xavier初始化"></a>1.2 正交初始化：Xavier初始化</h3><p>&#8195;&#8195;从第二节可以得出结论：不考虑激活函数的情况下，正交初始化比较好界定。==考虑激活函数==得情况就比较麻烦。relu函数可以从N(0,1/2m)采样来初始化W。如果是是其它激活函数，分期起来比较麻烦甚至做不到完全正交初始化。那么可以考虑的方案是==微调激活函数（激活函数进行各种变换）==<br>详情见第二节</p>
<h3 id="1-3-直接标准化"><a href="#1-3-直接标准化" class="headerlink" title="1.3 直接标准化"></a>1.3 直接标准化</h3><p>&#8195;&#8195;相比上一节各种“微调”，更直接的处理方法是各种Normalization方法，如Batch Normalization、Instance Normalization、Layer Normalization等，这类方法直接计算当前数据的均值方差来将输出结果标准化，而不用事先估计积分，有时候我们也称其为“归一化”。<br>&#8195;&#8195;这三种标准化方法大体上都是类似的，除了Batch Normalization多了一步滑动平均预测用的均值方差外，它们只不过是标准化的维度不一样。比如NLP尤其是Transformer模型用得比较多就是Layer Normalization是：</p>
<p>&#8195;&#8195;Normalization一般都包含了减均值（center）和除以标准差（scale）两个部分，但近来的一些工作逐渐尝试去掉center这一步，甚至有些工作的结果显示去掉center这一步后性能还略有提升。</p>
<p>&#8195;&#8195;比如2019年的论文《Root Mean Square Layer Normalization》比较了去掉center后的Layer Normalization，文章称之为RMS Norm。论文总的结果显示：==RMS Norm比Layer Normalization更快，效果也基本一致==。</p>
<p>&#8195;&#8195;比如2019年的论文《Root Mean Square Layer Normalization》比较了去掉center后的Layer Normalization，文章称之为RMS Norm。</p>
<p>&#8195;&#8195;一个直观的猜测是，center操作，类似于全连接层的bias项，储存到的是关于数据的一种先验分布信息，而把这种先验分布信息直接储存在模型中，反而可能会导致模型的迁移能力下降。所以T5不仅去掉了Layer Normalization的center操作，它把每一层的bias项也都去掉了。</p>
<h3 id="1-4-NTK参数化"><a href="#1-4-NTK参数化" class="headerlink" title="1.4 NTK参数化"></a>1.4 NTK参数化</h3><ul>
<li>Xavier初始化是用“均值为0、方差为1/m的随机分布”初始化。</li>
<li>NTK参数化：用“均值为0、方差为1的随机分布”来初始化，但是将输出结果除以$\sqrt{m}$。高斯过程中被称为“NTK参数化”</li>
</ul>
<p>&#8195;&#8195;==NTK参数化跟直接用Xavier初始化相比，有什么好处吗？==<br>&#8195;&#8195;理论上，利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的量级大致都是相同的O(1)级别，于是我们可以设置较大的学习率，比如$10^{−2}$，并且如果使用自适应优化器，其更新量大致是$\frac{梯度}{\sqrt{梯度\bigotimes 梯度}}\times 学习率$，那么我们就知道$10^{−2}$的学习率每一步对参数的调整程度大致是1%。<br>&#8195;&#8195;总的来说，NTK参数化能让我们更平等地处理每一个参数，并且比较形象地了解到训练的更新幅度，以便我们更好地调整参数。</p>
<p>==为什么Attention中除以$\sqrt{d}$这么重要？==<br>&#8195;&#8195;对于两个d维向量q,k，假设它们都采样自“均值为0、方差为1”的分布，那么它们的内积的二阶矩是：$E[(q,k)]^{2}=d$。由于均值也为0，所以这也意味着方差也是d。</p>
<p>&#8195;&#8195;Attention是内积后softmax，主要设计的运算是$e^{q⋅k}$，我们可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而$e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。</p>
<p>&#8195;&#8195;相应地，解决方法就有两个:</p>
<ul>
<li>像NTK参数化那样，在内积之后除以$\sqrt{d}$，使q⋅k的方差变为1，对应$e^3$,$e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法</li>
<li>另外就是不除以$\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</li>
</ul>
<h3 id="1-5-残差连接"><a href="#1-5-残差连接" class="headerlink" title="1.5 残差连接"></a>1.5 残差连接</h3><h2 id="2-模型参数的初始化"><a href="#2-模型参数的初始化" class="headerlink" title="2.模型参数的初始化"></a>2.模型参数的初始化</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/7180">《从几何视角来理解模型参数的初始化策略》</a></p>
<h3 id="2-1-总结"><a href="#2-1-总结" class="headerlink" title="2.1 总结"></a>2.1 总结</h3><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>初始化</th>
</tr>
</thead>
<tbody>
<tr>
<td>m=n</td>
<td>Xavier初始化：从N(0,1/n)采样来初始化W</td>
</tr>
<tr>
<td>m&lt;n</td>
<td>无法做到正交初始化</td>
</tr>
<tr>
<td>m≥n</td>
<td>从N(0,1/m)采样来初始化W</td>
</tr>
<tr>
<td>tanh激活函数</td>
<td>Xavier初始化直接适用于tanh激活</td>
</tr>
<tr>
<td>relu激活函数</td>
<td>从N(0,1/2m)采样来初始化W</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-2-为啥要正交初始化"><a href="#2-2-为啥要正交初始化" class="headerlink" title="2.2 为啥要正交初始化"></a>2.2 为啥要正交初始化</h3><p>&#8195;&#8195;对于复杂模型来说，参数的初始化显得尤为重要。糟糕的初始化，很多时候已经不单是模型效果变差的问题了，还更有可能是模型根本训练不动或者不收敛。在深度学习中常见的自适应初始化策略是Xavier初始化。<br>&#8195;&#8195;深度学习模型本身上就是一个个全连接层的嵌套，所以为了使模型最后的输出不至于在初始化阶段就过于“膨胀”或者“退化”，一个想法就是让模型在初始化时能保持模长不变。<br>&#8195;&#8195;正交矩阵是指满足$W^⊤W=I$的矩阵，也就是说它的逆等于转置。正交矩阵的重要意义在于它在变换过程中保持了向量的模长不变.用数学公式来表达，就是设$W\in \mathbb{R}^{n\times n}$是一个正交矩阵，而$x\in \mathbb{R}^{n}$是任意向量，则x的模长等于$W_{x}$的模长：</p>
<script type="math/tex; mode=display">\left \| W_{x} \right \|^{2}=x^{T}W^{T}Wx=x^{T}x=x</script><p>&#8195;&#8195;这个想法形成的一个自然的初始化策略就是“以全零初始化b，以随机正交矩阵初始化W”</p>
<h3 id="2-3-Xavier初始化"><a href="#2-3-Xavier初始化" class="headerlink" title="2.3 Xavier初始化"></a>2.3 Xavier初始化</h3><p>推论1： 高维空间中的任意两个随机向量几乎都是垂直的<br>推论2： 从N(0,1/n)中随机选取$n^2$个数，组成一个n×n的矩阵，这个矩阵近似为正交矩阵，且n越大，近似程度越好。<br>推论3： 从任意的均值为0、方差为1/n的分布p(x)中独立重复采样出来的n×n矩阵，都接近正交矩阵。<br>&#8195;&#8195;==Xavier初始化：从N(0,1/n)采样来初始化W==。从推论2可以看出，从N(0,1/n)采样而来的n×n矩阵就已经接近正交矩阵了。这种初始化也叫Glorot初始化，作者叫Xavier Glorot～<br>&#8195;&#8195;此外，采样分布也不一定是N(0,1/n)，前面推论3说了你可以从任意均值为0、方差为1/n的分布中采样。</p>
<p>&#8195;&#8195;==上面说的是输入和输出维度都是n的情况==，如果输入是n维，输出是m维呢？这时候$W\in \mathbb{R}^{m\times n}$，保持Wx模长不变的条件依然是$W^⊤W=I$。</p>
<ul>
<li>m&lt;n时，这是不可能的；</li>
<li>推论四：当m≥n时，从任意的均值为0、方差为1/m的分布p(x)中独立重复采样出来的m×n矩阵，近似满足$W^⊤W=I$（只需要把采样分布的方差改为1/m就好）。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/af9b1f597694442d841b7b07cb9c45c3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>sigmoid函数：W服从$U[-\sqrt{\frac{96}{n<em>{i}+n</em>{i+1}}},\sqrt{\frac{96}{n<em>{i}+n</em>{i+1}}}]$</p>
<p>Relu函数：W服从$U[-\sqrt{\frac{12}{n<em>{i}+n</em>{i+1}}},\sqrt{\frac{12}{n<em>{i}+n</em>{i+1}}}]$</p>
<h3 id="2-4-考虑激活函数"><a href="#2-4-考虑激活函数" class="headerlink" title="2.4 考虑激活函数"></a>2.4 考虑激活函数</h3><p>==以上都是没有考虑激活函数得场景==，考虑激活函数有：</p>
<ul>
<li>tanh(x) 在x比较小的时候有tanh(x)≈x，所以可以认为 Xavier初始化直接适用于tanh激活；</li>
<li>relu时可以认为relu(y)会有 大约一半的元素被置零，所以模长大约变为原来的$\frac{1}{\sqrt{2}}$&lt;/font&gt;，而要保持模长不变，可以让W乘上$\sqrt{2}$，也就是说初始化方差从1/m变成2/m</li>
</ul>
<p>&#8195;&#8195;事实上很难针对每一个激活函数都做好方差的调整，所以一个更通用的做法就是直接在激活函数后面加上一个类似Layer Normalization的操作，直接显式地恢复模长。这时候就轮到各种Normalization技巧登场了。</p>
<h2 id="3-BN为什么起作用"><a href="#3-BN为什么起作用" class="headerlink" title="3.BN为什么起作用"></a>3.BN为什么起作用</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/6992">《BN究竟起了什么作用？一个闭门造车的分析》</a></p>
<h3 id="3-1-简述"><a href="#3-1-简述" class="headerlink" title="3.1 简述"></a>3.1 简述</h3><p>&#8195;&#8195;BN在深度学习中可以加速训练，甚至有一定的抗过拟合作用，还允许我们用更大的学习率。因为BN使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练。<br>&#8195;&#8195;BN降低了神经网络的梯度的L常数，从而使得神经网络的学习更加容易，比如可以使用更大的学习率。而降低梯度的L常数，直观来看就是让损失函数没那么“跌宕起伏”，也就是使得landscape更光滑的意思了。（归一化、L常数请参考<a target="_blank" rel="noopener" href="https://kexue.fm/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>）<br>推导：</p>
<ol>
<li>假设f(θ)是损失函数，满足L约束。可以推导出梯度下降更新公式：<script type="math/tex; mode=display">Δθ=−η∇θf(θ)(4)</script>代入不等式2得出：保证损失函数下降，要么学习率η 足够小，要不就要L足够小。</li>
<li>神经网络中激活函数是非线性的。根据柯西不等式得出：降低L的最直接方式是降低$∣Ex∼p(x)[x⊗x]∣_{1}$。</li>
<li>在不会明显降低原来神经网络拟合能力的前提下，降低这一项最好的办法就是对输入x进行变换，即平移和缩放。推出两个结论：</li>
</ol>
<ul>
<li>结论1： 将输入减去所有样本的均值，能降低梯度的L常数，是一个有利于优化又不降低神经网络拟合能力的操作</li>
<li>结论2： 缩放输入x最佳选择是除以标准差。这更像是一个自适应的学习率校正项，可以一定程度上消除了不同层级的输入对参数优化的差异性，使得整个网络的优化更为“同步”，或者说使得神经网络的每一层更为“平权”，从而更充分地利用好了整个神经网络，减少了在某一层过拟合的可能性。当然，如果输入的量级过大时，除以标准差这一项也有助于降低梯度的L常数。</li>
</ul>
<h3 id="3-2-具体推导"><a href="#3-2-具体推导" class="headerlink" title="3.2 具体推导"></a>3.2 具体推导</h3><p>&#8195;&#8195;BN，也就是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a>，是当前深度学习模型（尤其是视觉相关模型）的一个相当重要的技巧，它能加速训练，甚至有一定的抗过拟合作用，还允许我们用更大的学习率，总的来说颇多好处（前提是你跑得起较大的batch size）</p>
<p>&#8195;&#8195;早期的解释主要是基于概率分布的，大概意思是将每一层的输入分布都归一化到N(0,1)上，减少了所谓的Internal Covariate Shift，从而稳定乃至加速了训练。<br>&#8195;&#8195;这种解释细思之下其实有问题的：不管哪一层的输入都不可能严格满足正态分布，从而单纯地将均值方差标准化无法实现标准分布N(0,1)；其次，就算能做到N(0,1)，这种诠释也无法进一步解释其他归一化手段（如Instance Normalization、Layer Normalization）起作用的原因。</p>
<p>&#8195;&#8195;2018年的论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.11604">《How Does Batch Normalization Help Optimization?》</a>里边，作者明确地提出了上述质疑，否定了原来的一些观点，并提出了自己关于BN的新理解：==BN主要作用是使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练==。<br>核心不等式：<br><img src="https://img-blog.csdnimg.cn/8951c52b00264283b6fe12bb4c7dcdcf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="核心不等式"><br>&#8195;&#8195;假设f(θ)是损失函数，而我们的目标是最小化f(θ)。我们自然希望f(θ)每一步都在下降，即$f(θ+Δθ)&lt;f(θ)$，而$\frac{1}{2}L\left | \Delta \theta  \right |<em>{2}^{2}$必然是非负的，所以要想下降的唯一选择就是$⟨∇</em>{θ}f(θ),Δθ⟩&lt;0$，这样一个自然的选择就是</p>
<script type="math/tex; mode=display">Δθ=−η∇θf(θ)(4)</script><p>&#8195;&#8195;这里η&gt;0是一个标量，即学习率。式(4)就是梯度下降的更新公式，它是一个严格的不等式，所以它还可以告诉我们关于训练的一些结论。<br><img src="https://img-blog.csdnimg.cn/f35d306359004137849730fa367377dc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="Lipschitz约束"><br>BN是怎样炼成的 ：<br>&#8195;&#8195;BN降低了神经网络的梯度的L常数，从而使得神经网络的学习更加容易，比如可以使用更大的学习率。而降低梯度的L常数，直观来看就是让损失函数没那么“跌宕起伏”，也就是使得landscape更光滑的意思了。</p>
<p>&#8195;&#8195;我们之前就讨论过L约束，之前我们讨论的是神经网络关于“输入”满足L约束，这导致了权重的谱正则和谱归一化，本文则是要讨论神经网络（的梯度）关于“参数”满足L约束，这导致了对输入的各种归一化手段，而BN是其中最自然的一种。</p>
<p>柯西不等式<br>探讨∇θf(θ)满足L约束的程度，并且探讨降低这个L的方法<br>结论：降低L常数最直接方法是，降低∣∣Ex∼p(x)[x⊗x]∣∣这一项（与参数无关）</p>
<p>式(12)的结果告诉我们，想办法降低L常数个做法就是对输入x进行变换，即平移和缩放。<br>结论1： 将输入减去所有样本的均值，能降低梯度的L常数，是一个有利于优化又不降低神经网络拟合能力的操作。（降低梯度的L常数前提是：必须不会明显降低原来神经网络拟合能力，否则只需要简单乘个0就可以让L降低到0了，但这并没有意义。）</p>
<p>如果一味追求更小的L，那直接σ→∞就好了，但这样的神经网络已经完全没有拟合能力了；但如果σ太小导致L过大，那又不利于优化。所以我们需要一个标准。<br>从公式可以看出：==一个相对自然的选择是将σ取为输入的标准差==。除以标准差更像是一个自适应的学习率校正项，它一定程度上消除了不同层级的输入对参数优化的差异性，使得整个网络的优化更为“同步”，或者说使得神经网络的每一层更为“平权”，从而更充分地利用好了整个神经网络，减少了在某一层过拟合的可能性。当然，如果输入的量级过大时，除以标准差这一项也有助于降低梯度的L常数。</p>
<h2 id="4-花式Mask预训练（我悟了）"><a href="#4-花式Mask预训练（我悟了）" class="headerlink" title="4. 花式Mask预训练（我悟了）"></a>4. 花式Mask预训练（我悟了）</h2><p>本节选自苏剑林的文章：<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6933#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83">《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》</a></p>
<p><strong>背景</strong><br>&#8195;&#8195;从Bert、GPT到XLNet等等，各种应用transformer结构的模型不断涌现，有基于现成的模型做应用的，有试图更好地去解释和可视化这些模型的，还有改进架构、改进预训练方式等以得到更好结果的。总的来说，这些以预训练为基础的工作层出不穷，有种琳琅满目的感觉</p>
<h3 id="4-1-单向语言模型"><a href="#4-1-单向语言模型" class="headerlink" title="4.1 单向语言模型"></a>4.1 单向语言模型</h3><p>&#8195;&#8195;语言模型可以说是一个无条件的文本生成模型（文本生成模型，可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5861">《玩转Keras之seq2seq自动生成标题》</a>）。单向语言模型相当于把训练语料通过下述条件概率分布的方式“记住”了：<br>p(x1,x2,x3,…,xn)=p(x1)p(x2|x1)p(x3|x1,x2)…p(xn|x1,…,xn−1)<br>&#8195;&#8195;我们一般说的“语言模型”，就是指单向的（更狭义的只是指正向的）语言模型。==语言模型的关键点是要防止看到“未来信息”==。如上式，预测x1的时候，是没有任何外部输入的；而预测x2的时候，只能输入x1，预测x3的时候，只能输入x1,x2；依此类推。<br><img src="https://img-blog.csdnimg.cn/8f13a78a86514fd78cc77b28520d04b3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="单向语言模型"><br>&#8195;&#8195;RNN模型是天然适合做语言模型的，因为它本身就是递归的运算；<strong>如果用CNN来做的话，则需要对卷积核进行Mask，即需要将卷积核对应右边的部分置零。如果是Transformer呢？那需要一个下三角矩阵形式的Attention矩阵，并将输入输出错开一位训练：</strong><br><img src="https://img-blog.csdnimg.cn/f5c140f2cc6e48a09da84954bd7ec656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="下三角矩阵"></p>
<p>&#8195;&#8195;如图所示，Attention矩阵的每一行事实上代表着输出，而每一列代表着输入，而<strong>Attention矩阵就表示输出和输入的关联</strong>。假定白色方格都代表0，那么第1行表示“北”只能跟起始标记$<s>$相关了，而第2行就表示“京”只能跟起始标记$<s>$和“北”相关了，依此类推。（（Mask的实现方式，也可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6810#Mask">《“让Keras更酷一些！”：层中层与mask》</a>）</p>
<h3 id="4-2-Transformer专属"><a href="#4-2-Transformer专属" class="headerlink" title="4.2 Transformer专属"></a>4.2 Transformer专属</h3><p>&#8195;&#8195;事实上，除了单向语言模型及其简单变体掩码语言模型之外，UNILM的Seq2Seq预训练、XLNet的乱序语言模型预训练，基本可以说是专为Transformer架构定制的。说白了，如果是RNN架构，根本就不能用乱序语言模型的方式来预训练。至于Seq2Seq的预训练方式，则必须同时引入两个模型（encoder和decoder），而无法像Transformer架构一样，可以一个模型搞定。</p>
<p>&#8195;&#8195;<strong>这其中的奥妙主要在Attention矩阵之上</strong>。Attention实际上相当于将输入两两地算相似度，这构成了一个$n^2$大小的相似度矩阵（即Attention矩阵，n是句子长度，本节的Attention均指Self Attention），这意味着它的空间占用量是O($n^2$)量级，相比之下，RNN模型、CNN模型只不过是O(n)，所以实际上Attention通常更耗显存。<br>&#8195;&#8195;然而，有弊也有利，更大的空间占用也意味着拥有了更多的可能性，==我们可以通过往这个O($n^2$)级别的Attention矩阵加入各种先验约束==，使得它可以做更灵活的任务。说白了，也就只有纯Attention的模型，才有那么大的“容量”去承载那么多的“花样”。</p>
<p>而==加入先验约束的方式，就是对Attention矩阵进行不同形式的Mask==，这便是本文要关注的焦点。</p>
<h3 id="4-3-乱序语言模型"><a href="#4-3-乱序语言模型" class="headerlink" title="4.3 乱序语言模型"></a>4.3 乱序语言模型</h3><p>&#8195;&#8195;乱序语言模型是XLNet提出来的概念，它主要用于XLNet的预训练上。<br>&#8195;&#8195;乱序语言模型跟语言模型一样，都是做条件概率分解，但是乱序语言模型的分解顺序是随机的：</p>
<script type="math/tex; mode=display">p(x1,x2,x3,…,xn)
=p(x1)p(x2|x1)p(x3|x1,x2)…p(xn|x1,x2,…,xn−1)
=p(x3)p(x1|x3)p(x2|x3,x1)…p(xn|x3,x1,…,xn−1)
=…
=p(xn−1)p(x1|xn−1)p(xn|xn−1,x1)…p(x2|xn−1,x1,…,x3)</script><p>&#8195;&#8195;总之，x1,x2,…,xn任意一种“出场顺序”都有可能。原则上来说，每一种顺序都对应着一个模型，所以原则上就有n!个语言模型。而基于Transformer的模型，则可以将这所有顺序都做到一个模型中去！<br>&#8195;&#8195;<strong>实现某种特定顺序的语言模型，就将原来的下三角形式的Mask以某种方式打乱</strong>。正因为Attention提供了这样的一个n×n的Attention矩阵，我们才有足够多的自由度去以不同的方式去Mask这个矩阵，从而实现多样化的效果。</p>
<p>&#8195;&#8195;以“北京欢迎你”的生成为例，假设随机的一种生成顺序为“$<s>$ → 迎 → 京 → 你 → 欢 → 北 → <e>”，那么我们只需要用下图中第二个子图的方式去Mask掉Attention矩阵，就可以达到目的了：<br><img src="https://img-blog.csdnimg.cn/2292a816c044400492613f9e2a105b9b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="l乱序语言模型"><br>&#8195;&#8195;跟前面的单向语言模型类似，第4行只有一个蓝色格，表示“迎”只能跟起始标记$<s>$相关，而第2行有两个蓝色格，表示“京”只能跟起始标记$<s>$和“迎”相关，依此类推。直观来看，这就像是把单向语言模型的下三角形式的Mask“打乱”了。</p>
<p>&#8195;&#8195;有人会问，打乱后的Mask似乎没看出什么规律呀，难道每次都要随机生成一个这样的似乎没有什么明显概率的Mask矩阵？事实上有一种更简单的、数学上等效的训练方案，即==在输入层面进行打乱==。<br>&#8195;&#8195;==纯Attention的模型本质上是一个无序的模型==，它里边的词序实际上是通过Position Embedding加上去的。也就是说，我们输入的不仅只有token本身，还包括token所在的位置id；再换言之，你觉得你是输入了序列“[北, 京, 欢, 迎, 你]”，实际上你输入的是集合“{(北, 1), (京, 2), (欢, 3), (迎, 4), (你, 5)}”。<br>&#8195;&#8195;既然只是一个集合，跟顺序无关，那么我们完全可以换一种顺序输入，比如刚才的“$<s>$→ 迎 → 京 → 你 → 欢 → 北 → $<e>$”，我们可以按“(迎, 4), (京, 2), (你, 5), (欢, 3), (北, 1)”的顺序输入，也就是说将token打乱为“迎,京,你,欢,北”输入到Transformer中，但是第1个token的position就不是1了，而是4；依此类推。这样换过来之后，Mask矩阵可以恢复为下三角矩阵。（讲了一通，乱序实现从乱序mask矩阵到乱序输入序列，有点迷。。。）</p>
<h3 id="4-4-Seq2Seq"><a href="#4-4-Seq2Seq" class="headerlink" title="4.4 Seq2Seq"></a>4.4 Seq2Seq</h3><p>&#8195;&#8195;原则上来说，任何NLP问题都可以转化为Seq2Seq来做，它是一个真正意义上的万能模型。所以如果能够做到Seq2Seq，理论上就可以实现任意任务了。<br>&#8195;&#8195;微软的<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.03197">UNILM</a>能将Bert与Seq2Seq优雅的结合起来。能够让我们==直接用单个Bert模型就可以做Seq2Seq任务，而不用区分encoder和decoder。而实现这一点几乎不费吹灰之力——只需要一个特别的Mask==。<br>&#8195;&#8195;UNILM直接将Seq2Seq当成句子补全来做。假如输入是“你想吃啥”，目标句子是“白切鸡”，那UNILM将这两个句子拼成一个：[CLS] 你 想 吃 啥 [SEP] 白 切 鸡 [SEP]。经过这样转化之后，最简单的方案就是训练一个语言模型，然后输入“[CLS] 你 想 吃 啥 [SEP]”来逐字预测“白 切 鸡”，直到出现“[SEP]”为止，即如下面的左图：<br><img src="https://img-blog.csdnimg.cn/61c52c89d2484fce9a5ab71efb6350db.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="unilm"></p>
<p>&#8195;&#8195;不过左图只是最朴素的方案，它把“你想吃啥”也加入了预测范围了（导致它这部分的Attention是单向的，即对应部分的Mask矩阵是下三角），事实上这是不必要的，属于==额外的约束==。真正要预测的只是“白切鸡”这部分，所以我们可以把“你想吃啥”这部分的Mask去掉，得到上面的右图的Mask。</p>
<p>UNILM单个Bert模型完成Seq2Seq任务的思路：<br>&#8195;&#8195;添加上述形状的Mask，==输入部分的Attention是双向的，输出部分的Attention是单向==，满足Seq2Seq的要求，而且==没有额外约束==。这样做不需要修改模型架构，并且还可以直接沿用Bert的Masked Language Model预训练权重，收敛更快。这符合“一Bert在手，天下我有”的万用模型的初衷，个人认为这是非常优雅的方案。</p>
<h3 id="4-5-实验"><a href="#4-5-实验" class="headerlink" title="4.5 实验"></a>4.5 实验</h3><p>&#8195;&#8195;事实上，上述的这些Mask方案，基本上都已经被集成在原作者写的<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6915">bert4keras</a>，读者可以直接用bert4keras加载bert的预训练权重，并且调用上述Mask方案来做相应的任务。具体UNILM实现例子，可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6933#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83">原文</a></p>
<h3 id="4-6-总结"><a href="#4-6-总结" class="headerlink" title="4.6 总结"></a>4.6 总结</h3><p>&#8195;&#8195;1.原始的seq2seq训练的是一个单向语言模型，语言模型的关键点是要防止看到“未来信息”。这一点可以通过循环神经网络的递归计算来实现，比如RNN。也可以通过CNN来做，只需要对卷积核进行Mask，即需要将卷积核对应右边的部分置零。如果是Transformer呢，那就需要一个下三角矩阵形式的Attention矩阵（表示输入与输出的关联）来实现。<br>&#8195;&#8195;2.不仅如此，通过Attention矩阵的不同Mask方式，还可以实现乱序语言模型和Seq2Seq。<br>&#8195;&#8195;前者只需要乱序原来的下三角形式的Masked-Attention矩阵（也等价于乱序输入序列），后者通过句子补全来做（类似输入一个词，预测接下来会输入的词，即输入法预测）。具体做的时候，只需要mask输入部分就行（感觉就是GPT2）。<br>&#8195;&#8195;3.之所以一个transformer结构能搞出后面那么多花样的玩法（Bert、GPT、XLNet等），==关键在于Attention矩阵==。Attention实际上相当于将输入两两地算相似度，这构成了一个$n^2$大小的相似度矩阵（复杂度O($n^2$)）。比起RNN、CNN模型只是O(n)，Attention通常更耗显存。<br>&#8195;&#8195;但正因如此，却也有了更多的可能性。==通过往O($n^2$)级别的Attention矩阵加入各种先验约束，使得它可以做更灵活的任务。这种先验约束就是mask玩法==。说白了，也就只有纯Attention的模型，才有那么大的“容量”去承载那么多的“花样”。（读到这里，我悟了）。</p>
<h2 id="5-词向量与Embedding究竟是怎么回事（有空补）"><a href="#5-词向量与Embedding究竟是怎么回事（有空补）" class="headerlink" title="5. 词向量与Embedding究竟是怎么回事（有空补）"></a>5. 词向量与Embedding究竟是怎么回事（有空补）</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/4122">词向量与Embedding究竟是怎么回事</a></p>
<h2 id="6-《Attention-is-All-You-Need》浅读（有空补）"><a href="#6-《Attention-is-All-You-Need》浅读（有空补）" class="headerlink" title="6. 《Attention is All You Need》浅读（有空补）"></a>6. 《Attention is All You Need》浅读（有空补）</h2><p><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/4765">《Attention is All You Need》浅读（简介+代码）</a><br>RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息：纯Attention！单靠注意力就可以。yt=f(xt,A,B)<br>Attention层的好处是能够一步到位捕捉到全局的联系，因为它直接把序列两两比较（代价是计算量变为$O(n^2)$，当然由于是纯矩阵运算，这个计算量相当也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task7%EF%BC%9ATransformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1/">task7：BERT token分类</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h1 id="Transformers解析序列标注任务"><a href="#Transformers解析序列标注任务" class="headerlink" title="Transformers解析序列标注任务"></a>Transformers解析序列标注任务</h1><p>本文主要来自datawhale的<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.2-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8">transformer教程4.2</a>和<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task07">天国之影学习笔记</a>。</p>
<h2 id="1-序列标注任务简介"><a href="#1-序列标注任务简介" class="headerlink" title="1 序列标注任务简介"></a>1 序列标注任务简介</h2><ul>
<li>序列标注可以看作时token级别的分类问题，为文本中的每一个token预测一个标签</li>
<li><p>token级别的分类任务：</p>
<ol>
<li>NER（Named-entity recognition 名词-实体识别）分辨出文本中的名词和实体（person人名, organization组织机构名, location地点名…）</li>
<li>POS（Part-of-speech tagging词性标注）根据语法对token进行词性标注（noun名词、verb动词、adjective形容词…）</li>
<li>Chunk（Chunking短语组块）将同一个短语的tokens组块放在一起</li>
</ol></div><a class="more" href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task7%EF%BC%9ATransformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task6%EF%BC%9ATransformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/">task6：BERT文本分类</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h1 id="Transformers解决文本分类任务、超参搜索"><a href="#Transformers解决文本分类任务、超参搜索" class="headerlink" title="Transformers解决文本分类任务、超参搜索"></a>Transformers解决文本分类任务、超参搜索</h1><p>本文主要内容转自天国之影笔记<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task06">Task06</a>，之后具体的API进行了一些查询，写了一些说明。<br>@[toc]</p>
<h2 id="1-文本分类任务简介"><a href="#1-文本分类任务简介" class="headerlink" title="1 文本分类任务简介"></a>1 文本分类任务简介</h2><ul>
<li>使用Transformers代码库中的模型来解决文本分类任务，任务来源于<a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE Benchmark</a></li>
<li>GLUE榜单的9个级别的分类任务：<ol>
<li>CoLA (Corpus of Linguistic Acceptability)：鉴别一个句子是否语法正确.</li>
<li>MNLI (Multi-Genre Natural Language Inference)：给定一个假设，判断另一个句子与该假设的关系：entails、contradicts、unrelated。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus)：判断两个句子是否互为paraphrases</li>
<li>QNLI (Question-answering Natural Language Inference)：判断第2句是否包含第1句问题的答案</li>
<li>QQP (Quora Question Pairs2)：判断两个问句是否语义相同</li>
<li>RTE (Recognizing Textual Entailment)：判断一个句子是否与假设成entail关系</li>
<li>SST-2 (Stanford Sentiment Treebank)：判断一个句子的情感正负向</li>
<li>STS-B (Semantic Textual Similarity Benchmark)：判断两个句子的相似性（分数为1-5分）</li>
<li>WNLI (Winograd Natural Language Inference)：判断带有匿名代词的句子中，是否存在能够替换该代词的子句</div><a class="more" href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task6%EF%BC%9ATransformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/24/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB1%EF%BC%9ATransformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89/">选读1：Transformer代码解读（Pytorch）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h1 id="Transformer代码解读（Pytorch）"><a href="#Transformer代码解读（Pytorch）" class="headerlink" title="Transformer代码解读（Pytorch）"></a>Transformer代码解读（Pytorch）</h1><p>本文是对transformer源代码的一点总结。原文在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.md">《Pytorch编写完整的Transformer》</a></p>
<p>本文涉及的jupter notebook在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.ipynb">Pytorch编写完整的Transformer</a><br>在阅读完<a href="./篇章2-Transformer相关原理/2.2-图解transformer.md">2.2-图解transformer</a>之后，希望大家能对transformer各个模块的设计和计算有一个形象的认识，本小节我们基于pytorch来实现一个Transformer，帮助大家进一步学习这个复杂的模型。<br>@[toc]</p>
<h2 id="0-读完总结：（这一段是自己的总结笔记）"><a href="#0-读完总结：（这一段是自己的总结笔记）" class="headerlink" title="0. 读完总结：（这一段是自己的总结笔记）"></a>0. 读完总结：（这一段是自己的总结笔记）</h2><ul>
<li>位置编码是最后加入的，输入输出形状不变。</li>
<li>每个attention层都有权重初始化（即输入映射成不同的QKV。而且每层权重不一样）<br>除了encoder-decoder-attention层q是来自前一层输出，kv是来自encoder层最后的输出memory会导致qkv维度不一致，其它层qkv维度都是一样的。并且只有第一维不一致，分别是L和S。</li>
<li>点积时会讲batch放到第一维，还有遮挡机制</li>
<li>encoderlayer1：输入src加入位置编码，进入 Multi-self-attention层。self.norm1(src + self.dropout1(src2))。即dropout输出src2，然后残差连接+Norm</li>
<li>encoderlayer2：全连接第一层3072神经元扩维4倍，之后激活并dropout，送入第二个全连接层降维回768维。之后同样的Add+Norm，即src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))、self.norm2(src + self.dropout2(src2))。</div><a class="more" href="/2021/08/24/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB1%EF%BC%9ATransformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>