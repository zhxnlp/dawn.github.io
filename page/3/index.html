<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="nlp"><meta name="keywords" content=""><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>You got a dream, you gotta protect it | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">48</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><nav id="nav" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="site-info"><div id="site-title">zhxnlpのBlog</div><div id="site-sub-title">You got a dream, you gotta protect it</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/zhxnlp/nlp-transformers" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fa"></i></a><a class="social-icon" href="https://gitee.com/zhxscut" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-google fa"></i></a><a class="social-icon" href="https://www.cnblogs.com/zhxnlp" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-bootstrap fa"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_56591814" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-weibo fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/11/10/CLUENER%20%E7%BB%86%E7%B2%92%E5%BA%A6%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/bert_softmax/">命名实体识别——bert_softmax模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/CLUENER-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">CLUENER 命名实体识别</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/NER/">NER</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/CRF/">CRF</a></span><div class="content"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True).
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&#x27;/content/drive/MyDrive/chinese task/CLUENER2020&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装</span></span><br><span class="line">!pip install transformers datasets seqeval</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> config</span><br></pre></td></tr></table></figure></div><a class="more" href="/2021/11/10/CLUENER%20%E7%BB%86%E7%B2%92%E5%BA%A6%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/bert_softmax/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/11/09/CLUENER%20%E7%BB%86%E7%B2%92%E5%BA%A6%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/README/">CLUENER 命名实体识别任务介绍</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/CLUENER-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">CLUENER 命名实体识别</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/NER/">NER</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/CRF/">CRF</a></span><div class="content"><h1 id="CLUENER-细粒度命名实体识别"><a href="#CLUENER-细粒度命名实体识别" class="headerlink" title="CLUENER 细粒度命名实体识别"></a>CLUENER 细粒度命名实体识别</h1><h2 id="一、任务说明："><a href="#一、任务说明：" class="headerlink" title="一、任务说明："></a>一、任务说明：</h2><ol>
<li>最开始是参考知乎文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346828049?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《用BERT做NER？教你用PyTorch轻松入门Roberta！》</a>，github项目地址：<a target="_blank" rel="noopener" href="https://github.com/hemingkx/CLUENER2020">《hemingkx/CLUENER2020》</a></li>
<li>任务介绍：本任务是中文语言理解测评基准(CLUE)任务之一：<a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/introduce.html">《CLUE Fine-Grain NER》</a>。</li>
<li>数据来源：本数据是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS.</li>
<li>平台github任务详情：<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUENER2020">《CLUENER 细粒度命名实体识别》</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/ner.html">CLUE命名实体任务排行榜</a></li>
<li>BERT-base-X部分的代码编写思路参考 <a target="_blank" rel="noopener" href="https://github.com/lemonhu/NER-BERT-pytorch">lemonhu</a> </li>
<li>参考文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/HUSTHY/article/details/109276404">《中文NER任务简析与深度算法模型总结和实战展示》</a></li>
</ol>
<h2 id="二、数据集介绍："><a href="#二、数据集介绍：" class="headerlink" title="二、数据集介绍："></a>二、数据集介绍：</h2><blockquote>
<p>cluener下载链接：<a target="_blank" rel="noopener" href="https://storage.googleapis.com/cluebenchmark/tasks/cluener_public.zip">数据下载</a></p>
<h3 id="2-1-数据集划分和数据内容"><a href="#2-1-数据集划分和数据内容" class="headerlink" title="2.1 数据集划分和数据内容"></a>2.1 数据集划分和数据内容</h3><ul>
<li>训练集:10748 </li>
<li>验证集:1343   </li>
<li>测试集(无标签）:1345</li>
<li>原始数据存储在json文件中。文件中的每一行是一条单独的数据，一条数据包括一个原始句子以及其上的标签，具体形式如下：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，&quot;</span>, <span class="string">&quot;label&quot;</span>: &#123;<span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;叶老桂&quot;</span>: [[<span class="number">9</span>, <span class="number">11</span>]]&#125;, <span class="string">&quot;company&quot;</span>: &#123;<span class="string">&quot;浙商银行&quot;</span>: [[<span class="number">0</span>, <span class="number">3</span>]]&#125;&#125;&#125;</span><br><span class="line">&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;生生不息CSOL生化狂潮让你填弹狂扫&quot;</span>, <span class="string">&quot;label&quot;</span>: &#123;<span class="string">&quot;game&quot;</span>: &#123;<span class="string">&quot;CSOL&quot;</span>: [[<span class="number">4</span>, <span class="number">7</span>]]&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
展开看就是：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="string">&quot;text&quot;</span>: <span class="string">&quot;浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，&quot;</span>,</span><br><span class="line">	<span class="string">&quot;label&quot;</span>: &#123;</span><br><span class="line">		<span class="string">&quot;name&quot;</span>: &#123;</span><br><span class="line">			<span class="string">&quot;叶老桂&quot;</span>: [</span><br><span class="line">				[<span class="number">9</span>, <span class="number">11</span>],</span><br><span class="line">				[<span class="number">32</span>, <span class="number">34</span>]</span><br><span class="line">			]</span><br><span class="line">		&#125;,</span><br><span class="line">		<span class="string">&quot;company&quot;</span>: &#123;</span><br><span class="line">			<span class="string">&quot;浙商银行&quot;</span>: [</span><br><span class="line">				[<span class="number">0</span>, <span class="number">3</span>]</span><br><span class="line">			]</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
数据字段解释：<br>以train.json为例，数据分为两列：text &amp; label，其中text列代表文本，label列代表文本中出现的所有包含在10个类别中的实体。例如：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text: <span class="string">&quot;北京勘察设计协会副会长兼秘书长周荫如&quot;</span></span><br><span class="line">label: &#123;<span class="string">&quot;organization&quot;</span>: &#123;<span class="string">&quot;北京勘察设计协会&quot;</span>: [[<span class="number">0</span>, <span class="number">7</span>]]&#125;, <span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;周荫如&quot;</span>: [[<span class="number">15</span>, <span class="number">17</span>]]&#125;, <span class="string">&quot;position&quot;</span>: &#123;<span class="string">&quot;副会长&quot;</span>: [[<span class="number">8</span>, <span class="number">10</span>]], <span class="string">&quot;秘书长&quot;</span>: [[<span class="number">12</span>, <span class="number">14</span>]]&#125;&#125;</span><br><span class="line">其中，organization，name，position代表实体类别，</span><br><span class="line"><span class="string">&quot;organization&quot;</span>: &#123;<span class="string">&quot;北京勘察设计协会&quot;</span>: [[<span class="number">0</span>, <span class="number">7</span>]]&#125;：表示原text中，<span class="string">&quot;北京勘察设计协会&quot;</span> 是类别为 <span class="string">&quot;组织机构（organization）&quot;</span> 的实体, 并且start_index为<span class="number">0</span>，end_index为<span class="number">7</span> （注：下标从<span class="number">0</span>开始计数）</span><br><span class="line"><span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;周荫如&quot;</span>: [[<span class="number">15</span>, <span class="number">17</span>]]&#125;：表示原text中，<span class="string">&quot;周荫如&quot;</span> 是类别为 <span class="string">&quot;姓名（name）&quot;</span> 的实体, 并且start_index为<span class="number">15</span>，end_index为<span class="number">17</span></span><br><span class="line"><span class="string">&quot;position&quot;</span>: &#123;<span class="string">&quot;副会长&quot;</span>: [[<span class="number">8</span>, <span class="number">10</span>]], <span class="string">&quot;秘书长&quot;</span>: [[<span class="number">12</span>, <span class="number">14</span>]]&#125;：表示原text中，<span class="string">&quot;副会长&quot;</span> 是类别为 <span class="string">&quot;职位（position）&quot;</span> 的实体, 并且start_index为<span class="number">8</span>，end_index为<span class="number">10</span>，同时，<span class="string">&quot;秘书长&quot;</span> 也是类别为 <span class="string">&quot;职位（position）&quot;</span> 的实体,</span><br><span class="line">并且start_index为<span class="number">12</span>，end_index为<span class="number">14</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h3 id="2-2-标签类别和定义："><a href="#2-2-标签类别和定义：" class="headerlink" title="2.2 标签类别和定义："></a>2.2 标签类别和定义：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">数据分为<span class="number">10</span>个标签类别，分别为: 地址（address），书名（book），公司（company），游戏（game），政府（goverment），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene）</span><br></pre></td></tr></table></figure>
<ul>
<li>标签定义与规则：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">地址（address）: **省**市**区**街**号，**路，**街道，**村等（如单独出现也标记），注意：地址需要标记完全, 标记到最细。</span><br><span class="line">书名（book）: 小说，杂志，习题集，教科书，教辅，地图册，食谱，书店里能买到的一类书籍，包含电子书。</span><br><span class="line">公司（company）: **公司，**集团，**银行（央行，中国人民银行除外，二者属于政府机构）, 如：新东方，包含新华网/中国军网等。</span><br><span class="line">游戏（game）: 常见的游戏，注意有一些从小说，电视剧改编的游戏，要分析具体场景到底是不是游戏。</span><br><span class="line">政府（goverment）: 包括中央行政机关和地方行政机关两级。 中央行政机关有国务院、国务院组成部门（包括各部、委员会、中国人民银行和审计署）、国务院直属机构（如海关、税务、工商、环保总局等），军队等。</span><br><span class="line">电影（movie）: 电影，也包括拍的一些在电影院上映的纪录片，如果是根据书名改编成电影，要根据场景上下文着重区分下是电影名字还是书名。</span><br><span class="line">姓名（name）: 一般指人名，也包括小说里面的人物，宋江，武松，郭靖，小说里面的人物绰号：及时雨，花和尚，著名人物的别称，通过这个别称能对应到某个具体人物。</span><br><span class="line">组织机构（organization）: 篮球队，足球队，乐团，社团等，另外包含小说里面的帮派如：少林寺，丐帮，铁掌帮，武当，峨眉等。</span><br><span class="line">职位（position）: 古时候的职称：巡抚，知州，国师等。现代的总经理，记者，总裁，艺术家，收藏家等。</span><br><span class="line">景点（scene）: 常见旅游景点如：长沙公园，深圳动物园，海洋馆，植物园，黄河，长江等。</span><br></pre></td></tr></table></figure>
<h3 id="2-3-数据分布"><a href="#2-3-数据分布" class="headerlink" title="2.3 数据分布"></a>2.3 数据分布</h3>训练集：10748 验证集：1343</li>
</ul>
<p>按照不同标签类别统计，训练集数据分布如下（注：一条数据中出现的所有实体都进行标注，如果一条数据出现两个地址（address）实体，那么统计地址（address）类别数据的时候，算两条数据）：</p>
<p>【训练集】标签数据分布如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">地址（address）:<span class="number">2829</span></span><br><span class="line">书名（book）:<span class="number">1131</span></span><br><span class="line">公司（company）:<span class="number">2897</span></span><br><span class="line">游戏（game）:<span class="number">2325</span></span><br><span class="line">政府（government）:<span class="number">1797</span></span><br><span class="line">电影（movie）:<span class="number">1109</span></span><br><span class="line">姓名（name）:<span class="number">3661</span></span><br><span class="line">组织机构（organization）:<span class="number">3075</span></span><br><span class="line">职位（position）:<span class="number">3052</span></span><br><span class="line">景点（scene）:<span class="number">1462</span></span><br><span class="line"></span><br><span class="line">【验证集】标签数据分布如下：</span><br><span class="line">地址（address）:<span class="number">364</span></span><br><span class="line">书名（book）:<span class="number">152</span></span><br><span class="line">公司（company）:<span class="number">366</span></span><br><span class="line">游戏（game）:<span class="number">287</span></span><br><span class="line">政府（government）:<span class="number">244</span></span><br><span class="line">电影（movie）:<span class="number">150</span></span><br><span class="line">姓名（name）:<span class="number">451</span></span><br><span class="line">组织机构（organization）:<span class="number">344</span></span><br><span class="line">职位（position）:<span class="number">425</span></span><br><span class="line">景点（scene）:<span class="number">199</span></span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/15c63d52477f4127a77009544cb6b08f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>测试训练集句子长度<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line"><span class="comment">#最大句子长度50</span></span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>] = train_df[<span class="string">&#x27;words&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x))</span><br><span class="line"><span class="built_in">print</span>(train_df[<span class="string">&#x27;text_len&#x27;</span>].describe())</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Populating the interactive namespace <span class="keyword">from</span> numpy <span class="keyword">and</span> matplotlib</span><br><span class="line">count    <span class="number">10748.000000</span></span><br><span class="line">mean        <span class="number">37.380350</span></span><br><span class="line">std         <span class="number">10.709827</span></span><br><span class="line"><span class="built_in">min</span>          <span class="number">2.000000</span></span><br><span class="line"><span class="number">25</span>%         <span class="number">32.000000</span></span><br><span class="line"><span class="number">50</span>%         <span class="number">41.000000</span></span><br><span class="line"><span class="number">75</span>%         <span class="number">46.000000</span></span><br><span class="line"><span class="built_in">max</span>         <span class="number">50.000000</span></span><br><span class="line">Name: text_len, dtype: float64</span><br></pre></td></tr></table></figure>
<p>平台测试结果：<br>Roberta指的chinese_roberta_wwm_large模型。（roberta-wwm-large-ext）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">模型	  BiLSTM+CRF	bert-base-chinese   Roberta+Softmax	  Roberta+CRF	   Roberta+BiLSTM+CRF</span><br><span class="line">overall	 <span class="number">70</span>/<span class="number">67</span>	       <span class="number">78.82</span>               <span class="number">75.90</span>	        <span class="number">80.4</span>/<span class="number">79.3</span>           <span class="number">79.64</span></span><br></pre></td></tr></table></figure>
<p>可见，Roberta+lstm和Roberta模型差别不大。<br>官方处理方法：softmax、crf和span，模型本体和运行代码见：<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUENER2020/blob/master/pytorch_version/models/albert_for_ner.py">CLUENER2020/pytorch_version/models/albert_for_ner.py</a>  | <a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUENER2020/blob/master/pytorch_version/run_ner_crf.py">run_ner_crf.py</a>。</p>
<p>为什么使用CRF提升这么大呢？ softmax最终分类，只能通过输入判断输出，但是 CRF 可以通过学习转移矩阵，看前后的输出来判断当前的输出。这样就能学到一些规律（比如“O 后面不能直接接 I”“B-brand 后面不可能接 I-color”），这些规律在有时会起到至关重要的作用。</p>
<p>例如下面的例子，A 是没加 CRF 的输出结果，B 是加了 CRF 的输出结果，一看就懂不细说了<br><img src="https://img-blog.csdnimg.cn/d9e359b8d06d48f89cca5010f8bfb392.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="三、处理json文件，转成BIOS标注"><a href="#三、处理json文件，转成BIOS标注" class="headerlink" title="三、处理json文件，转成BIOS标注"></a>三、处理json文件，转成BIOS标注</h2><p>本文选取Roberta+lstm+lstm，标注方法选择BIOS。<br>“B”:（实体开始的token）前缀<br>“I” :（实体中间的token）前缀<br>“O”：无特别实体（no special entity）<br>“S”:  即Single，“S-X”表示该字单独标记为X标签</p>
<p>另外还有BIO、BIOE（“E-X”表示该字是标签X的词片段末尾的终止字）等。</p>
<h3 id="3-1-分词和标签预处理"><a href="#3-1-分词和标签预处理" class="headerlink" title="3.1 分词和标签预处理"></a>3.1 分词和标签预处理</h3><p>NER作为序列标注任务，输出需要确定实体边界和类型。如果预先进行了分词处理，由于分词工具原本就无法保证绝对正确的分词方案，势必会产生错误的分词结果，而这将进一步影响序列标注结果。因此，我们不进行分词，在字层面进行BIOS标注。</p>
<p>我们采用BIOS标注对原始标签进行转换。B-X 代表实体X的开头，I-X代表实体的结尾，O代表不属于任何类型，S表示改词本身就是一个实体。范例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，&quot;</span>, </span><br><span class="line"><span class="string">&quot;label&quot;</span>: &#123;<span class="string">&quot;name&quot;</span>: &#123;<span class="string">&quot;叶老桂&quot;</span>: [[<span class="number">9</span>, <span class="number">11</span>],[<span class="number">32</span>, <span class="number">34</span>]]&#125;, <span class="string">&quot;company&quot;</span>: &#123;<span class="string">&quot;浙商银行&quot;</span>: [[<span class="number">0</span>, <span class="number">3</span>]]&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><br>转换结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;B-company&#x27;</span>, <span class="string">&#x27;I-company&#x27;</span>, <span class="string">&#x27;I-company&#x27;</span>, <span class="string">&#x27;I-company&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-name&#x27;</span>, <span class="string">&#x27;I-name&#x27;</span>, <span class="string">&#x27;I-name&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-name&#x27;</span>, <span class="string">&#x27;I-name&#x27;</span>, <span class="string">&#x27;I-name&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>这部分处理在data_process.ipynb文件中。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;address&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;company&#x27;</span>, <span class="string">&#x27;game&#x27;</span>, <span class="string">&#x27;government&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;movie&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;organization&#x27;</span>, <span class="string">&#x27;position&#x27;</span>, <span class="string">&#x27;scene&#x27;</span>]</span><br><span class="line"></span><br><span class="line">label2id = &#123;</span><br><span class="line">    <span class="string">&quot;O&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;B-address&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;B-book&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;B-company&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&#x27;B-game&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&#x27;B-government&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;B-movie&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&#x27;B-name&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&#x27;B-organization&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&#x27;B-position&#x27;</span>: <span class="number">9</span>,</span><br><span class="line">    <span class="string">&#x27;B-scene&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;I-address&quot;</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="string">&quot;I-book&quot;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&quot;I-company&quot;</span>: <span class="number">13</span>,</span><br><span class="line">    <span class="string">&#x27;I-game&#x27;</span>: <span class="number">14</span>,</span><br><span class="line">    <span class="string">&#x27;I-government&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;I-movie&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;I-name&#x27;</span>: <span class="number">17</span>,</span><br><span class="line">    <span class="string">&#x27;I-organization&#x27;</span>: <span class="number">18</span>,</span><br><span class="line">    <span class="string">&#x27;I-position&#x27;</span>: <span class="number">19</span>,</span><br><span class="line">    <span class="string">&#x27;I-scene&#x27;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;S-address&quot;</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">&quot;S-book&quot;</span>: <span class="number">22</span>,</span><br><span class="line">    <span class="string">&quot;S-company&quot;</span>: <span class="number">23</span>,</span><br><span class="line">    <span class="string">&#x27;S-game&#x27;</span>: <span class="number">24</span>,</span><br><span class="line">    <span class="string">&#x27;S-government&#x27;</span>: <span class="number">25</span>,</span><br><span class="line">    <span class="string">&#x27;S-movie&#x27;</span>: <span class="number">26</span>,</span><br><span class="line">    <span class="string">&#x27;S-name&#x27;</span>: <span class="number">27</span>,</span><br><span class="line">    <span class="string">&#x27;S-organization&#x27;</span>: <span class="number">28</span>,</span><br><span class="line">    <span class="string">&#x27;S-position&#x27;</span>: <span class="number">29</span>,</span><br><span class="line">    <span class="string">&#x27;S-scene&#x27;</span>: <span class="number">30</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">id2label = &#123;_<span class="built_in">id</span>: _label <span class="keyword">for</span> _label, _<span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">list</span>(label2id.items())&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="3-2-模型输入"><a href="#3-2-模型输入" class="headerlink" title="3.2 模型输入"></a>3.2 模型输入</h3><p>再将BIOS标记转换为数字，pad之后装入dataloader输入模型。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#每个句子都被pad到52的长度</span></span><br><span class="line">train_df[<span class="string">&#x27;label_len&#x27;</span>] = train_df[<span class="string">&#x27;pad_labels&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x))</span><br><span class="line"><span class="built_in">print</span>(train_df[<span class="string">&#x27;label_len&#x27;</span>].describe())</span><br><span class="line"></span><br><span class="line">count    <span class="number">10748.0</span></span><br><span class="line">mean        <span class="number">52.0</span></span><br><span class="line">std          <span class="number">0.0</span></span><br><span class="line"><span class="built_in">min</span>         <span class="number">52.0</span></span><br><span class="line"><span class="number">25</span>%         <span class="number">52.0</span></span><br><span class="line"><span class="number">50</span>%         <span class="number">52.0</span></span><br><span class="line"><span class="number">75</span>%         <span class="number">52.0</span></span><br><span class="line"><span class="built_in">max</span>         <span class="number">52.0</span></span><br><span class="line">Name: label_len, dtype: float64</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">                      words	                                                               labels</span><br><span class="line"><span class="number">0</span>	[彭, 小, 军, 认, 为, ，, 国, 内, 银, 行, 现, 在, 走, 的, 是, ...	[<span class="number">7</span>, <span class="number">17</span>, <span class="number">17</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.</span>..</span><br><span class="line"><span class="number">1</span>	[温, 格, 的, 球, 队, 终, 于, 又, 踢, 了, 一, 场, 经, 典, 的, ...	[<span class="number">7</span>, <span class="number">17</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,...</span><br><span class="line"><span class="number">2</span>	[突, 袭, 黑, 暗, 雅, 典, 娜, 》, 中, R, i, d, d, i, c, ...	[<span class="number">4</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">17</span>, <span class="number">17</span>, ...</span><br><span class="line"><span class="number">3</span>	[郑, 阿, 姨, 就, 赶, 到, 文, 汇, 路, 排, 队, 拿, 钱, ，, 希, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">11</span>, <span class="number">11</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.</span>..</span><br><span class="line"><span class="number">4</span>	[我, 想, 站, 在, 雪, 山, 脚, 下, 你, 会, 被, 那, 巍, 峨, 的, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.</span>..</span><br><span class="line">...	...	...</span><br><span class="line"><span class="number">1338</span>	[在, 这, 个, 非, 常, 喜, 庆, 的, 日, 子, 里, ，, 我, 们, 首, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ...</span><br><span class="line"><span class="number">1339</span>	[姜, 哲, 中, ：, 公, 共, 之, 敌, <span class="number">1</span>, -, <span class="number">1</span>, 》, 、, 《, 神, ...	[<span class="number">6</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16.</span>..</span><br><span class="line"><span class="number">1340</span>	[目, 前, ，, 日, 本, 松, 山, 海, 上, 保, 安, 部, 正, 在, 就, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">15</span>, <span class="number">0.</span>..</span><br><span class="line"><span class="number">1341</span>	[也, 就, 是, 说, 英, 国, 人, 在, 世, 博, 会, 上, 的, 英, 国, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ...</span><br><span class="line"><span class="number">1342</span>	[另, 外, 意, 大, 利, 的, P, l, a, y, G, e, n, e, r, ...	[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">12</span>, ...</span><br><span class="line"><span class="number">1343</span> rows × <span class="number">2</span> columns</span><br></pre></td></tr></table></figure>
<h2 id="四、定义bert模型、优化器、和训练部分"><a href="#四、定义bert模型、优化器、和训练部分" class="headerlink" title="四、定义bert模型、优化器、和训练部分"></a>四、定义bert模型、优化器、和训练部分</h2><p>这部分代码在bert_softmax.ipynb和bert_lstm_crf.ipynb文件中。</p>
<h3 id="4-1-bert-softmax："><a href="#4-1-bert-softmax：" class="headerlink" title="4.1 bert-softmax："></a>4.1 bert-softmax：</h3><p>训练过程为：</p>
<p><img src="https://user-images.githubusercontent.com/88963272/147417762-5f335c0a-e85c-4519-9ea0-f1a567b26705.png" alt="image"></p>
<p>验证集分数为：</p>
<p><img src="https://user-images.githubusercontent.com/88963272/147417775-63c4ecbe-6f48-4a0c-a601-46dd3a9ee366.png" alt="image"></p>
<p>训练完，用模型预测验证集结果，与原标签对比</p>
<p><img src="https://user-images.githubusercontent.com/88963272/147417783-787a3430-98ea-462f-8c67-efd837349a05.png" alt="image"></p>
<h3 id="4-2-bert-lstm-CRF"><a href="#4-2-bert-lstm-CRF" class="headerlink" title="4.2  bert+lstm+CRF"></a>4.2  bert+lstm+CRF</h3><p>epoch6的时候f1=0.78，没有达到预期，还需要调整。<br><img src="https://user-images.githubusercontent.com/88963272/147417806-25d99f69-e29e-4660-9443-e679556a32f4.png" alt="image"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/09/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/textcnn/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%20NLP%20%E5%85%A5%E9%97%A8%E8%B5%9B%20TextCNN%20%E6%96%B9%E6%A1%88%E4%BB%A3%E7%A0%81%E8%AF%A6%E7%BB%86%E6%B3%A8%E9%87%8A%E5%92%8C%E6%B5%81%E7%A8%8B%E8%AE%B2%E8%A7%A3/">张贤：textcnn做天池-新闻文本分类</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><div class="content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇文章用于记录阿里天池 NLP 入门赛，详细讲解了整个数据处理流程，以及如何从零构建一个模型，适合新手入门。</p>
<p>赛题以新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。实质上是一个 14 分类问题。</p>
<p>赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。</p>
<p>比赛地址：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">https://tianchi.aliyun.com/competition/entrance/531810/introduction</a><br></div><a class="more" href="/2021/10/09/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/textcnn/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%20NLP%20%E5%85%A5%E9%97%A8%E8%B5%9B%20TextCNN%20%E6%96%B9%E6%A1%88%E4%BB%A3%E7%A0%81%E8%AF%A6%E7%BB%86%E6%B3%A8%E9%87%8A%E5%92%8C%E6%B5%81%E7%A8%8B%E8%AE%B2%E8%A7%A3/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/08/huggingface/hugging%20face%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94datasets%E3%80%81optimizer/">Hugging Face官方文档——datasets、optimizer</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/HuggingFace/">HuggingFace</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></span><div class="content"><p>@[toc]<br>trainer参数设定参考：<a target="_blank" rel="noopener" href="https://www.yanxishe.com/columnDetail/26409">《huggingface transformers使用指南之二——方便的trainer》</a></p>
<h2 id="一、Load-dataset"><a href="#一、Load-dataset" class="headerlink" title="一、Load dataset"></a>一、Load dataset</h2><p>本节参考官方文档：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/loading.html">Load</a><br>数据集存储在各种位置，比如 Hub 、本地计算机的磁盘上、Github 存储库中以及内存中的数据结构（如 Python 词典和 Pandas DataFrames）中。无论您的数据集存储在何处，🤗 Datasets 都为您提供了一种加载和使用它进行训练的方法。</p>
<p>本节将向您展示如何从以下位置加载数据集：</p>
<ul>
<li>没有数据集加载脚本的 Hub</li>
<li>本地文件</li>
<li>内存数据</li>
<li>离线</li>
<li>拆分的特定切片</li>
<li>解决常见错误，以及如何加载指标的特定配置。</div><a class="more" href="/2021/10/08/huggingface/hugging%20face%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94datasets%E3%80%81optimizer/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/07/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task3%EF%BC%9A%E5%8D%95%E4%B8%AAbert%E6%A8%A1%E5%9E%8B%E5%88%86%E6%95%B00.961/">天池-新闻文本分类task3：bert模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><div class="content"><h2 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h2><p>&#8195;&#8195;比赛官方链接为：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">《零基础入门NLP - 新闻文本分类》</a>。<br>&#8195;&#8195;讨论区有大佬张帆、惊鹊和张贤等人的代码，值得大家仔细阅读。<br>&#8195;&#8195;最后我的模型参考了这些代码的一些config，比如bert.config，lr等等。然后大佬们的代码对我来说还是太复杂，pytorch功力不够，看的吃力。所以自己用huggingface实现了。<br>&#8195;&#8195;第一步分词我就考虑了很久，没有像张帆他们那样用pytorch具体一步步写，而是参考HF主页的教程。所以一开始我是翻译了构建tokenizer的教程，如果对比赛代码中分词有疑问的可以参考。</p>
<h2 id="三、最终代码及解析"><a href="#三、最终代码及解析" class="headerlink" title="三、最终代码及解析"></a>三、最终代码及解析</h2><p>主要思路：</p>
<ol>
<li>构建分词器。参考HF教程<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">《How to train and use your very own tokenizer》</a>。3750、648、900这三个应该是标点符号（详见张帆task02的分析），直接把这三个替换成‘，’、‘.’和‘！’。主要是为了断句。在预分词器pre_tokenizers.BertPreTokenizer中，有根据标点进行断句的方法，直接将文本换成带标点的格式就行，预分词器会自动断句。<ul>
<li>和BERT 有关的 Tokenizer 主要写在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py">models/bert/tokenization_bert.py</a>中。这部分内容其实在<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT">nlp教程3.1</a>里面有写。</div><a class="more" href="/2021/10/07/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task3%EF%BC%9A%E5%8D%95%E4%B8%AAbert%E6%A8%A1%E5%9E%8B%E5%88%86%E6%95%B00.961/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/05/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task2%20%EF%BC%9Afasttext/">天池-新闻文本分类task2：fasttext模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-05</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/fasttext/">fasttext</a></span><div class="content"><p>FastText：快速的文本分类器</p>
<h2 id="一、word2vec"><a href="#一、word2vec" class="headerlink" title="一、word2vec"></a>一、word2vec</h2><p>参考文档<a target="_blank" rel="noopener" href="https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80">《word2vec原理和gensim实现》</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">《深入浅出Word2Vec原理解析》</a></p>
<h3 id="1-1-word2vec为什么-不用现成的DNN模型"><a href="#1-1-word2vec为什么-不用现成的DNN模型" class="headerlink" title="1.1 word2vec为什么 不用现成的DNN模型"></a>1.1 word2vec为什么 不用现成的DNN模型</h3><ol>
<li>最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，==从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值==。解决办法有两个：霍夫曼树和负采样。</li>
<li>对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。输入从多个词向量变成了一个词向量</li>
<li>在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量</div><a class="more" href="/2021/10/05/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task2%20%EF%BC%9Afasttext/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/04/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task1%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/">天池-新闻文本分类task1: 机器学习模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/TF-IDF/">TF-IDF</a></span><div class="content"><p>本文重点使用TF-IDF+LightGBM</p>
<h1 id="二、LightGBM介绍"><a href="#二、LightGBM介绍" class="headerlink" title="二、LightGBM介绍"></a>二、LightGBM介绍</h1><p>LightGBM：一种高效的gbdt梯度提升决策树<br>gbdt 是决策树的集成模型，它基于三个重要原则：</p>
<ul>
<li>弱学习器（决策树） </li>
<li>梯度优化 </li>
<li><p>提升技巧<br>所以在gbdt方法中我们有很多决策树（弱学习器）。这些树是按顺序构建的：</p>
</li>
<li><p>第一棵树学习如何适应目标变量</p>
</li>
<li>第二棵树学习如何适应第一棵树的预测和基本事实之间的残差（差异）</li>
<li>第三棵树学习如何拟合第二棵树的残差，依此类推。</li>
</ul></div><a class="more" href="/2021/10/04/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task1%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/01/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/README/">天池-新闻文本分类task0：任务简介</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></span><div class="content"><p>task1:机器学习算法<br>用TF-IDF作为文档向量，分类器分别使用了岭回归、朴素贝叶斯、SVM、随机森林、Xgboost和lightGBM。<br>关于集成学习中的随机森林、Xgboost和lightGBM可以参考我在CSDN的帖子<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/122138831">《集成学习4：整理总结》</a></p>
<p>task2：fasttext<br>参考帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_64375823/article/details/121581268?spm=1001.2014.3001.5501">《学习笔记四：word2vec和fasttext》</a></p>
<p>task3：bert<br>参考帖子<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/120582114">《天池 入门赛-新闻文本分类-单个bert模型分数0.961》</a></p>
<h2 id="一、赛事说明"><a href="#一、赛事说明" class="headerlink" title="一、赛事说明"></a>一、赛事说明</h2><ul>
<li>比赛官方链接为：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">《零基础入门NLP - 新闻文本分类》</a>。</li>
<li>讨论区有大佬张帆、惊鹊和张贤等人的代码，值得大家仔细阅读。</li>
<li>最后我的模型参考了这些代码的一些config，比如bert.config，lr等等。然后大佬们的代码对我来说还是太复杂，pytorch功力不够，看的吃力。所以参考HF主页的教程，自己用huggingface实现了。</div><a class="more" href="/2021/10/01/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/README/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/27/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E3%80%81%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/">PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9APytorch/">10月组队学习：Pytorch</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Pytorch/">Pytorch</a></span><div class="content"><p>@[toc]</p>
<blockquote>
<p>推荐文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/265394674">《PyTorch 学习笔记汇总（完结撒花）》</a></p>
<h2 id="一、基础介绍"><a href="#一、基础介绍" class="headerlink" title="一、基础介绍"></a>一、基础介绍</h2><h3 id="1-1PyTorch-简介："><a href="#1-1PyTorch-简介：" class="headerlink" title="1.1PyTorch 简介："></a>1.1PyTorch 简介：</h3><ul>
<li>Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高。</li>
<li>PyTorch是一个基于Torch的Python开源机器学习库，提供了两个高级功能： <ul>
<li>具有强大的GPU加速的张量计算（如Numpy） </li>
<li>包含自动求导系统的深度神经网络</li>
<li>PyTorch，通过反向求导技术，可以让你零延迟地任意改变神经网络的行为，而且其实现速度快</li>
<li>底层代码易于理解 +命令式体验 +自定义扩展</li>
<li>缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势。例如目前PyTorch还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量等</li>
</ul>
</li>
</ul>
</blockquote></div><a class="more" href="/2021/09/27/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E3%80%81%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/09/25/huggingface/%E7%BC%96%E5%86%99transformers%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89pytorch%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%EF%BC%88Dataset%E5%92%8CDataLoader%E8%A7%A3%E6%9E%90%E5%92%8C%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%89/">Hugging Face官方文档——编写transformers的自定义pytorch训练循环</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/HuggingFace/">HuggingFace</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformers/">transformers</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></span><div class="content"><p>@[toc]</p>
<h1 id="一、Dataset和DataLoader加载数据集"><a href="#一、Dataset和DataLoader加载数据集" class="headerlink" title="一、Dataset和DataLoader加载数据集"></a>一、Dataset和DataLoader加载数据集</h1><h2 id="1-torch-utils-data"><a href="#1-torch-utils-data" class="headerlink" title="1.torch.utils.data"></a>1.torch.utils.data</h2><p>torch.utils.data主要包括以下三个类： </p>
<ol>
<li>class torch.utils.data.Dataset<br>其他的数据集类必须是torch.utils.data.Dataset的子类,比如说torchvision.ImageFolder. </li>
<li>class torch.utils.data.sampler.Sampler(data_source)<br>参数: data_source (Dataset) – dataset to sample from<br>作用: 创建一个采样器, class torch.utils.data.sampler.Sampler是所有的Sampler的基类, 其中,iter(self)函数来获取一个迭代器,对数据集中元素的索引进行迭代,len(self)方法返回迭代器中包含元素的长度. </li>
<li>class torch.utils.data.DataLoader</div><a class="more" href="/2021/09/25/huggingface/%E7%BC%96%E5%86%99transformers%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89pytorch%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%EF%BC%88Dataset%E5%92%8CDataLoader%E8%A7%A3%E6%9E%90%E5%92%8C%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%89/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>