<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="nlp"><meta name="keywords" content=""><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>You got a dream, you gotta protect it | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">49</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><nav id="nav" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="site-info"><div id="site-title">zhxnlpのBlog</div><div id="site-sub-title">You got a dream, you gotta protect it</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/zhxnlp/nlp-transformers" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fa"></i></a><a class="social-icon" href="https://gitee.com/zhxscut" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-google fa"></i></a><a class="social-icon" href="https://www.cnblogs.com/zhxnlp" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-bootstrap fa"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_56591814" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-weibo fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/24/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB1%EF%BC%9ATransformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89/">选读1：Transformer代码解读（Pytorch）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h1 id="Transformer代码解读（Pytorch）"><a href="#Transformer代码解读（Pytorch）" class="headerlink" title="Transformer代码解读（Pytorch）"></a>Transformer代码解读（Pytorch）</h1><p>本文是对transformer源代码的一点总结。原文在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.md">《Pytorch编写完整的Transformer》</a></p>
<p>本文涉及的jupter notebook在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.ipynb">Pytorch编写完整的Transformer</a><br>在阅读完<a href="./篇章2-Transformer相关原理/2.2-图解transformer.md">2.2-图解transformer</a>之后，希望大家能对transformer各个模块的设计和计算有一个形象的认识，本小节我们基于pytorch来实现一个Transformer，帮助大家进一步学习这个复杂的模型。<br>@[toc]</p>
<h2 id="0-读完总结：（这一段是自己的总结笔记）"><a href="#0-读完总结：（这一段是自己的总结笔记）" class="headerlink" title="0. 读完总结：（这一段是自己的总结笔记）"></a>0. 读完总结：（这一段是自己的总结笔记）</h2><ul>
<li>位置编码是最后加入的，输入输出形状不变。</li>
<li>每个attention层都有权重初始化（即输入映射成不同的QKV。而且每层权重不一样）<br>除了encoder-decoder-attention层q是来自前一层输出，kv是来自encoder层最后的输出memory会导致qkv维度不一致，其它层qkv维度都是一样的。并且只有第一维不一致，分别是L和S。</li>
<li>点积时会讲batch放到第一维，还有遮挡机制</li>
<li>encoderlayer1：输入src加入位置编码，进入 Multi-self-attention层。self.norm1(src + self.dropout1(src2))。即dropout输出src2，然后残差连接+Norm</li>
<li>encoderlayer2：全连接第一层3072神经元扩维4倍，之后激活并dropout，送入第二个全连接层降维回768维。之后同样的Add+Norm，即src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))、self.norm2(src + self.dropout2(src2))。</div><a class="more" href="/2021/08/24/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB1%EF%BC%9ATransformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/23/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task5%EF%BC%9ABERT%E5%85%B7%E4%BD%93%E5%BA%94%E7%94%A8%EF%BC%88%E5%BE%85%E8%A1%A5%E5%85%85%EF%BC%89/">task5：BERT具体应用</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-23</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h2 id="如何应用-BERT"><a href="#如何应用-BERT" class="headerlink" title="如何应用 BERT"></a>如何应用 BERT</h2><p>@[toc]<br>&#8195;&#8195;尝试 BERT 的最佳方式是通过托管在 Google Colab 上的 BERT FineTuning with Cloud TPUs。 BERT 代码可以运行在 TPU、CPU 和 GPU。</p>
<p>上一章我们查看 了BERT 仓库 中的代码：</p>
<h3 id="1-BERT代码总结："><a href="#1-BERT代码总结：" class="headerlink" title="1.BERT代码总结："></a>1.BERT代码总结：</h3><h4 id="1-1-BertTokenizer（Tokenization分词）"><a href="#1-1-BertTokenizer（Tokenization分词）" class="headerlink" title="1.1 BertTokenizer（Tokenization分词）"></a>1.1 BertTokenizer（Tokenization分词）</h4><ul>
<li>组成结构：BasicTokenizer和WordPieceTokenizer</li>
<li>BasicTokenizer主要作用：<ol>
<li>按标点、空格分割句子，对于中文字符，通过预处理（加空格方式）进行按字分割</li>
<li>通过never_split指定对某些词不进行分割</li>
<li>处理是否统一小写</li>
<li>清理非法字符</li>
</ol>
</li>
<li>WordPieceTokenizer主要作用：<ol>
<li>进一步将词分解为子词(subword)，例如，tokenizer 这个词就可以拆解为“token”和“##izer”两部分，注意后面一个词的“##”表示接在前一个词后面</li>
<li>subword介于char和word之间，保留了词的含义，又能够解决英文中单复数、时态导致的词表爆炸和未登录词的OOV问题</li>
<li>将词根和时态词缀分割，减小词表，降低训练难度</div><a class="more" href="/2021/08/23/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task5%EF%BC%9ABERT%E5%85%B7%E4%BD%93%E5%BA%94%E7%94%A8%EF%BC%88%E5%BE%85%E8%A1%A5%E5%85%85%EF%BC%89/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/22/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task4%EF%BC%9ABERT%E4%BB%A3%E7%A0%81%E8%AE%B2%E8%A7%A3/">task4：BERT代码讲解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-22</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><p>BERT代码实现<br>@[toc]<br>前言<br>&#8195;&#8195;本文是复制datawhale关于transformer教程里的一章<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT?id=%E5%89%8D%E8%A8%80">《如何实现一个BERT》</a>。本文包含大量源码和讲解，通过段落和横线分割了各个模块，同时网站配备了侧边栏，帮助大家在各个小节中快速跳转，希望大家阅读完能对BERT有深刻的了解。同时建议通过pycharm、vscode等工具对bert源码进行单步调试，调试到对应的模块再对比看本章节的讲解。</p>
<p>&#8195;&#8195;本篇章将基于<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">HuggingFace/Transformers, 48.9k Star</a>进行学习。本章节的全部代码在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert">huggingface bert</a>，注意由于版本更新较快，可能存在差别，请以4.4.2版本为准。<br>&#8195;&#8195;HuggingFace 是一家总部位于纽约的聊天机器人初创服务商，很早就捕捉到 BERT 大潮流的信号并着手实现基于 pytorch 的 BERT 模型。这一项目最初名为 pytorch-pretrained-bert，在复现了原始效果的同时，提供了易用的方法以方便在这一强大模型的基础上进行各种玩耍和研究。</p>
<p>&#8195;&#8195;随着使用人数的增加，这一项目也发展成为一个较大的开源社区，合并了各种预训练语言模型以及增加了 Tensorflow 的实现，并且在 2019 年下半年改名为 Transformers。截止写文章时（2021 年 3 月 30 日）这一项目已经拥有 43k+ 的star，可以说 Transformers 已经成为事实上的 NLP 基本工具。</p>
<p>本文基于 Transformers 版本 4.4.2（2021 年 3 月 19 日发布）项目中，pytorch 版的 BERT 相关代码，从代码结构、具体实现与原理，以及使用的角度进行分析。<br></div><a class="more" href="/2021/08/22/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task4%EF%BC%9ABERT%E4%BB%A3%E7%A0%81%E8%AE%B2%E8%A7%A3/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B%EF%BC%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/">GitHub 进阶教程：分支管理</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-21</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/">软件应用</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/github/">github</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%BD%AF%E4%BB%B6/">软件</a></span><div class="content"><h3 id="4-Git-进阶"><a href="#4-Git-进阶" class="headerlink" title="4.Git 进阶"></a>4.Git 进阶</h3><h4 id="4-1用户名和邮箱"><a href="#4-1用户名和邮箱" class="headerlink" title="4.1用户名和邮箱"></a>4.1用户名和邮箱</h4><p>&#8195;&#8195;每一次commit都会产生一条log，==log标记了提交人的姓名与邮箱==，以便其他人方便的查看与联系提交人，所以我们在进行提交代码的第一步就是要设置自己的用户名与邮箱。执行以下代码：<br>git config —global user.name “stormzhang”<br>git config —global user.email “stormzhang.dev@gmail.com”<br>&#8195;&#8195;以上进行了==全局配置==，当然有些时候我们的某一个项目想要用特定的邮箱，这个时候只需切换到你的项目，以上代码把 —global 参数去除，再重新执行一遍就ok了。<br></div><a class="more" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B%EF%BC%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/">GitHub 安装使用详细教程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-21</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/">软件应用</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/github/">github</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%BD%AF%E4%BB%B6/">软件</a></span><div class="content"><h2 id="GitHub发现开源项目，提高工作效率"><a href="#GitHub发现开源项目，提高工作效率" class="headerlink" title="GitHub发现开源项目，提高工作效率"></a>GitHub发现开源项目，提高工作效率</h2><p>&#8195;&#8195;本文是<a target="_blank" rel="noopener" href="https://github.com/stormzhang">《learn-github-from-zero》</a>读书笔记，做了一些简洁化修改。<br>&#8195;&#8195;主要内容是GitHub页面介绍、Git Bash基础命令、GIT进阶、Git Flow分支管理流程和最后的开源项目参与。不包含GitHub账号注册、Git Bash下载安装、ssh密钥等内容。（这部分可参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/Hanani_Jia/article/details/77950594">《GitHub 新手详细教程》</a>）</p>
<p>初始化本地仓库，推送到空的git 仓库。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">create a new repository on the command line</span><br><span class="line">echo <span class="string">&quot;# nlp-transformers&quot;</span> &gt;&gt; README.md</span><br><span class="line">git init<span class="comment">#本地仓库初始化</span></span><br><span class="line">git add README.md</span><br><span class="line">git add .<span class="comment">#添加当前目录所有文件</span></span><br><span class="line">git commit -m <span class="string">&quot;first commit&quot;</span></span><br><span class="line"><span class="comment">#此时本地仓库默认分支是master，直接提交需要合并到远程仓库mian，比较麻烦</span></span><br><span class="line">git branch -M main<span class="comment">#将本地仓库默认改为mian分支</span></span><br><span class="line">git remote add origin https://github.com/zhxnlp/nlp-transformers.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><br></div><a class="more" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/lightGBM/">GitHub 安装使用详细教程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-21</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/">软件应用</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/github/">github</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E8%BD%AF%E4%BB%B6/">软件</a></span><div class="content"><ul>
<li><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-Intro.html" title="官方文档">LightGBM 官方文档</a></li>
<li>阿水知乎贴：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266865429" title="《你应该知道的LightGBM各种操作》">《你应该知道的LightGBM各种操作》</a></li>
<li><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html" title="Python API">Python API（包括Scikit-learn API）</a></li>
<li><a target="_blank" rel="noopener" href="https://coggle.club/blog/30days-of-ml-202201" title="《Coggle 30 Days of ML（22年1&amp;2月）》">《Coggle 30 Days of ML（22年1&amp;2月）》</a></li>
</ul>
<p>学习内容：</p>
<p>LightGBM（Light Gradient Boosting Machine）是微软开源的一个实现 GBDT 算法的框架，支持高效率的并行训练。LightGBM 提出的主要原因是为了解决 GBDT 在海量数据遇到的问题。本次学习内容包括使用LightGBM完成各种操作，包括竞赛和数据挖掘中的模型训练、验证和调参过程。</p>
<p>打卡汇总：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务名称</th>
<th>难度、分数</th>
<th>所需技能</th>
</tr>
</thead>
<tbody>
<tr>
<td>任务1模型训练与预测</td>
<td>低、1</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务2：模型保存与加载</td>
<td>低、1</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务3：分类、回归和排序任务</td>
<td>高、3</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务4：模型可视化</td>
<td>低、1</td>
<td>graphviz</td>
</tr>
<tr>
<td>任务5：模型调参（网格、随机、贝叶斯）</td>
<td>中、2</td>
<td>模型调参</td>
</tr>
<tr>
<td>任务6：模型微调与参数衰减</td>
<td>中、2</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务7：特征筛选方法</td>
<td>高、3</td>
<td>特征筛选方法</td>
</tr>
<tr>
<td>任务8：自定义损失函数</td>
<td>中、2</td>
<td>损失函数&amp;评价函数</td>
</tr>
<tr>
<td>任务9：模型部署与加速</td>
<td>高、3</td>
<td>Treelite</td>
</tr>
</tbody>
</table>
</div>
<h2 id="一、使用LGBMClassifier对iris进行训练"><a href="#一、使用LGBMClassifier对iris进行训练" class="headerlink" title="一、使用LGBMClassifier对iris进行训练"></a>一、使用LGBMClassifier对iris进行训练</h2></div><a class="more" href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/lightGBM/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/21/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task3%EF%BC%9A%E5%9B%BE%E8%A7%A3GPT-2/">task3：图解GPT-2</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-21</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h2 id="图解GPT-2"><a href="#图解GPT-2" class="headerlink" title="图解GPT-2"></a>图解GPT-2</h2><p>@[toc]<br>引言：<br>&#8195;&#8195;本文是datawhale教程的读书笔记，由datawhale翻译自Jay Alammar的文章<a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-gpt2/">《The Illustrated GPT-2 (Visualizing Transformer Language Models)》</a>。中文翻译版原文在datawhale课程<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.4-%E5%9B%BE%E8%A7%A3GPT?id=gpt-2-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">《图解GPT》</a></p>
<h3 id="1-GPT简介"><a href="#1-GPT简介" class="headerlink" title="1.GPT简介"></a>1.GPT简介</h3><p>&#8195;&#8195;GPT是OpenAI公司2018年提出的生成式预训练模型（Generative Pre-Trainning，GPT），用来提升自然语言理解任务的效果。GPT的出现打破了NLP各个任务之间的壁垒，不需要根据特定任务了解太多任务背景。根据预训练模型就能得到不错的任务效果。<br>&#8195;&#8195;GPT提出了“生成式预训练+判别式任务精调”的范式来处理NLP任务。</p>
<ul>
<li>生成式预训练：在大规模无监督语料上进行预训练一个高容量的语言模型，学习丰富的上下文信息，掌握文本的通用语义。</li>
<li>判别式任务精调：在通用语义基础上根据下游任务进行领域适配。具体的在预训练好的模型上增加一个与任务相关的神经网络层，比如一个全连接层，预测最终的标签。并在该任务的监督数据上进行微调训练（微调的一种理解：学习率较小，训练epoch数量较少，对模型整体参数进行轻微调整）</div><a class="more" href="/2021/08/21/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task3%EF%BC%9A%E5%9B%BE%E8%A7%A3GPT-2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/18/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task2%EF%BC%9ABERT%E5%85%A5%E9%97%A8/">task2：BERT入门</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p>@[toc]<br>&#8195;&#8195;开篇先分享苏神的两篇文章<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/4765">《Attention is All You Need》浅读（简介+代码）</a>、<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6933">《Sep从语言模型到Seq2Seq：Transformer如戏，全靠Mask》</a>。还有一篇讲BERT输入的<a target="_blank" rel="noopener" href="https://www.cnblogs.com/d0main/p/10447853.html">《为什么BERT有3个嵌入层，它们都是如何实现的》</a>（这篇不感兴趣可以不看）</p>
<h4 id="1-1-BERT简介"><a href="#1-1-BERT简介" class="headerlink" title="1.1 BERT简介"></a>1.1 BERT简介</h4><p>&#8195;&#8195;BERT是2018年10月由Google AI研究院提出的一种预训练模型，在多种不同NLP测试中创出SOTA表现，成为NLP发展史上的里程碑式的模型成就。BERT的出现标志着NLP 新时代的开始。<br>&#8195;&#8195;BERT全称是“Bidirectional Encoder Representation from Transformers“，即双向Transformer解码器。是一种NLP领域的龙骨模型，用来提取各类任务的基础特征，即作为预训练模型为下游NLP任务提供帮助。<br></div><a class="more" href="/2021/08/18/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task2%EF%BC%9ABERT%E5%85%A5%E9%97%A8/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/08/17/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task1%EF%BC%9A%E5%A4%9A%E5%9B%BE%E8%AF%A6%E8%A7%A3attention%E5%92%8Cmask%E3%80%82%E4%BB%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81transformer%E5%88%B0GPT2/">task1：多图详解attention</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-17</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nlp/">nlp</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/transformer/">transformer</a></span><div class="content"><h2 id="transformer原理"><a href="#transformer原理" class="headerlink" title="transformer原理"></a>transformer原理</h2><p>见CSDN帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/119759105">https://blog.csdn.net/qq_56591814/article/details/119759105</a></p>
<p>说明：<br>&#8195;&#8195;本文主要来自datawhale的开源教程<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers">《基于transformers的自然语言处理(NLP)入门》</a>，此项目也发布在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers">github</a>。部分内容（章节2.1-2.5，3.1-3.2，4.1-4.2）来自北大博士后卢菁老师的《速通机器学习》一书。</p>
<p>&#8195;&#8195; 另外篇幅有限，关于多头注意力的encoder-decoder attention模块进行运算的更详细内容可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38224810/article/details/115587885">《Transformer概览总结》</a>。从attention到transformer的API实现和自编程代码实现，可以查阅<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task02">《Task02 学习Attention和Transformer》</a>（这篇文章排版很好，干净简洁，看着非常舒服，非常推荐）</p>
<p>&#8195;&#8195; Transformer代码参考<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.2-Pytorch%E7%BC%96%E5%86%99Transformer-%E9%80%89%E8%AF%BB">《Transformer源代码解释之PyTorch篇》</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/120278245">《《The Annotated Transformer》翻译————注释和代码实现《Attention Is All You Need》》</a>。<br></div><a class="more" href="/2021/08/17/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task1%EF%BC%9A%E5%A4%9A%E5%9B%BE%E8%AF%A6%E8%A7%A3attention%E5%92%8Cmask%E3%80%82%E4%BB%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81transformer%E5%88%B0GPT2/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>