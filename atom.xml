<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhxnlpのBlog</title>
  
  <subtitle>You got a dream, you gotta protect it</subtitle>
  <link href="https://zhxnlp.github.io/atom.xml" rel="self"/>
  
  <link href="https://zhxnlp.github.io/"/>
  <updated>2022-01-03T20:46:27.391Z</updated>
  <id>https://zhxnlp.github.io/</id>
  
  <author>
    <name>zhxnlp</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2021使用hexo+github搭建个人博客</title>
    <link href="https://zhxnlp.github.io/2021/12/31/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/2021%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8hexo+github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://zhxnlp.github.io/2021/12/31/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/2021%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8hexo+github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-12-30T22:45:50.000Z</published>
    <updated>2022-01-03T20:46:27.391Z</updated>
    
    <content type="html"><![CDATA[<p>参考资料：</p><ul><li>主要参考文档：<a href="https://hexo.io/zh-cn/docs/">hexo文档</a>、<a href="https://molunerfinn.com/hexo-theme-melody-doc/#features">melody主题文档</a>、<a href="https://www.youtube.com/watch?v=xvIRGmKWpFM">youtube教学视频</a>、<a href="https://space.bilibili.com/362224537/channel/series">bilibili的hexo教学视频</a>、<a href="https://hexo.io/themes/">hexo主题网站</a>。</li><li><a href="https://blog.csdn.net/wapchief/article/details/54602515?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164088400916780366527950%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164088400916780366527950&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-10-54602515.pc_search_insert_es_download_v2&amp;utm_term=github%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA&amp;spm=1018.2226.3001.4187">《使用hexo+github免费搭建个人博客网站超详细教程》</a>、hexo文档：<a href="https://hexo.io/zh-cn/docs/github-pages">将 Hexo 部署到 GitHub Pages</a>。</li><li>bilibili搭建博客视频资料<a href="https://www.bilibili.com/video/BV1mU4y1j72n?p=5">《【2021最新版】保姆级Hexo+github搭建个人博客》</a></li><li><a href="https://yarn.bootcss.com/">yarn1中文文档</a>、<a href="https://www.yarnpkg.cn/">yarn2文档</a>、<a href="https://www.npmjs.cn/">npm中文文档</a>、<a href="http://bit.ly/2QhJTNaYaml">Yaml官网</a>、<a href="http://bit.ly/2zxeDCC">Yaml教程</a> </li><li><a href="https://hexo.io/zh-cn/docs/commands">hexo常用命令</a>、<a href="https://github.com/hexojs/awesome-hexo">Hexo Awesome文档</a>、<a href="https://github.com/hexojs/hexo">Hexo Github</a><span id="more"></span> <h2 id="一、必备软件安装"><a href="#一、必备软件安装" class="headerlink" title="一、必备软件安装"></a>一、必备软件安装</h2>首先安装Node.js、git，再安装hexo-cli（cli是命令行接口，即commend  line interface。本身也会装hexo，且可以通过命令操作hexo）。<br>安装nodejs参考帖子<a href="https://blog.csdn.net/qq_56591814/article/details/121980469?spm=1001.2014.3001.5501">《Datawhale第32期组队学习——task0：新闻推荐系统项目搭建：centos下前端配置》</a><br>安装git工具，以及github注册创建仓库，参考我另一篇帖子<a href="https://blog.csdn.net/qq_56591814/article/details/119841785?spm=1001.2014.3001.5501">《GitHub 详细教程》</a>。<br>hexo支持yaml和json。凡是使用Yaml的地方也可以使用json替代。但是json相较yaml更加烦琐而功能相同，且各大网站基本都支持Yaml，所以推荐大家使用Yaml。</li></ul><h2 id="二、-hexo本地搭建博客"><a href="#二、-hexo本地搭建博客" class="headerlink" title="二、 hexo本地搭建博客"></a>二、 hexo本地搭建博客</h2><p>常用命令：</p><ul><li>hexo clean：清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令</li><li>hexo g -d :Hexo 在生成完毕后自动部署网站</li><li>hexo s：本地部署网站</li><li>hexo —draft：显示草稿</li><li><h3 id="2-1-本地生成博客内容"><a href="#2-1-本地生成博客内容" class="headerlink" title="2.1 本地生成博客内容"></a>2.1 本地生成博客内容</h3>新建blog文件夹，运行<code>hexo init [folder]</code>。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。<br>本命令相当于执行了以下几步：</li></ul><ol><li>Git clone hexo-starter 和 hexo-theme-landscape 主题到当前目录或指定目录。</li><li>使用 Yarn 1、pnpm 或 npm 包管理器下载依赖（如有已安装多个，则列在前面的优先）。npm 默认随 Node.js 安装。</li></ol><p>有些包没有安装成功，直接运行<code>npm install</code>安装。<br>输入dir可以查看目录下文件。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">dir</span><span class="comment">#我这个是已经发布过的hexo 博客</span></span><br><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure><ul><li>_config.yml：网站的 <a href="https://hexo.io/zh-cn/docs/configuration">配置</a> 信息，您可以在此配置大部分的参数。配置文件遵循yaml语法。</li><li>package.json：应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。</li><li>scaffolds：<a href="https://hexo.io/zh-cn/docs/writing">模版</a> 文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。<br>Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。</li><li>source：资源文件夹。除 <em>posts 文件夹之外，开头命名为 </em> (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。</li><li>themes：主题 文件夹，Hexo 会根据主题来生成静态页面。</li></ul><p>输入<code>hexo g</code>静态生成本地网页。（g就是generate生成的缩写）<br><img src="https://img-blog.csdnimg.cn/3480ea39bd374d73816019e2e6afe7cf.png" alt="在这里插入图片描述"><br>此时会发现根目录下多了一个<code>public</code>的文件夹，里面就是生成的网页文件。<br>再次运行<code>hexo s</code>启动网站（s就是server的意思）。此时就可以在<a href="http://localhost:4000/中查看本地生成的博客网站。">http://localhost:4000/中查看本地生成的博客网站。</a></p><p>您可执行下列的其中一个命令，让 Hexo 在生成完毕后自动部署网站，两个命令的作用是相同的：<code>hexo g -d hexo d -g</code>。</p><h3 id="2-2-文章写作、自动摘录"><a href="#2-2-文章写作、自动摘录" class="headerlink" title="2.2 文章写作、自动摘录"></a>2.2 文章写作、自动摘录</h3><blockquote><p><a href="https://www.youtube.com/watch?v=HLJ9jJy7CMg">youtube视频</a></p><ul><li>实现博客主页只显示部分文章内容，只需要在md文件某一行插入<code>&lt;!--more--&gt;</code>。首页只会显示标记以前的内容，标记后的内容可以通过阅读更多查阅，也可以设置自动摘录。</li><li>ipynb转md文件，在文件目录下打开cmd运行：<code>jupyter nbconvert --to markdown bert.ipynb</code>就行。<br>注意：使用自动摘录可能会导致代码区出错。所以如果你想在索引页中显示代码，那么你最好不要使用这个功能！</li></ul></blockquote><p>自动摘录，设置melody.yml：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true</span><br><span class="line">  length: <span class="number">150</span></span><br></pre></td></tr></table></figure></p><ol><li><code>hexo new [layout] &lt;title&gt;</code>新建文章，生成一个标题为post title的md文件<br>如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;post title&quot;</span></span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  Created: E:\Git Bash\blog\source\_posts\post-title.md</span><br></pre></td></tr></table></figure>打开md文件发现开头已经写了front-master（开头这部分自动生成的就是front master）：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: post title</span><br><span class="line">date: <span class="number">2021</span>-<span class="number">12</span>-<span class="number">31</span> <span class="number">22</span>:07:<span class="number">24</span></span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li></ol><ul><li>-p, —path    自定义新文章的路径</li><li>-r, —replace    如果存在同名文章，将其替换</li><li>-s, —slug    文章的 Slug，作为新文章的文件名和发布后的 URL</li></ul><p>默认情况下，Hexo 会使用文章的标题来决定文章文件的路径。对于独立页面来说，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 index.md 文件。你可以使用 —path 参数来覆盖上述行为、自行决定文件的目录：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo new page --path about/me <span class="string">&quot;About me&quot;</span></span><br></pre></td></tr></table></figure></p><p>以上命令会创建一个 source/about/me.md 文件，同时 Front Matter 中的 title 为 “About me”</p><ol><li>运行<code>hexo new draft &quot;draft title&quot;</code>可以生成草稿。草稿可以预览其效果，但是不会被hexo渲染生成，即网站不会存在草稿文章。</li><li>运行<code>hexo new page &quot;page title&quot;</code>可以生成纯网页html。</li></ol><p>这三种类型的默认模板在scaffolds文件夹下面。即post.md，draft.md，page.md。模板使用nonjunks访问系统变量和函数。</p><h3 id="2-3-博客发布到网上"><a href="#2-3-博客发布到网上" class="headerlink" title="2.3 博客发布到网上"></a>2.3 博客发布到网上</h3><h3 id="2-3-1-配置主题模板"><a href="#2-3-1-配置主题模板" class="headerlink" title="2.3.1 配置主题模板"></a>2.3.1 配置主题模板</h3><p>下载next主题：<code>git clone git://github.com/theme-next/hexo-theme-next themes/next</code>。这句代码表示创建themes/next文件夹，并下载github.com/theme-next/hexo-theme-next仓库的文件。</p><p>回到hexo根目录下用记事本打开_config.yml文件。<br>配置模板，参考<a href="https://github.com/theme-next/hexo-theme-next">themes/next仓库文档</a>和<a href="http://theme-next.iissnan.com/getting-started.html">配置文档</a>。<br><img src="https://img-blog.csdnimg.cn/e181732eb6cc4a4a98cb664d9cca716e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>更改主题参考：<a href="https://blog.csdn.net/weixin_42419856/article/details/81141546?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164090083216780265493528%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164090083216780265493528&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-81141546.pc_search_insert_es_download_v2&amp;utm_term=github%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98&amp;spm=1018.2226.3001.4187">《如何更改使用hexo-github搭建博客的主题 theme》</a>。<br>hexo主题在这：<a href="https://hexo.io/themes/">Themes</a>。我安装的主题是<a href="https://molunerfinn.com/">melody</a>，<a href="https://molunerfinn.com/hexo-theme-melody-doc/quick-start.html#installation">说明文档</a>很详细。<br>安装后网页打开显示：<code>extends includes/layout.pug block content #recent-posts.recent-posts include includes/rec</code>。</p><p>运行<code>npm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code><br>提示有些包版本低，升级就行。npm查看本地安装的包的版本号<code>npm ls &lt;packageName&gt;</code>。<br>npm install —save constantinople jstransformer hexo-renderer-pug pug debug</p><h3 id="2-3-2-配置自己的远程仓库地址"><a href="#2-3-2-配置自己的远程仓库地址" class="headerlink" title="2.3.2 配置自己的远程仓库地址"></a>2.3.2 配置自己的远程仓库地址</h3><p><a href="https://blog.csdn.net/qq_38225558/article/details/90765491?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164088400916780366557862%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164088400916780366557862&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-8-90765491.pc_search_insert_es_download_v2&amp;utm_term=github%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA&amp;spm=1018.2226.3001.4187"><GitHub搭建个人博客教程></a><br>在开始之前，必须先在 _config.yml 中修改部署配置参数，可同时使用多个 deployer，Hexo 会依照顺序执行每个 deployer：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">- <span class="built_in">type</span>: git</span><br><span class="line">  repo:</span><br><span class="line">- <span class="built_in">type</span>: heroku</span><br><span class="line">  repo:</span><br></pre></td></tr></table></figure><p>我配置的是：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git<span class="comment">#英文：接一个空格，下同</span></span><br><span class="line">  repo: https://github.com/zhxnlp/zhxnlp.github.io<span class="comment">#自己的仓库地址</span></span><br><span class="line">  branch: mian<span class="comment">#github在今年默认展示main分支，而不是master。</span></span><br></pre></td></tr></table></figure></p><h3 id="2-3-3-发布github博客"><a href="#2-3-3-发布github博客" class="headerlink" title="2.3.3 发布github博客"></a>2.3.3 发布github博客</h3><p>继续在本目录命令行，安装部署工具<code>npm install hexo-deployer-git -save</code>（方便以后更新）。<br>在blog目录（hexo项目根目录）运行<code>git init</code>本地仓库初始化。然后绑定仓库：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git branch -M main<span class="comment">#将本地仓库默认改为mian分支</span></span><br><span class="line">git remote add origin git@github.com:zhxnlp/zhxnlp.github.io.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><p>已有仓库估计出错。新建仓库名为<code>zhxnlp.github.io</code>，绑定好仓库后，运行<code>hexo g</code>生成页面。<br>运行<code>hexo d</code>即deployee，将本地仓库文件上传到网上。也可直接输入 <code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>。</p><p>打开博客网址<code>https://zhxnlp.github.io/</code>，网页界面报错：<br>extends includes/layout.pug block content include includes/recent-posts.pug include includes/pagination.pug。<br>这是确少hexo插件，安装插件见上一节内容。</p><p>hexo g报错：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ERROR TypeError: E:\Git Bash\blog\node_modules\hexo-theme-melody\layout\includes</span><br><span class="line">\layout.pug:<span class="number">11</span></span><br><span class="line">    <span class="number">9</span>|</span><br><span class="line">    <span class="number">10</span>| - var pageDescription = page.description || page.title || config.descrip</span><br><span class="line">tion || <span class="string">&#x27;&#x27;</span></span><br><span class="line">  &gt; <span class="number">11</span>| - var pageKeywords = (config.keywords || []).join(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="number">12</span>| - <span class="keyword">if</span> (page.tags &amp;&amp; page.tags.data) pageKeywords = page.tags.data.<span class="built_in">map</span>(fun</span><br><span class="line">ction(tag) &#123;<span class="keyword">return</span> tag.name;&#125;).join(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="number">13</span>| - <span class="keyword">if</span> (page.keywords) pageKeywords = page.keywords.join(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="number">14</span>| - var pageAuthor = config.email ? config.author + <span class="string">&#x27;,&#x27;</span> + config.email : c</span><br><span class="line">onfig.author</span><br><span class="line"></span><br><span class="line">(config.keywords || []).join <span class="keyword">is</span> <span class="keyword">not</span> a function</span><br><span class="line">    at <span class="built_in">eval</span> (<span class="built_in">eval</span> at wrap (E:\Git Bash\blog\node_modules\pug-runtime\wrap.js:<span class="number">6</span>:<span class="number">1</span></span><br><span class="line"><span class="number">0</span>), &lt;anonymous&gt;:<span class="number">24</span>:<span class="number">44</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这是因为根目录下<code>keyword</code>报错（估计是格式不对），导致hexo-theme-melody\layout\includes<br>\layout.pug文件第11行执行错误，改了就好了。</p><p>又报错10054，</p><blockquote><p>我是已经有仓库了，但是为了搭建博客改了域名为<code>https://github.com/zhxnlp/zhxnlp.github.io</code>，但是本地没有改，结果pull时出错。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ git pull origin mian</span><br><span class="line">starting fsmonitor-daemon <span class="keyword">in</span> <span class="string">&#x27;E:/Git Bash/bert2&#x27;</span></span><br><span class="line">fatal: couldn<span class="string">&#x27;t find remote ref mian</span></span><br></pre></td></tr></table></figure><br><code>fatal: couldn&#39;t find remote ref mian</code>表示仓库没有文件（域名被我改了，原来的仓库地址啥也没有）<br>git隐藏文件夹下：将 .git/cofig 文件中的 url 进行修改：<code>url = git@github.com:zhxnlp/zhxnlp.github.io</code>。</p></blockquote><h3 id="2-4-主题配置"><a href="#2-4-主题配置" class="headerlink" title="2.4 主题配置"></a>2.4 主题配置</h3><p>从这里开始都是配置主题的内容，可以在前面部分搭好之后再慢慢弄。<br>2.4这一节都是<a href="https://hexo.io/zh-cn/docs/front-matter">hexo文档内容</a>。</p><h4 id="2-4-1-front-master参数"><a href="#2-4-1-front-master参数" class="headerlink" title="2.4.1 front master参数"></a>2.4.1 front master参数</h4><blockquote><p><a href="https://www.youtube.com/watch?v=Rl48Yk4A_V8">youtube视频</a></p></blockquote><p>Front-matter 是文件最上方以 —- 分隔的区域，用于指定个别文件的变量，默认是：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: post title</span><br><span class="line">date: <span class="number">2021</span>-<span class="number">12</span>-<span class="number">31</span> <span class="number">22</span>:07:<span class="number">24</span></span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><br>可以用以下<a href="https://hexo.io/zh-cn/docs/front-matter">参数设置</a>其它属性：</p><div class="table-container"><table><thead><tr><th>参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>layout</td><td>布局</td><td>config.default_layout</td></tr><tr><td>title</td><td>标题</td><td>文章的文件名</td></tr><tr><td>date</td><td>建立日期</td><td>文件建立日期</td></tr><tr><td>updated</td><td>更新日期</td><td>文件更新日期</td></tr><tr><td>comments</td><td>开启文章的评论功能</td><td>true/on/off</td></tr><tr><td>tags</td><td>标签（不适用于分页）</td><td></td></tr><tr><td>categories</td><td>分类（不适用于分页）</td><td></td></tr><tr><td>permalink</td><td>覆盖文章网址</td><td></td></tr><tr><td>excerpt</td><td>纯文本页面摘录。</td><td></td></tr><tr><td>lang</td><td>设置语言覆盖</td><td>Inherited from _config.yml</td></tr></tbody></table></div><h4 id="2-4-2-布局"><a href="#2-4-2-布局" class="headerlink" title="2.4.2  布局"></a>2.4.2  布局</h4><p>默认布局为 post，根据 _config.yml 中 default_layout 设置的值。 当文章中禁用布局（设为layout: false时），将不会对其进行主题处理。 但是，它仍然会被任何可用的渲染器渲染：如果一篇文章是用 Markdown 编写的，并且安装了 Markdown 渲染器（如默认的 hexo-renderer-marked），它将被渲染为 HTML。<br>标签插件默认启用，除非设置disableNunjucks禁用或者被渲染器禁用。</p><h4 id="2-4-3-标签和分类"><a href="#2-4-3-标签和分类" class="headerlink" title="2.4.3 标签和分类"></a>2.4.3 标签和分类</h4><p>启用首页显示分类和标签，只需要_config.melody.yml文件设置：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Main menu navigation</span></span><br><span class="line">menu:</span><br><span class="line">  Home: /</span><br><span class="line">  Archives: /archives</span><br><span class="line">  Tags: /tags</span><br><span class="line">  Categories: /categories</span><br><span class="line">  <span class="comment">#about: /about</span></span><br></pre></td></tr></table></figure><p>修改source/tags/index.md文件内容为：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: tags</span><br><span class="line">date: <span class="number">2018</span>-01-05 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="built_in">type</span>: <span class="string">&quot;tags&quot;</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>修改source/categories/index.md内容为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: categories</span><br><span class="line">date: <span class="number">2018</span>-01-05 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="built_in">type</span>: <span class="string">&quot;categories&quot;</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><ul><li>只有md文章支持分类和标签。分类具有顺序性和层次性，而标签没有顺序和层次。一个文章可以有多个标签，多个分类，用中括号[]就可以达到并列效果。</li><li><p>标签和分类插件使用的是YAML语法，直接categories: 1,2会把1,2作为一个分类。正确是用- 的形式设置分类和多标签：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- Diary</span><br><span class="line">tags:</span><br><span class="line">- PS3</span><br><span class="line">- Games</span><br></pre></td></tr></table></figure><p>Hexo 不支持指定多个同级分类。下面的指定方法，会使分类Life成为Diary的子分类，而不是并列分类：</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line"> - Diary</span><br><span class="line"> - Life</span><br></pre></td></tr></table></figure><p>如果想设置并列分类，可以写成：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line"> - [Diary]</span><br><span class="line"> - [Life]</span><br></pre></td></tr></table></figure><br>如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [Diary, PlayStation]</span><br><span class="line">- [Diary, Games]</span><br><span class="line">- [Life]</span><br></pre></td></tr></table></figure><p>此时这篇文章同时包括三个分类： PlayStation 和 Games 分别都是父分类 Diary 的并列子分类，同时 Life 是一个没有子分类的分类。</p><h4 id="2-4-4-标签插件（Tag-Plugins）"><a href="#2-4-4-标签插件（Tag-Plugins）" class="headerlink" title="2.4.4 标签插件（Tag Plugins）"></a>2.4.4 标签插件（Tag Plugins）</h4><p>标签插件和 Front-matter 中的标签不同，它们是用于在文章中快速插入特定内容的插件。具体查看<a href="https://hexo.io/zh-cn/docs/tag-plugins">官方文档</a>。感觉这部分内容不重要，我用得少。</p><h4 id="2-4-5-资源文件夹-图片嵌入"><a href="#2-4-5-资源文件夹-图片嵌入" class="headerlink" title="2.4.5 资源文件夹/图片嵌入"></a>2.4.5 资源文件夹/图片嵌入</h4><ol><li>资源文件夹</li></ol><ul><li>资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。</li><li>例如你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 source/images 文件夹中。然后通过类似于<code>![](/images/image.jpg)</code>的方法访问它们。</li><li>Hexo也提供了更组织化的方式来管理资源。管理资源功能可以通过将 config.yml 文件中的 <code>post_asset_folder</code> 选项设为 <code>true</code> 来打开。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure><ul><li>当资源文件管理功能打开后，Hexo将会在你每一次通过 <code>hexo new [layout] &lt;title&gt;</code> 命令创建新文章时自动创建一个和文章文件同名的资源文件夹，里面放入所有和文章有关的资源，然后通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。</li></ul><ol><li>相对路径引用标签插件：<br>Hexo 3 中，许多新的<a href="https://hexo.io/docs/tag-plugins#Include-Assets">标签插件</a>被加入到了核心代码中，这使得你可以更简单地在文章中引用你的资源。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>比如说：当你<code>打开文章资源文件夹功能</code>后，你把一个 example.jpg 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 <code>![](example.jpg)</code> ，它将 不会 出现在首页上。正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img example.jpg This <span class="keyword">is</span> an example image %&#125;</span><br></pre></td></tr></table></figure><p>通过这种方式，图片将会同时出现在文章和主页以及归档页中。</p><ol><li>使用 Markdown 嵌入图像<br>hexo-renderer-marked 3.1.0 引入了一个新选项，允许您在不使用 <code>asset_img</code> 标签插件的情况下在 Markdown 中嵌入图像。</li></ol><p>启用功能：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">post_asset_folder: true</span><br><span class="line">marked:</span><br><span class="line">  prependRoot: true</span><br><span class="line">  postAsset: true</span><br></pre></td></tr></table></figure><p>启用后，asset image将自动解析为其相应帖子的路径。 例如，“image.jpg”位于“/2020/01/02/foo/image.jpg”，表示它是“/2020/01/02/foo/”帖子的asset image ，<code>![](image.jpg)</code>将呈现为 <code>&lt;img src=&quot;/2020/01/02/foo/image.jpg&quot;&gt;</code>。</p><h4 id="2-4-6-数据文件夹"><a href="#2-4-6-数据文件夹" class="headerlink" title="2.4.6 数据文件夹"></a>2.4.6 数据文件夹</h4><p>有时您可能需要在主题中使用某些资料，而这些资料并不在文章内，并且是需要重复使用的，那么您可以考虑使用 Hexo 3.0 新增的「数据文件」功能。此功能会载入 source/_data 内的 YAML 或 JSON 文件，如此一来您便能在网站中复用这些文件了。<br>举例来说，在 source/_data 文件夹中新建 menu.yml 文件：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Home: /</span><br><span class="line">Gallery: /gallery/</span><br><span class="line">Archives: /archives/</span><br></pre></td></tr></table></figure><p>您就能在模板中使用这些资料：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;% <span class="keyword">for</span> (var link <span class="keyword">in</span> site.data.menu) &#123; %&gt;</span><br><span class="line">  &lt;a href=<span class="string">&quot;&lt;%= site.data.menu[link] %&gt;&quot;</span>&gt; &lt;%= link %&gt; &lt;/a&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><p>渲染结果如下 :</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;a href=<span class="string">&quot;/&quot;</span>&gt; Home &lt;/a&gt;</span><br><span class="line">&lt;a href=<span class="string">&quot;/gallery/&quot;</span>&gt; Gallery &lt;/a&gt;</span><br><span class="line">&lt;a href=<span class="string">&quot;/archives/&quot;</span>&gt; Archives &lt;/a&gt;</span><br></pre></td></tr></table></figure><h4 id="2-4-7-服务器"><a href="#2-4-7-服务器" class="headerlink" title="2.4.7 服务器"></a>2.4.7 服务器</h4><p>直接参考<a href="https://hexo.io/zh-cn/docs/server">官方文档</a></p><h2 id="三-、自定义配置"><a href="#三-、自定义配置" class="headerlink" title="三 、自定义配置"></a>三 、自定义配置</h2><blockquote><p>参考本帖开头给的hexo文档和molody文档。最后一些修改我没写（第三方服务等）。</p><h3 id="3-1-配置"><a href="#3-1-配置" class="headerlink" title="3.1 配置"></a>3.1 配置</h3><p>在 网站配置文件_config.yml 中可以修改大部分的配置：</p><h4 id="3-1-1-网站、网址参数"><a href="#3-1-1-网站、网址参数" class="headerlink" title="3.1.1 网站、网址参数"></a>3.1.1 网站、网址参数</h4><div class="table-container"><table><thead><tr><th>网站参数</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td>网站标题</td></tr><tr><td>subtitle</td><td>网站副标题</td></tr><tr><td>description</td><td>网站描述</td></tr><tr><td>keywords</td><td>网站的关键词。支持多个关键词。</td></tr><tr><td>author</td><td>文章作者。也可以通过修改front matter修改单篇文章的作者</td></tr><tr><td>language</td><td>简体中文为 zh-Hans或 zh-CN，请参考你的主题的文档自行设置，也支持多国语言</td></tr><tr><td>timezone</td><td>网站时区。Hexo 默认使用您电脑的时区。其它请参考 时区列表 进行设置</td></tr></tbody></table></div></blockquote><ul><li>description主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，会显示在网站源代码中而不会显示在网页中。通常建议在其中包含您网站的关键词。</li><li>时区一般不需要改。如果你自动生成网页不在本地，而是海外，会造成时区差异，则可能需要设置。</li></ul><div class="table-container"><table><thead><tr><th>网址参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>url</td><td>网址, 必须以 http:// 或 https:// 开头</td><td></td></tr><tr><td>root</td><td>网站根目录</td><td>url’s pathname</td></tr><tr><td>permalink</td><td>文章的 永久链接 格式</td><td>:year/:month/:day/:title/</td></tr><tr><td>permalink_defaults</td><td>永久链接中各部分的默认值</td><td></td></tr><tr><td>pretty_urls</td><td>改写 permalink 的值来美化 URL</td><td></td></tr><tr><td>pretty_urls.trailing_index</td><td>设置为 false 时去除永久链接中尾部的 index.html</td><td>true</td></tr><tr><td>pretty_urls.trailing_html</td><td>设置为 false 时去除永久链接中尾部的 .html (对尾部的 index.html无效)</td><td>true</td></tr></tbody></table></div><ul><li>文章链接默认是年月日标题的格式。比如我的hello world文章链接是<code>https://zhxnlp.github.io/2021/12/31/hello-world/</code></li><li>如果您的<code>网站存放在子目录</code>中，例如 <a href="http://example.com/blog，则请将您的">http://example.com/blog，则请将您的</a> url 设为 <a href="http://example.com/blog">http://example.com/blog</a> 并把 root 设为 /blog/。(貌似是私人仓库设置网站在子目录，参见后面文档）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 比如，一个页面的永久链接是 http://example.com/foo/bar/index.html</span></span><br><span class="line">pretty_urls:</span><br><span class="line">  trailing_index: false</span><br><span class="line"><span class="comment"># 此时页面的永久链接会变为 http://example.com/foo/bar/</span></span><br></pre></td></tr></table></figure><h4 id="3-1-2-目录-amp-文章"><a href="#3-1-2-目录-amp-文章" class="headerlink" title="3.1.2 目录&amp;文章"></a>3.1.2 目录&amp;文章</h4><p>如果您刚刚开始接触 Hexo，通常没有必要修改这一部分的值。</p><div class="table-container"><table><thead><tr><th>目录参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>source_dir</td><td>资源文件夹，这个文件夹用来存放内容。</td><td>source</td></tr><tr><td>public_dir</td><td>公共文件夹，这个文件夹用于存放生成的站点文件。</td><td>public</td></tr><tr><td>tag_dir</td><td>标签文件夹</td><td>tags</td></tr><tr><td>archive_dir</td><td>归档文件夹</td><td>archives</td></tr><tr><td>category_dir</td><td>分类文件夹</td><td>categories</td></tr><tr><td>code_dir</td><td>Include code 文件夹，source_dir 下的子目录</td><td>downloads/code</td></tr><tr><td>i18n_dir</td><td>国际化（i18n）文件夹</td><td>:lang</td></tr><tr><td>skip_render</td><td>跳过hexo对指定文件的渲染。一般用于发布网站密钥等。匹配到的文件将会被不做改动地复制到 public 目录中。您可使用 glob 表达式来匹配路径。</td><td></td></tr></tbody></table></div><p>例如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">skip_render: <span class="string">&quot;mypage/**/*&quot;</span></span><br><span class="line"><span class="comment"># 将会直接将 `source/mypage/index.html` 和 `source/mypage/code.js` 不做改动地输出到 &#x27;public&#x27; 目录</span></span><br><span class="line"><span class="comment"># 你也可以用这种方法来跳过对指定文章文件的渲染</span></span><br><span class="line">skip_render: <span class="string">&quot;_posts/test-post.md&quot;</span></span><br><span class="line"><span class="comment"># 这将会忽略对 &#x27;test-post.md&#x27; 的渲染</span></span><br></pre></td></tr></table></figure></p><div class="table-container"><table><thead><tr><th>文章参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>new_post_name</td><td>新文章的文件名称</td><td>:title.md</td></tr><tr><td>default_layout</td><td>预设布局</td><td>post</td></tr><tr><td>auto_spacing</td><td>在中文和英文之间加入空格</td><td>false</td></tr><tr><td>titlecase</td><td>把标题转换为 title case</td><td>false</td></tr><tr><td>external_link</td><td>在新标签中打开链接</td><td>true</td></tr><tr><td>external_link.enable</td><td>在新标签中打开链接</td><td>true</td></tr><tr><td>external_link.field</td><td>对整个网站（site）生效或仅对文章（post）生效</td><td>site</td></tr><tr><td>external_link.exclude</td><td>需要排除的域名。主域名和子域名如 www 需分别配置</td><td>[]</td></tr><tr><td>filename_case</td><td>把文件名称转换为 (1) 小写或 (2) 大写</td><td>0</td></tr><tr><td>render_drafts</td><td>是否在网站上显示草稿</td><td>false</td></tr><tr><td>post_asset_folder</td><td>启动 <a href="https://hexo.io/zh-cn/docs/asset-folders">Asset 文件夹</a></td><td>false</td></tr><tr><td>relative_link</td><td>把链接改为与根目录的相对位置</td><td>false</td></tr><tr><td>future</td><td>显示未来的文章</td><td>true</td></tr><tr><td>highlight</td><td>代码高亮设置, 请参考 <a href="https://hexo.io/docs/syntax-highlight#Highlight-js">Highlight.js</a> 进行设置</td></tr><tr><td>prismjs</td><td>代码块的设置, 请参考 <a href="https://hexo.io/docs/syntax-highlight#PrismJS">PrismJS</a> 进行设置</td></tr></tbody></table></div><h4 id="3-1-3-分类-amp-标签-amp-日期-时间-amp-分页扩展"><a href="#3-1-3-分类-amp-标签-amp-日期-时间-amp-分页扩展" class="headerlink" title="3.1.3 分类 &amp; 标签&amp;日期 / 时间&amp;分页扩展"></a>3.1.3 分类 &amp; 标签&amp;日期 / 时间&amp;分页扩展</h4><div class="table-container"><table><thead><tr><th>分类标签参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>default_category</td><td>默认分类</td><td>uncategorized</td></tr><tr><td>category_map</td><td>分类别名</td><td></td></tr><tr><td>tag_map</td><td>标签别名</td><td></td></tr></tbody></table></div><p>Hexo 使用 Moment.js 来解析和显示时间</p><div class="table-container"><table><thead><tr><th>时间参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>date_format</td><td>日期格式</td><td>YYYY-MM-DD</td></tr><tr><td>time_format</td><td>时间格式</td><td>HH:mm:ss</td></tr><tr><td>updated_option</td><td>当 Front Matter 中没有指定 updated 时 updated 的取值</td><td>mtime</td></tr></tbody></table></div><p>分页和扩展参数：</p><div class="table-container"><table><thead><tr><th>分页参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td>per_page</td><td>每页显示的文章量 (0 = 关闭分页功能)</td><td>10</td></tr><tr><td>pagination_dir</td><td>分页目录位置</td><td>page</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>扩展参数</th><th>描述</th></tr></thead><tbody><tr><td>theme</td><td>当前3主题名称。值为false时禁用主题</td></tr><tr><td>theme_config</td><td>主题的配置文件。在这里放置的配置会覆盖主题目录下的 _config.yml 中的配置</td></tr><tr><td>deploy</td><td>部署部分的设置，比如git、heroke。</td></tr><tr><td>meta_generator</td><td>Meta generator标签。 值为 false 时 Hexo 不会在头部插入该标签</td></tr></tbody></table></div><h4 id="3-1-4-include-和-exclude文件"><a href="#3-1-4-include-和-exclude文件" class="headerlink" title="3.1.4 include 和 exclude文件"></a>3.1.4 include 和 exclude文件</h4><ul><li>Hexo 默认会不包括 source/ 下的文件和文件夹（包括名称以下划线和 . 开头的文件和文件夹，Hexo 的 _posts 和 _data 等目录除外）</li><li><p>在 Hexo 配置文件中，通过设置 include/exclude 可以让 Hexo 进行处理或忽略某些目录和文件夹。你可以使用 <a href="https://github.com/isaacs/minimatch">glob 表达式</a> 对目录和文件进行匹配。</p></li><li><p>include 和 exclude 选项只会应用到 source/ ，而 ignore 选项会应用到所有文件夹。</p></li></ul><div class="table-container"><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>include</td><td>如果想发布隐藏文件，需要设置在include里</td></tr><tr><td>exclude</td><td>剔除 source/ 下的这些文件和目录不再发布到网上。</td></tr><tr><td>ignore</td><td>Hexo 会忽略整个 Hexo 项目下的这些文件夹或文件</td></tr></tbody></table></div><p>举例：<br><code>.nojekyll文件</code>告诉github不要使用jekyll引擎渲染，因为会有一些其它操作，比如忽略Vander目录，会造成一些网站的奇怪错误。所以建议github发布博客，把.nojekyll文件放在根目录下，<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 处理或不处理目录或文件</span></span><br><span class="line">include:</span><br><span class="line">  - <span class="string">&quot;.nojekyll&quot;</span> </span><br><span class="line">  - <span class="string">&quot;css/_typing.css&quot;</span><span class="comment"># 处理 &#x27;source/css/_typing.css&#x27;</span></span><br><span class="line">  - <span class="string">&quot;_css/*&quot;</span> <span class="comment"># 处理 &#x27;source/_css/&#x27; 中的任何文件，但不包括子目录及其其中的文件。 </span></span><br><span class="line">  - <span class="string">&quot;_css/**/*&quot;</span> <span class="comment"># 处理 &#x27;source/_css/&#x27; 中的任何文件和子目录下的任何文件</span></span><br><span class="line"></span><br><span class="line">exclude:<span class="comment">#剔除文件，使其不包括在网站发布文件中</span></span><br><span class="line">  - <span class="string">&quot;js/test.js&quot;</span><span class="comment"># 不处理 &#x27;source/js/test.js&#x27;  </span></span><br><span class="line">  - <span class="string">&quot;js/*&quot;</span><span class="comment"># 不处理 &#x27;source/js/&#x27; 中的文件、但包括子目录下的所有目录和文件</span></span><br><span class="line">    </span><br><span class="line">ignore: </span><br><span class="line">  - <span class="string">&quot;**/foo&quot;</span> <span class="comment"># 忽略任何一个名叫 &#x27;foo&#x27; 的文件夹 </span></span><br><span class="line">  - <span class="string">&quot;**/themes/*/foo&quot;</span> <span class="comment"># 只忽略 &#x27;themes/&#x27; 下的 &#x27;foo&#x27; 文件夹 </span></span><br><span class="line">  - <span class="string">&quot;**/themes/**/foo&quot;</span> <span class="comment"># 对 &#x27;themes/&#x27; 目录下的每个文件夹中忽略名叫 &#x27;foo&#x27; 的子文件夹</span></span><br></pre></td></tr></table></figure></p><p>具体示例<a href="https://hexo.io/zh-cn/docs/configuration">参考文档</a></p><h4 id="3-1-5代替配置文件"><a href="#3-1-5代替配置文件" class="headerlink" title="3.1.5代替配置文件"></a>3.1.5代替配置文件</h4><p>这个的好处是：</p><ol><li>测试配置文件时，不需要来回更改主配置文件，只需要把主配置文件复制一份作为测试版就行。</li><li>同一根目录下，可以运行多个网站<br>使用 —config 参数来指定自定义配置文件的路径，例如：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 &#x27;custom.yml&#x27; 和 &#x27;custom2.json&#x27;，优先使用 &#x27;_config.yml&#x27;，然后是 &#x27;_config.yml&#x27;</span></span><br><span class="line">$ hexo generate --config custom.yml,custom2.json,_config.yml</span><br></pre></td></tr></table></figure>当你指定了多个配置文件以后，Hexo 会按顺序将这部分配置文件合并成一个 _multiconfig.yml。如果遇到重复的配置，靠后的配置文件覆盖靠前的配置文件。这个原则适用于任意数量、任意深度的 YAML 和 JSON 文件。</li></ol><p>例如，使用 —options 指定了两个自定义配置文件：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ hexo generate --config custom.yml,custom2.json</span><br></pre></td></tr></table></figure><p>如果 custom.yml 中指定了 foo: bar，在 custom2.json 中指定了 “foo”: “dinosaur”，那么在 _multiconfig.yml 中你会得到 foo: dinosaur。</p><hr><p><strong>代替主题配置文件</strong><br>Hexo 5.0.0 起提供独立的 _config.[theme].yml 文件。默认主题配置文件会覆盖主网站配置文件。主题配置文件应放置于站点根目录下，并配置站点 _config.yml 文件中的 theme。例如配置melody主题：</p><ol><li>安装主题：<code>npm install hexo-theme-melody</code></li><li>根目录创建<code>_config.melody.yml</code>文件</li><li>将<code>./node_modules/hexo-theme-melody/_config.yml</code>的内容复制到刚才的<code>_config.melody.yml</code>里面。</li></ol><p>强烈建议你将所有的主题配置集中在一处。<br>Hexo 在合并主题配置时，Hexo 配置文件中的 theme_config 的优先级最高，其次是 _config.[theme].yml 文件，最后是位于主题目录下的 _config.yml 文件。</p><h3 id="3-2-Permalinks永久链接"><a href="#3-2-Permalinks永久链接" class="headerlink" title="3.2 Permalinks永久链接"></a>3.2 Permalinks永久链接</h3><p>可以在 _config.yml 配置中调整网站的永久链接或者在每篇文章的 Front-matter 中指定。</p><div class="table-container"><table><thead><tr><th>变量</th><th>描述</th></tr></thead><tbody><tr><td>:year</td><td>文章的发表年份（4 位数）</td></tr><tr><td>:month</td><td>文章的发表月份（2 位数）</td></tr><tr><td>:i_month</td><td>文章的发表月份（去掉开头的零）</td></tr><tr><td>:day</td><td>文章的发表日期 (2 位数)</td></tr><tr><td>:i_day</td><td>文章的发表日期（去掉开头的零）</td></tr><tr><td>:hour</td><td>文章发表时的小时 (2 位数)</td></tr><tr><td>:minute</td><td>文章发表时的分钟 (2 位数)</td></tr><tr><td>:second</td><td>文章发表时的秒钟 (2 位数)</td></tr><tr><td>:title</td><td>文件名称 (relative to “source/_posts/“ folder)</td></tr><tr><td>:name</td><td>文件名称</td></tr><tr><td>:post_title</td><td>文章标题</td></tr><tr><td>:id</td><td>文章 ID (not persistent across cache reset)，每次发布id可能会不一样</td></tr><tr><td>:category</td><td>分类。如果文章没有分类，则是 default_category 配置信息。</td></tr><tr><td>:hash</td><td>SHA1 hash of filename (same as :title) and date (12-hexadecimal)</td></tr></tbody></table></div><p>使用第三方插件，例如disqus管理评论，会在每个网页下面生成对话窗口。如果id不唯一，多个文章评论会揉在一起。如果要生成唯一id，可以使用下面这个插件。<br><img src="https://img-blog.csdnimg.cn/28f3856e48ca437ab6064e06ede56c4d.png" alt=""></p><p>可在 permalink_defaults 参数下调整永久链接中各变量的默认值：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">permalink_defaults:</span><br><span class="line">  lang: en</span><br></pre></td></tr></table></figure><br>例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source/_posts/hello-world.md</span><br><span class="line">title: Hello World</span><br><span class="line">date: <span class="number">2013</span>-07-<span class="number">14</span> <span class="number">17</span>:01:<span class="number">34</span></span><br><span class="line">categories:</span><br><span class="line">- foo</span><br><span class="line">- bar</span><br></pre></td></tr></table></figure><h3 id="3-3-主题"><a href="#3-3-主题" class="headerlink" title="3.3 主题"></a>3.3 主题</h3><p>只要在 themes 文件夹内，新增一个任意名称的文件夹，并修改 _config.yml 内的 theme 设定，即可切换主题。一个主题可能会有以下的结构：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── languages</span><br><span class="line">├── layout</span><br><span class="line">├── scripts</span><br><span class="line">└── source</span><br></pre></td></tr></table></figure><ul><li>_config.yml：主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启 Hexo Server。</li><li>languages：语言文件夹。请参见 <a href="https://hexo.io/zh-cn/docs/internationalization">国际化 (i18n)</a>。</li><li>layout：布局文件夹。用于存放主题的模板文件，决定了网站内容的呈现方式，Hexo 内建 <a href="https://github.com/node-swig/swig-templates">Swig</a> 模板引擎，您可以另外安装插件来获得 <a href="https://github.com/hexojs/hexo-renderer-ejs">EJS</a>、<a href="https://github.com/hexojs/hexo-renderer-haml">Haml</a>、<a href="https://github.com/hexojs/hexo-renderer-jade">Jade</a> 或 <a href="https://github.com/maxknee/hexo-render-pug">Pug</a> 支持。Hexo 根据模板文件的扩展名来决定所使用的模板引擎，参考 <a href="https://hexo.io/zh-cn/docs/templates">模板</a> 以获得更多信息。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">archive.pug   - 使用 Pug</span><br><span class="line">layout.swig   - 使用 Swig</span><br></pre></td></tr></table></figure><ul><li>scripts：脚本文件夹。在启动时，Hexo 会载入此文件夹内的 JavaScript 文件，请参见 <a href="https://hexo.io/zh-cn/docs/plugins">插件</a> 以获得更多信息。</li><li>source：资源文件夹，除了模板以外的 Asset，例如 CSS、JavaScript 文件等，都应该放在这个文件夹中。文件或文件夹开头名称为 _（下划线线）或隐藏的文件会被忽略。<br>如果文件可以被渲染的话，会经过解析然后储存到 public 文件夹，否则会直接拷贝到 public 文件夹。</li></ul><p>分享发布主题，请参考文档。</p><h3 id="3-4-模版"><a href="#3-4-模版" class="headerlink" title="3.4 模版"></a>3.4 模版</h3><h4 id="3-4-1-模板名称"><a href="#3-4-1-模板名称" class="headerlink" title="3.4.1 模板名称"></a>3.4.1 模板名称</h4><p>模板决定了网站内容的呈现方式，每个主题至少都应包含一个 index 模板，以下是各页面相对应的模板名称：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">模板         用途      回退</span><br><span class="line">index     首页</span><br><span class="line">post     文章     index</span><br><span class="line">page     分页     index</span><br><span class="line">archive     归档     index</span><br><span class="line">category 分类归档 archive</span><br><span class="line">tag         标签归档 archive</span><br></pre></td></tr></table></figure><h4 id="3-4-2-布局（Layout）"><a href="#3-4-2-布局（Layout）" class="headerlink" title="3.4.2 布局（Layout）"></a>3.4.2 布局（Layout）</h4><p>每个模板都默认使用 layout 布局，您可在 front-matter 指定其他布局，或是设为 false 来关闭布局功能，您甚至可在布局中再使用其他布局来建立嵌套布局。<br>其它内容参考<a href="https://hexo.io/zh-cn/docs/templates">官方文档</a>。</p><h3 id="3-5-变量和函数、代码高亮"><a href="#3-5-变量和函数、代码高亮" class="headerlink" title="3.5 变量和函数、代码高亮"></a>3.5 变量和函数、代码高亮</h3><p>变量包括全局变量和网站变量、page变量等等<br>辅助函数包括取得路径、gravatar根据邮箱地址返回 Gravatar 头像 URL等等。这部分内容参考官网文档。</p><h3 id="3-6-安装搜索"><a href="#3-6-安装搜索" class="headerlink" title="3.6 安装搜索"></a>3.6 安装搜索</h3><p>参考帖子：<a href="https://blog.csdn.net/weixin_33826268/article/details/88022470?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164099797316780264072146%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164099797316780264072146&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-88022470.pc_search_insert_es_download_v2&amp;utm_term=hexo-algolia&amp;spm=1018.2226.3001.4187">Hexo+Next集成Algolia搜索</a>、<a href="https://blog.csdn.net/qq_35479468/article/details/107335663?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164099797316780264072146%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164099797316780264072146&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-5-107335663.pc_search_insert_es_download_v2&amp;utm_term=hexo-algolia&amp;spm=1018.2226.3001.4187">《为Hexo增加algolia搜索功能》</a><br> <a href="https://www.algolia.com/users/sign_in用github注册登录后，创建index。然后进入API">https://www.algolia.com/users/sign_in用github注册登录后，创建index。然后进入API</a> Keys。点击ALL API Keys-NEW API Keys新建API Keys。<br><img src="https://img-blog.csdnimg.cn/1e019db7a74547a58efa5f41253322a4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>创建完后保存好API Keys。<br><img src="https://img-blog.csdnimg.cn/3811cf7a5af34f12b76c03dbce2af133.png" alt="在这里插入图片描述"><br>在Hexo根目录<code>npm install hexo-algolia --save</code>安装，然后在_config.yml中加入algolia的配置，注意改成前面API Keys页面相应配置。这里注意apikey填写刚才你创建的那个有权限的，其余的在your api keys里面可以找到。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">algolia:</span><br><span class="line">  applicationID: <span class="string">&#x27;your applicationID&#x27;</span></span><br><span class="line">  apiKey: <span class="string">&#x27;your apiKey&#x27;</span><span class="comment">#我写的是刚才创建的额那个key</span></span><br><span class="line">  adminApiKey: <span class="string">&#x27;your adminApiKey&#x27;</span></span><br><span class="line">  indexName: <span class="string">&#x27;your indexName&#x27;</span><span class="comment">#github-io</span></span><br><span class="line">  chunkSize: <span class="number">5000</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/c78753e5ff3241429d69ad8d30160838.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>设置环境变量：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">export HEXO_ALGOLIA_INDEXING_KEY=your apiKey<span class="comment">#刚才创建那个</span></span><br><span class="line">hexo algolia</span><br></pre></td></tr></table></figure><p>到主题目录下的_config.yml，修改为开启：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">algolia_search:</span><br><span class="line">  enable: true</span><br><span class="line">  hits:</span><br><span class="line">    per_page: <span class="number">10</span> </span><br><span class="line">  labels:</span><br><span class="line">    input_placeholder: Search <span class="keyword">for</span> Posts</span><br><span class="line">    hits_empty: <span class="string">&quot;我们没有找到任何搜索结果: $&#123;query&#125;&quot;</span> </span><br><span class="line">    hits_stats: <span class="string">&quot;找到$&#123;hits&#125;条结果，用时$&#123;time&#125; ms&quot;</span></span><br></pre></td></tr></table></figure><p>然后运行<code>hexo g -d</code>就行。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主要参考文档：&lt;a href=&quot;https://hexo.io/zh-cn/docs/&quot;&gt;hexo文档&lt;/a&gt;、&lt;a href=&quot;https://molunerfinn.com/hexo-theme-melody-doc/#features&quot;&gt;melody主题文档&lt;/a&gt;、&lt;a href=&quot;https://www.youtube.com/watch?v=xvIRGmKWpFM&quot;&gt;youtube教学视频&lt;/a&gt;、&lt;a href=&quot;https://space.bilibili.com/362224537/channel/series&quot;&gt;bilibili的hexo教学视频&lt;/a&gt;、&lt;a href=&quot;https://hexo.io/themes/&quot;&gt;hexo主题网站&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/wapchief/article/details/54602515?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164088400916780366527950%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;amp;request_id=164088400916780366527950&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-10-54602515.pc_search_insert_es_download_v2&amp;amp;utm_term=github%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA&amp;amp;spm=1018.2226.3001.4187&quot;&gt;《使用hexo+github免费搭建个人博客网站超详细教程》&lt;/a&gt;、hexo文档：&lt;a href=&quot;https://hexo.io/zh-cn/docs/github-pages&quot;&gt;将 Hexo 部署到 GitHub Pages&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;bilibili搭建博客视频资料&lt;a href=&quot;https://www.bilibili.com/video/BV1mU4y1j72n?p=5&quot;&gt;《【2021最新版】保姆级Hexo+github搭建个人博客》&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yarn.bootcss.com/&quot;&gt;yarn1中文文档&lt;/a&gt;、&lt;a href=&quot;https://www.yarnpkg.cn/&quot;&gt;yarn2文档&lt;/a&gt;、&lt;a href=&quot;https://www.npmjs.cn/&quot;&gt;npm中文文档&lt;/a&gt;、&lt;a href=&quot;http://bit.ly/2QhJTNaYaml&quot;&gt;Yaml官网&lt;/a&gt;、&lt;a href=&quot;http://bit.ly/2zxeDCC&quot;&gt;Yaml教程&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hexo.io/zh-cn/docs/commands&quot;&gt;hexo常用命令&lt;/a&gt;、&lt;a href=&quot;https://github.com/hexojs/awesome-hexo&quot;&gt;Hexo Awesome文档&lt;/a&gt;、&lt;a href=&quot;https://github.com/hexojs/hexo&quot;&gt;Hexo Github&lt;/a&gt;</summary>
    
    
    
    <category term="软件应用" scheme="https://zhxnlp.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="前端开发" scheme="https://zhxnlp.github.io/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
    <category term="github" scheme="https://zhxnlp.github.io/tags/github/"/>
    
    <category term="hexo" scheme="https://zhxnlp.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记10：统计学习方法_——HMM和CRF</title>
    <link href="https://zhxnlp.github.io/2021/12/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_%E2%80%94%E2%80%94HMM%E5%92%8CCRF/"/>
    <id>https://zhxnlp.github.io/2021/12/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_%E2%80%94%E2%80%94HMM%E5%92%8CCRF/</id>
    <published>2021-12-25T15:32:54.000Z</published>
    <updated>2022-01-02T20:45:01.029Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考文章<a href="https://www.zhihu.com/question/35866596/answer/236886066?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_content=group3_Answer&amp;utm_campaign=shareopn">《如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？》</a><br><a href="https://zhuanlan.zhihu.com/p/178731739?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《条件随机场CRF之从公式到代码》</a><br><a href="https://zhuanlan.zhihu.com/p/148813079?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《CRF条件随机场的原理、例子、公式推导和应用》</a><br><span id="more"></span></p><h2 id="一、概率图模型"><a href="#一、概率图模型" class="headerlink" title="一、概率图模型"></a>一、概率图模型</h2><h3 id="1-1-概览"><a href="#1-1-概览" class="headerlink" title="1.1 概览"></a>1.1 概览</h3><p>在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构：</p></blockquote><p><img src="https://img-blog.csdnimg.cn/c8b569f3f6d04c488431dce83ed52f8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>在概率图模型中，数据(样本)由图$G=(V,E)$建模表示：</p><ul><li>V：节点v的集合。v∈V表示随机变量$Y_v$。</li><li>E：边e的集合。e∈E表示随机变量之间的概率依赖关系。</li><li>P(Y):由图表示的联合概率分布</li></ul><p>有向图和无向图区别在于如何求概率分布P(Y)。</p><h3 id="1-2-有向图"><a href="#1-2-有向图" class="headerlink" title="1.2 有向图"></a>1.2 有向图</h3><p>有向图模型，这么求联合概率： </p><script type="math/tex; mode=display">P(x_{1}...x_{n})=\prod_{i=0}P(x_{i}|\pi (x_{i}))</script><p>对于下图求概率有：<br><img src="https://img-blog.csdnimg.cn/7256a9d4eb6e4d7ea9647c6d47746801.png" alt="在这里插入图片描述"></p><script type="math/tex; mode=display">P(x_{1}...x_{n})=P(x_{1})\cdot P(x_{2}| x_{1})\cdot P(x_{3}| x_{2})\cdot P(x_{4})|P(x_{2})P(x_{5}|x_{3},x_{4})</script><h3 id="1-3-无向图"><a href="#1-3-无向图" class="headerlink" title="1.3 无向图"></a>1.3 无向图</h3><p>基本概念：</p><ol><li>团：节点子集，子集中任何两个节点均有边相连</li><li>最大团：不能再加入节点使其更大的团</li><li>因子分解：联合概率分布P(Y)表示为<code>所有最大团上随机变量函数的乘积的形式</code>为因子分解。</li></ol><p>所以有：联合概率分布为最大团势函数的乘积。</p><script type="math/tex; mode=display">P(Y)=\frac{1}{Z}\prod_{C}\psi _{C}(Y_{C})=\frac{1}{Z}\prod_{C}exp^{-E(Y_{C})}</script><ul><li>C：无向图的最大团</li><li>$Y_{C}$:最大团C上的节点(随机变量）</li><li>Z：规范化因子。$Z=\sum<em>{Y}\prod</em>{C}\psi <em>{C}(Y</em>{C})$,使得输出P(Y)具有概率意义。</li><li>$\psi <em>{C}(Y</em>{C})$:严格正的势函数，通常为指数函数$\psi <em>{C}(Y</em>{C})=exp^{-E(Y_{C})}$。</li></ul><p>马尔科夫性，保证概率图为概率无向图：</p><ul><li>成对马尔科夫性：u和v没有边相连，O为其它所有节点，$Y_u和Y_v$互相独立。</li><li>局部马尔科夫性：对于任意节点v、相连节点集合W和无边相连集合O，给定$Y_W$情况下，$Y_v和Y_O$互相独立：</li><li>全局马尔科夫性：节点A和B被节点集合C分割，$Y_A和Y_B$互相独立：</li></ul><p>总之就是没有边相连的节点概率互相独立。</p><p>对于一个无向图，举例如下：</p><p><img src="https://img-blog.csdnimg.cn/f77a15a1b61a4829868472dce880feb5.png" alt="在这里插入图片描述"></p><script type="math/tex; mode=display">P(Y)=\frac{1}{Z}\prod_{C}\psi( _{X_{1},X_{3},X_{4}})\psi( _{X_{2},X_{3},X_{4}})</script><h3 id="1-4-生成式模型和判别式模型"><a href="#1-4-生成式模型和判别式模型" class="headerlink" title="1.4 生成式模型和判别式模型"></a>1.4 生成式模型和判别式模型</h3><h3 id="1-4-1生成式模型和判别式模型区别"><a href="#1-4-1生成式模型和判别式模型区别" class="headerlink" title="1.4.1生成式模型和判别式模型区别"></a>1.4.1生成式模型和判别式模型区别</h3><p>有监督学习中，训练数据包括输入X和标签Y。所以模型求的是X和Y的概率分布。根据概率论的知识可以知道，对应的概率分布（以概率密度函数指代概率分布）有两种：</p><ul><li>联合概率分布：$P_{\theta }(X,Y)$，表示数据和标签同时出现的概率，对应于生成式模型。</li><li>条件概率分布：P_{\theta }(Y|X)，表示给定数据条件下，对应标签的概率，对应于判别式模型。</li></ul><p>进一步理解：</p><ul><li>生成式模型：除了能够根据输入数据 X 来预测对应的标签 Y ,还能根据训练得到的模型产生服从训练数据集分布的数据( X ，Y），相当于生成一组新的数据，所以称之为生成式模型。</li><li>判别式模型：仅仅根据X由条件概率$P_{\theta }(Y|X)$来预测标签Y。牺牲了生成数据的能力，但是比生成式模型的预测准确率高。<h4 id="1-4-2-为啥判别式模型预测效果更好"><a href="#1-4-2-为啥判别式模型预测效果更好" class="headerlink" title="1.4.2 为啥判别式模型预测效果更好"></a>1.4.2 为啥判别式模型预测效果更好</h4>原因如下：由全概率公式和信息熵公式可以得到：<script type="math/tex; mode=display">P(X,Y)=\int P(Y|X)P(X)dX</script>即计算全概率公式$P(X,Y)$时引入了输入数据的概率分布$P(X)$，而这个并不是我们关心的。我们只关心给定X情况下Y的分布，这就相对削弱了模型的预测能力。<br>另外从信息熵的角度进行定量分析。</li></ul><ol><li>X的信息熵定义为：<script type="math/tex; mode=display">H(X)=-\int P(X)logP(X)dX</script></li><li>两个离散随机变量 X  和 Y  的联合熵 (Joint Entropy) 表示两事件同时发生系统的不确定度:<script type="math/tex; mode=display">H(X,Y)=-\int P(X,Y)logP(X,Y)dXdY</script></li><li>条件熵 (Conditional Entropy) H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性：<script type="math/tex; mode=display">H(Y|X)=-\int P(Y|X)logP(Y|X)dX</script></li></ol><p>可以推导出来$H(Y|X)=H(X,Y)-H(X)$.一般H(X)&gt;0（所有离散分布和很多连续分布满足这个条件），可以知道条件分布的信息熵小于联合分布，即判别模型比生成式模型含有更多的信息，所以同条件下比生成式模型效果更好。&lt;/font &gt;</p><h2 id="二、隐式马尔科夫模型HMM"><a href="#二、隐式马尔科夫模型HMM" class="headerlink" title="二、隐式马尔科夫模型HMM"></a>二、隐式马尔科夫模型HMM</h2><h3 id="2-1-HMM定义"><a href="#2-1-HMM定义" class="headerlink" title="2.1 HMM定义"></a>2.1 HMM定义</h3><ul><li>隐马尔可夫模型是关于时序的概率模型</li><li>描述由一个<code>隐藏的马尔可夫链</code>随机生成<code>不可观测的状态随机序列</code>(state sequence)，再由各个状态生成一个观测而产生<code>观测随机序列</code>(observation sequence )的过程,序列的每一个位置又可以看作是一个时刻。</li></ul><p>设Q是所有可能状态的集合，V是所有可能观测的集合：</p><script type="math/tex; mode=display">Q=(q_{1},q_{2},...q_{N})和V=(v_{1},v_{2},...v_{M})</script><p>对于长度为T的状态序列I和观测序列O有：</p><script type="math/tex; mode=display">i=(i_{1},i_{2},...i_{T})和O=(o_{1},o_{2},...o_{T})</script><p>其中:</p><ul><li>状态转移概率矩阵$A=(a<em>{ij})</em>{N\times N}\qquad i,j\epsilon (1,N)$。$a_{ij}$表示t时刻状态$q_i$转移到t+1时刻$q_j$的概率</li><li>观测概率（发射概率）矩阵$B=[b<em>{j}(k)]</em>{N\times M} \quad j\epsilon (1,N)k\epsilon (1,M)$。$b_{j}(k)$表示t时刻状态$q_i$生成观测$v_k$的概率。&lt;/font&gt;</li><li>初始状态概率向量$\pi =(\pi <em>{i})=P(i</em>{1}=q_{i})\quad i\epsilon (1,N)$。表示初始时刻处于状态$q_i$的概率。<br><img src="https://img-blog.csdnimg.cn/82df5f741379490f83e123408e52d70d.png" alt="在这里插入图片描述"></li><li>隐状态节点$i<em>t$在A的指导下生成下一个隐状态节点$i</em>{t+1}$，并且$i_t$在B的指导下生成观测节点$o_t$ , 并且我只能观测到序列O。</li><li>根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模:<br><img src="https://img-blog.csdnimg.cn/a468a47db0c34f2db5239e2c4bbf0d91.png" alt="在这里插入图片描述"><blockquote><p>只是我们都去这么来表示HMM是个生成式模型,实际不这么计算。</p><h3 id="2-2-HMM三要素和两个基本假设"><a href="#2-2-HMM三要素和两个基本假设" class="headerlink" title="2.2 HMM三要素和两个基本假设"></a>2.2 HMM三要素和两个基本假设</h3></blockquote></li></ul><ol><li>HMM由<code>初始状态概率向量π、状态转移概率矩阵A、 观测概率矩阵B</code>三元素构成。<br>所以HMM模型$\lambda$可以写成：$\lambda =(A,B,\pi)$。<code>三者共同决定了隐藏的马尔可夫链生成不可观测的状态序列</code>。而状态序列和矩阵B综合产生观测序列。</li><li>HMM模型基本假设<ul><li>齐次马尔科夫性假设：隐马尔可夫链任意时刻t的状态只依赖前一时刻t-1的状态，即$P(i<em>{t}|i</em>{i-1})$。&lt;/font&gt;</li><li>观测独立性假设：任意时刻的观测只依赖当前时刻的状态，即$P(o<em>{t}|i</em>{i})$。&lt;/font&gt;</li></ul></li></ol><h3 id="2-3-HMM三个基本问题"><a href="#2-3-HMM三个基本问题" class="headerlink" title="2.3 HMM三个基本问题"></a>2.3 HMM三个基本问题</h3><ol><li>概率计算：给定模型$\lambda =(A,B,\pi)$和观测序列O，计算观测序列O出现的概率$P(O|\lambda)$。</li><li>学习问题：已知观测序列O，用最大似然估计的方法计算模型$\lambda =(A,B,\pi)$的参数。（该模型下观测序列O的概率最大）</li><li>预测（解码）问题：已知模型$\lambda =(A,B,\pi)$和观测序列，求最有可能的对应状态序列。</li></ol><ul><li>==HMM可以用于序列标记，观测序列O为tokens，状态序列I为其对应的标记。此时问题是给定序列O预测对应序列I。==</li><li>问题2对应模型建立过程，问题3 对应解码过程（crf.decode）<h3 id="2-4-HMM基本解法"><a href="#2-4-HMM基本解法" class="headerlink" title="2.4 HMM基本解法"></a>2.4 HMM基本解法</h3><h4 id="2-4-1-极大似然估计（根据I和O求λ）"><a href="#2-4-1-极大似然估计（根据I和O求λ）" class="headerlink" title="2.4.1 极大似然估计（根据I和O求λ）"></a>2.4.1 极大似然估计（根据I和O求λ）</h4>一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的，即根据观测序列O和状态序列I求模型$\lambda =(A,B,\pi)$的参数，是一个有监督学习。</li></ul><ol><li>根据状态序列求状态转移矩阵A：<script type="math/tex; mode=display">\mathbf{a_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}}</script></li><li>根据状态序列I和观测序列O求观测概率矩阵B：<script type="math/tex; mode=display">\mathbf{b_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}}</script></li><li>直接估计π<h4 id="2-4-2-前向后向算法（没有I）"><a href="#2-4-2-前向后向算法（没有I）" class="headerlink" title="2.4.2 前向后向算法（没有I）"></a>2.4.2 前向后向算法（没有I）</h4>只有观测序列O，没有状态序列I，无监督过程。计算就是一个就EM的过程。<br><img src="https://img-blog.csdnimg.cn/1d4febff1b6043bda7cc7d466111afe4.png" alt=""><h4 id="2-4-3-序列标注（解码）过程"><a href="#2-4-3-序列标注（解码）过程" class="headerlink" title="2.4.3 序列标注（解码）过程"></a>2.4.3 序列标注（解码）过程</h4></li></ol><ul><li>学习完了HMM的分布参数，也就确定了一个HMM模型。序列标注问题也就是“预测过程”(解码过程)。对应了序列建模问题3。</li><li>学习后已知了 联合概率P(I,O),现在要求出条件概率P(I|O)：<script type="math/tex; mode=display">I_{max}=\underset{all I}{argmax}\frac{P(I,O)}{P(O)}</script></li><li>用Viterbi算法解码，在给定的观测序列下找出一条概率最大的隐状态序列。&lt;/font&gt;</li><li>Viterbi计算有向无环图的一条最大路径，用DP思想减少重复的计算。如图：<br><img src="https://img-blog.csdnimg.cn/8c629c14f21748149969da86a07b677d.png" alt="在这里插入图片描述"><h2 id="三、最大熵马尔科夫MEMM模型"><a href="#三、最大熵马尔科夫MEMM模型" class="headerlink" title="三、最大熵马尔科夫MEMM模型"></a>三、最大熵马尔科夫MEMM模型</h2><h3 id="3-1-MEMM原理和区别"><a href="#3-1-MEMM原理和区别" class="headerlink" title="3.1 MEMM原理和区别"></a>3.1 MEMM原理和区别</h3>MEMM是判别式模型，直接对条件概率建模：<br><img src="https://img-blog.csdnimg.cn/9f9ee1ef4db54e94826a3f87bf85770e.png" alt="在这里插入图片描述"><br>MEMM需要注意：</li></ul><ol><li><p>HMM是$o<em>t$只依赖当前时刻的隐藏状态$i_t$，HEMM是当前时刻隐状态$i_t$依赖观测节点$o_t$和上一时刻状态$i</em>{t-1}$。&lt;/font&gt;</p></li><li><p><code>判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定</code>。HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等</p></li><li>需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。</li><li>序列标注解码时，一样用维特比算法求概率最大的隐状态序列。</li></ol><ul><li>HMM中，观测节点$o_t$只依赖当前时刻的隐藏状态$i_t$。</li><li>更多的实际场景下，观测序列是需要很多的特征来刻画的。比如说，我在做NER时，我的标注$i<em>t$不仅跟当前状态 $o_t$相关，而且还跟前后标注 $i</em>{j}(j≠i)$相关，比如字母大小写、词性等等。&lt;/font&gt;</li><li>MEMM模型:允许“定义特征”，直接学习条件概率&lt;/font&gt;，即：<br><img src="https://img-blog.csdnimg.cn/87be161d470f4af4ba6e3a6a69fbce8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>$Z(o,i{}’)$：归一化系数</li><li>$f(o,i)$：特征函数，需要自定义，其个数可任意制定</li><li>λ：特征函数系数，需要训练得到<br><img src="https://img-blog.csdnimg.cn/09d86caafb094b6f8a63771e7e77e4e5.png" alt="在这里插入图片描述"><h3 id="3-2-标注偏置"><a href="#3-2-标注偏置" class="headerlink" title="3.2 标注偏置"></a>3.2 标注偏置</h3><img src="https://img-blog.csdnimg.cn/a42fb328efa94ae4b8898d3169acd9db.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 过程细节：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">P(<span class="number">1</span>-&gt; <span class="number">1</span>-&gt; <span class="number">1</span>-&gt; <span class="number">1</span>)= <span class="number">0.4</span> x <span class="number">0.45</span> x <span class="number">0.5</span> = <span class="number">0.09</span> ，</span><br><span class="line">P(<span class="number">2</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>)= <span class="number">0.2</span> X <span class="number">0.3</span> X <span class="number">0.3</span> = <span class="number">0.018</span>，</span><br><span class="line">P(<span class="number">1</span>-&gt;<span class="number">2</span>-&gt;<span class="number">1</span>-&gt;<span class="number">2</span>)= <span class="number">0.6</span> X <span class="number">0.2</span> X <span class="number">0.5</span> = <span class="number">0.06</span>，</span><br><span class="line">P(<span class="number">1</span>-&gt;<span class="number">1</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>)= <span class="number">0.4</span> X <span class="number">0.55</span> X <span class="number">0.3</span> = <span class="number">0.066</span> </span><br></pre></td></tr></table></figure><p>但是得到的最优的状态转换路径是1-&gt;1-&gt;1-&gt;1，<br>为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态&lt;/font&gt;。原因如下：</p><p><img src="https://img-blog.csdnimg.cn/d8f64347c3df42a58170e7768592096d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="四、条件随机场CRF"><a href="#四、条件随机场CRF" class="headerlink" title="四、条件随机场CRF"></a>四、条件随机场CRF</h2><h3 id="4-1-CRF定义"><a href="#4-1-CRF定义" class="headerlink" title="4.1 CRF定义"></a>4.1 CRF定义</h3><ol><li>条件随机场：给定随机变量X条件下，输出随机变量Y的条件概率模型，其中Y构成无向图G=(V,E)表示的马尔科夫随机场。&lt;/font&gt;&lt;/font &gt;<br>对任意节点v，条件随机场满足：<script type="math/tex; mode=display">P(Y_{v}|X,Y_{w},w\neq v)=P(Y_{v}|X,Y_{w},w\sim v)</script>w≠ v表示v之外的所有结点，w~v表示与v有边相连的所有结点。即$P(Y_{v}$之和与v有边连接的结点有关。</li><li><code>线性链条件随机场，最大团是相邻两个结点的集合</code>。满足马尔科夫性(隐状态只和前后时刻状态有关）：<script type="math/tex; mode=display">P(Y_{i}|X,Y_{1},Y_{2}...Y_{n})=P(Y_{i}|X,Y_{i+1},Y_{i-1})</script></li><li><p>线性链CRF是判别模型，学习方法是利用训练数据的（正则化）极大似然估计得到条件概率模型P(Y|X)。可用于序列标注问题。此时条件概率P(Y|X)中：</p><ul><li>Y为输出变量，即标记序列（状态序列）</li><li>X为输入变量，即需要标注的状态序列。</li></ul></li><li><p>预测时，对于给定输入序列x，求出条件概率最大的输出序列y。<br><img src="https://img-blog.csdnimg.cn/8720f5b7ee7040339fd7e12f6cfee126.png" alt="在这里插入图片描述"></p><h3 id="4-2-线性链CRF的计算"><a href="#4-2-线性链CRF的计算" class="headerlink" title="4.2 线性链CRF的计算"></a>4.2 线性链CRF的计算</h3><p>概率无向图的联合概率分布可以在因子分解下表示为：<br><img src="https://img-blog.csdnimg.cn/984eb14e02934c4da21d22985b7d3255.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li>下标i表示我当前所在的节点（token）位置。&lt;/font&gt;</li><li>下标k表示我这是第几个特征函数&lt;/font&gt;，并且每个特征函数都附属一个权重$\lambda_{k}$ 。&lt;/font&gt;即每个团里面，我将为$token_i$构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。 </li><li>Z(O)是用来归一化的，形成概率值。</li><li>$P(I|O)$表示了在给定的一条观测序列 O的条件下，我用CRF所求出来的隐状态序列$I=(i<em>{1},i</em>{2},…i_{T})$的概率。而至于观测序列 O，它可以是一整个训练语料的所有的观测序列；也可以是在推断阶段的一句sample。比如序列标注进行预测，最终选的是最大概率的那条（by viterbi）。</li><li>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 我们将建模总公式展开：</li></ol><p><img src="https://img-blog.csdnimg.cn/bc28595886384e76ab860cf1eb9175d9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 转移特征针对的是前后token之间的限定。&lt;/font&gt;</p><p>为了简单起见，将转移特征和状态特征及其权值用统一符号表示。条件随机场简化公式如下：<br><img src="https://img-blog.csdnimg.cn/a407494d8818498dbde165b83a141b1a.png" alt="在这里插入图片描述"><br>再进一步理解的话，我们需要把特征函数部分抠出来：<br><img src="https://img-blog.csdnimg.cn/69ed4fa41ce64a27b87f8ecd717cf56d.png" alt="在这里插入图片描述"><br>我们为$token_i$打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值。<br>具体应用求解参考<a href="https://www.zhihu.com/question/35866596/answer/236886066?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_content=group3_Answer&amp;utm_campaign=shareopn">《如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？》</a>。</p><h3 id="4-3-从公式到代码的理解"><a href="#4-3-从公式到代码的理解" class="headerlink" title="4.3 从公式到代码的理解"></a>4.3 从公式到代码的理解</h3><p>实际计算时，采用概率的对数形式，即logP(Y)。使用最大似然估计来计算分布的参数，即我们的目标就是最大化ogP(Y)。<br><img src="https://img-blog.csdnimg.cn/12330b71b141418f9fb7cc3ec9039943.png" alt="在这里插入图片描述"><br>即$-logP(Y)=logZ(x)-score$。<br>对应到代码中，forward_score 就是$logZ(x)$，gold_score就是特征函数部分的score。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span>(<span class="params">self, sentence, tags</span>):</span></span><br><span class="line">    feats = self._get_lstm_features(sentence)</span><br><span class="line">    forward_score = self._forward_alg(feats)</span><br><span class="line">    gold_score = self._score_sentence(feats, tags)</span><br><span class="line">    <span class="keyword">return</span> forward_score - gold_score</span><br></pre></td></tr></table></figure></p><ul><li>因为模型建立的初衷就是要考虑到$i<em>{k-1}$对$i</em>{k}$的影响和X对观测序列的影响.所以我们将图分解成若干个$(i<em>{k-1},i</em>{k},X)$。</li><li>其中$i<em>{k}$表示观测变量的状态值，比如在BIO标注中状态取值范围是{B,I,O,START,STOP}，则k最大取5，$i</em>{k}$有5个状态值可取。<br><img src="https://img-blog.csdnimg.cn/6081457d466243bbbbf3e97d99cd0e9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>只关注其中的某一个团$C<em>i$,特征函数部分的gold_score表示给定序列X下，表现出的$(i</em>{k-1}，i_{k})$的费归一化概率，与两个东西有关：</li></ul><ol><li>给定序列X下出现$i<em>{k}$的概率，以$h(i</em>{k},X)$表示。这个概率使用lstm、cnn建模X对$i_{k})$映射就可以得到，对应结点上的状态特征。</li><li>给定序列X下由$i<em>{k-1}$转移到$i</em>{k}$的概率，由$g(i<em>{k-1},i</em>{k};X)$表示，对应边上的转移特征。在CRF中，观测变量只受临近节点的影响。</li><li>考虑到深度学习模型已经能比较充分捕捉各个$i<em>{k}$与X 的联系，所以假设 $i</em>{k-1}$ 转移到$i<em>{k}$的概率与X无关，所以有：$g(i</em>{k-1},i<em>{k};X)=g(i</em>{k-1},i_{k})$</li></ol><p>考虑以上几点，可以得到：</p><script type="math/tex; mode=display">gold-score=\sum_{c}\sum_{k}\lambda _{k}f_{k}(c,y,x)=\sum_{c}\sum_{k}(g(i_{k-1},i_{k})+h(i_{k},X))</script><p>剩下计算过程参考：<a href="https://zhuanlan.zhihu.com/p/178731739?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《条件随机场CRF之从公式到代码》</a><br><img src="https://img-blog.csdnimg.cn/b2612debc2c94e7fbdcaf918a69bc0a7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="五、-HMM-vs-MEMM-vs-CRF"><a href="#五、-HMM-vs-MEMM-vs-CRF" class="headerlink" title="五、 HMM vs. MEMM vs. CRF"></a>五、 HMM vs. MEMM vs. CRF</h2><h3 id="5-1-HMM-vs-MEMM"><a href="#5-1-HMM-vs-MEMM" class="headerlink" title="5.1 HMM vs MEMM"></a>5.1 HMM vs MEMM</h3><p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中<code>当前状态只与前一状态有关</code>。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。<code>MEMM解决了HMM输出独立性假设的问题</code>。因为HMM只限定在了观测与状态之间的依赖，而<code>MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖</code>。</p><h3 id="5-2-MEMM-vs-CRF"><a href="#5-2-MEMM-vs-CRF" class="headerlink" title="5.2 MEMM vs CRF"></a>5.2 MEMM vs CRF</h3><p>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，<code>MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布</code>，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。HMM、MEMM属于有向图，所以考虑了x与y的影响，但没将x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;参考文章&lt;a href=&quot;https://www.zhihu.com/question/35866596/answer/236886066?utm_source=wechat_session&amp;amp;utm_medium=social&amp;amp;utm_oi=1400823417357139968&amp;amp;utm_content=group3_Answer&amp;amp;utm_campaign=shareopn&quot;&gt;《如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？》&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/178731739?utm_source=wechat_session&amp;amp;utm_medium=social&amp;amp;utm_oi=1400823417357139968&amp;amp;utm_campaign=shareopn&quot;&gt;《条件随机场CRF之从公式到代码》&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/148813079?utm_source=wechat_session&amp;amp;utm_medium=social&amp;amp;utm_oi=1400823417357139968&amp;amp;utm_campaign=shareopn&quot;&gt;《CRF条件随机场的原理、例子、公式推导和应用》&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="CRF" scheme="https://zhxnlp.github.io/tags/CRF/"/>
    
    <category term="机器学习" scheme="https://zhxnlp.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="统计学习方法" scheme="https://zhxnlp.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>集成学习4： 整理总结</title>
    <link href="https://zhxnlp.github.io/2021/12/24/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/lightGBM/"/>
    <id>https://zhxnlp.github.io/2021/12/24/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/lightGBM/</id>
    <published>2021-12-23T21:07:27.000Z</published>
    <updated>2022-01-14T19:30:21.708Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="https://lightgbm.readthedocs.io/en/latest/Python-Intro.html" title="官方文档">LightGBM 官方文档</a></li><li>阿水知乎贴：<a href="https://zhuanlan.zhihu.com/p/266865429" title="《你应该知道的LightGBM各种操作》">《你应该知道的LightGBM各种操作》</a></li><li><a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html" title="Python API">Python API（包括Scikit-learn API）</a></li><li><a href="https://coggle.club/blog/30days-of-ml-202201" title="《Coggle 30 Days of ML（22年1&amp;2月）》">《Coggle 30 Days of ML（22年1&amp;2月）》</a></li></ul><p>学习内容：</p><p>LightGBM（Light Gradient Boosting Machine）是微软开源的一个实现 GBDT 算法的框架，支持高效率的并行训练。LightGBM 提出的主要原因是为了解决 GBDT 在海量数据遇到的问题。本次学习内容包括使用LightGBM完成各种操作，包括竞赛和数据挖掘中的模型训练、验证和调参过程。</p><p>打卡汇总：</p><div class="table-container"><table><thead><tr><th>任务名称</th><th>难度、分数</th><th>所需技能</th></tr></thead><tbody><tr><td>任务1模型训练与预测</td><td>低、1</td><td>LightGBM</td></tr><tr><td>任务2：模型保存与加载</td><td>低、1</td><td>LightGBM</td></tr><tr><td>任务3：分类、回归和排序任务</td><td>高、3</td><td>LightGBM</td></tr><tr><td>任务4：模型可视化</td><td>低、1</td><td>graphviz</td></tr><tr><td>任务5：模型调参（网格、随机、贝叶斯）</td><td>中、2</td><td>模型调参</td></tr><tr><td>任务6：模型微调与参数衰减</td><td>中、2</td><td>LightGBM</td></tr><tr><td>任务7：特征筛选方法</td><td>高、3</td><td>特征筛选方法</td></tr><tr><td>任务8：自定义损失函数</td><td>中、2</td><td>损失函数&amp;评价函数</td></tr><tr><td>任务9：模型部署与加速</td><td>高、3</td><td>Treelite</td></tr></tbody></table></div><h2 id="一、使用LGBMClassifier对iris进行训练"><a href="#一、使用LGBMClassifier对iris进行训练" class="headerlink" title="一、使用LGBMClassifier对iris进行训练"></a>一、使用LGBMClassifier对iris进行训练</h2><span id="more"></span><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br></pre></td></tr></table></figure><h3 id="1-1-使用lgb-LGBMClassifier"><a href="#1-1-使用lgb-LGBMClassifier" class="headerlink" title="1.1 使用lgb.LGBMClassifier"></a>1.1 使用lgb.LGBMClassifier</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">10</span>,</span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">2000</span>,<span class="comment">#提升迭代次数</span></span><br><span class="line">            objective=<span class="string">&#x27;multi:softmax&#x27;</span>,<span class="comment">#默认regression，用于设置损失函数</span></span><br><span class="line">            num_class=<span class="number">3</span> ,          </span><br><span class="line">            nthread=-<span class="number">1</span>,<span class="comment">#LightGBM 的线程数</span></span><br><span class="line">            min_child_weight=<span class="number">1</span>,</span><br><span class="line">            max_delta_step=<span class="number">0</span>,</span><br><span class="line">            subsample=<span class="number">0.85</span>,</span><br><span class="line">            colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">            reg_alpha=<span class="number">0</span>,<span class="comment">#L1正则化系数</span></span><br><span class="line">            reg_lambda=<span class="number">1</span>,<span class="comment">#L2正则化系数</span></span><br><span class="line">            scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line">gbm.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=-1 will be ignored. Current value: num_threads=-1accuarcy: 93.33%</code></pre><h4 id="1-1-2使用pickle进行保存模型，然后加载预测"><a href="#1-1-2使用pickle进行保存模型，然后加载预测" class="headerlink" title="1.1.2使用pickle进行保存模型，然后加载预测"></a>1.1.2使用pickle进行保存模型，然后加载预测</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">    pickle.dump(gbm, fout)</span><br><span class="line"><span class="comment"># load model with pickle to predict</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    pkl_bst = pickle.load(fin)</span><br><span class="line"><span class="comment"># can predict with any iteration when loaded in pickle way</span></span><br><span class="line">y_pred = pkl_bst.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 93.33%</code></pre><h4 id="1-1-3-使用txt和json保存模型并加载"><a href="#1-1-3-使用txt和json保存模型并加载" class="headerlink" title="1.1.3 使用txt和json保存模型并加载"></a>1.1.3 使用txt和json保存模型并加载</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># txt格式</span></span><br><span class="line">gbm.booster_.save_model(<span class="string">&quot;skmodel.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">clf_loads = lgb.Booster(model_file=<span class="string">&#x27;skmodel.txt&#x27;</span>)</span><br><span class="line">y_pred = clf_loads.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 93.33%</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#json格式</span></span><br><span class="line">gbm.booster_.save_model(<span class="string">&quot;skmodel.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">clf_loads = lgb.Booster(model_file=<span class="string">&#x27;skmodel.json&#x27;</span>)</span><br><span class="line">y_pred = clf_loads.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 93.33%</code></pre><h3 id="1-2使用原生的API进行模型训练和预测"><a href="#1-2使用原生的API进行模型训练和预测" class="headerlink" title="1.2使用原生的API进行模型训练和预测"></a>1.2使用原生的API进行模型训练和预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line"><span class="comment">#reference:如果这是用于验证的数据集，则应使用训练数据作为参考</span></span><br><span class="line"><span class="comment">#weight : list每个实例的权重</span></span><br><span class="line"><span class="comment">#free_raw_data：default=True，构建内部 Dataset 后释放原始数据，节省内存。</span></span><br><span class="line"><span class="comment">#silent:布尔类型，default=False。是否在构建过程中打印消息。</span></span><br><span class="line"><span class="comment">#init_score：数据集初始分数</span></span><br><span class="line"><span class="comment">#feature_name：设为 &#x27;auto&#x27; 时，如果 data 是 pandas DataFrame，则使用数据列名称。</span></span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="comment">#多分类的objective为multiclass或者别名softmax</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.85</span>,</span><br><span class="line">    <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>:<span class="number">0.01</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[10]    valid_0&#39;s multi_logloss: 0.96537accuarcy: 93.33%</code></pre><h4 id="1-2-2-使用txt-json格式保存模型"><a href="#1-2-2-使用txt-json格式保存模型" class="headerlink" title="1.2.2 使用txt/json格式保存模型"></a>1.2.2 使用txt/json格式保存模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用txt保存模型</span></span><br><span class="line">gbm.save_model(<span class="string">&#x27;model.txt&#x27;</span>)</span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.txt&#x27;</span>)</span><br><span class="line">y_pred = bst.predict(X_test, num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用json格式保存模型</span></span><br><span class="line">    </span><br><span class="line">gbm.save_model(<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">y_pred = bst.predict(X_test, num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 93.33%accuarcy: 93.33%</code></pre><h4 id="1-2-3-使用pickle进行保存模型"><a href="#1-2-3-使用pickle进行保存模型" class="headerlink" title="1.2.3 使用pickle进行保存模型"></a>1.2.3 使用pickle进行保存模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">    pickle.dump(gbm, fout)</span><br><span class="line"><span class="comment"># load model with pickle to predict</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    pkl_bst = pickle.load(fin)</span><br><span class="line"><span class="comment"># can predict with any iteration when loaded in pickle way</span></span><br><span class="line">y_pred = pkl_bst.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 93.33%</code></pre><h2 id="三、任务3-分类、回归和排序任务"><a href="#三、任务3-分类、回归和排序任务" class="headerlink" title="三、任务3 分类、回归和排序任务"></a>三、任务3 分类、回归和排序任务</h2><h3 id="3-1使用-make-classification生成二分类数据进行训练"><a href="#3-1使用-make-classification生成二分类数据进行训练" class="headerlink" title="3.1使用 make_classification生成二分类数据进行训练"></a>3.1使用 make_classification生成二分类数据进行训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#n_samples:样本数量，默认100</span></span><br><span class="line"><span class="comment">#n_features:特征数，默认20</span></span><br><span class="line"><span class="comment">#n_informative：有效特征数量，默认2</span></span><br><span class="line"><span class="comment">#n_redundant:冗余特征，默认2</span></span><br><span class="line"><span class="comment">#n_repeated :重复的特征个数，默认0</span></span><br><span class="line"><span class="comment">#n_clusters_per_class：每个类别中cluster数量，默认2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#weight：各个类的占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#n_classes* n_clusters_per_class 必须≤ 2**有效特征数</span></span><br><span class="line">data, target = make_classification(n_samples=<span class="number">1000</span>,n_features=<span class="number">3</span>,n_informative=<span class="number">3</span>,n_redundant=<span class="number">0</span>,n_classes=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df[<span class="string">&#x27;target&#x27;</span>] = target</span><br><span class="line"></span><br><span class="line">df1 = df[df[<span class="string">&#x27;target&#x27;</span>]==<span class="number">0</span>]</span><br><span class="line">df2 = df[df[<span class="string">&#x27;target&#x27;</span>]==<span class="number">1</span>]</span><br><span class="line">df1.index = <span class="built_in">range</span>(<span class="built_in">len</span>(df1))</span><br><span class="line">df2.index = <span class="built_in">range</span>(<span class="built_in">len</span>(df2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出数据集的数据分布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">plt.scatter(df1[<span class="number">0</span>],df1[<span class="number">1</span>],color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.scatter(df2[<span class="number">0</span>],df2[<span class="number">1</span>],color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">2</span>))</span><br><span class="line">df1[<span class="number">0</span>].hist()</span><br><span class="line">df1[<span class="number">0</span>].plot(kind = <span class="string">&#x27;kde&#x27;</span>, secondary_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mean_ = df1[<span class="number">0</span>].mean()</span><br><span class="line">std_ = df1[<span class="number">0</span>].std()</span><br><span class="line"></span><br><span class="line">stats.kstest(df1[<span class="number">0</span>], <span class="string">&#x27;norm&#x27;</span>, (mean_, std_))</span><br></pre></td></tr></table></figure><pre><code>KstestResult(statistic=0.03723785150172143, pvalue=0.4930944895472954)</code></pre><p><img src="lightGBM_files/lightGBM_17_1.png" alt="png"></p><p><img src="lightGBM_files/lightGBM_17_2.png" alt="png"></p><h4 id="3-1-1-sklearn接口"><a href="#3-1-1-sklearn接口" class="headerlink" title="3.1.1 sklearn接口"></a>3.1.1 sklearn接口</h4><blockquote><p><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier" title="sklearn接口lgb分类器参考文档">sklearn接口lgb分类器参考文档</a><br>注意：每次产生的数据都不一样，所以</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier()</span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[39]    valid_0&#39;s binary_logloss: 0.255538accuarcy: 88.50%</code></pre><h4 id="3-1-2-原生train接口"><a href="#3-1-2-原生train接口" class="headerlink" title="3.1.2 原生train接口"></a>3.1.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">y_pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[10]    valid_0&#39;s binary_logloss: 0.371903accuarcy: 88.00%</code></pre><h3 id="3-2使用-make-classification生成多分类数据进行训练"><a href="#3-2使用-make-classification生成多分类数据进行训练" class="headerlink" title="3.2使用 make_classification生成多分类数据进行训练"></a>3.2使用 make_classification生成多分类数据进行训练</h3><h4 id="3-2-1-sklearn接口"><a href="#3-2-1-sklearn接口" class="headerlink" title="3.2.1 sklearn接口"></a>3.2.1 sklearn接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data, target = make_classification(n_samples=<span class="number">1000</span>,n_features=<span class="number">3</span>,n_informative=<span class="number">3</span>,n_redundant=<span class="number">0</span>,n_classes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier()</span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;logloss&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[39]    valid_0&#39;s binary_logloss: 0.255538accuarcy: 88.50%</code></pre><h4 id="3-2-2-原生train接口"><a href="#3-2-2-原生train接口" class="headerlink" title="3.2.2 原生train接口"></a>3.2.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[10]    valid_0&#39;s multi_logloss: 0.322024accuarcy: 87.50%</code></pre><h3 id="3-3使用-make-regression生成回归数据"><a href="#3-3使用-make-regression生成回归数据" class="headerlink" title="3.3使用 make_regression生成回归数据"></a>3.3使用 make_regression生成回归数据</h3><h4 id="3-3-1-sklearn接口"><a href="#3-3-1-sklearn接口" class="headerlink" title="3.3.1 sklearn接口"></a>3.3.1 sklearn接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">100</span>, n_informative=<span class="number">10</span>, n_targets=<span class="number">1</span>, bias=<span class="number">0.0</span>, </span><br><span class="line">                effective_rank=<span class="literal">None</span>, tail_strength=<span class="number">0.5</span>, noise=<span class="number">0.0</span>, shuffle=<span class="literal">True</span>, coef=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">```                </span><br><span class="line">- n_samples：样本数</span><br><span class="line">- n_features：特征数(自变量个数)</span><br><span class="line">- n_informative：参与建模特征数</span><br><span class="line">- n_targets：因变量个数</span><br><span class="line">- noise：噪音</span><br><span class="line">- bias：偏差(截距)</span><br><span class="line">- coef：是否输出coef标识</span><br><span class="line">- random_state：随机状态若为固定值则每次产生的数据都一样</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data, target = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">5</span>,n_targets=<span class="number">1</span>,noise=<span class="number">1.5</span>,random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMRegressor()<span class="comment">#直接使用默认参数，mse较小。</span></span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;l1&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">mse= mean_squared_error(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mse: %.2f&quot;</span> % (mse))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[99]    valid_0&#39;s l1: 8.13036    valid_0&#39;s l2: 119.246mse: 119.25</code></pre><h4 id="3-3-2-原生train接口"><a href="#3-3-2-原生train接口" class="headerlink" title="3.3.2 原生train接口"></a>3.3.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数为lgb.LGBMRegressor的默认参数。</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;num_leaves&quot;</span>:<span class="number">31</span>,</span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&quot;n_estimators&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;min_child_samples&quot;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">5</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">mse= mean_squared_error(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mse: %.2f&quot;</span> % (mse))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[99]    valid_0&#39;s l2: 119.246mse: 119.25</code></pre><h2 id="四、graphviz可视化"><a href="#四、graphviz可视化" class="headerlink" title="四、graphviz可视化"></a>四、graphviz可视化</h2><blockquote><p>参考文档：<a href="https://blog.csdn.net/kyle1314608/article/details/111245782">《lightgbm 决策树 可视化 graphviz》</a>、<a href="https://graphviz.readthedocs.io/en/stable/manual.html"> graphviz参考文档</a> 、<a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting">《xgboost 可视化API文档》</a>、<a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#plotting">《lightgbm可视化API》</a></p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line">lgb_clf = lgb.LGBMClassifier()</span><br><span class="line">lgb_clf.fit(X_train, y_train)</span><br><span class="line">lgb.create_tree_digraph(lgb_clf, tree_index=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="comment">#多分类的objective为multiclass或者别名softmax</span></span><br></pre></td></tr></table></figure><p><img src="lightGBM_files/lightGBM_33_0.svg" alt="svg"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#lgb没有to_graphviz，无法这样保存图片通</span></span><br><span class="line">digraph = lgb.to_graphviz(lgb_clf , num_trees=<span class="number">1</span>)<span class="comment">#报错module &#x27;lightgbm&#x27; has no attribute &#x27;to_graphviz&#x27;</span></span><br><span class="line">digraph.<span class="built_in">format</span> = <span class="string">&#x27;png&#x27;</span></span><br><span class="line">digraph.view(<span class="string">&#x27;./iris_lgb&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)&lt;ipython-input-20-c0cca38d5eee&gt; in &lt;module&gt;      1 #lgb没有to_graphviz，无法这样保存图片通----&gt; 2 digraph = lgb.to_graphviz(lgb_clf , num_trees=1)#报错module &#39;lightgbm&#39; has no attribute &#39;to_graphviz&#39;      3 digraph.format = &#39;png&#39;      4 digraph.view(&#39;./iris_lgb&#39;)AttributeError: module &#39;lightgbm&#39; has no attribute &#39;to_graphviz&#39;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">xgb_clf = xgb.XGBClassifier()</span><br><span class="line">xgb_clf.fit(iris.data, iris.target)</span><br><span class="line">xgb.to_graphviz(xgb_clf, num_trees=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>[02:55:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.</code></pre><p><img src="lightGBM_files/lightGBM_35_1.svg" alt="svg"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过Digraph对象可以将保存文件并查看</span></span><br><span class="line">digraph = xgb.to_graphviz(xgb_clf, num_trees=<span class="number">1</span>)</span><br><span class="line">digraph.<span class="built_in">format</span> = <span class="string">&#x27;png&#x27;</span><span class="comment">#将图像保存为png图片</span></span><br><span class="line">digraph.view(<span class="string">&#x27;./iris_xgb&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;iris_xgb.png&#39;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 步骤3：读取任务2的json格式模型文件</span></span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">lgb.create_tree_digraph(bst, tree_index=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="lightGBM_files/lightGBM_37_0.svg" alt="svg"></p><h2 id="五、模型调参（网格、随机、贝叶斯）"><a href="#五、模型调参（网格、随机、贝叶斯）" class="headerlink" title="五、模型调参（网格、随机、贝叶斯）"></a>五、模型调参（网格、随机、贝叶斯）</h2><h3 id="5-1-加载数据集"><a href="#5-1-加载数据集" class="headerlink" title="5.1 加载数据集"></a>5.1 加载数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下四列数据转换为类别</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">train, test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">data</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>MONTH</th>      <th>DAY</th>      <th>DAY_OF_WEEK</th>      <th>AIRLINE</th>      <th>FLIGHT_NUMBER</th>      <th>DESTINATION_AIRPORT</th>      <th>ORIGIN_AIRPORT</th>      <th>AIR_TIME</th>      <th>DEPARTURE_TIME</th>      <th>DISTANCE</th>      <th>ARRIVAL_DELAY</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>2</td>      <td>88</td>      <td>253</td>      <td>13</td>      <td>169.0</td>      <td>2354.0</td>      <td>1448</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>1</td>      <td>2120</td>      <td>213</td>      <td>164</td>      <td>263.0</td>      <td>2.0</td>      <td>2330</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>12</td>      <td>803</td>      <td>60</td>      <td>262</td>      <td>266.0</td>      <td>18.0</td>      <td>2296</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>1</td>      <td>238</td>      <td>185</td>      <td>164</td>      <td>258.0</td>      <td>15.0</td>      <td>2342</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>2</td>      <td>122</td>      <td>14</td>      <td>261</td>      <td>199.0</td>      <td>24.0</td>      <td>1448</td>      <td>0</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>9994</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>8</td>      <td>2399</td>      <td>44</td>      <td>215</td>      <td>62.0</td>      <td>1710.0</td>      <td>473</td>      <td>0</td>    </tr>    <tr>      <th>9995</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>7</td>      <td>149</td>      <td>128</td>      <td>210</td>      <td>28.0</td>      <td>1716.0</td>      <td>100</td>      <td>1</td>    </tr>    <tr>      <th>9996</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>8</td>      <td>2510</td>      <td>208</td>      <td>76</td>      <td>29.0</td>      <td>1653.0</td>      <td>147</td>      <td>0</td>    </tr>    <tr>      <th>9997</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>8</td>      <td>2512</td>      <td>62</td>      <td>215</td>      <td>28.0</td>      <td>1721.0</td>      <td>135</td>      <td>1</td>    </tr>    <tr>      <th>9998</th>      <td>1</td>      <td>1</td>      <td>4</td>      <td>8</td>      <td>2541</td>      <td>208</td>      <td>182</td>      <td>103.0</td>      <td>2000.0</td>      <td>594</td>      <td>1</td>    </tr>  </tbody></table><p>9592 rows × 11 columns</p></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br></pre></td></tr></table></figure><h3 id="5-2-步骤2-设置树模型深度分别为-3-5-6-9-，记录下验证集AUC精度。"><a href="#5-2-步骤2-设置树模型深度分别为-3-5-6-9-，记录下验证集AUC精度。" class="headerlink" title="5.2:步骤2 设置树模型深度分别为[3,5,6,9]，记录下验证集AUC精度。"></a>5.2:步骤2 设置树模型深度分别为[3,5,6,9]，记录下验证集AUC精度。</h3><ul><li>predict：lgb.LGBMClassifier()等sklearn接口中是返回预测的类别</li><li>predict_proba：klearn接口中是返回预测的概率。重点是求auc时，我们必须用predict_proba。因为roc曲线的阀值是根据其正样本的概率求的。</li></ul><p>参考<a href="https://blog.csdn.net/weixin_43827767/article/details/120586336">《数据挖掘竞赛lightgbm通过求最大auc调参》</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#sklearn接口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_depth</span>(<span class="params">max_depth</span>):</span>    </span><br><span class="line">    gbm = lgb.LGBMClassifier(max_depth=max_depth)</span><br><span class="line">    gbm.fit(train, y_train,</span><br><span class="line">            eval_set=[(test, y_test)],</span><br><span class="line">            eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">            callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line">    <span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line">    <span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">    y_pred = gbm.predict(test)</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">    auc_score=metrics.roc_auc_score(y_test,gbm.predict_proba(test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;max_depth=&quot;</span>,max_depth,<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line">test_depth(<span class="number">3</span>)</span><br><span class="line">test_depth(<span class="number">5</span>)</span><br><span class="line">test_depth(<span class="number">6</span>)</span><br><span class="line">test_depth(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[98]    valid_0&#39;s binary_logloss: 0.429334max_depth= 3 accuarcy: 81.90% auc_score: 76.32%Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[60]    valid_0&#39;s binary_logloss: 0.430826max_depth= 5 accuarcy: 81.98% auc_score: 75.54%Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[65]    valid_0&#39;s binary_logloss: 0.429341max_depth= 6 accuarcy: 81.69% auc_score: 75.63%Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[52]    valid_0&#39;s binary_logloss: 0.429146max_depth= 9 accuarcy: 81.94% auc_score: 76.07%</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#原生train接口</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">lgb_train = lgb.Dataset(train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_depth</span>(<span class="params">max_depth</span>):</span> </span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;learning_rate&quot;</span>:<span class="number">0.1</span>, </span><br><span class="line">        <span class="string">&quot;min_child_samples&quot;</span>:<span class="number">20</span>,</span><br><span class="line">         <span class="string">&quot;num_leaves&quot;</span>:<span class="number">31</span>,</span><br><span class="line">         <span class="string">&quot;max_depth&quot;</span>:max_depth&#125;</span><br><span class="line"></span><br><span class="line">    gbm2 = lgb.train(params,</span><br><span class="line">                    lgb_train,</span><br><span class="line">                    num_boost_round=<span class="number">10</span>,</span><br><span class="line">                    valid_sets=lgb_eval,</span><br><span class="line">                    callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">    y_pred = gbm2.predict(test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">    pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">    accuracy = accuracy_score(y_test,pred)</span><br><span class="line">    auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;max_depth=&quot;</span>,max_depth,<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line">test_depth(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-步骤3：category变量设置为categorical-feature"><a href="#5-3-步骤3：category变量设置为categorical-feature" class="headerlink" title="5.3  步骤3：category变量设置为categorical_feature"></a>5.3  步骤3：category变量设置为categorical_feature</h3><blockquote><p>参考<a href="https://blog.csdn.net/anshuai_aw1/article/details/83275299">《Lightgbm如何处理类别特征？》</a><br>参考kaggle教程<a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">《Feature Selection with Null Importances》</a>中的代码。</p></blockquote><p>lightGBM比XGBoost的1个改进之处在于对类别特征的处理, 不再需要将类别特征转为one-hot形式。这一步通过设置categorical_feature来实现。</p><p>唯一疑惑的是真正的object特征只有’AIRLINE’, ‘DESTINATION_AIRPORT’, ‘ORIGIN_AIRPORT’，但是’FLIGHT_NUMBER’也设置成类别特征效果更好。</p><ul><li>‘FLIGHT_NUMBER’也为类别特征：accuarcy: 81.82% auc_score: 77.52%</li><li>‘FLIGHT_NUMBER’不是类别特征：accuarcy: 81.69% auc_score: 76.48%</li></ul><p>估计跟数据集有关系，没有仔细研究数据集。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line">categorical_feats  = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="comment">#categorical_feats = [f for f in data.columns if data[f].dtype == &#x27;object&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上面四列特征转为类别特征，但不是one-hot编码</span></span><br><span class="line"><span class="keyword">for</span> f_ <span class="keyword">in</span> categorical_feats:</span><br><span class="line">    data[f_], _ = pd.factorize(data[f_])</span><br><span class="line">    <span class="comment"># Set feature type as categorical</span></span><br><span class="line">    data[f_] = data[f_].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">categorical_feats</span><br></pre></td></tr></table></figure><pre><code>[&#39;AIRLINE&#39;, &#39;FLIGHT_NUMBER&#39;, &#39;DESTINATION_AIRPORT&#39;, &#39;ORIGIN_AIRPORT&#39;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征命名</span></span><br><span class="line"><span class="comment">#num_train, num_feature = X_train.shape#X_train是7194行10列的数据集，num_feature=10表示特征数量</span></span><br><span class="line"><span class="comment">#feature_name = [&#x27;feature_&#x27; + str(col) for col in range(num_feature)]#feature_0到9</span></span><br><span class="line"></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,  <span class="comment">#验证集设置</span></span><br><span class="line">                <span class="comment">#feature_name=feature_name,  #特征命名</span></span><br><span class="line">                categorical_feature=categorical_feats,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)]) <span class="comment">#设置分类变量</span></span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[10]    valid_0&#39;s binary_logloss: 0.424384accuarcy: 81.82% auc_score: 77.52%</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不设置categorical_feature结果一样啊，不知道为何？</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行one-hot编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">train, test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                 callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)]) </span><br><span class="line"></span><br><span class="line">y_pred2 = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred2 =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred2]</span><br><span class="line">accuracy2 = accuracy_score(y_test,pred2)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,y_pred2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsDid not meet early stopping. Best iteration is:[10]    valid_0&#39;s binary_logloss: 0.424384accuarcy: 81.82% auc_score: 77.52%</code></pre><h3 id="5-4-步骤4：超参搜索"><a href="#5-4-步骤4：超参搜索" class="headerlink" title="5.4  步骤4：超参搜索"></a>5.4  步骤4：超参搜索</h3><h4 id="5-4-1-GridSearchCV"><a href="#5-4-1-GridSearchCV" class="headerlink" title="5.4.1 GridSearchCV"></a>5.4.1 GridSearchCV</h4><blockquote><p><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection">GridSearchCV参考文档</a></p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, refit=<span class="literal">True</span>, cv=<span class="literal">None</span>, verbose=<span class="number">0</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>, error_score=nan, return_train_score=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>其中 scoring是字符串格式或者str列表、字典。具体的参数列表参考文档<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">scoring-parameter</a></p><p>网格搜索——尝试所有可能的组合：<br><img src="https://img-blog.csdnimg.cn/fc0dcfd497d04ab38597dc9a4b02afe7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data = data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>],</span><br><span class="line">              <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">100</span>, <span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line">              <span class="string">&quot;num_leaves&quot;</span>:[<span class="number">25</span>,<span class="number">31</span>,<span class="number">36</span>]&#125;</span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">10</span>,<span class="comment">#构建树的深度，越大越容易过拟合</span></span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">100</span>,         </span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">gs = GridSearchCV(gbm, param_grid=parameters, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">3</span>)</span><br><span class="line">gs.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score: %0.3f&quot;</span> % gs.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set: %s&quot;</span> % gs.best_params_ )</span><br></pre></td></tr></table></figure><pre><code>Best score: 0.805Best parameters set: &#123;&#39;learning_rate&#39;: 0.05, &#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 100, &#39;num_leaves&#39;: 36&#125;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用最优参数预测验证集</span></span><br><span class="line">y_pred = gs.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gs.predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 82.07% auc_score: 75.92%</code></pre><h3 id="5-4-3-随机搜索"><a href="#5-4-3-随机搜索" class="headerlink" title="5.4.3 随机搜索"></a>5.4.3 随机搜索</h3><blockquote><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">参考文档</a></p></blockquote><p>网格搜索尝试超参数的所有组合，因此增加了计算的时间复杂度，在数据量较大，或者模型较为复杂等等情况下，可能导致不可行的计算成本，这样网格搜索调参方法就不适用了。然而，随机搜索提供更便利的替代方案，该方法只测试你选择的超参数组成的元组，并且超参数值的选择是完全随机的，如下图所示。</p><p><img src="https://img-blog.csdnimg.cn/424f79626ce841ae95ee8bd3247808e9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line">param = <span class="built_in">dict</span>(n_estimators=[<span class="number">80</span>,<span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">             max_depth=[<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],</span><br><span class="line">            learning_rate= [<span class="number">0.02</span>,<span class="number">0.05</span>, <span class="number">0.1</span>],</span><br><span class="line">            num_leaves=[<span class="number">25</span>,<span class="number">31</span>,<span class="number">36</span>])</span><br><span class="line"></span><br><span class="line">grid = RandomizedSearchCV(estimator=lgb.LGBMClassifier(),</span><br><span class="line">                          param_distributions=param,scoring=<span class="string">&#x27;accuracy&#x27;</span>,cv=<span class="number">3</span>)</span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score: %0.3f&quot;</span> % grid.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set: %s&quot;</span> % grid.best_params_ )</span><br><span class="line"><span class="comment"># 找到最好的模型</span></span><br><span class="line">grid.best_estimator_</span><br></pre></td></tr></table></figure><pre><code>Best score: 0.806Best parameters set: &#123;&#39;num_leaves&#39;: 36, &#39;n_estimators&#39;: 80, &#39;max_depth&#39;: 6, &#39;learning_rate&#39;: 0.1&#125;LGBMClassifier(max_depth=6, n_estimators=80, num_leaves=36)</code></pre><p>最优模型直接用grid或者rid.best<em>estimator</em>都行</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用最优参数预测验证集</span></span><br><span class="line">y_pred = grid .predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,grid .predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到最好的模型</span></span><br><span class="line">gd=grid.best_estimator_</span><br><span class="line">y_pred = gd .predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gd .predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 81.69% auc_score: 75.62%accuarcy: 81.69% auc_score: 75.62%</code></pre><h3 id="5-4-4-贝叶斯搜索"><a href="#5-4-4-贝叶斯搜索" class="headerlink" title="5.4.4 贝叶斯搜索"></a>5.4.4 贝叶斯搜索</h3><blockquote><p>参考<a href="https://blog.csdn.net/qq_42283960/article/details/88317003">《贝叶斯全局优化（LightGBM调参）》</a></p></blockquote><p>贝叶斯搜索使用贝叶斯优化技术对搜索空间进行建模，以尽快获得优化的参数值。它使用搜索空间的结构来优化搜索时间。贝叶斯搜索方法使用过去的评估结果来采样最有可能提供更好结果的新候选参数（如下图所示）:</p><p><img src="https://img-blog.csdnimg.cn/02eaae7e2d104f418e5d0331ca9cd870.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设定贝叶斯优化的黑盒函数LGB_bayesian</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LGB_bayesian</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    num_leaves,  <span class="comment"># int</span></span></span></span><br><span class="line"><span class="params"><span class="function">    min_data_in_leaf,  <span class="comment"># int</span></span></span></span><br><span class="line"><span class="params"><span class="function">    learning_rate,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_sum_hessian_in_leaf,    <span class="comment"># int  </span></span></span></span><br><span class="line"><span class="params"><span class="function">    feature_fraction,</span></span></span><br><span class="line"><span class="params"><span class="function">    lambda_l1,</span></span></span><br><span class="line"><span class="params"><span class="function">    lambda_l2,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_gain_to_split,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_depth</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LightGBM expects next three parameters need to be integer. So we make them integer</span></span><br><span class="line">    num_leaves = <span class="built_in">int</span>(num_leaves)</span><br><span class="line">    min_data_in_leaf = <span class="built_in">int</span>(min_data_in_leaf)</span><br><span class="line">    max_depth = <span class="built_in">int</span>(max_depth)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(num_leaves) == <span class="built_in">int</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(min_data_in_leaf) == <span class="built_in">int</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(max_depth) == <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: num_leaves,</span><br><span class="line">        <span class="string">&#x27;max_bin&#x27;</span>: <span class="number">63</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: min_data_in_leaf,</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: learning_rate,</span><br><span class="line">        <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: min_sum_hessian_in_leaf,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: feature_fraction,</span><br><span class="line">        <span class="string">&#x27;lambda_l1&#x27;</span>: lambda_l1,</span><br><span class="line">        <span class="string">&#x27;lambda_l2&#x27;</span>: lambda_l2,</span><br><span class="line">        <span class="string">&#x27;min_gain_to_split&#x27;</span>: min_gain_to_split,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: max_depth,</span><br><span class="line">        <span class="string">&#x27;save_binary&#x27;</span>: <span class="literal">True</span>, </span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;drop_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;data_random_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;is_unbalance&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;boost_from_average&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    &#125;    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    lgb_train = lgb.Dataset(X_train,</span><br><span class="line">                           label=y_train)</span><br><span class="line">    lgb_valid = lgb.Dataset(X_test,label=y_test,reference=lgb_train)   </span><br><span class="line"></span><br><span class="line">    num_round = <span class="number">500</span></span><br><span class="line">    gbm= lgb.train(param, lgb_train, num_round, valid_sets = [lgb_valid],callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])   </span><br><span class="line">    predictions = gbm.predict(X_test,num_iteration=gbm.best_iteration)</span><br><span class="line">    score = metrics.roc_auc_score(y_test, predictions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>LGB_bayesian函数从贝叶斯优化框架获取num_leaves，min_data_in_leaf，learning_rate，min_sum_hessian_in_leaf，feature_fraction，lambda_l1，lambda_l2，min_gain_to_split，max_depth的值。 请记住，对于LightGBM，num_leaves，min_data_in_leaf和max_depth应该是整数。 但贝叶斯优化会发送连续的函数。 所以我强制它们是整数。 我只会找到它们的最佳参数值。 读者可以增加或减少要优化的参数数量。<br>现在需要为这些参数提供边界，以便贝叶斯优化仅在边界内搜索</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bounds_LGB = &#123;</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: (<span class="number">5</span>, <span class="number">20</span>), </span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: (<span class="number">5</span>, <span class="number">20</span>),  </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: (<span class="number">0.01</span>, <span class="number">0.3</span>),</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: (<span class="number">0.00001</span>, <span class="number">0.01</span>),    </span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: (<span class="number">0.05</span>, <span class="number">0.5</span>),</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: (<span class="number">0</span>, <span class="number">5.0</span>), </span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: (<span class="number">0</span>, <span class="number">5.0</span>), </span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: (<span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>:(<span class="number">3</span>,<span class="number">15</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#将它们全部放在BayesianOptimization对象中</span></span><br><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line">LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=<span class="number">13</span>)</span><br><span class="line"><span class="built_in">print</span>(LGB_BO.space.keys)<span class="comment">#显示要优化的参数</span></span><br></pre></td></tr></table></figure><pre><code>[&#39;feature_fraction&#39;, &#39;lambda_l1&#39;, &#39;lambda_l2&#39;, &#39;learning_rate&#39;, &#39;max_depth&#39;, &#39;min_data_in_leaf&#39;, &#39;min_gain_to_split&#39;, &#39;min_sum_hessian_in_leaf&#39;, &#39;num_leaves&#39;]</code></pre><p>调用maximize方法LGB_BO才会开始搜索。</p><ul><li>init_points：我们想要执行的随机探索的初始随机运行次数。 在我们的例子中，LGB_bayesian将被运行n_iter次。</li><li>n_iter：运行init_points数后，我们要执行多少次贝叶斯优化运行。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">init_points = <span class="number">5</span></span><br><span class="line">n_iter = <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">130</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> warnings.catch_warnings():</span><br><span class="line">    warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq=<span class="string">&#x27;ucb&#x27;</span>, xi=<span class="number">0.0</span>, alpha=<span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure><pre><code>----------------------------------------------------------------------------------------------------------------------------------|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |-------------------------------------------------------------------------------------------------------------------------------------[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[22]    valid_0&#39;s auc: 0.770384| [0m 1       [0m | [0m 0.7704  [0m | [0m 0.4     [0m | [0m 1.188   [0m | [0m 4.121   [0m | [0m 0.2901  [0m | [0m 14.67   [0m | [0m 11.8    [0m | [0m 0.609   [0m | [0m 0.007758[0m | [0m 14.62   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[5]    valid_0&#39;s auc: 0.737399| [0m 2       [0m | [0m 0.7374  [0m | [0m 0.3749  [0m | [0m 0.1752  [0m | [0m 1.492   [0m | [0m 0.02697 [0m | [0m 13.28   [0m | [0m 10.59   [0m | [0m 0.6798  [0m | [0m 0.00257 [0m | [0m 10.21   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[8]    valid_0&#39;s auc: 0.719501| [0m 3       [0m | [0m 0.7195  [0m | [0m 0.05424 [0m | [0m 1.792   [0m | [0m 4.745   [0m | [0m 0.07319 [0m | [0m 6.833   [0m | [0m 18.77   [0m | [0m 0.0319  [0m | [0m 0.000660[0m | [0m 14.45   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[19]    valid_0&#39;s auc: 0.760323| [0m 4       [0m | [0m 0.7603  [0m | [0m 0.4432  [0m | [0m 0.04358 [0m | [0m 3.733   [0m | [0m 0.2457  [0m | [0m 3.909   [0m | [0m 14.85   [0m | [0m 0.5093  [0m | [0m 0.004804[0m | [0m 19.33   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[8]    valid_0&#39;s auc: 0.719412| [0m 5       [0m | [0m 0.7194  [0m | [0m 0.05001 [0m | [0m 1.235   [0m | [0m 3.561   [0m | [0m 0.1041  [0m | [0m 6.324   [0m | [0m 15.43   [0m | [0m 0.9186  [0m | [0m 0.002452[0m | [0m 11.87   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[11]    valid_0&#39;s auc: 0.761779| [0m 6       [0m | [0m 0.7618  [0m | [0m 0.5     [0m | [0m 1.457   [0m | [0m 5.0     [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 11.16   [0m | [0m 0.5786  [0m | [0m 0.01    [0m | [0m 17.3    [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[8]    valid_0&#39;s auc: 0.723696| [0m 7       [0m | [0m 0.7237  [0m | [0m 0.05    [0m | [0m 5.0     [0m | [0m 5.0     [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 12.07   [0m | [0m 0.0     [0m | [0m 0.01    [0m | [0m 14.22   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770585| [95m 8       [0m | [95m 0.7706  [0m | [95m 0.5     [0m | [95m 0.0     [0m | [95m 2.9     [0m | [95m 0.3     [0m | [95m 14.75   [0m | [95m 11.78   [0m | [95m 1.0     [0m | [95m 0.003764[0m | [95m 16.12   [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[19]    valid_0&#39;s auc: 0.769728| [0m 9       [0m | [0m 0.7697  [0m | [0m 0.5     [0m | [0m 0.0     [0m | [0m 4.272   [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 8.6     [0m | [0m 1.0     [0m | [0m 0.009985[0m | [0m 15.0    [0m |[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[28]    valid_0&#39;s auc: 0.770488| [0m 10      [0m | [0m 0.7705  [0m | [0m 0.5     [0m | [0m 0.0     [0m | [0m 4.457   [0m | [0m 0.3     [0m | [0m 10.79   [0m | [0m 9.347   [0m | [0m 1.0     [0m | [0m 0.01    [0m | [0m 16.83   [0m |=====================================================================================================================================</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;target&#x27;</span>])<span class="comment">#最佳的auc值</span></span><br><span class="line">LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>]<span class="comment">#最佳模型参数</span></span><br></pre></td></tr></table></figure><pre><code>0.7705848546741305&#123;&#39;feature_fraction&#39;: 0.5, &#39;lambda_l1&#39;: 0.0, &#39;lambda_l2&#39;: 2.899605369776912, &#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 14.752822601781512, &#39;min_data_in_leaf&#39;: 11.782200828907708, &#39;min_gain_to_split&#39;: 1.0, &#39;min_sum_hessian_in_leaf&#39;: 0.0037639771497955552, &#39;num_leaves&#39;: 16.11909067874899&#125;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将这些参数用于我们的最终模型</span></span><br><span class="line">LGB_BO.probe(</span><br><span class="line">    params=&#123;<span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">            <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">            <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">            <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">            <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">            <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">            <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">            <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">            <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>&#125;,</span><br><span class="line">            lazy=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对LGB_BO对象进行最大化调用。</span></span><br><span class="line">LGB_BO.maximize(init_points=<span class="number">0</span>, n_iter=<span class="number">0</span>) </span><br></pre></td></tr></table></figure><pre><code>|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |-------------------------------------------------------------------------------------------------------------------------------------[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586| [95m 11      [0m | [95m 0.7706  [0m | [95m 0.5     [0m | [95m 0.0     [0m | [95m 2.9     [0m | [95m 0.3     [0m | [95m 15.0    [0m | [95m 12.0    [0m | [95m 1.0     [0m | [95m 0.0038  [0m | [95m 16.0    [0m |=====================================================================================================================================</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过属性LGB_BO.res可以获得探测的所有参数列表及其相应的目标值。</span></span><br><span class="line"><span class="keyword">for</span> i, res <span class="keyword">in</span> <span class="built_in">enumerate</span>(LGB_BO.res):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Iteration &#123;&#125;: \n\t&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, res))</span><br></pre></td></tr></table></figure><p>将LGB_BO的最佳参数保存到param_lgb字典中，然后进行5折交叉训练</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> rankdata</span><br><span class="line">param_lgb = &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;num_leaves&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;max_bin&#x27;</span>: <span class="number">63</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_data_in_leaf&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;learning_rate&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1.0</span>, </span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">5</span>, </span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;feature_fraction&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;lambda_l1&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;lambda_l1&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;lambda_l2&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;lambda_l2&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;min_gain_to_split&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_gain_to_split&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;max_depth&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;save_binary&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;drop_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;data_random_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;is_unbalance&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;boost_from_average&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">nfold = <span class="number">5</span></span><br><span class="line">gc.collect()</span><br><span class="line">skf = StratifiedKFold(n_splits=nfold, shuffle=<span class="literal">True</span>, random_state=<span class="number">2019</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">oof = np.zeros(<span class="built_in">len</span>(y_train))</span><br><span class="line">predictions = np.zeros((<span class="built_in">len</span>(X_test),nfold))</span><br><span class="line"></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> train_index, valid_index <span class="keyword">in</span> skf.split(X_train, y_train):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nfold &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    lgb_train = lgb.Dataset(X_train,label=y_train)</span><br><span class="line">    lgb_valid = lgb.Dataset(X_test,label=y_test,reference=lgb_train)  </span><br><span class="line">   </span><br><span class="line">    clf = lgb.train(param_lgb, lgb_train,<span class="number">500</span>, valid_sets = [lgb_valid ], verbose_eval=<span class="number">250</span>, callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line">    <span class="built_in">print</span>(clf.predict(X_train, num_iteration=clf.best_iteration) )</span><br><span class="line">    oof[valid_index] = clf.predict(X_train.iloc[valid_index].values, num_iteration=clf.best_iteration) </span><br><span class="line">    </span><br><span class="line">    predictions[:,i-<span class="number">1</span>] += clf.predict(X_test, num_iteration=clf.best_iteration)</span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n\nCV AUC: &#123;:&lt;0.2f&#125;&quot;</span>.<span class="built_in">format</span>(metrics.roc_auc_score(y_train, oof)))</span><br></pre></td></tr></table></figure><pre><code>fold 1[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000288 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 393[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]fold 2[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 393[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]fold 3[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 393[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]fold 4[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 393[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]fold 5[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 393[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[17]    valid_0&#39;s auc: 0.770586[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]CV AUC: 0.81</code></pre><p>另一种贝叶斯搜索，参考:<br>[《网格搜索、随机搜索和贝叶斯搜索实用教程》[(<a href="https://blog.csdn.net/fengdu78/article/details/121134090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164200107216780274194329%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164200107216780274194329&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-121134090.pc_search_insert_es_download&amp;utm_term=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%90%9C%E7%B4%A2&amp;spm=1018.2226.3001.4187">https://blog.csdn.net/fengdu78/article/details/121134090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164200107216780274194329%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164200107216780274194329&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-121134090.pc_search_insert_es_download&amp;utm_term=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%90%9C%E7%B4%A2&amp;spm=1018.2226.3001.4187</a>)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#还没有写完，并不能正确运行</span></span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> BayesSearchCV</span><br><span class="line"><span class="comment"># 参数范围由下面的一个指定</span></span><br><span class="line"><span class="keyword">from</span> skopt.space <span class="keyword">import</span> Real, Categorical, Integer</span><br><span class="line">search_spaces = &#123;</span><br><span class="line">  <span class="string">&#x27;C&#x27;</span>: Real(<span class="number">0.1</span>, <span class="number">1e+4</span>),</span><br><span class="line">  <span class="string">&#x27;gamma&#x27;</span>: Real(<span class="number">1e-6</span>, <span class="number">1e+1</span>, <span class="string">&#x27;log-uniform&#x27;</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#接下来创建一个RandomizedSearchCV带参数n_iter_search的对象，并将使用训练数据来训练模型。</span></span><br><span class="line"></span><br><span class="line">n_iter_search = <span class="number">20</span> </span><br><span class="line">bayes_search = BayesSearchCV( </span><br><span class="line">    lgb.LGBMClassifier(), </span><br><span class="line">    search_spaces, </span><br><span class="line">    n_iter=n_iter_search, </span><br><span class="line">    cv=<span class="number">3</span>, </span><br><span class="line">    verbose=<span class="number">3</span> </span><br><span class="line">) </span><br><span class="line">bayes_search.fit(X_train, y_train)</span><br><span class="line">bayes_search.best_params_</span><br></pre></td></tr></table></figure><h2 id="六、模型微调与参数衰减"><a href="#六、模型微调与参数衰减" class="headerlink" title="六、模型微调与参数衰减"></a>六、模型微调与参数衰减</h2><h3 id="6-2-学习率衰减"><a href="#6-2-学习率衰减" class="headerlink" title="6.2 学习率衰减"></a>6.2 学习率衰减</h3><p>参考<a href="https://www.codenong.com/cs108978573/https://www.codenong.com/cs108978573/">《python实现LightGBM(进阶)python实现LightGBM(进阶)》</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data = data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 学习率指数衰减,learning_rates弃用了</span></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                learning_rates=<span class="keyword">lambda</span> <span class="built_in">iter</span>: <span class="number">0.3</span> * (<span class="number">0.99</span> ** <span class="built_in">iter</span>),<span class="comment"># 学习率衰减</span></span><br><span class="line">                valid_sets=lgb_eval)</span><br><span class="line"><span class="comment">#设置learning_rates结果是accuarcy: 82.07% auc_score: 75.40%</span></span><br><span class="line"><span class="comment">#不设置learning_rates结果是accuarcy: 81.61% auc_score: 75.74%,还是不一样</span></span><br><span class="line"><span class="comment"># 学习率指数衰减</span></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=gbm,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.reset_parameter(bagging_fraction=<span class="keyword">lambda</span> <span class="built_in">iter</span>: <span class="number">0.3</span> * (<span class="number">0.99</span> ** <span class="built_in">iter</span>))])</span><br><span class="line"><span class="comment">#不设置init_model，结果是accuarcy: 81.69% auc_score: 75.25%</span></span><br><span class="line"><span class="comment">#  设置init_model，结果是accuarcy: 81.94% auc_score: 76.32%</span></span><br><span class="line"><span class="comment">#lgb.reset_parameter参数可以是列表或者衰减函数，不知道为啥bagging_fraction设置不同值结果是一样的</span></span><br><span class="line">y_pred1,y_pred2 = gbm.predict(X_test,num_iteration=gbm.best_iteration),gbm2.predict(X_test,num_iteration=gbm2.best_iteration)</span><br><span class="line">pred1,pred2 =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred1],[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred2]</span><br><span class="line">accuracy1,accuracy2 = accuracy_score(y_test,pred1),accuracy_score(y_test,pred2)</span><br><span class="line">auc_score1,auc_score2=metrics.roc_auc_score(y_test,y_pred1),metrics.roc_auc_score(y_test,y_pred2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy1*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score1*<span class="number">100.0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>[1]    valid_0&#39;s binary_logloss: 0.48425[2]    valid_0&#39;s binary_logloss: 0.471031[3]    valid_0&#39;s binary_logloss: 0.46278[4]    valid_0&#39;s binary_logloss: 0.456369[5]    valid_0&#39;s binary_logloss: 0.449357[6]    valid_0&#39;s binary_logloss: 0.444377[7]    valid_0&#39;s binary_logloss: 0.440908[8]    valid_0&#39;s binary_logloss: 0.438597[9]    valid_0&#39;s binary_logloss: 0.435632[10]    valid_0&#39;s binary_logloss: 0.434647accuarcy: 82.07% auc_score: 75.40%accuarcy: 82.19% auc_score: 76.08%</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 学习率阶梯衰减,bagging_fraction&#x27;如果使用列表，列表元素数量要和  &#x27;num_boost_round&#x27;值一样</span></span><br><span class="line">gbm3 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=gbm,<span class="comment">#</span></span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.reset_parameter(bagging_fraction=[<span class="number">0.6</span>]*<span class="number">5</span>+[<span class="number">0.2</span>]*<span class="number">3</span>+[<span class="number">0.1</span>]*<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm3.predict(X_test,num_iteration=gbm3.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>accuarcy: 81.94% auc_score: 76.30%</code></pre><h2 id="七、特征筛选方法"><a href="#七、特征筛选方法" class="headerlink" title="七、特征筛选方法"></a>七、特征筛选方法</h2><h3 id="7-1-筛选最重要的3个特征"><a href="#7-1-筛选最重要的3个特征" class="headerlink" title="7.1 筛选最重要的3个特征"></a>7.1 筛选最重要的3个特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过feature_importances_方法得到特征重要性，值越高越重要</span></span><br><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">9</span>)</span><br><span class="line">gbm.fit(train, y_train,</span><br><span class="line">            eval_set=[(test, y_test)],</span><br><span class="line">            eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">            callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line">df=pd.DataFrame(gbm.feature_importances_,gbm.feature_name_,columns=[<span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">df.sort_values(<span class="string">&#x27;value&#x27;</span>,inplace=<span class="literal">True</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[52]    valid_0&#39;s binary_logloss: 0.429146</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>value</th>    </tr>  </thead>  <tbody>    <tr>      <th>DEPARTURE_TIME</th>      <td>276</td>    </tr>    <tr>      <th>ORIGIN_AIRPORT</th>      <td>262</td>    </tr>    <tr>      <th>DESTINATION_AIRPORT</th>      <td>250</td>    </tr>    <tr>      <th>FLIGHT_NUMBER</th>      <td>236</td>    </tr>    <tr>      <th>AIR_TIME</th>      <td>227</td>    </tr>    <tr>      <th>DISTANCE</th>      <td>184</td>    </tr>    <tr>      <th>AIRLINE</th>      <td>124</td>    </tr>    <tr>      <th>MONTH</th>      <td>0</td>    </tr>    <tr>      <th>DAY</th>      <td>0</td>    </tr>    <tr>      <th>DAY_OF_WEEK</th>      <td>0</td>    </tr>  </tbody></table></div><p>上图看出，最重要的是DEPARTURE_TIME、ORIGIN_AIRPORT、DESTINATION_AIRPORT</p><h3 id="7-2-利用PermutationImportance排列特征重要性"><a href="#7-2-利用PermutationImportance排列特征重要性" class="headerlink" title="7.2 利用PermutationImportance排列特征重要性"></a>7.2 利用PermutationImportance排列特征重要性</h3><ul><li><a href="https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance">eli5文档</a></li><li><a href="https://blog.csdn.net/lz_peter/article/details/88654198?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164201788816780357244048%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164201788816780357244048&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-88654198.pc_search_insert_es_download&amp;utm_term=PermutationImportance&amp;spm=1018.2226.3001.4187">利用PermutationImportance挑选变量</a></li><li><a href="https://www.kaggle.com/dansbecker/permutation-importance">kaggle教程</a></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> eli5</span><br><span class="line"><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line">perm = PermutationImportance(gbm, random_state=<span class="number">1</span>).fit(test,y_test)</span><br><span class="line">eli5.show_weights(perm, feature_names =gbm.feature_name_)</span><br></pre></td></tr></table></figure><pre><code>Training until validation scores don&#39;t improve for 5 roundsEarly stopping, best iteration is:[52]    valid_0&#39;s binary_logloss: 0.429146&lt;style&gt;table.eli5-weights tr:hover &#123;    filter: brightness(85%);&#125;</code></pre><p>&lt;/style&gt;</p><pre><code>    &lt;table class=&quot;eli5-weights eli5-feature-importances&quot; style=&quot;border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;&quot;&gt;&lt;thead&gt;&lt;tr style=&quot;border: none;&quot;&gt;    &lt;th style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;Weight&lt;/th&gt;    &lt;th style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;Feature&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 80.00%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0396                &amp;plusmn; 0.0096        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            DEPARTURE_TIME        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 90.14%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0144                &amp;plusmn; 0.0057        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            DESTINATION_AIRPORT        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 90.50%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0137                &amp;plusmn; 0.0043        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            ORIGIN_AIRPORT        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 92.95%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0089                &amp;plusmn; 0.0067        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            AIR_TIME        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 95.47%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0048                &amp;plusmn; 0.0041        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            AIRLINE        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 95.86%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0042                &amp;plusmn; 0.0045        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            DISTANCE        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 96.10%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0.0038                &amp;plusmn; 0.0029        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            FLIGHT_NUMBER        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0                &amp;plusmn; 0.0000        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            DAY_OF_WEEK        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0                &amp;plusmn; 0.0000        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            DAY        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;            0                &amp;plusmn; 0.0000        &lt;/td&gt;        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;            MONTH        &lt;/td&gt;    &lt;/tr&gt;&lt;/tbody&gt;</code></pre><p>&lt;/table&gt;</p><p>所以前三重要的特征是DEPARTURE_TIME、DESTINATION_AIRPORT、ORIGIN_AIRPORT</p><h3 id="7-3-Null-Importances进行特征选择"><a href="#7-3-Null-Importances进行特征选择" class="headerlink" title="7.3 Null Importances进行特征选择"></a>7.3 Null Importances进行特征选择</h3><blockquote><p>参考<a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">kaggkle教程</a><br><a href="https://blog.csdn.net/weixin_39681171/article/details/109919282?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164216516716780357210720%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164216516716780357210720&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-109919282.pc_search_insert_es_download&amp;utm_term=null%20importance%E9%87%8D%E8%A6%81%E6%80%A7&amp;spm=1018.2226.3001.4187">《数据竞赛】99%情况下都有效的特征筛选策略—Null Importance》</a></p></blockquote><p>特征筛选策略 — Null Importance 特征筛选</p><h4 id="7-3-1-主要思想："><a href="#7-3-1-主要思想：" class="headerlink" title="7.3.1 主要思想："></a>7.3.1 主要思想：</h4><p>通过利用跑树模型得到特征的importance来判断特征的稳定性和好坏。</p><ol><li><p>将构建好的特征和正确的标签扔进树模型中，此时可以得到每个特征的重要性（split 和 gain）</p></li><li><p>将数据的标签打乱，再扔进模型中，得到打乱标签后，每个特征的重要性（split和gain）；重复n次；取n次特征重要性的平均值。</p></li><li><p>将1中正确标签跑的特征的重要性和2中打乱标签的特征中重要性进行比较；具体比较方式可以参考上面的kernel</p></li></ol><ul><li>当一个特征非常work，那它在正确标签的树模型中的importance应该很高，但它在打乱标签的树模型中的importance将很低（无法识别随机标签）；反之，一个垃圾特征，那它在正确标签的模型中importance很一般，打乱标签的树模型中importance将大于等于正确标签模型的importance。所以通过同时判断每个特征在正确标签的模型和打乱标签的模型中的importance（split和gain），可以选择特征稳定和work的特征。</li><li>思想大概就是这样吧，importance受到特征相关性的影响，特征的重要性会被相关特征的重要性稀释，看importance也不一定准，用这个来对暴力特征进行筛选还是可以的。</li></ul><h4 id="7-3-2实现步骤"><a href="#7-3-2实现步骤" class="headerlink" title="7.3.2实现步骤"></a>7.3.2实现步骤</h4><p>Null Importance算法的实现步骤为：</p><ol><li>在原始数据集上运行模型并且记录每个特征重要性。以此作为基准；</li><li>构建Null importances分布：对我们的标签进行随机Shuffle，并且计算shuffle之后的特征的重要性；</li><li>对2进行多循环操作，得到多个不同shuffle之后的特征重要性；</li><li>设计score函数，得到未shuffle的特征重要性与shuffle之后特征重要性的偏离度，并以此设计特征筛选策略；</li><li>计算不同筛选情况下的模型的分数，并进行记录；</li><li>将分数最好的几个分数对应的特征进行返回。实现步骤</li></ol><h4 id="7-3-3-读取数据集，计算Real-Targe和shuffle-Target下的特征重要度"><a href="#7-3-3-读取数据集，计算Real-Targe和shuffle-Target下的特征重要度" class="headerlink" title="7.3.3 读取数据集，计算Real Targe和shuffle Target下的特征重要度"></a>7.3.3 读取数据集，计算Real Targe和shuffle Target下的特征重要度</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.simplefilter(<span class="string">&#x27;ignore&#x27;</span>, UserWarning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">gc.enable()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"><span class="comment">#categorical_feats  = [&quot;AIRLINE&quot;,&quot;FLIGHT_NUMBER&quot;,&quot;DESTINATION_AIRPORT&quot;,&quot;ORIGIN_AIRPORT&quot;]</span></span><br><span class="line">categorical_feats = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> data[f].dtype == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#将上面四列特征转为类别特征，但不是one-hot编码</span></span><br><span class="line"><span class="keyword">for</span> f_ <span class="keyword">in</span> categorical_feats:</span><br><span class="line">    data[f_], _ = pd.factorize(data[f_])</span><br><span class="line">    <span class="comment"># Set feature type as categorical</span></span><br><span class="line">    data[f_] = data[f_].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><p>创建评分函数<br>feature<em>importances</em> :特征重要性的类型。default=’split’。</p><ul><li>如果是split，则结果包含该特征在模型中使用的次数。 </li><li>如果为“gain”，则结果包含使用该特征的分割的总增益。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span>(<span class="params">X_train, X_test, y_train, y_test,shuffle, seed=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 获取特征</span></span><br><span class="line">    train_features = <span class="built_in">list</span>(X_train.columns)   </span><br><span class="line">    <span class="comment"># 判断是否shuffle TARGET</span></span><br><span class="line">    y_train,y_test= y_train.copy(),y_test.copy()</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Here you could as well use a binomial distribution</span></span><br><span class="line">        y_train,y_test= y_train.copy().sample(frac=<span class="number">1.0</span>),y_test.copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    clf = lgb.train(params=lgb_params,train_set=lgb_train,valid_sets=lgb_eval,num_boost_round=<span class="number">10</span>, categorical_feature=categorical_feats)<span class="comment">#将object特征设置为分类特征，但是并不需要进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#得到特征重要性</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">&quot;feature&quot;</span>] = <span class="built_in">list</span>(train_features)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_gain&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;gain&#x27;</span>)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_split&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;split&#x27;</span>)</span><br><span class="line">    imp_df[<span class="string">&#x27;trn_score&#x27;</span>] = roc_auc_score(y_test, clf.predict( X_test))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="comment"># 获得市实际的特征重要性，即没有shuffletarget</span></span><br><span class="line">actual_imp_df = get_feature_importances(X_train, X_test, y_train, y_test, shuffle=<span class="literal">False</span>)</span><br><span class="line">actual_imp_df</span><br></pre></td></tr></table></figure><pre><code>[LightGBM] [Info] Number of positive: 1600, number of negative: 5594[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 1406[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7[1]    valid_0&#39;s binary_logloss: 0.479157[2]    valid_0&#39;s binary_logloss: 0.46882[3]    valid_0&#39;s binary_logloss: 0.454724[4]    valid_0&#39;s binary_logloss: 0.445913[5]    valid_0&#39;s binary_logloss: 0.440924[6]    valid_0&#39;s binary_logloss: 0.438309[7]    valid_0&#39;s binary_logloss: 0.433886[8]    valid_0&#39;s binary_logloss: 0.432747[9]    valid_0&#39;s binary_logloss: 0.431001[10]    valid_0&#39;s binary_logloss: 0.429621</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>feature</th>      <th>importance_gain</th>      <th>importance_split</th>      <th>trn_score</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>AIRLINE</td>      <td>153.229680</td>      <td>15</td>      <td>0.764829</td>    </tr>    <tr>      <th>1</th>      <td>FLIGHT_NUMBER</td>      <td>189.481180</td>      <td>23</td>      <td>0.764829</td>    </tr>    <tr>      <th>2</th>      <td>DESTINATION_AIRPORT</td>      <td>1036.401096</td>      <td>23</td>      <td>0.764829</td>    </tr>    <tr>      <th>3</th>      <td>ORIGIN_AIRPORT</td>      <td>650.938854</td>      <td>22</td>      <td>0.764829</td>    </tr>    <tr>      <th>4</th>      <td>AIR_TIME</td>      <td>119.763649</td>      <td>17</td>      <td>0.764829</td>    </tr>    <tr>      <th>5</th>      <td>DEPARTURE_TIME</td>      <td>994.109417</td>      <td>37</td>      <td>0.764829</td>    </tr>    <tr>      <th>6</th>      <td>DISTANCE</td>      <td>93.170790</td>      <td>13</td>      <td>0.764829</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">10</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_runs):</span><br><span class="line">    <span class="comment"># 获取当前的特征重要性</span></span><br><span class="line">    imp_df = get_feature_importances(X_train, X_test, y_train, y_test, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">&#x27;run&#x27;</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># 将特征重要性连起来</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 删除上一条信息</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dsp)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\b&#x27;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Display current run and time used</span></span><br><span class="line">    spent = (time.time() - start) / <span class="number">60</span></span><br><span class="line">    dsp = <span class="string">&#x27;Done with %4d of %4d (Spent %5.1f min)&#x27;</span> % (i + <span class="number">1</span>, nb_runs, spent)</span><br><span class="line">    <span class="built_in">print</span>(dsp, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_imp_df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>feature</th>      <th>importance_gain</th>      <th>importance_split</th>      <th>trn_score</th>      <th>run</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>AIRLINE</td>      <td>26.436000</td>      <td>8</td>      <td>0.525050</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>FLIGHT_NUMBER</td>      <td>142.159161</td>      <td>35</td>      <td>0.525050</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>DESTINATION_AIRPORT</td>      <td>231.459383</td>      <td>20</td>      <td>0.525050</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>ORIGIN_AIRPORT</td>      <td>319.862975</td>      <td>26</td>      <td>0.525050</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>AIR_TIME</td>      <td>97.764902</td>      <td>24</td>      <td>0.525050</td>      <td>1</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>2</th>      <td>DESTINATION_AIRPORT</td>      <td>254.016771</td>      <td>20</td>      <td>0.509197</td>      <td>10</td>    </tr>    <tr>      <th>3</th>      <td>ORIGIN_AIRPORT</td>      <td>271.220462</td>      <td>20</td>      <td>0.509197</td>      <td>10</td>    </tr>    <tr>      <th>4</th>      <td>AIR_TIME</td>      <td>82.260759</td>      <td>17</td>      <td>0.509197</td>      <td>10</td>    </tr>    <tr>      <th>5</th>      <td>DEPARTURE_TIME</td>      <td>137.511192</td>      <td>25</td>      <td>0.509197</td>      <td>10</td>    </tr>    <tr>      <th>6</th>      <td>DISTANCE</td>      <td>73.353821</td>      <td>19</td>      <td>0.509197</td>      <td>10</td>    </tr>  </tbody></table><p>70 rows × 5 columns</p></div><p>可视化演示</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_distributions</span>(<span class="params">actual_imp_df_, null_imp_df_, feature_</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">6</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 画出 Split importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_split&#x27;</span>].values, label=<span class="string">&#x27;Null importances&#x27;</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_split&#x27;</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.<span class="built_in">max</span>(a[<span class="number">0</span>]), color=<span class="string">&#x27;r&#x27;</span>,linewidth=<span class="number">10</span>, label=<span class="string">&#x27;Real Target&#x27;</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">&#x27;Split Importance of %s&#x27;</span> % feature_.upper(), fontweight=<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Null Importance (split) Distribution for %s &#x27;</span> % feature_.upper())</span><br><span class="line">    <span class="comment"># 画出 Gain importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_gain&#x27;</span>].values, label=<span class="string">&#x27;Null importances&#x27;</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_gain&#x27;</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.<span class="built_in">max</span>(a[<span class="number">0</span>]), color=<span class="string">&#x27;r&#x27;</span>,linewidth=<span class="number">10</span>, label=<span class="string">&#x27;Real Target&#x27;</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">&#x27;Gain Importance of %s&#x27;</span> % feature_.upper(), fontweight=<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Null Importance (gain) Distribution for %s &#x27;</span> % feature_.upper())</span><br><span class="line"></span><br><span class="line"><span class="comment">#画出“DESTINATION_AIRPORT”的特征重要性</span></span><br><span class="line">display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=<span class="string">&#x27;DESTINATION_AIRPORT&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="lightGBM_files/lightGBM_91_0.png" alt="png"></p><h4 id="7-3-4计算Score"><a href="#7-3-4计算Score" class="headerlink" title="7.3.4计算Score"></a>7.3.4计算Score</h4><ol><li>以未进行特征shuffle的特征重要性除以shuffle之后的0.75分位数作为我们的score<br>因为’MONTH’,’DAY’,’DAY_OF_WEEK’三个特征没有什么用，画的的图结果不好看，所以把这三个去掉了。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">&#x27;feature&#x27;</span>].unique():</span><br><span class="line">    f_null_imps_gain = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    f_act_imps_gain = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].mean()</span><br><span class="line">    gain_score = np.log(<span class="number">1e-10</span> + f_act_imps_gain / (<span class="number">1</span> + np.percentile(f_null_imps_gain, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    f_null_imps_split = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    f_act_imps_split = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].mean()</span><br><span class="line">    split_score = np.log(<span class="number">1e-10</span> + f_act_imps_split / (<span class="number">1</span> + np.percentile(f_null_imps_split, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    feature_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">scores_df = pd.DataFrame(feature_scores, columns=[<span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;split_score&#x27;</span>, <span class="string">&#x27;gain_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Plot Split importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;split_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;split_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt split importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># Plot Gain importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;gain_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;gain_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt gain importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line">null_imp_df.to_csv(<span class="string">&#x27;null_importances_distribution_rf.csv&#x27;</span>)</span><br><span class="line">actual_imp_df.to_csv(<span class="string">&#x27;actual_importances_ditribution_rf.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="lightGBM_files/lightGBM_93_0.png" alt="png"></p><ol><li>shuffle target之后特征重要性低于实际target对应特征的重要性0.25分位数的次数百分比。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">&#x27;feature&#x27;</span>].unique():</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    gain_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).<span class="built_in">sum</span>() / f_null_imps.size</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    split_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).<span class="built_in">sum</span>() / f_null_imps.size</span><br><span class="line">    correlation_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">corr_scores_df = pd.DataFrame(correlation_scores, columns=[<span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;split_score&#x27;</span>, <span class="string">&#x27;gain_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Plot Split importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;split_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=corr_scores_df.sort_values(<span class="string">&#x27;split_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt split importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># Plot Gain importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;gain_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=corr_scores_df.sort_values(<span class="string">&#x27;gain_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt gain importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.suptitle(<span class="string">&quot;Features&#x27; split and gain scores&quot;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">fig.subplots_adjust(top=<span class="number">0.93</span>)</span><br></pre></td></tr></table></figure><p><img src="lightGBM_files/lightGBM_95_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_scores</span><br></pre></td></tr></table></figure><pre><code>[(&#39;AIRLINE&#39;, 100.0, 100.0), (&#39;FLIGHT_NUMBER&#39;, 0.0, 60.0), (&#39;DESTINATION_AIRPORT&#39;, 100.0, 100.0), (&#39;ORIGIN_AIRPORT&#39;, 50.0, 100.0), (&#39;AIR_TIME&#39;, 10.0, 90.0), (&#39;DEPARTURE_TIME&#39;, 100.0, 100.0), (&#39;DISTANCE&#39;, 30.0, 100.0)]</code></pre><ol><li>计算特征筛选之后的最佳分数并记录相应特征<br>通过运行下面的代码，train_features选择不同的特征来拟合模型，最终Results for threshold  20/30效果最好。此时的模型特征为<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;=<span class="number">20</span>]</span><br><span class="line">split_feats</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;AIRLINE&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DESTINATION_AIRPORT&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ORIGIN_AIRPORT&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DEPARTURE_TIME&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DISTANCE&#x27;</span>]</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#此时的特征为</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_feature_selection</span>(<span class="params">data,train_features=<span class="literal">None</span>, cat_feats=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># Fit LightGBM </span></span><br><span class="line">    lgb_train = lgb.Dataset(data[train_features], data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>],free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line">    <span class="comment">#&quot;force_col_wise&quot;:true&#125;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    hist = lgb.cv(params=lgb_params,train_set=lgb_train,</span><br><span class="line">    num_boost_round=<span class="number">10</span>, categorical_feature=cat_feats,</span><br><span class="line">    nfold=<span class="number">5</span>,stratified=<span class="literal">True</span>,shuffle=<span class="literal">True</span>,early_stopping_rounds=<span class="number">5</span>,seed=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the last mean / std values </span></span><br><span class="line">    <span class="keyword">return</span> hist[<span class="string">&#x27;auc-mean&#x27;</span>][-<span class="number">1</span>], hist[<span class="string">&#x27;auc-stdv&#x27;</span>][-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># features = [f for f in data.columns if f not in [&#x27;SK_ID_CURR&#x27;, &#x27;TARGET&#x27;]]</span></span><br><span class="line"><span class="comment"># score_feature_selection(df=data[features], train_features=features, target=data[&#x27;TARGET&#x27;])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span> , <span class="number">40</span>, <span class="number">50</span> ,<span class="number">60</span> , <span class="number">70</span>, <span class="number">80</span> , <span class="number">90</span>, <span class="number">99</span>]:</span><br><span class="line">    split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    split_cat_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">    gain_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    gain_cat_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">                                                                                             </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Results for threshold %3d&#x27;</span> % threshold)</span><br><span class="line">    split_results = score_feature_selection(data,train_features=split_feats, cat_feats=split_cat_feats)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\t SPLIT : %.6f +/- %.6f&#x27;</span> % (split_results[<span class="number">0</span>], split_results[<span class="number">1</span>]))</span><br><span class="line">    gain_results = score_feature_selection(data,train_features=gain_feats, cat_feats=gain_cat_feats)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\t GAIN  : %.6f +/- %.6f&#x27;</span> % (gain_results[<span class="number">0</span>], gain_results[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Results for threshold   0     SPLIT : 0.757882 +/- 0.012114     GAIN  : 0.757882 +/- 0.012114Results for threshold  10     SPLIT : 0.756999 +/- 0.011506     GAIN  : 0.757882 +/- 0.012114Results for threshold  20     SPLIT : 0.757959 +/- 0.012558     GAIN  : 0.757882 +/- 0.012114Results for threshold  30     SPLIT : 0.757959 +/- 0.012558     GAIN  : 0.757882 +/- 0.012114Results for threshold  40     SPLIT : 0.745729 +/- 0.013217     GAIN  : 0.757882 +/- 0.012114Results for threshold  50     SPLIT : 0.745729 +/- 0.013217     GAIN  : 0.757882 +/- 0.012114Results for threshold  60     SPLIT : 0.727063 +/- 0.006758     GAIN  : 0.757882 +/- 0.012114Results for threshold  70     SPLIT : 0.727063 +/- 0.006758     GAIN  : 0.756999 +/- 0.011506Results for threshold  80     SPLIT : 0.727063 +/- 0.006758     GAIN  : 0.756999 +/- 0.011506Results for threshold  90     SPLIT : 0.727063 +/- 0.006758     GAIN  : 0.756999 +/- 0.011506Results for threshold  99     SPLIT : 0.727063 +/- 0.006758     GAIN  : 0.757959 +/- 0.012558</code></pre><h2 id="八、自定义损失函数和评测函数"><a href="#八、自定义损失函数和评测函数" class="headerlink" title="八、自定义损失函数和评测函数"></a>八、自定义损失函数和评测函数</h2><blockquote><p>参考<a href="https://blog.csdn.net/zwqjoy/article/details/121289448?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164217860316780255221706%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164217860316780255221706&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-121289448.pc_search_insert_es_download&amp;utm_term=lgb%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187">XGB/LGB—-自定义损失函数与评价函数</a><br><a href="https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py">参考示例</a></p><ul><li>自定义损失函数，预测概率小于0.1的正样本（标签为正样本，但模型预测概率小于0.1），梯度增加一倍。</li><li>自定义评价函数，阈值大于0.8视为正样本（标签为正样本，但模型预测概率大于0.8）。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#正常模型效果</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征去掉&#x27;MONTH&#x27;,&#x27;DAY&#x27;,&#x27;DAY_OF_WEEK&#x27;三个没用的之后，正常模型阈值0.5时accuarcy: 83.03% auc_score: 83.67%</span></span><br><span class="line"><span class="comment">#acc阈值 0.8时accuarcy: 80.40% auc_score: 83.67%</span></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">lgb_params = &#123;</span><br><span class="line"><span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,<span class="comment">#别名binary_error</span></span><br><span class="line"><span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line"><span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line"><span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line"><span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line"><span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line"><span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"><span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line"><span class="string">&quot;verbosity&quot;</span>:-<span class="number">5</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">clf2 = lgb.train(params=lgb_params,train_set=lgb_train,valid_sets=lgb_eval,num_boost_round=<span class="number">10</span>, </span><br><span class="line">                categorical_feature=categorical_feats)</span><br><span class="line"></span><br><span class="line">y_pred = clf2.predict(X_test,num_iteration=clf2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>[1]    valid_0&#39;s binary_logloss: 0.479157[2]    valid_0&#39;s binary_logloss: 0.46882[3]    valid_0&#39;s binary_logloss: 0.454724[4]    valid_0&#39;s binary_logloss: 0.445913[5]    valid_0&#39;s binary_logloss: 0.440924[6]    valid_0&#39;s binary_logloss: 0.438309[7]    valid_0&#39;s binary_logloss: 0.433886[8]    valid_0&#39;s binary_logloss: 0.432747[9]    valid_0&#39;s binary_logloss: 0.431001[10]    valid_0&#39;s binary_logloss: 0.429621accuarcy: 81.69% auc_score: 76.48%</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义目标函数，预测概率小于0.1的正样本（标签为正样本，但模型预测概率小于0.1），梯度增加一倍。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span>(<span class="params">preds, train_data</span>):</span></span><br><span class="line">    labels=train_data.get_label()</span><br><span class="line">    preds=<span class="number">1.</span>/(<span class="number">1.</span>+np.exp(-preds))</span><br><span class="line">    </span><br><span class="line">    grad=[(p-l) <span class="keyword">if</span> p&gt;=<span class="number">0.1</span> <span class="keyword">else</span> <span class="number">2</span>*(p-l) <span class="keyword">for</span> (p,l) <span class="keyword">in</span> <span class="built_in">zip</span>(preds,labels) ]</span><br><span class="line">    hess=[p*(<span class="number">1.</span>-p) <span class="keyword">if</span> p&gt;=<span class="number">0.1</span> <span class="keyword">else</span> <span class="number">2</span>*p*(<span class="number">1.</span>-p) <span class="keyword">for</span> p <span class="keyword">in</span> preds ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad, hess</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义评价指标binary_error，阈值大于0.8视为正样本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_error</span>(<span class="params">preds, train_data</span>):</span></span><br><span class="line">    labels = train_data.get_label()</span><br><span class="line">    preds = <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-preds))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;error&#x27;</span>, np.mean(labels != (preds &gt; <span class="number">0.8</span>)), <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">clf3 = lgb.train(lgb_params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=clf2,</span><br><span class="line">                fobj=loglikelihood, <span class="comment"># 目标函数</span></span><br><span class="line">                feval=binary_error, <span class="comment"># 评价指标</span></span><br><span class="line">                valid_sets=lgb_eval)</span><br><span class="line"></span><br><span class="line">y_pred = clf3.predict(X_test,num_iteration=clf3.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.8</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy2 = accuracy_score(y_test,pred)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>[11]    valid_0&#39;s binary_logloss: 4.68218    valid_0&#39;s error: 0.196414[12]    valid_0&#39;s binary_logloss: 4.51541    valid_0&#39;s error: 0.196414[13]    valid_0&#39;s binary_logloss: 4.4647    valid_0&#39;s error: 0.19558[14]    valid_0&#39;s binary_logloss: 4.5248    valid_0&#39;s error: 0.196414[15]    valid_0&#39;s binary_logloss: 4.51904    valid_0&#39;s error: 0.196414[16]    valid_0&#39;s binary_logloss: 4.52481    valid_0&#39;s error: 0.196414[17]    valid_0&#39;s binary_logloss: 4.4928    valid_0&#39;s error: 0.196414[18]    valid_0&#39;s binary_logloss: 4.43027    valid_0&#39;s error: 0.196414[19]    valid_0&#39;s binary_logloss: 4.4285    valid_0&#39;s error: 0.196414[20]    valid_0&#39;s binary_logloss: 4.42314    valid_0&#39;s error: 0.196831accuarcy: 81.19% auc_score: 76.47%</code></pre><h2 id="九-模型部署与加速"><a href="#九-模型部署与加速" class="headerlink" title="九 模型部署与加速"></a>九 模型部署与加速</h2><blockquote><p><a href="https://treelite.readthedocs.io/en/latest/tutorials/import.html">参考文档</a><br><a href="https://blog.csdn.net/sinat_26917383/article/details/113287642?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164218367916780271548371%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164218367916780271548371&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-113287642.pc_search_insert_es_download&amp;utm_term=import+treelite.sklearn%E5%A4%B1%E8%B4%A5&amp;spm=1018.2226.3001.4187">python+Treelite：Sklearn树模型训练迁移到c、java部署</a></p></blockquote><p>由于 Treelite 的范围仅限于预测，因此必须使用其他机器学习包来训练决策树集成模型。在本文档中，我们将展示如何导入已在其他地方训练过的集成模型。</p><p>import treelite失败，无法导入，不知道为什么</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm9 = lgb.LGBMClassifier()</span><br><span class="line">gbm9.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = gbm9.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gbm9.predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line">gbm9.booster_.save_model(<span class="string">&quot;model9.txt&quot;</span>)<span class="comment">#保存模型为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> treelite</span><br><span class="line"><span class="keyword">import</span> treelite.sklearn</span><br><span class="line">model = treelite.sklearn.import_model(gbm9)<span class="comment">#导入 scikit-learn 模型</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy2 = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,model.predict_proba(X_test)[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/Python-Intro.html&quot; title=&quot;官方文档&quot;&gt;LightGBM 官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;阿水知乎贴：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/266865429&quot; title=&quot;《你应该知道的LightGBM各种操作》&quot;&gt;《你应该知道的LightGBM各种操作》&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/Python-API.html&quot; title=&quot;Python API&quot;&gt;Python API（包括Scikit-learn API）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://coggle.club/blog/30days-of-ml-202201&quot; title=&quot;《Coggle 30 Days of ML（22年1&amp;amp;2月）》&quot;&gt;《Coggle 30 Days of ML（22年1&amp;amp;2月）》&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学习内容：&lt;/p&gt;
&lt;p&gt;LightGBM（Light Gradient Boosting Machine）是微软开源的一个实现 GBDT 算法的框架，支持高效率的并行训练。LightGBM 提出的主要原因是为了解决 GBDT 在海量数据遇到的问题。本次学习内容包括使用LightGBM完成各种操作，包括竞赛和数据挖掘中的模型训练、验证和调参过程。&lt;/p&gt;
&lt;p&gt;打卡汇总：&lt;/p&gt;
&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;任务名称&lt;/th&gt;
&lt;th&gt;难度、分数&lt;/th&gt;
&lt;th&gt;所需技能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;任务1模型训练与预测&lt;/td&gt;
&lt;td&gt;低、1&lt;/td&gt;
&lt;td&gt;LightGBM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务2：模型保存与加载&lt;/td&gt;
&lt;td&gt;低、1&lt;/td&gt;
&lt;td&gt;LightGBM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务3：分类、回归和排序任务&lt;/td&gt;
&lt;td&gt;高、3&lt;/td&gt;
&lt;td&gt;LightGBM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务4：模型可视化&lt;/td&gt;
&lt;td&gt;低、1&lt;/td&gt;
&lt;td&gt;graphviz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务5：模型调参（网格、随机、贝叶斯）&lt;/td&gt;
&lt;td&gt;中、2&lt;/td&gt;
&lt;td&gt;模型调参&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务6：模型微调与参数衰减&lt;/td&gt;
&lt;td&gt;中、2&lt;/td&gt;
&lt;td&gt;LightGBM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务7：特征筛选方法&lt;/td&gt;
&lt;td&gt;高、3&lt;/td&gt;
&lt;td&gt;特征筛选方法&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务8：自定义损失函数&lt;/td&gt;
&lt;td&gt;中、2&lt;/td&gt;
&lt;td&gt;损失函数&amp;amp;评价函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;任务9：模型部署与加速&lt;/td&gt;
&lt;td&gt;高、3&lt;/td&gt;
&lt;td&gt;Treelite&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&quot;一、使用LGBMClassifier对iris进行训练&quot;&gt;&lt;a href=&quot;#一、使用LGBMClassifier对iris进行训练&quot; class=&quot;headerlink&quot; title=&quot;一、使用LGBMClassifier对iris进行训练&quot;&gt;&lt;/a&gt;一、使用LGBMClassifier对iris进行训练&lt;/h2&gt;</summary>
    
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Boosting" scheme="https://zhxnlp.github.io/tags/Boosting/"/>
    
    <category term="Bagging" scheme="https://zhxnlp.github.io/tags/Bagging/"/>
    
    <category term="Stacking" scheme="https://zhxnlp.github.io/tags/Stacking/"/>
    
  </entry>
  
  <entry>
    <title>集成学习4： 整理总结</title>
    <link href="https://zhxnlp.github.io/2021/12/24/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A04%EF%BC%9A%E6%95%B4%E7%90%86%E6%80%BB%E7%BB%93%20/"/>
    <id>https://zhxnlp.github.io/2021/12/24/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A04%EF%BC%9A%E6%95%B4%E7%90%86%E6%80%BB%E7%BB%93%20/</id>
    <published>2021-12-23T21:07:27.000Z</published>
    <updated>2022-01-03T20:52:11.427Z</updated>
    
    <content type="html"><![CDATA[<p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。</p><h2 id="一、bagging"><a href="#一、bagging" class="headerlink" title="一、bagging"></a>一、bagging</h2><h3 id="1-1-bagging基本原理"><a href="#1-1-bagging基本原理" class="headerlink" title="1.1 bagging基本原理"></a>1.1 bagging基本原理</h3><ul><li>自助采样(bootstrap)：有放回的从数据集中进行采样。</li><li>Bagging：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差</li><li>Bagging的一个典型应用是随机森林。由许多“树”bagging组成的。每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）<span id="more"></span><h3 id="1-2-决策树"><a href="#1-2-决策树" class="headerlink" title="1.2 决策树"></a>1.2 决策树</h3></li><li>决策树，它是一种树形结构，树的每个非叶子节点表示对样本在一个特征上的判断，节点下方的分支代表对样本的划分。</li><li>每次划分中，首先要选择用于划分的特征，之后要确定划分的方案（类别/阈值）。我们希望通过划分，决策树的分支节点所包含的样本“纯度”尽可能地高。节点划分过程中所用的指标主要是<code>max信息增益</code>或者<code>min GINI系数</code></li><li><code>信息增益 IG=划分前的信息熵H(Y)-划分后的条件熵H(Y|X)</code></li><li>选择信息增益最大或者gini指数最小的划分方式，划分过程直到样本的类别被完全分开，所有特征都已使用，或达到树的最大深度为止。<h2 id="二、boosting"><a href="#二、boosting" class="headerlink" title="二、boosting"></a>二、boosting</h2><img src="https://img-blog.csdnimg.cn/44c45807c23048b8a6bd97f353e89bf3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><h3 id="2-1-Boosting算法原理"><a href="#2-1-Boosting算法原理" class="headerlink" title="2.1 Boosting算法原理"></a>2.1 Boosting算法原理</h3><ul><li>Boosting：使用同一组数据集进行反复学习，得到一系列简单模型，然后组合这些模型构成一个预测性能十分强大的机器学习模型。</li><li>Boosting通过不断减少偏差的形式提高最终的预测效果，与Bagging有着本质的不同。</li></ul><p>Boosting方法关键点：</p><ol><li>每一轮学习应该如何改变数据的概率分布</li><li>如何将各个弱分类器组合起来<h3 id="2-2-Adaboost算法原理"><a href="#2-2-Adaboost算法原理" class="headerlink" title="2.2 Adaboost算法原理"></a>2.2 Adaboost算法原理</h3>Adaboost不改变训练数据，而是改变其权值分布，使每一轮的基学习器学习不同权重分布的样本集，最后加权组合表决组合。<br>Adaboost解决上述的两个问题的方式是：</li></ol><ul><li>提高那些被前一轮分类器错误分类的样本的权重&lt;/font&gt;，来改变数据的概率分布</li><li>各个弱分类器通过采取加权多数表决的方式组合&lt;/font&gt;。</li><li>Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</li><li>加法模型：最终的强分类器是由若干个弱分类器加权平均得到的。</li></ul><p>简单来说：训练M个基本分类器，计算每个分类器的错误率、模型权重及样本权重。</p><ol><li>均匀初始化样本权重$D_{1}$</li><li>对于轮次m，针对当前权重$D<em>{m}$ 学习分类器 $G</em>{m}(x)$，并计算其分类错误率$e<em>{m}$。<br>$$e</em>{m}=\sum<em>{i=1}^{N} P\left(G</em>{m}\left(x<em>{i}\right) \neq y</em>{i}\right)=\sum<em>{i=1}^{N} w</em>{m i} I\left(G<em>{m}\left(x</em>{i}\right) \neq y<em>{i}\right)$$$w</em>{m i}$代表了在$G_m(x)$中分类错误的样本权重和，这点直接说明了权重分布$D_m$与$G_m(x)$的分类错误率$e_m$有直接关系。</li><li>计算分类器$G<em>m(x)$的权重系数$\alpha</em>{m}=\frac{1}{2} \log \frac{1-e<em>{m}}{e</em>{m}}$&lt;/font&gt;。$e<em>{m} \leqslant \frac{1}{2}$时，$\alpha</em>{m} \geqslant 0$，并且$\alpha_m$随着$e_m$的减少而增大，因此分类错误率越小的基本分类器在最终分类器的作用越大！</li><li>更新权重分布 <script type="math/tex">w_{m+1, i}=\left\{\begin{array}{ll}\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}\end{array}\right.</script><br>这里的$Z<em>m$是规范化因子，使得$D</em>{m+1}$成为概率分布。<br>一般来说$\alpha<em>{m} \geqslant 0，e^0=1$。被基本分类器$G_m(x)$错误分类的样本的权重扩大，被正确分类的样本权重减少。$e</em>{m}$减小，$\alpha<em>{m}$增大，${w</em>{m+1, i}}$增大。</li><li>基本分类器加权组合表决<script type="math/tex; mode=display">f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)</script><script type="math/tex; mode=display">\begin{aligned}G(x) &=\operatorname{sign}(f(x)) \\&=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)\end{aligned}</script><script type="math/tex; mode=display">sign(x)=\begin{cases}1 & \text{ if } x\geqslant 0 \\ -1 & \text{ if } x< 0 \end{cases}</script>线性组合$f(x)$实现了将M个基本分类器的加权表决，系数$\alpha_m$标志了基本分类器$G_m(x)$的重要性，值得注意的是：所有的$\alpha_m$之和不为1。$f(x)$的符号决定了样本x属于哪一类,其绝对值表示分类的确信度。</li></ol><p>下面是用前向分布算法，从指数损失函数到分类错误率$e<em>{m}$、分类器$G_m(x)$的权重系数$\alpha</em>{m}$、样本权重更新公式的推导：<br><img src="https://img-blog.csdnimg.cn/9935e62b675c41bd8e83059a091d97ff.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e48a0ed59a15493aa99f33f0e89a6ce6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/ee0e0b810f93461a8ebd4d101bc0da29.png" alt="在这里插入图片描述"><br>这样就得到了样本权重更新公式。</p><h3 id="2-3-GBDT"><a href="#2-3-GBDT" class="headerlink" title="2.3 GBDT"></a>2.3 GBDT</h3><ul><li>GBDT 的全称是 <code>Gradient Boosting Decision Tree</code>，梯度提升树。GBDT使用的决策树是<code>CART回归树</code>。因为<code>GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树</code>。</li><li>CART假设决策树都是二叉树，内部节点特征取值为“是”和“否”，等价于递归二分每个特征。对回归树用平方误差最小化准则（回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适），对分类树用基尼系数最小化准则，进行特征选择生成二叉树</li><li>回归问题没有分类错误率可言，用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题。</li></ul><p>GBDT和Adaboost区别：</p><ol><li>拟合思路</li></ol><ul><li>Adaboost算法：使用了<code>分类错误率修正样本权重以及计算每个基本分类器的权重</code>。通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。</li><li>GBDT：拟合残差来学习基模型。残差定义为损失函数相对于前一轮组合树模型的负梯度方向的值作为残差的近似值&lt;/font &gt;（损失函数的负梯度在当前模型的值）希望最快速度地最小化预测值与真实值之间的差异。</li><li>除了均方差损失函数时，负梯度值等于残差。<script type="math/tex; mode=display">r_{m i}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}</script></li><li>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而<code>对与分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本</code>。</li></ul><ol><li>Adaboost是分类树，GBDT是回归树，但是也可以做分类。</li></ol><ul><li>AdaBoost 是通过提升错分数据点的权重来定位模型的不足，而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost, Gradient Boosting 可以使用更多种类的目标函数（5种）AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数。</li><li>基于残差 GBDT 容易对异常值敏感：<br><img src="https://img-blog.csdnimg.cn/1b459870e18d41d89c40e6aed450e8bd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><p>很明显后续的模型会对第 4 个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<code>绝对损失或者 Huber 损失函数来代替平方损失函数</code>。</p><h3 id="2-4-XGBoost"><a href="#2-4-XGBoost" class="headerlink" title="2.4 XGBoost"></a>2.4 XGBoost</h3><h3 id="2-4-1-目标函数"><a href="#2-4-1-目标函数" class="headerlink" title="2.4.1 目标函数"></a>2.4.1 目标函数</h3><p>==XGBoost 是 Boosting 框架的一种实现结构， lightgbm 也是一种框架实现结构，而 GBDT 则是一种算法实现。XGBoost本质也是GBDT， 相比于 GBDT 的差别主要就是 XGBoost 做的优化。==</p><ol><li>构造目标函数为：<script type="math/tex; mode=display">\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)</script>$\sum<em>{i} l\left(\hat{y}</em>{i}, y<em>{i}\right)$为loss function，$\sum</em>{k} \Omega\left(f_{k}\right)$为正则化项。</li><li>叠加式训练 <script type="math/tex">\hat{y}_i^{(K)} = \hat{y}_i^{(K-1)} + f_K(x_i)</script> 其中，$\hat{y}_i^{(K-1)}$ 为前K-1棵树的预测结果，$f_K(x_i)$ 为第K棵树的预测结果。 </li><li>目标函数分解：<br>由于正则化项也可以分解为前K-1棵树的复杂度加第K棵树的复杂度，因此：<script type="math/tex">\mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathrm{x}_{i}\right)\right)+\sum_{k=1} ^{K-1}\Omega\left(f_{k}\right)+\Omega\left(f_{K}\right)</script></li></ol><p>由于$\sum<em>{k=1} ^{K-1}\Omega\left(f</em>{k}\right)$在模型构建到第K棵树的时候已经固定，无法改变，因此是一个已知的常数，可以在最优化的时候省去，故：                     </p><script type="math/tex; mode=display">\mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathrm{x}_{i}\right)\right)+\Omega\left(f_{K}\right)</script><ol><li>使用泰勒级数<strong>近似</strong>目标函数：                                      <script type="math/tex; mode=display">\mathcal{L}^{(K)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(K-1)}\right)+g_{i} f_{K}\left(\mathrm{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathrm{x}_{i}\right)\right]+\Omega\left(f_{K}\right)</script>其中，$g<em>{i}=\partial</em>{\hat{y}(t-1)} l\left(y<em>{i}, \hat{y}^{(t-1)}\right)$和$h</em>{i}=\partial<em>{\hat{y}^{(t-1)}}^{2} l\left(y</em>{i}, \hat{y}^{(t-1)}\right)$           </li></ol><p>由于$\sum<em>{i=1}^{n}l\left(y</em>{i}, \hat{y}^{(K-1)}\right)$在模型构建到第K棵树的时候已经固定，无法改变，因此是一个已知的常数，可以在最优化的时候省去，故：                               </p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(K)}=\sum_{i=1}^{n}\left[g_{i} f_{K}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{K}\right)</script><h3 id="2-4-2-正则项"><a href="#2-4-2-正则项" class="headerlink" title="2.4.2 正则项"></a>2.4.2 正则项</h3><p>模型复杂度$\Omega\left(f<em>{K}\right)$，它可以<code>由叶子节点的个数以及节点函数值来构建</code>，则：$\Omega\left(f</em>{K}\right) = \gamma T+\frac{1}{2} \lambda \sum<em>{j=1}^{T} w</em>{j}^{2}$<br>目标函数用以上符号替代后：                                      </p><script type="math/tex; mode=display">\begin{aligned}\tilde{\mathcal{L}}^{(K)} &=\sum_{i=1}^{n}\left[g_{i} f_{K}\left(\mathrm{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathrm{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T\end{aligned}</script><p>我们的目标就是最小化目标函数,根据二次函数求极值的公式：$y=ax^2 +bx +c$求极值有：</p><script type="math/tex; mode=display">w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}</script><p>以及</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(K)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T</script><p>分割节点的标准为$max{\tilde{\mathcal{L}}^{(old)} - \tilde{\mathcal{L}}^{(new)} }$，即：                               </p><script type="math/tex; mode=display">\mathcal{L}_{\text {split }}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma</script><p><code>节点分裂标准是：目标函数的值在分裂前后的差值最大</code>      </p><h3 id="2-4-3-分割策略"><a href="#2-4-3-分割策略" class="headerlink" title="2.4.3 分割策略"></a>2.4.3 分割策略</h3><p>为了找到找到最优特征及最优切分点，有三种策略：</p><ol><li>精确贪心分裂算法：首先找到所有的候 选特征及所有的候选切分点, 求其 $\mathcal{L}<em>{\text {split }}$, 然后 选择使$\mathcal{L}</em>{\mathrm{split}}$ 最大的特征及 对应切分点作为最优特征和最优切分点。节点分裂时只选择当前最优的分裂策略, 而非全局最优的分裂策略。<ul><li>精确贪心算法优点：它计算了所有特征、所有切分点的收益, 并从中选择了最优的, 从而保证模型能比较好地拟合了训练数据。</li><li>精确贪心算法缺点：当数据不能完全加载到内存时非常低效。算法在计算过程中需要不断在内存与磁盘之间进行数据交换，非常耗时, 并且在分布式环境中面临同样的问题</li></ul></li><li>基于直方图的近似算法：更高效地选 择最优特征及切分点<ol><li>对某一特征寻找最优切分点时，首先对该特征的所有切分点按分位数 (如百分位) 分桶, 得到一个候选切分点集。</li><li>特征的每一个切分点都可以分到对应的分桶，对每个桶计算特征统计G和H得到直方图, G为该桶内所有样本一阶特征统计g之和, H为该桶内所有样本二阶特征统计h之和</li><li>选择所有候选特征及候选切分点中对应桶的特征统计收益最大的作为最优特征及最优切分点</li></ol></li><li>近似算法实现了两种候选切分点的构建策略：全局策略和本地策略。<br>Global全局策略：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割，整个过程候选切分点集合不改变<br>Local本地策略：<code>每次分裂前将重新提出候选切分点</code>。<ul><li>全局策略是在树构建的初始阶段对每一个特征确定一个候选切分点的集合, 并在该树每一层的节点分裂中均采用此集合计算收益, 整个过程候选切分点集合不改变。 Global 策略因为节点没有划分所以需要更多的候选点，即需要更细的分桶才能达到本地策略的精确度。</li><li>本地策略则是在每一次节点分裂时均重新确定候选切分点。</li><li>在XGBoost系统中, 用户可以根据需求自由选择使用精确贪心算法、近似算法全局策略、近似算法本地策略, 算法均可通过参数进行配置。<h3 id="2-5-XGBoost和GBDT主要区别"><a href="#2-5-XGBoost和GBDT主要区别" class="headerlink" title="2.5 XGBoost和GBDT主要区别"></a>2.5 XGBoost和GBDT主要区别</h3>XGBoost优点：</li></ul></li><li><code>灵活性更强</code>：传统GBDT以CART作为基分类器，<code>xgboost还支持线性分类器</code>。xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li><code>用到二阶导数和正则项，精度更高</code>。 GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。并在代价函数里加入了正则项，用于控制模型的复杂度，使学习出来的模型更加简单，防止过拟合。&lt;/font&gt;并且拟合方向更准、速度更快。（模型复杂度$\Omega\left(f_{K}\right)$由叶子节点的个数以及节点函数值来构建）</li><li>xgboost支持<code>特征粒度上的并行</code>，而不是tree粒度的并行。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。&lt;/font&gt;这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么`各个特征的增益计算就可以开多线程进行。</li><li><code>近似直方图算法</code>。树节点在进行分裂时，贪心算法枚举所有可能的分割点，计算了所有特征、所有切分点的收益。当数据无法一次载入内存或者在分布式情况下，计算时需要不断在内存与磁盘之间进行数据交换，非常耗时、效率很低&lt;/font&gt;。近似直方图算法，用于高效地生成候选的分割点。（就是 XGBoost 论文中介绍的加权直方图，这里权值是特征的二阶梯度，因为其目标函数可以化简为二次项系数为 H 的二次多项式）</li><li>Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li><li>支持列抽样和缺失值处理。x列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向（稀疏感知算法）。</li><li>XGBoost, GBDT 均支持自定义损失函数，但 XGBoost 进行基分类器拟合的时候需要一阶、二阶梯度信息，故而需要自定义损失函数提供一阶、二阶梯度信息，而 GBDT 只需提供自定义损失函数的一阶梯度信息。</li></ol><hr><p>XGBoost缺点：</p><ul><li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li><li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存</li></ul><p>2.6 LightGBM<br>轻量级（Light）的梯度提升机（GBM），主要用于解决 GDBT 在海量数据中遇到的问题。其相对 XGBoost 具有训练速度快、内存占用低的特点。<br>下图分别显示了 XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 三者之间针对不同数据集情况下的内存和训练时间的对比：<br><img src="https://img-blog.csdnimg.cn/66b9384db24444e981921f659875ae75.png" alt="在这里插入图片描述"><br>LightGBM 为了解决XGBoost的问题提出了以下几点解决方案：</p><ol><li>单边梯度抽样算法；</li><li>直方图算法；</li><li>互斥特征捆绑算法；</li><li>基于最大深度的 Leaf-wise 的垂直生长算法；</li><li>类别特征最优分割；</li><li>特征并行和数据并行；</li><li>缓存优化。<h3 id="2-6-1-单边梯度抽样算法"><a href="#2-6-1-单边梯度抽样算法" class="headerlink" title="2.6.1 单边梯度抽样算法"></a>2.6.1 单边梯度抽样算法</h3></li></ol><ul><li>梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好。单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）<code>保留了梯度大的样本，并对梯度小的样本进行随机抽样，减少了大量梯度小的样本</code>，极大的减少了计算量。(在接下来的计算锅中只需关注梯度高的样本)</li><li>为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。</li></ul><p>GOSS 事先基于梯度的绝对值对样本进行排序（无需保存排序后结果），然后拿到前 a% 的梯度大的样本，和总体样本的 b%，在计算增益时，通过乘上$\frac{1-a}{b}$来放大梯度小的样本的权重。一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</p><h3 id="2-6-2-直方图算法"><a href="#2-6-2-直方图算法" class="headerlink" title="2.6.2 直方图算法"></a>2.6.2 直方图算法</h3><ul><li>直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用<code>直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点</code>。</li><li>直方图算法存储方便、内存占用更小、运算更快、鲁棒性强、模型更加稳定</li><li>虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。</li><li>直方图加速。在构建叶节点的直方图时，可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。<br><img src="https://img-blog.csdnimg.cn/d8c248f7ff9c48c3a8aad100067421f6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>只用非零特征构建直方图。（XGBoost也一样）<br>其它见帖子<a href="https://zhuanlan.zhihu.com/p/87885678?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《决策树（下）——XGBoost、LightGBM（非常详细）》</a>。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。&lt;/p&gt;
&lt;h2 id=&quot;一、bagging&quot;&gt;&lt;a href=&quot;#一、bagging&quot; class=&quot;headerlink&quot; title=&quot;一、bagging&quot;&gt;&lt;/a&gt;一、bagging&lt;/h2&gt;&lt;h3 id=&quot;1-1-bagging基本原理&quot;&gt;&lt;a href=&quot;#1-1-bagging基本原理&quot; class=&quot;headerlink&quot; title=&quot;1.1 bagging基本原理&quot;&gt;&lt;/a&gt;1.1 bagging基本原理&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;自助采样(bootstrap)：有放回的从数据集中进行采样。&lt;/li&gt;
&lt;li&gt;Bagging：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差&lt;/li&gt;
&lt;li&gt;Bagging的一个典型应用是随机森林。由许多“树”bagging组成的。每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）</summary>
    
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Boosting" scheme="https://zhxnlp.github.io/tags/Boosting/"/>
    
    <category term="Bagging" scheme="https://zhxnlp.github.io/tags/Bagging/"/>
    
    <category term="Stacking" scheme="https://zhxnlp.github.io/tags/Stacking/"/>
    
  </entry>
  
  <entry>
    <title>task3：新闻推荐系统项目调试</title>
    <link href="https://zhxnlp.github.io/2021/12/17/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task3%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/"/>
    <id>https://zhxnlp.github.io/2021/12/17/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task3%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/</id>
    <published>2021-12-16T17:08:37.000Z</published>
    <updated>2022-01-02T20:50:36.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-项目运行环境"><a href="#0-项目运行环境" class="headerlink" title="0    项目运行环境"></a>0    项目运行环境</h2><p>0.1    获取软件安装包<br>软件安装包地址：<a href="https://share.weiyun.com/u3ZIjZfg">https://share.weiyun.com/u3ZIjZfg</a><br> <span id="more"></span><br>0.2    使用软件版本<br>操作系统：Windows10<br>MySQL：8.0.25<br>Redis：5.0.14<br>Mongodb：5.0.5<br>Mini-Conda Python 3.8<br>Node.js：16.13.1<br>前端IDE：WebStorm 2021.1<br>后端IDE：PyCharm Professional 2021.1<br>访问MySQL和Mongodb的数据库工具：DataGrip 2021.1<br>访问Redis的工具：redis-desktop-manager-0.9.9.99.exe</p><h2 id="1-项目下载与IDE导入"><a href="#1-项目下载与IDE导入" class="headerlink" title="1    项目下载与IDE导入"></a>1    项目下载与IDE导入</h2><p>项目地址：<br><a href="https://github.com/datawhalechina/fun-rec">https://github.com/datawhalechina/fun-rec</a><br>1.1    前端项目导入<br>使用WebStrom IDE工具，导入前端项目</p><p>1.2    后端项目导入<br>使用PyCharm IDE工具，导入后端项目</p><h2 id="2-数据库安装与使用（Windows10）"><a href="#2-数据库安装与使用（Windows10）" class="headerlink" title="2    数据库安装与使用（Windows10）"></a>2    数据库安装与使用（Windows10）</h2><h3 id="2-1-MySQL数据库安装与使用"><a href="#2-1-MySQL数据库安装与使用" class="headerlink" title="2.1    MySQL数据库安装与使用"></a>2.1    MySQL数据库安装与使用</h3><p>卸载mysql：<a href="https://blog.csdn.net/dh12313012/article/details/87274385?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163976655316780269838594%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163976655316780269838594&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-87274385.pc_search_insert_es_download_v2&amp;utm_term=centos%E5%AE%89%E8%A3%85%E5%8D%B8%E8%BD%BDmysql&amp;spm=1018.2226.3001.4187">帖子1</a>、<a href="https://blog.csdn.net/weixin_44443884/article/details/106231811?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163976655316780269838594%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163976655316780269838594&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-106231811.pc_search_insert_es_download_v2&amp;utm_term=centos%E5%AE%89%E8%A3%85%E5%8D%B8%E8%BD%BDmysql&amp;spm=1018.2226.3001.4187">帖子2</a><br>参考胡瑞峰文档和帖子<a href="https://blog.csdn.net/hu10131013/article/details/107711192?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163957716016780269846658%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163957716016780269846658&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107711192.pc_search_insert_es_download_v2&amp;utm_term=windows%E5%AE%89%E8%A3%85mysql&amp;spm=1018.2226.3001.4187">《Windows环境安装 安装mysql-8.0.18-winx64详细图解(zip包版本)》</a></p><p>2.0 centos装的mysql8无法启动，运行<code>service mysql start</code>显示这个命令找不到</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不装这个下面安装报错，缺少安装包</span></span><br><span class="line">wget http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/<span class="number">5.5</span><span class="number">.37</span>-<span class="number">25.10</span>/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-<span class="number">55</span>-<span class="number">5.5</span><span class="number">.37</span>-<span class="number">25.10</span><span class="number">.756</span>.el6.x86_64.rpm</span><br><span class="line">rpm -ivh Percona-XtraDB-Cluster-shared-<span class="number">55</span>-<span class="number">5.5</span><span class="number">.37</span>-<span class="number">25.10</span><span class="number">.756</span>.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">yum install -y mariadb-server</span><br></pre></td></tr></table></figure><h4 id="2-1-1-MySQL数据库安装"><a href="#2-1-1-MySQL数据库安装" class="headerlink" title="2.1.1    MySQL数据库安装"></a>2.1.1    MySQL数据库安装</h4><p>（1）安装包下载<br>下载地址：<a href="https://dev.mysql.com/downloads/mysql/">https://dev.mysql.com/downloads/mysql/</a><br>安装包版本：8.0.25</p><p>（2）配置环境变量<br>变量名：MYSQL_HOME<br>变量值：D:\mysql-8.0.25-winx64<br>在桌面上点击”此电脑–右击–选择属性–选择高级–环境变量”，上方点新建系统变量</p><p> <img src="https://img-blog.csdnimg.cn/70bb500defa44234bacf8095ed29e9e7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>在下方环境变量PATH添加：%MYSQL_HOME%\bin<br> <img src="https://img-blog.csdnimg.cn/2ed4bd1facab45aaa5f5f31c4bfa10bb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>（3）生成data文件<br>在解压后mysql-8.0.18-winx64的文件下创建my.ini配置文件<br>具体内容如下<br>将下面的内容复制到刚创建的文件中 ，主要需要修改的字段为basedir和datadir<br>basedir=自己的mysql目录<br>datadir=mysql的data存储的目录</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="comment"># 设置3306端口</span></span><br><span class="line">port=<span class="number">3306</span></span><br><span class="line"><span class="comment"># 设置mysql的安装目录</span></span><br><span class="line">basedir=D:/Java/Database/mysql-<span class="number">8.0</span><span class="number">.18</span>-winx64</span><br><span class="line"><span class="comment"># 设置mysql数据库的数据的存放目录 (data文件夹如果没有的话会自动创建)</span></span><br><span class="line">datadir=D:/Java/Database/mysql-<span class="number">8.0</span><span class="number">.18</span>-winx64/data</span><br><span class="line"><span class="comment"># 允许最大连接数</span></span><br><span class="line">max_connections=<span class="number">200</span></span><br><span class="line"><span class="comment"># 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统</span></span><br><span class="line">max_connect_errors=<span class="number">10</span></span><br><span class="line"><span class="comment"># 服务端使用的字符集默认为UTF8</span></span><br><span class="line">character-<span class="built_in">set</span>-server=utf8</span><br><span class="line"><span class="comment"># 创建新表时将使用的默认存储引擎</span></span><br><span class="line">default-storage-engine=INNODB</span><br><span class="line"><span class="comment"># 默认使用“mysql_native_password”插件认证</span></span><br><span class="line">default_authentication_plugin=mysql_native_password</span><br><span class="line">[mysql]</span><br><span class="line"><span class="comment"># 设置mysql客户端默认字符集</span></span><br><span class="line">default-character-<span class="built_in">set</span>=utf8</span><br><span class="line">[client]</span><br><span class="line"><span class="comment"># 设置mysql客户端连接服务端时默认使用的端口</span></span><br><span class="line">port=<span class="number">3306</span></span><br><span class="line">default-character-<span class="built_in">set</span>=utf8</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/691660bdac224b65999438778b5a2136.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>打开CMD，进入D:\mysql-8.0.25-winx64\bin目录，执行如下命令初始化创建data目录。<br>cd D:\mysql-8.0.25-winx64\bin<br>mysqld —initialize-insecure —user=mysql</p><p>（5）启动MySQL服务，并配置成系统服务<br><img src="https://img-blog.csdnimg.cn/a03d94e194ac4975806f082908c1b9dd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>使用系统管理员身份，启动CMD，执行如下命令将MySQL配置成Windows系统服务：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mysqld -install --serviceName <span class="string">&quot;MySQL&quot;</span></span><br><span class="line">Service successfully installed.</span><br></pre></td></tr></table></figure><p>安装mysql服务方便以后启动：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D:\Java\Database\mysql-<span class="number">8.0</span><span class="number">.18</span>-winx64\<span class="built_in">bin</span>&gt;mysqld.exe install mysql</span><br><span class="line">Service successfully installed</span><br></pre></td></tr></table></figure><p>在服务列表中能找到刚刚安装的mysql服务，可设置其启动的方式 </p><p>右键单击此电脑打开任务管理器的服务，启动MySQL服务。<br> <img src="https://img-blog.csdnimg.cn/dea6a6cef5d4492aa5941fe029a440f6.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/b425fee1e9a44921807b7f3df002101a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><code>net start mysql</code>：启动mysql服务<br><code>net stop mysql</code>： 停止mysql服务</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动mysql服务需要使用管理员角色</span></span><br><span class="line"><span class="comment"># 通过net start命令启动mysql服务 (net stop mysql --终止mysql服务命令)</span></span><br><span class="line">  D:\Java\Database\mysql-<span class="number">8.0</span><span class="number">.18</span>-winx64\<span class="built_in">bin</span>&gt;net start mysql</span><br><span class="line">  mysql 服务正在启动 .</span><br><span class="line">  mysql 服务已经启动成功</span><br></pre></td></tr></table></figure><h4 id="2-1-2-设置root用户密码"><a href="#2-1-2-设置root用户密码" class="headerlink" title="2.1.2    设置root用户密码"></a>2.1.2    设置root用户密码</h4><p>（1）登录MySQL<br>在CMD中，输入以下命令登录MySQL（新安装的MySQL，可以无密码登录）：<br><code>mysql -u root</code> -p</p><p>（2）设置root用户密码<br>输入如下命令，键入回车后执行SQL语句，设置root用户密码为123456：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ALTER USER <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> IDENTIFIED WITH mysql_native_password BY <span class="string">&#x27;123456&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）刷新保存配置<br>输入如下命令，保存配置并生效：<code>flush privileges;</code><br>输入<code>quit</code>退出数据库。</p><h4 id="2-1-3-使用DataGrip连接MySQL数据库"><a href="#2-1-3-使用DataGrip连接MySQL数据库" class="headerlink" title="2.1.3    使用DataGrip连接MySQL数据库"></a>2.1.3    使用DataGrip连接MySQL数据库</h4><p>DataGrip2021安装参考<a href="https://blog.csdn.net/qq_31762741/article/details/115134775?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163957944416780265418841%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163957944416780265418841&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-115134775.pc_search_insert_es_download_v2&amp;utm_term=datagrip%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187">帖子1</a>、<a href="https://blog.csdn.net/weixin_45078818/article/details/116054375?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163957944416780265418841%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163957944416780265418841&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-116054375.pc_search_insert_es_download_v2&amp;utm_term=datagrip%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187">帖子2</a>。<br>（1）打开DataGrip工具，新建MySQL连接</p><p><img src="https://img-blog.csdnimg.cn/2c81891e8fa144e293df9d0fbdc37cf2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>（2）配置MySQL连接<br> <img src="https://img-blog.csdnimg.cn/0f8c2899749f493195e7b5f486b8dd56.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>（3）连接MySQL数据库<br>第一次连接mysql会报错，提示你缺少驱动，点击Download Driver Files就会自动帮你安装连接驱动。<img src="https://img-blog.csdnimg.cn/09b34b59e4e34be7a5fec1394f5dd1fe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>（4）更换中文语言教程，参考<a href="https://blog.csdn.net/qq_31762741/article/details/115134775?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163957944416780265418841%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163957944416780265418841&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-115134775.pc_search_insert_es_download_v2&amp;utm_term=datagrip%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187">此贴</a>。<br>（5）创建userinfo和loginfo数据库<br>新建一个console窗口，在mysql窗口中输入如下SQL语句，创建数据库：<br>create database userinfo;<br>create database loginfo;<br>（此时不能创建mongodb连接，因为还没装mongodb，也没有启动。装了也连不上）</p><p>2.2    MongoDB数据库安装与使用<br>参考帖子<a href="https://blog.csdn.net/qq_46092061/article/details/119811965?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163958533516780271568946%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163958533516780271568946&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-119811965.pc_search_insert_es_download_v2&amp;utm_term=windows%E9%85%8D%E7%BD%AEmongodb%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187">《【2021/8/19-最新教程】Windows安装MongoDB及配置（超详细）》</a><br>2.2.1    MongoDB数据库安装<br>（1）安装包下载<br>下载地址：<a href="https://www.mongodb.com/try/download/community">https://www.mongodb.com/try/download/community</a><br>安装包版本：5.0.5</p><p>（2）配置环境变量<br>在PATH下添加环境变量：D:\mongodb-win32-x86_64-windows-5.0.5\bin</p><p>（3）创建目录及配置文件<br>在bin目录同级的目录创建data目录，继续在data目录下创建db以及log，log目录中还需要创建mongod.log文件。<br> <img src="https://img-blog.csdnimg.cn/b03fdda88be74e648b00a70204c5cfd5.png" alt="在这里插入图片描述"></p><p>然后在bin目录的同级目录创建mongod.cfg文件：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">systemLog:</span><br><span class="line">    destination: file</span><br><span class="line">    path: D:\mongodb-win32-x86_64-windows-<span class="number">5.0</span><span class="number">.5</span>\data\log\mongod.log</span><br><span class="line">storage:</span><br><span class="line">    dbPath: D:\mongodb-win32-x86_64-windows-<span class="number">5.0</span><span class="number">.5</span>\data\db</span><br><span class="line">net:</span><br><span class="line">    port: <span class="number">27017</span></span><br></pre></td></tr></table></figure><ul><li>path是配置打印日志的目录</li><li>dbpath是配置数据的存储位置</li><li>port是配置的端口号</li></ul><p>（4）启动MongoDB服务，并配置成系统服务</p><p>使用系统管理员身份，启动CMD，在D:\mongodb-win32-x86_64-windows-5.0.5\bin目录下执行如下命令将MongoDB配置成Windows系统服务：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mongod --config D:\mongodb-win32-x86_64-windows-<span class="number">5.0</span><span class="number">.5</span>\mongod.cfg --install --serviceName <span class="string">&quot;MongoDB&quot;</span></span><br></pre></td></tr></table></figure></p><p>打开任务管理器的服务，查看MongoDB服务。<br> 此时就可以通过<code>net start MongoDB</code>和<code>net stop MongoDB</code>以及<code>net delete MongoDB</code>开启、关闭、删除MongoDB。</p><p>2.2.2    使用DataGrip连接MongoDB数据库<br>（1）打开DataGrip工具，新建MongoDB连接</p><p>（2）配置MongoDB连接</p><p>（3）连接MongoDB数据库<br>连接MongoDB数据库，在console中输入语句创建两个库（由于库中没有数据，在MongoDB中还看不到这两个库，等完成项目部署并运行调试之后，刷新MongoDB之后会出来这两个库）：<br>use NewsRecSys;<br>use SinaNews;</p><h3 id="2-3-Redis数据库安装与使用"><a href="#2-3-Redis数据库安装与使用" class="headerlink" title="2.3    Redis数据库安装与使用"></a>2.3    Redis数据库安装与使用</h3><p>2.3.1    Redis数据库安装<br>（1）安装包下载<br>下载地址：<a href="https://github.com/tporadowski/redis/releases">https://github.com/tporadowski/redis/releases</a><br>安装包版本：5.0.14</p><p>（2）启动Redis服务，并配置成系统服务<br>使用系统管理员身份，启动CMD，执行如下命令将Redis配置成Windows系统服务：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd D:\Redis-x64-<span class="number">5.0</span><span class="number">.14</span><span class="comment">#命令地址</span></span><br><span class="line">redis-server.exe --service-install redis.windows.conf --serviceName <span class="string">&quot;Redis5.0.14&quot;</span></span><br><span class="line"><span class="comment">#或者运行下面这个</span></span><br><span class="line">redis-server --service-install redis.windows.conf --loglevel verbose</span><br></pre></td></tr></table></figure><p>打开任务管理器的服务，查看Redis服务。</p><p>启动Redis：<code>net start Redis</code></p><h4 id="2-3-2-使用redis-desktop-manager连接Redis数据库"><a href="#2-3-2-使用redis-desktop-manager连接Redis数据库" class="headerlink" title="2.3.2    使用redis-desktop-manager连接Redis数据库"></a>2.3.2    使用redis-desktop-manager连接Redis数据库</h4><blockquote><p>参考<a href="https://blog.csdn.net/weixin_40668023/article/details/91905748?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=Redis%20Desktop%20Manager%E6%95%99%E7%A8%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-91905748.pc_search_insert_es_download_v2&amp;spm=1018.2226.3001.4187">《Redis可视化工具Redis Desktop Manager使用教程》</a><br><a href="https://blog.csdn.net/weixin_33859504/article/details/93832144?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163958926516780255243373%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163958926516780255243373&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-93832144.pc_search_insert_es_download_v2&amp;utm_term=Redis%20Desktop%20Manager%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187">《Redis DeskTop Manager 使用教程》</a></p></blockquote><p>（1）安装Redis Desktop Manager软件<br>下载地址：从腾讯微云中获取，参见0.1节</p><p>（2）连接Redis数据库<br> <img src="https://img-blog.csdnimg.cn/2b34a7914af54615bf17a245e738a4b8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 最后点ok就行。点击左侧Redis-@localhost出现下拉列表：<br><img src="https://img-blog.csdnimg.cn/332005dea44d4513b09e0d126f8e5e33.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="3-前端项目运行"><a href="#3-前端项目运行" class="headerlink" title="3    前端项目运行"></a>3    前端项目运行</h2><p><a href="https://blog.csdn.net/xudali_1012/article/details/117534094?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163959359416780274139155%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163959359416780274139155&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-117534094.pc_search_insert_es_download_v2&amp;utm_term=webstorm%E5%AE%89%E8%A3%85&amp;spm=1018.2226.3001.4187">安装WebStorm-2021</a><br><img src="https://img-blog.csdnimg.cn/122c2d83f4484699a0adfefe7aa58c84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>导入前端项目：open打开vue文件夹就行<br><img src="https://img-blog.csdnimg.cn/a39350a1ceac4cf5b27541f3c675f4e2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="3-1-安装依赖包"><a href="#3-1-安装依赖包" class="headerlink" title="3.1    安装依赖包"></a>3.1    安装依赖包</h3><p>安装<br>安装node<br><img src="https://img-blog.csdnimg.cn/bbad58c531214107817c7e3a0406a0b4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>首先安装淘宝的npm，在Terminal中执行如下命令：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">cnpm install</span><br></pre></td></tr></table></figure><h3 id="3-2-修改前端访问IP和端口"><a href="#3-2-修改前端访问IP和端口" class="headerlink" title="3.2    修改前端访问IP和端口"></a>3.2    修改前端访问IP和端口</h3><p>打开文件package.json，修改第49行的IP和端口，修改内容如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;scripts&quot;</span>: &#123;</span><br><span class="line">  <span class="string">&quot;test&quot;</span>: <span class="string">&quot;echo \&quot;Error: no test specified\&quot; &amp;&amp; exit 1&quot;</span>,</span><br><span class="line">  <span class="string">&quot;dev&quot;</span>: <span class="string">&quot;webpack-dev-server --open --port 8686 --contentBase src --hot --host 0.0.0.0&quot;</span>,</span><br><span class="line">  <span class="string">&quot;start&quot;</span>: <span class="string">&quot;nodemon src/main.js&quot;</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="comment">#锐锋是127.0.0.1</span></span><br></pre></td></tr></table></figure><p>127.0.0.1表示游览器的访问IP（也称为本地IP），8686表示访问端口</p><h3 id="3-3-修改访问后端API接口的IP和端口"><a href="#3-3-修改访问后端API接口的IP和端口" class="headerlink" title="3.3    修改访问后端API接口的IP和端口"></a>3.3    修改访问后端API接口的IP和端口</h3><p>打开文件main.js，文件路径：src/main.js，修改第23行的IP和端口，修改内容如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// Vue.prototype.$http = axios</span><br><span class="line">Vue.use(VueAxios, axios);</span><br><span class="line">// axios公共基路径，以后所有的请求都会在前面加上这个路径</span><br><span class="line">// axios.defaults.baseURL = <span class="string">&quot;http://10.170.4.60:3000&quot;</span>;</span><br><span class="line">// axios.defaults.baseURL = <span class="string">&quot;http://47.108.56.188:3000&quot;</span>;</span><br><span class="line">axios.defaults.baseURL = <span class="string">&quot;http://127.0.0.1:5000&quot;</span></span><br></pre></td></tr></table></figure><p>127.0.0.1表示后端项目的访问IP（也称为本地IP），5000表示访问端口</p><h3 id="3-4-运行前端项目"><a href="#3-4-运行前端项目" class="headerlink" title="3.4    运行前端项目"></a>3.4    运行前端项目</h3><p>在Terminal中执行命令运行前端项目:<code>npm run dev</code><br><img src="https://img-blog.csdnimg.cn/0993baf2421249babfed2927ecaeb7bf.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>浏览器会自动访问地址：<a href="http://127.0.0.1:8686/#/">http://127.0.0.1:8686/#/</a><br><img src="https://img-blog.csdnimg.cn/83ecc2fc191940a3be7e5e1e9d5b34a1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>通过打开“开发者工具”，调节设备工具栏，显示正常比例的页面<br> <img src="https://img-blog.csdnimg.cn/59ef280c39df421d814fa4174b65f836.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="4-后端项目运行"><a href="#4-后端项目运行" class="headerlink" title="4    后端项目运行"></a>4    后端项目运行</h2><h3 id="4-1-配置环境-安装依赖"><a href="#4-1-配置环境-安装依赖" class="headerlink" title="4.1    配置环境,安装依赖"></a>4.1    配置环境,安装依赖</h3><ol><li>安装conda环境，并创建虚拟环境<br>创建指定路径的Python环境：<code>conda create --prefix venv python=3.8</code></li></ol><p>虚拟环境位置：<br><img src="https://img-blog.csdnimg.cn/f5457bb18bdd400383c037add2dc7b52.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>包装在libs下面的site-pakeages:<br><img src="https://img-blog.csdnimg.cn/6156da64eced4666966c649f2091d7c6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/d54e9b59faaa44dbb4a1418d5103bfee.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>在PyCharm中，设置Python解释器<br> <img src="https://img-blog.csdnimg.cn/c6c89560e48f47348d21338b5be58911.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ol><li>安装依赖文件<br>在Terminal中执行命令安装依赖包：<code>pip install -r requirements.txt</code></li></ol><p><img src="https://img-blog.csdnimg.cn/6e4b9062a1064cfcbabc9aba8baf960e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="4-2-修改端口，配置文件"><a href="#4-2-修改端口，配置文件" class="headerlink" title="4.2 修改端口，配置文件"></a>4.2 修改端口，配置文件</h3><ol><li>修改后端项目的IP和端口<br>打开文件server.py，修改第233行的IP和端口，修改内容如下：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 允许服务器被公开访问</span></span><br><span class="line">    <span class="comment"># app.run(debug=True, host=&#x27;0.0.0.0&#x27;, port=3000, threaded=True)</span></span><br><span class="line">    <span class="comment"># 只能被自己的机子访问</span></span><br><span class="line">    app.run(debug=<span class="literal">True</span>, host=<span class="string">&#x27;127.0.0.1&#x27;</span>, port=<span class="number">5000</span>, threaded=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>127.0.0.1表示后端提供给前端的IP（也称为本地IP），5000表示端口。</p><ol><li>修改项目路径配置文件proj_path.py<br>因为没有配置home路径，所以改为读取项目地址。修改项目路径配置文件proj_path.py，文件路径：conf/proj_path.py</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># home_path = os.environ[&#x27;HOME&#x27;]</span></span><br><span class="line"><span class="comment"># proj_path = home_path + &quot;/fun-rec/codes/news_recsys/news_rec_server/&quot;</span></span><br><span class="line">proj_path = os.path.join(sys.path[<span class="number">1</span>], <span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><ol><li>核对数据库配置文件dao_config.py<br>打开数据库配置文件dao_config.py，文件路径：conf/dao_config.py，核对以下配置：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MySQL默认配置</span></span><br><span class="line">mysql_username = <span class="string">&quot;root&quot;</span></span><br><span class="line">mysql_passwd = <span class="string">&quot;123456&quot;</span></span><br><span class="line">mysql_hostname = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">mysql_port = <span class="string">&quot;3306&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MongoDB配置</span></span><br><span class="line">mongo_hostname = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">mongo_port = <span class="number">27017</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis配置</span></span><br><span class="line">redis_hostname = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">redis_port = <span class="number">6379</span></span><br></pre></td></tr></table></figure><h3 id="4-3-启动雪花算法服务"><a href="#4-3-启动雪花算法服务" class="headerlink" title="4.3     启动雪花算法服务"></a>4.3     启动雪花算法服务</h3><p>在Terminal中执行命令启动雪花算法服务，用于生成用户ID，启动命令如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">snowflake_start_server --address=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --port=<span class="number">8910</span> --dc=<span class="number">1</span> --worker=<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="4-4-创建logs目录"><a href="#4-4-创建logs目录" class="headerlink" title="4.4    创建logs目录"></a>4.4    创建logs目录</h3><p>在根目录下，创建logs目录，如下图所示：<br><img src="https://img-blog.csdnimg.cn/3582ab8e23ff4b07b3a9989699e6c4d2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="4-5-启动后端项目"><a href="#4-5-启动后端项目" class="headerlink" title="4.5 启动后端项目"></a>4.5 启动后端项目</h3><p>启动server.py程序（注：在此之前，必须启动数据库并创建数据库，详见2.1.3节和2.2.2节），执行如下命令：<br>python server.py<br><img src="https://img-blog.csdnimg.cn/12c5b5947bd84a71aaab73cf9e51670e.png" alt="在这里插入图片描述"></p><h2 id="5-项目整体运行与调试"><a href="#5-项目整体运行与调试" class="headerlink" title="5    项目整体运行与调试"></a>5    项目整体运行与调试</h2><p>注册用户</p><h3 id="5-1-爬取新浪新闻"><a href="#5-1-爬取新浪新闻" class="headerlink" title="5.1    爬取新浪新闻"></a>5.1    爬取新浪新闻</h3><p>通过查看crawl_news.sh文件（文件路径：scheduler/crawl_news.sh），可知爬取新浪新闻的代码在如下目录<br>/materials/news_scrapy/sinanews/run.py<br>使用PyCharm的Run按钮，手动执行该代码，需要配置参数：<br>—pages=30</p><p><img src="https://img-blog.csdnimg.cn/3a0d7d8f91774f90b1ca2f709db08c52.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c65de70dd5f9404abdc7d56d20804897.png" alt="在这里插入图片描述"></p><h3 id="5-2-更新物料画像"><a href="#5-2-更新物料画像" class="headerlink" title="5.2    更新物料画像"></a>5.2    更新物料画像</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知更新物料画像的代码在如下目录：<br>materials/process_material.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/17cd2ccaf3d94bf5ac8d8b93a167f6e5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="5-3-更新用户画像"><a href="#5-3-更新用户画像" class="headerlink" title="5.3    更新用户画像"></a>5.3    更新用户画像</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知更新用户画像的代码在如下目录：<br>materials/process_user.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/ba2509dd448a450caff81142bfe76b6f.png" alt="在这里插入图片描述"></p><h3 id="5-4-清除前一天redis中的数据，更新最新今天最新的数据"><a href="#5-4-清除前一天redis中的数据，更新最新今天最新的数据" class="headerlink" title="5.4    清除前一天redis中的数据，更新最新今天最新的数据"></a>5.4    清除前一天redis中的数据，更新最新今天最新的数据</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知清除前一天redis中的数据，更新最新今天最新的数据的代码在如下目录：<br>materials/update_redis.py<br>使用PyCharm的Run按钮，手动执行该代码</p><p><img src="https://img-blog.csdnimg.cn/f4265cf10e72450da4b50457a9c301c4.png" alt="在这里插入图片描述"></p><h3 id="5-5-离线将推荐列表和热门列表存入redis"><a href="#5-5-离线将推荐列表和热门列表存入redis" class="headerlink" title="5.5    离线将推荐列表和热门列表存入redis"></a>5.5    离线将推荐列表和热门列表存入redis</h3><p>通过查看run_offline.sh文件（文件路径：scheduler/run_offline.sh），可知离线将推荐列表和热门列表存入redis的代码在如下目录：<br>recprocess/offline.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/6fd3cdaa10e64cc19040601fa4313ea9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="5-6-重新登录用户查看新闻"><a href="#5-6-重新登录用户查看新闻" class="headerlink" title="5.6    重新登录用户查看新闻"></a>5.6    重新登录用户查看新闻</h3><p> <img src="https://img-blog.csdnimg.cn/65be2740b23243a6b677e8e0fad1f1b6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-项目运行环境&quot;&gt;&lt;a href=&quot;#0-项目运行环境&quot; class=&quot;headerlink&quot; title=&quot;0    项目运行环境&quot;&gt;&lt;/a&gt;0    项目运行环境&lt;/h2&gt;&lt;p&gt;0.1    获取软件安装包&lt;br&gt;软件安装包地址：&lt;a href=&quot;https://share.weiyun.com/u3ZIjZfg&quot;&gt;https://share.weiyun.com/u3ZIjZfg&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    <category term="12月组队学习：推荐系统" scheme="https://zhxnlp.github.io/categories/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="推荐系统" scheme="https://zhxnlp.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="数据库" scheme="https://zhxnlp.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="前端开发" scheme="https://zhxnlp.github.io/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>task2：新闻推荐系统项目搭建：centos下前端配置</title>
    <link href="https://zhxnlp.github.io/2021/12/16/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task2%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%EF%BC%9Acentos%E4%B8%8B%E5%89%8D%E7%AB%AF%E9%85%8D%E7%BD%AE/"/>
    <id>https://zhxnlp.github.io/2021/12/16/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task2%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%EF%BC%9Acentos%E4%B8%8B%E5%89%8D%E7%AB%AF%E9%85%8D%E7%BD%AE/</id>
    <published>2021-12-15T22:01:41.000Z</published>
    <updated>2021-12-31T23:24:30.237Z</updated>
    
    <content type="html"><![CDATA[<p>@[toc]</p><h2 id="0-解决npm命令语法不正确问题"><a href="#0-解决npm命令语法不正确问题" class="headerlink" title="0.解决npm命令语法不正确问题"></a>0.解决npm命令语法不正确问题</h2><h3 id="0-1-powershell报错"><a href="#0-1-powershell报错" class="headerlink" title="0.1  powershell报错"></a>0.1  powershell报错</h3><p><img src="https://img-blog.csdnimg.cn/2a4d50e19ca7476a8ac33976be4071c0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>解决方案：<br>根据上面提升报错的环境变量把环境变量Path中含有 ； 的分开写<br><span id="more"></span><br><img src="https://img-blog.csdnimg.cn/99d033d1644845a38a4ddf0fcda8e270.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ol><li>第二个报错<br><img src="https://img-blog.csdnimg.cn/10342ec9d4534a2ea2828cf81b5953ef.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ol><ul><li>PowerShell默认禁止运行脚本，但是因为安装Anaconda后再启动PowerShell时需要运行脚本，所以会报错。</li><li>以管理员身份运行PowerShell，执行 <code>set-ExecutionPolicy RemoteSigned</code>，然后输入 <code>Y</code>，重启PowerShell：<br><img src="https://img-blog.csdnimg.cn/cd8fb0e578a24c169c0592a9476b0281.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="0-2-win10家庭版升级"><a href="#0-2-win10家庭版升级" class="headerlink" title="0.2 win10家庭版升级"></a>0.2 win10家庭版升级</h3>参考帖子<a href="https://blog.csdn.net/weixin_30540691/article/details/101459865?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164069499916780265470211%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164069499916780265470211&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-101459865.pc_search_insert_es_download_v2&amp;utm_term=windows10%E6%BF%80%E6%B4%BB%E5%AF%86%E9%92%A5&amp;spm=1018.2226.3001.4187">《win10激活密钥》</a>。<br>专业版秘钥用不了，装的教育版。直接此电脑——右键属性——产品密钥，输入合适的密钥就安装对应的win10版本。（教育版：NW6C2-QMPVW-D7KKK-3GKT6-VCFB2）<br><img src="https://img-blog.csdnimg.cn/256f5169649c47089d4edf7832ce946f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>激活步骤：<br>管理员打开powershell，输入以下命令：（cmd打开会提示没有slmgr.vbs命令）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">slmgr.vbs /upk                               //卸载秘钥</span><br><span class="line">slmgr /ipk NW6C2-QMPVW-D7KKK-3GKT6-VCFB2     //产品密钥可用并且需要连接互联网</span><br><span class="line">slmgr /skms kms<span class="number">.03</span>k.org                     //指定KMS服务器的地址和端口,服务器地址和端口请根据实际情况修改。</span><br><span class="line">slmgr /ato                                  //进行激活</span><br></pre></td></tr></table></figure></li></ul><h2 id="nodejs"><a href="#nodejs" class="headerlink" title="nodejs"></a>nodejs</h2><h3 id="1-1-centos安装nodejs"><a href="#1-1-centos安装nodejs" class="headerlink" title="1.1 centos安装nodejs"></a>1.1 centos安装nodejs</h3><p>切换到目录：<code>cd /usr/local</code><br>压缩包解压:<code>tar -zxvf node-v15.10.0-linux-x64.tar.gz</code><br>mv命令将解压文件夹重命名为nodejs<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tar -xvf file.tar //解压 tar包</span><br><span class="line">tar xf node-v16<span class="number">.13</span><span class="number">.1</span>-linux-x64.tar.xz//解压 tar.xz包</span><br><span class="line">tar -xzvf file.tar.gz //解压tar.gz</span><br><span class="line">tar -xjvf file.tar.bz2   //解压 tar.bz2</span><br><span class="line">tar –xZvf file.tar.Z   //解压tar.Z</span><br><span class="line">unrar e file.rar //解压rar</span><br><span class="line">unzip file.<span class="built_in">zip</span> //解压<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line">rm-f　　-force　　忽略不存在的文件，强制删除，无任何提示</span><br><span class="line">-i　　　--interactive　　　 进行交互式地删除</span><br><span class="line">rm -ivrf dirname 删除目录</span><br></pre></td></tr></table></figure><br>建立软连接：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ln -s /usr/local/nodejs/<span class="built_in">bin</span>/node /usr/local/<span class="built_in">bin</span></span><br><span class="line">ln -s /usr/local/nodejs/<span class="built_in">bin</span>/npm /usr/local/<span class="built_in">bin</span></span><br></pre></td></tr></table></figure><p>查看是否正确安装<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@<span class="number">192</span> local]<span class="comment"># node -v</span></span><br><span class="line">v15<span class="number">.10</span><span class="number">.0</span></span><br><span class="line">[root@<span class="number">192</span> local]<span class="comment"># npm -v</span></span><br><span class="line"><span class="number">7.5</span><span class="number">.3</span></span><br></pre></td></tr></table></figure><br>各种报错：</p><ol><li><p>安装nodejs15后执行<code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code>报错：<br><img src="https://img-blog.csdnimg.cn/f050c16ed60942cfbb875cac6d2df79e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这是npm的版本太低了版本过低不支持nodejs。</p></li><li><p>结果再次<code>npm install -g cnpm --registry=https://registry.npm.taobao.orgnpm</code>报错<br><img src="https://img-blog.csdnimg.cn/57130bfa4bde44d99b2a3ad0128d26a2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>怀疑是上一次的nodejs没删干净，但其实是ping www.baidu.com都有问题<br><img src="https://img-blog.csdnimg.cn/cfab59f2a1254876a6a8a2a82a83d7d2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li></ol><p>之后参考：<a href="https://blog.csdn.net/weixin_43700340/article/details/88393833?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163965925416780274135285%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163965925416780274135285&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-5-88393833.pc_search_insert_es_download_v2&amp;utm_term=ping%20www.baidu.com&amp;spm=1018.2226.3001.4187">linux里面ping www.baidu.com ping不通的问题</a>。就ok了<br>配置DNS：打开文件 vim /etc/resolv.conf，注释第二行并输入：<br><img src="https://img-blog.csdnimg.cn/f149a64bfda74cba96a0855d5bf95291.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Google</span></span><br><span class="line">nameserver <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span></span><br><span class="line">nameserver <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure><p>现在ping registry.npmjs.org也ok了。<br>输入<code>npm config set registry http://registry.npm.taobao.org</code>。测试<code>ping registry.npm.taobao.org</code>成功。</p><ul><li>查看npm源地址<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm config get registry</span><br><span class="line"><span class="comment">#http://registry.npmjs.org 为国外镜像地址</span></span><br><span class="line"><span class="comment">#设置阿里云镜像</span></span><br><span class="line">npm config <span class="built_in">set</span> registry http://registry.npm.taobao.org</span><br><span class="line"><span class="comment">#http://registry.npm.taobao.org/现在是淘宝镜像</span></span><br><span class="line">npm config <span class="built_in">set</span> registry http://registry.npm.taobao.org<span class="comment">#安装cnpm</span></span><br></pre></td></tr></table></figure>这个是不知道中间执行了啥报错了。即越做越错。<br><img src="https://img-blog.csdnimg.cn/7ea41e5a166c4a01a36e5575aa615546.png" alt="在这里插入图片描述"></li></ul><p>装完nodejs16，弄好DNS服务。<code>npm install -g npm@8.3.0</code>升级npm到8.3。</p><h3 id="1-2-win10安装nodejs"><a href="#1-2-win10安装nodejs" class="headerlink" title="1.2  win10安装nodejs"></a>1.2  win10安装nodejs</h3><p>官网下载安装node-v16.13.1-x64后，cmd运行<code>npm -v</code>一直显示命令语法不正确。升级win10家庭版到专业版，各种操作都不行。打开git-bash，运行就ok了。<br><img src="https://img-blog.csdnimg.cn/5ccfc4ee6f3c4252a31b4d6b51e0467a.png" alt="在这里插入图片描述"><br>运行<code>npm config set registry http://registry.npm.taobao.org</code><br>运行<code>npm install -g npm@8.3.0</code>升级npm到8.3。<br>安装vue：<code>pip install vue</code><br>安装vue-cli：<code>npm install -g @vue/cli</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ npm install joi</span><br><span class="line">npm ERR! code EPERM</span><br><span class="line">npm ERR! syscall mkdir</span><br><span class="line">npm ERR! path D:\</span><br><span class="line">npm ERR! errno -<span class="number">4048</span></span><br><span class="line">npm ERR! Error: EPERM: operation <span class="keyword">not</span> permitted, mkdir <span class="string">&#x27;D:\&#x27;</span></span><br><span class="line"><span class="string">npm ERR!  [Error: EPERM: operation not permitted, mkdir &#x27;</span>D:\<span class="string">&#x27;] &#123;</span></span><br><span class="line"><span class="string">npm ERR!   errno: -4048,</span></span><br><span class="line"><span class="string">npm ERR!   code: &#x27;</span>EPERM<span class="string">&#x27;,</span></span><br><span class="line"><span class="string">npm ERR!   syscall: &#x27;</span>mkdi<span class="string">r&#x27;,</span></span><br><span class="line"><span class="string">npm ERR!   path: &#x27;</span>D:\\<span class="string">&#x27;</span></span><br><span class="line"><span class="string">npm ERR! &#125;</span></span><br><span class="line"><span class="string">npm ERR!</span></span><br><span class="line"><span class="string">npm ERR! The operation was rejected by your operating system.</span></span><br><span class="line"><span class="string">npm ERR! It&#x27;</span>s possible that the file was already <span class="keyword">in</span> use (by a text editor <span class="keyword">or</span> ant</span><br><span class="line">ivirus),</span><br><span class="line">npm ERR! <span class="keyword">or</span> that you lack permissions to access it.</span><br><span class="line">npm ERR!</span><br><span class="line">npm ERR! If you believe this might be a permissions issue, please double-check t</span><br><span class="line">he</span><br><span class="line">npm ERR! permissions of the file <span class="keyword">and</span> its containing directories, <span class="keyword">or</span> <span class="keyword">try</span> running</span><br><span class="line">npm ERR! the command again <span class="keyword">as</span> root/Administrator.</span><br><span class="line"></span><br><span class="line">npm ERR! A complete log of this run can be found <span class="keyword">in</span>:</span><br><span class="line">npm ERR!     C:\Users\LS\AppData\Local\npm-cache\_logs\<span class="number">2021</span>-<span class="number">12</span>-30T16_48_16_213Z-</span><br><span class="line">debug-<span class="number">0.</span>log</span><br></pre></td></tr></table></figure><p>bash没有管理员权限。此时右键单击git-bash.exe，以管理员身份运行<code>npm install joi</code>。<br><img src="https://img-blog.csdnimg.cn/4e900e52f9e54dbc8a01ecfeff76941b.png" alt="在这里插入图片描述"></p><h2 id="2-vue"><a href="#2-vue" class="headerlink" title="2. vue"></a>2. vue</h2><h3 id="2-1-安装vue"><a href="#2-1-安装vue" class="headerlink" title="2.1 安装vue"></a>2.1 安装vue</h3><p>安装vue：<code>pip install vue</code><br>安装vue-cli：<code>npm install -g @vue/cli</code><br><img src="https://img-blog.csdnimg.cn/ee2645568b0444388b67156108860054.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>报错一大堆，我一直以为是安装失败。后来才知道是有些包过时弃用了不碍事。改了两处：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm install uuid@<span class="number">8.3</span><span class="number">.2</span></span><br><span class="line">npm install joi</span><br></pre></td></tr></table></figure><p>之后建立软连接：<code>ln -s /usr/local/nodejs/bin/vue /usr/local/bin/vue</code><br>输入 <code>vue -v</code><br><img src="https://img-blog.csdnimg.cn/cdf0eea5932d4a5fb9e12c6e627865a0.png" alt="在这里插入图片描述"><br>原来已经装成功了。</p><p>wget <a href="http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.37-25.10/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm">http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.37-25.10/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm</a><br>wget <a href="http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.37-25.10/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm">http://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.37-25.10/RPM/rhel6/x86_64/Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm</a><br>rpm -ivh Percona-XtraDB-Cluster-shared-55-5.5.37-25.10.756.el6.x86_64.rpm</p><h3 id="2-2-创建vue项目"><a href="#2-2-创建vue项目" class="headerlink" title="2.2 创建vue项目"></a>2.2 创建vue项目</h3><p>官网教程：<a href="https://cli.vuejs.org/zh/guide/creating-a-project.html#%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%95%8C%E9%9D%A2">创建一个项目</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vue create hello-world</span><br></pre></td></tr></table></figure><p>可以选默认的包含了基本的 Babel + ESLint 设置的 preset，也可以选“手动选择特性”来选取需要的特性。<br><img src="https://img-blog.csdnimg.cn/ae49532f1d9640b2b91814fbaec5ff33.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>vue create 命令有一些可选项，你可以通过运行以下命令进行探索：<code>vue create --help</code><br>你也可以通过 vue ui 命令以图形化界面创建和管理项目：<code>vue ui</code><br>上述命令会打开一个浏览器窗口，并以图形化界面将你引导至项目创建的流程。<br><img src="https://img-blog.csdnimg.cn/4e72d8d9381b47aa9e84f7e6cbae730e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>Vue CLI &gt;= 3 和旧版使用了相同的 vue 命令，所以 Vue CLI 2 (vue-cli) 被覆盖了。如果你仍然需要使用旧版本的 vue init 功能，你可以全局安装一个桥接工具：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm install -g @vue/cli-init</span><br><span class="line"><span class="comment"># `vue init` 的运行效果将会跟 `vue-cli@2.x` 相同</span></span><br><span class="line">vue init webpack my-project</span><br></pre></td></tr></table></figure><p>Vue项目目录和说明见教程：</p><h3 id="2-3-使用Vue开发H5页面"><a href="#2-3-使用Vue开发H5页面" class="headerlink" title="2.3 使用Vue开发H5页面"></a>2.3 使用Vue开发H5页面</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vue create创建项目</span></span><br><span class="line">vue create test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入项目具体路径</span></span><br><span class="line">cd test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载依赖</span></span><br><span class="line">npm install</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动运行项目</span></span><br><span class="line">npm run serve </span><br></pre></td></tr></table></figure><p>此时输不了命令，也没有app界面弹出（minicentos没有图形界面）<br>ctrl+z退出后输入npm run build：<br><img src="https://img-blog.csdnimg.cn/193c0c2d582b4cc59f17cd4e80bc8ed5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/0b33543fc78340a8b0ec8f70d659adad.png" alt="在这里插入图片描述"><br>构建完成后，可以看到前端项目根目录下多了一个dist文件夹，这就是要部署的前台文件。</p><blockquote><p>参考<a href="https://blog.csdn.net/weixin_30367543/article/details/99511206?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163968487616780265472282%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163968487616780265472282&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-99511206.pc_search_insert_es_download_v2&amp;utm_term=centos%E9%83%A8%E7%BD%B2vue%E9%A1%B9%E7%9B%AE&amp;spm=1018.2226.3001.4187">《centos部署vue项目》</a><a href="https://blog.csdn.net/qq_41082746/article/details/106698019?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=centos%E9%83%A8%E7%BD%B2vue%E9%A1%B9%E7%9B%AE&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-106698019.pc_search_insert_es_download_v2&amp;spm=1018.2226.3001.4187">《Centos7部署Vue项目》</a><br>参考链接：<br><a href="https://www.runoob.com/vue3/vue3-directory-structure.html">https://www.runoob.com/vue3/vue3-directory-structure.html</a><br><a href="https://blog.csdn.net/weixin_41887155/article/details/107648969">https://blog.csdn.net/weixin_41887155/article/details/107648969</a><br><a href="https://www.cnblogs.com/jhpy/p/11873270.html">https://www.cnblogs.com/jhpy/p/11873270.html</a><br><a href="https://blog.csdn.net/chao2458/article/details/81284522">https://blog.csdn.net/chao2458/article/details/81284522</a></p></blockquote><h3 id="2-4-部署新闻推荐前端项目"><a href="#2-4-部署新闻推荐前端项目" class="headerlink" title="2.4 部署新闻推荐前端项目"></a>2.4 部署新闻推荐前端项目</h3><ol><li><p>跳转到前端项目文件目录：cd Vue-newsinfo</p></li><li><p>本地安装node环境，在项目根目录命令行输入命令<code>npm install</code>安装依赖包</p></li></ol><p>如果因为版本或者网络问题下载失败请执行npm install -g cnpm -registry=<a href="https://registry.npm.taobao.org/">https://registry.npm.taobao.org/</a> 和cnpm install</p><ol><li>启动前端服务：<code>npm run dev</code></li></ol><p>本机访问地址<a href="http://localhost:8686/#/">http://localhost:8686/#/</a></p><ol><li>根据需要修改package.json下<code>&quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;webpack-dev-server --open --port 8686 --contentBase src --hot --host 0.0.0.0&quot;&#125;</code>,中的ip和端口号)</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm ERR! JSON.parse package.json must be actual JSON, <span class="keyword">not</span> just JavaScript</span><br><span class="line"><span class="comment">#修改package.json错误导致</span></span><br></pre></td></tr></table></figure><ol><li><p>修改main.js：<code>vim /home/Vue-newsinfo/src/main.js</code><br>这个是后端访问的地址，也就是网页F12时用户操作的地址。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Vue.use(VueAxios, axios);</span><br><span class="line">// axios公共基路径，以后所有的请求都会在前面加上这个路径</span><br><span class="line">//axios.defaults.baseURL = <span class="string">&quot;http://47.108.56.188:3000&quot;</span>;</span><br><span class="line">//axios.defaults.baseURL = <span class="string">&quot;http://127.0.0.1:3000&quot;</span></span><br><span class="line">axios.defaults.baseURL = <span class="string">&quot;http://0.0.0.0:3000&quot;</span></span><br></pre></td></tr></table></figure><p>centos本地服务启动后想退出按<code>ctrl+c</code>不是ctrl+z。<br><code>netstat -tunlp | grep 8080</code>查看端口是否被占用<br><code>systemctl stop firewalld.service</code>关闭防火墙<br><code>yum install telnet httpd</code>安装telnet、httpd。<br><code>netstat -nlp</code> 查看是否映射成功<br><code>telnet 192.168.112.1 10022</code>连接主机<br><code>telnet 192.168.112.1 6379</code>连接redis</p></li><li><p>虚拟机端口映射<br>参考<a href="https://blog.csdn.net/aod83029/article/details/102163544?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-102163544.nonecase&amp;spm=1018.2226.3001.4187">此贴</a><br><img src="https://img-blog.csdnimg.cn/dacfd75c157e49ffaf91f4dc348df679.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/14430c3bc0fe4bcfa8b8ffd3a0e8ed5f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>最后输入<code>http://localhost:8686/#/</code>访问。</p></li><li><p>点击F12或者右键选择检查打开开发者模式,选中移动端浏览（点击左上角箭头右边的手机按钮）开始体验</p></li></ol><h2 id="3-后端配置"><a href="#3-后端配置" class="headerlink" title="3. 后端配置"></a>3. 后端配置</h2><h3 id="3-1-创建conda虚拟环境"><a href="#3-1-创建conda虚拟环境" class="headerlink" title="3.1 创建conda虚拟环境"></a>3.1 创建conda虚拟环境</h3><p>输入conda命令，如正常返回，说明conda安装成功。<br>查看已有环境列表，*表示当前环境，base表示默认环境：<code>conda env list</code><br><img src="https://img-blog.csdnimg.cn/4aa8a52871144e4e9bb81ee9f7fd7159.png" alt="在这里插入图片描述"><br><code>python --version</code>查看python版本</p><ol><li>使用命令<code>conda create -n sina python=3.6.8</code>创建环境，这里创建了名称为sina的python版本号为3.6.8的虚拟环境，稍微等待，过程中输入“y”。</li></ol><p>将后端文件全部拷贝到centos上，切换到目录：<code>cd /home/news_rec_server</code></p><ol><li>创建指定路径的Python环境：<code>conda create --prefix venv python=3.6.8</code>指定路径venv，名字是sina。<br><img src="https://img-blog.csdnimg.cn/8422680ac4a14d999f41439b1fa161fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ol><p>进入其他环境需要使用<code>conda activate</code>手动切换（一开始没有命令，结果悲剧了）<br>退出当前环境，使用<code>conda deactivate</code>，默认回到base 环境</p><ul><li>在新的环境中使用下面的代码安装旧的环境中对应的包：<code>pip install -r requirements.txt</code></li><li>直接安装失败报错;</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Could <span class="keyword">not</span> fetch URL https://pypi.tuna.tsinghua.edu.cn/simple/pip/: There was a problem confirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:<span class="number">852</span>) - skipping</span><br></pre></td></tr></table></figure><p>解决方法:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m pip install --upgrade pip --user -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></p><p>中间报错一次，将selenium==4.0.0改成3.14.1就ok了。<br><img src="https://img-blog.csdnimg.cn/80c20c68c2624b679714085067ec21af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>删除虚拟环境，直接找到环境所在的目录，手动删除</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda remove -n your_env_name(虚拟环境名称) --<span class="built_in">all</span> 删除虚拟环境</span><br><span class="line">conda remove --name your_env_name package_name 删除虚拟环境中的某个包</span><br></pre></td></tr></table></figure><p>旧环境导出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#切换到需要生成requirements.txt 的虚拟环境中;在终端运行下面的代码</span></span><br><span class="line">pip freeze &gt;requirements.txt</span><br><span class="line"></span><br><span class="line">pip <span class="built_in">list</span> --<span class="built_in">format</span>=freeze &gt; requirements.txt</span><br><span class="line"><span class="comment">#该命令可以得到安装在本地的库的版本名</span></span><br></pre></td></tr></table></figure><ul><li>进入环境后，可使用如下命令安装依赖的包，使用的是已经配置好的清华的源，这里以“opencv-python”包为例，由于使用了清华大学的镜像源，下载速度很快。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python</span><br></pre></td></tr></table></figure><ul><li>换回conda默认的源，访问起来可能有些慢，但总比无法访问好。<br>conda config —remove-key channels<h3 id="3-2-后端文件配置"><a href="#3-2-后端文件配置" class="headerlink" title="3.2 后端文件配置"></a>3.2 后端文件配置</h3><h3 id="4-2-修改端口，配置文件"><a href="#4-2-修改端口，配置文件" class="headerlink" title="4.2 修改端口，配置文件"></a>4.2 修改端口，配置文件</h3></li></ul><ol><li>修改后端项目的IP和端口<br>后端获取网页地址的代码在最后一行（点击登录的时候login信息）<br><img src="https://img-blog.csdnimg.cn/524998e228654f8ab9af0f49894fe994.png" alt="在这里插入图片描述"></li></ol><p>打开文件server.py，修改第233行的IP和端口，修改内容如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 允许服务器被公开访问</span></span><br><span class="line">     app.run(debug=<span class="literal">True</span>, host=<span class="string">&#x27;0.0.0.0&#x27;</span>, port=<span class="number">3000</span>, threaded=<span class="literal">True</span>)<span class="comment">#我的这个不变</span></span><br><span class="line">    <span class="comment"># 只能被自己的机子访问</span></span><br><span class="line">    <span class="comment">#app.run(debug=True, host=&#x27;127.0.0.1&#x27;, port=5000, threaded=True)</span></span><br></pre></td></tr></table></figure><p>127.0.0.1表示后端提供给前端的IP（也称为本地IP），5000表示端口。</p><ol><li>修改项目路径配置文件proj_path.py<br>因为没有配置home路径，所以改为读取项目地址。修改项目路径配置文件proj_path.py，文件路径：conf/proj_path.py</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># home_path = os.environ[&#x27;HOME&#x27;]</span></span><br><span class="line"><span class="comment"># proj_path = home_path + &quot;/fun-rec/codes/news_recsys/news_rec_server/&quot;</span></span><br><span class="line">proj_path = os.path.join(sys.path[<span class="number">1</span>], <span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><ol><li>核对数据库配置文件dao_config.py<br>打开数据库配置文件dao_config.py，文件路径：conf/dao_config.py，核对以下配置：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MySQL默认配置</span></span><br><span class="line">mysql_username = <span class="string">&quot;root&quot;</span></span><br><span class="line">mysql_passwd = <span class="string">&quot;123456&quot;</span></span><br><span class="line">mysql_hostname = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">mysql_port = <span class="string">&quot;3306&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MongoDB配置</span></span><br><span class="line">mongo_hostname = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">mongo_port = <span class="number">27017</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redis配置</span></span><br><span class="line">redis_hostname = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">redis_port = <span class="number">6379</span></span><br></pre></td></tr></table></figure><h3 id="4-3-启动雪花算法服务"><a href="#4-3-启动雪花算法服务" class="headerlink" title="4.3     启动雪花算法服务"></a>4.3     启动雪花算法服务</h3><p>news_rec_server下在Terminal中执行命令启动雪花算法服务，用于生成用户ID，启动命令如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">snowflake_start_server --address=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --port=<span class="number">8910</span> --dc=<span class="number">1</span> --worker=<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="4-4-创建数据库"><a href="#4-4-创建数据库" class="headerlink" title="4.4 创建数据库"></a>4.4 创建数据库</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mysql -u root -p <span class="comment">#登录mysql</span></span><br><span class="line">grep <span class="string">&quot;password&quot;</span> /var/log/mysqld.log<span class="comment">#查看密码，直接复制登录</span></span><br><span class="line">CREATE DATABASE loginfo；</span><br><span class="line">CREATE DATABASE userinfo;</span><br></pre></td></tr></table></figure><p>创建mongodb库</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mongo</span><br><span class="line">use SinaNews</span><br><span class="line">use NewsRecSys</span><br></pre></td></tr></table></figure><h3 id="4-4-创建logs目录"><a href="#4-4-创建logs目录" class="headerlink" title="4.4    创建logs目录"></a>4.4    创建logs目录</h3><p>在根目录下，创建logs目录，如下图所示：<br><img src="https://img-blog.csdnimg.cn/3582ab8e23ff4b07b3a9989699e6c4d2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="4-5-启动后端项目"><a href="#4-5-启动后端项目" class="headerlink" title="4.5 启动后端项目"></a>4.5 启动后端项目</h3><p>启动server.py程序（注：在此之前，必须启动数据库并创建数据库，详见2.1.3节和2.2.2节），执行如下命令：<br>先在sina虚拟环境下安装flask：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate sina</span><br><span class="line">pip install Flask</span><br></pre></td></tr></table></figure><p>python server.py<br><img src="https://img-blog.csdnimg.cn/12c5b5947bd84a71aaab73cf9e51670e.png" alt="在这里插入图片描述"></p><h2 id="5-项目整体运行与调试"><a href="#5-项目整体运行与调试" class="headerlink" title="5    项目整体运行与调试"></a>5    项目整体运行与调试</h2><p>注册用户，注册的用户数据在mysql的userinfo table下面：<br><img src="https://img-blog.csdnimg.cn/6f7d11017cfa4727921003d6aa673d25.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>可以直接删除就没有用户注册信息了（datagrap删除后要刷新）<br> 现在就可以登录网址注册登录了，但是没有新闻展示，因为还没有开始推送新闻，所以用户看不到新闻。终端报错：<img src="https://img-blog.csdnimg.cn/6736c30e0356408d97c8184ea616c0c8.png" alt="在这里插入图片描述"></p><h3 id="5-1-爬取新浪新闻"><a href="#5-1-爬取新浪新闻" class="headerlink" title="5.1    爬取新浪新闻"></a>5.1    爬取新浪新闻</h3><p>通过查看crawl_news.sh文件（文件路径：scheduler/crawl_news.sh），可知爬取新浪新闻的代码在如下目录<br>/materials/news_scrapy/sinanews/run.py<br>使用PyCharm的Run按钮，手动执行该代码，需要配置参数：<br>—pages=30</p><p><img src="https://img-blog.csdnimg.cn/3a0d7d8f91774f90b1ca2f709db08c52.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c65de70dd5f9404abdc7d56d20804897.png" alt="在这里插入图片描述"></p><h3 id="5-2-更新物料画像"><a href="#5-2-更新物料画像" class="headerlink" title="5.2    更新物料画像"></a>5.2    更新物料画像</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知更新物料画像的代码在如下目录：<br>materials/process_material.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/17cd2ccaf3d94bf5ac8d8b93a167f6e5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="5-3-更新用户画像"><a href="#5-3-更新用户画像" class="headerlink" title="5.3    更新用户画像"></a>5.3    更新用户画像</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知更新用户画像的代码在如下目录：<br>materials/process_user.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/ba2509dd448a450caff81142bfe76b6f.png" alt="在这里插入图片描述"></p><h3 id="5-4-清除前一天redis中的数据，更新最新今天最新的数据"><a href="#5-4-清除前一天redis中的数据，更新最新今天最新的数据" class="headerlink" title="5.4    清除前一天redis中的数据，更新最新今天最新的数据"></a>5.4    清除前一天redis中的数据，更新最新今天最新的数据</h3><p>通过查看offline_material_and_user_process.sh文件（文件路径：scheduler/offline_material_and_user_process.sh），可知清除前一天redis中的数据，更新最新今天最新的数据的代码在如下目录：<br>materials/update_redis.py<br>使用PyCharm的Run按钮，手动执行该代码</p><p><img src="https://img-blog.csdnimg.cn/f4265cf10e72450da4b50457a9c301c4.png" alt="在这里插入图片描述"></p><h3 id="5-5-离线将推荐列表和热门列表存入redis"><a href="#5-5-离线将推荐列表和热门列表存入redis" class="headerlink" title="5.5    离线将推荐列表和热门列表存入redis"></a>5.5    离线将推荐列表和热门列表存入redis</h3><p>通过查看run_offline.sh文件（文件路径：scheduler/run_offline.sh），可知离线将推荐列表和热门列表存入redis的代码在如下目录：<br>recprocess/offline.py<br>使用PyCharm的Run按钮，手动执行该代码<br> <img src="https://img-blog.csdnimg.cn/6fd3cdaa10e64cc19040601fa4313ea9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;@[toc]&lt;/p&gt;
&lt;h2 id=&quot;0-解决npm命令语法不正确问题&quot;&gt;&lt;a href=&quot;#0-解决npm命令语法不正确问题&quot; class=&quot;headerlink&quot; title=&quot;0.解决npm命令语法不正确问题&quot;&gt;&lt;/a&gt;0.解决npm命令语法不正确问题&lt;/h2&gt;&lt;h3 id=&quot;0-1-powershell报错&quot;&gt;&lt;a href=&quot;#0-1-powershell报错&quot; class=&quot;headerlink&quot; title=&quot;0.1  powershell报错&quot;&gt;&lt;/a&gt;0.1  powershell报错&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2a4d50e19ca7476a8ac33976be4071c0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;br&gt;解决方案：&lt;br&gt;根据上面提升报错的环境变量把环境变量Path中含有 ； 的分开写&lt;br&gt;</summary>
    
    
    
    <category term="12月组队学习：推荐系统" scheme="https://zhxnlp.github.io/categories/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="推荐系统" scheme="https://zhxnlp.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="数据库" scheme="https://zhxnlp.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="前端开发" scheme="https://zhxnlp.github.io/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>task1：新闻推荐系统项目搭建：数据库安装与使用</title>
    <link href="https://zhxnlp.github.io/2021/12/14/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task1%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://zhxnlp.github.io/2021/12/14/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task1%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2021-12-13T16:41:19.000Z</published>
    <updated>2022-01-02T19:49:12.689Z</updated>
    
    <content type="html"><![CDATA[<p>@[toc]</p><blockquote><p>参考：<a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/2.2%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/docs/2.2.1.1%20Mysql%E5%9F%BA%E7%A1%80.md">Mysql基础.md</a><br>本项目来自<a href="https://github.com/datawhalechina/fun-rec/tree/master/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/2.2%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98">《fun-rec/docs/第二章 推荐系统实战/2.2新闻推荐系统实战/》</a><br><a href="https://share.weiyun.com/u3ZIjZfg">【交流分享-腾讯微云】</a></p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> vim /etc/resolv.conf</span><br><span class="line"><span class="comment">#Google</span></span><br><span class="line">nameserver <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span></span><br><span class="line">nameserver <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span></span><br><span class="line"></span><br><span class="line">conda activate</span><br></pre></td></tr></table></figure><ol><li><p>mysql命令</p><span id="more"></span><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">systemctl start mysqld<span class="comment">#启动mysql</span></span><br><span class="line">systemctl status mysqld.service<span class="comment">#查看mysql状态，running就是启动中</span></span><br><span class="line">systemctl stop mysqld.service<span class="comment">#关闭mysql服务</span></span><br><span class="line">mysql -u root -p <span class="comment">#登录mysql</span></span><br><span class="line">grep <span class="string">&quot;password&quot;</span> /var/log/mysqld.log<span class="comment">#查看密码，直接复制登录</span></span><br><span class="line">netstat -nlp <span class="comment">#查看是否映射成功</span></span><br></pre></td></tr></table></figure><p>安装成功后没有mysql服务报错 没有此服务<br>我们应该找到 自己的mysq安装文件夹（博主的mysql在“ /home/tool/mysql_5.7.22”）<br>将/home/tool/mysql_5.7.22/support-files/mysql.server 拷贝到 /etc/init.d/mysql</p></li><li><p>mongodb命令：</p></li></ol><ul><li>添加路径：<code>export PATH=/usr/local/mongodb/bin:$PATH</code></li><li>启动mongo服务：<code>mongod --dbpath /var/lib/mongo --logpath /var/log/mongodb/mongod.log --fork</code></li><li>通过 ps ax | grep mongod查看数据库启动情况，如下图表示启动成功：</li><li>关闭mongodb服务通过ps ax | grep mongod命令查看mongodb运行的id<br><img src="https://img-blog.csdnimg.cn/890bf81dcb7a4899886c3e77f3ec1a4e.png" alt="在这里插入图片描述"><br>然后输入kill -9 进程id，杀死mongodb服务，如上图执行命令</li></ul><ol><li>redis命令</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">systemctl start redis.service   <span class="comment">#启动redis服务</span></span><br><span class="line">systemctl stop redis.service   <span class="comment">#停止redis服务</span></span><br><span class="line">systemctl restart redis.service   <span class="comment">#重新启动服务</span></span><br><span class="line">systemctl status redis.service   <span class="comment">#查看服务当前状态</span></span><br><span class="line">systemctl enable redis.service   <span class="comment">#设置开机自启动</span></span><br><span class="line">systemctl disable redis.service   <span class="comment">#停止开机自启动</span></span><br></pre></td></tr></table></figure><h2 id="一、Mysql用户"><a href="#一、Mysql用户" class="headerlink" title="一、Mysql用户"></a>一、Mysql用户</h2><p>启动MySQL/重启</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@master software]<span class="comment"># service mysql start</span></span><br><span class="line">[root@master software]<span class="comment"># service mysql restart</span></span><br><span class="line">systemctl status mysqld.service<span class="comment">#查看mysql状态，running就是启动中</span></span><br><span class="line">systemctl stop mysqld.service<span class="comment">#关闭mysql服务</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用户登录</span></span><br><span class="line">[root@master software]<span class="comment"># mysql -uroot -p</span></span><br><span class="line">mysql&gt; exit<span class="comment">#退出</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">CREATE USER <span class="string">&#x27;用户名&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;你的密码&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋予admin用户全部的权限，你也可以只授予部分权限</span></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;用户名&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;</span><br></pre></td></tr></table></figure><p>监听不到mysql端口查看<a href="https://blog.csdn.net/weixin_43671497/article/details/84931578?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E7%9C%8B%E4%B8%8D%E5%88%B0mysql%E7%AB%AF%E5%8F%A3&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-9-84931578.pc_search_insert_es_download_v2&amp;spm=1018.2226.3001.4187">帖子</a>。<br>即之前为了免密登陆在<code>vi /etc/my.cnf</code>打开mysql设置文件，在最后一行加了<code>skip-grant-tables</code>或<code>skip-network</code>导致3306端口无法被监听到。注释掉这句话之后重启mysql服务就行。</p><h2 id="二、Mysql-数据库"><a href="#二、Mysql-数据库" class="headerlink" title="二、Mysql-数据库"></a>二、Mysql-数据库</h2><div class="table-container"><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td> CREATE DATABASE [IF NOT EXISTS] &lt;数据库名称&gt;;</td><td>查看数据库。IF NOT EXISTS（可选项）避免数据库重名</td></tr><tr><td> SHOW DATABASES [LIKE ‘数据库名’];;</td><td>查看所有存在的数据库</td></tr><tr><td>USE &lt;数据库名&gt;</td><td>选择数据库</td></tr><tr><td> DROP DATABASE [IF EXISTS] &lt;数据库名&gt;</td><td>删除数据库</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE DATABASE shop;<span class="comment">#创建名为shop的数据库。</span></span><br><span class="line">SHOW DATABASES;<span class="comment">#查看数据库</span></span><br><span class="line">SHOW CREATE DATABASE shop;<span class="comment">#查看创建的数据库shop</span></span><br><span class="line">DROP DATABASE shop;<span class="comment">#删除shop数据库</span></span><br></pre></td></tr></table></figure><h2 id="三、Mysql-表的基本操作"><a href="#三、Mysql-表的基本操作" class="headerlink" title="三、Mysql-表的基本操作"></a>三、Mysql-表的基本操作</h2><h3 id="3-1-表的创建和修改"><a href="#3-1-表的创建和修改" class="headerlink" title="3.1 表的创建和修改"></a>3.1 表的创建和修改</h3><p>创建表的语法结构如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &lt;表名&gt; （&lt;字段<span class="number">1</span>&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;，</span><br><span class="line">   &lt;字段<span class="number">2</span>&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;，</span><br><span class="line">   &lt;字段<span class="number">3</span>&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;，</span><br><span class="line">   &lt;字段<span class="number">4</span>&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;，</span><br><span class="line">   .</span><br><span class="line">   .</span><br><span class="line">   .</span><br><span class="line">   &lt;该表的约束<span class="number">1</span>&gt;， &lt;该表的约束<span class="number">2</span>&gt;，……）；</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">-- 创建一个名为Product的表</span><br><span class="line">CREATE TABLE Product(</span><br><span class="line">  product_id CHAR(<span class="number">4</span>) NOT NULL,</span><br><span class="line">  product_name VARCHAR(<span class="number">100</span>) NOT NULL,</span><br><span class="line">  product_type VARCHAR(<span class="number">32</span>) NOT NULL,</span><br><span class="line">  sale_price INT,</span><br><span class="line">  purchase_price INT,</span><br><span class="line">  regist_date DATE,</span><br><span class="line">  PRIMARY KEY (product_id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td>DESC Product;</td><td>查询表product</td></tr><tr><td>DROP TABLE &lt;表名&gt;;</td><td>删除表，无法恢复</td></tr><tr><td>rename命令：ALTER TABLE Student RENAME Students;</td><td>更新表名Student =&gt; Students。</td></tr><tr><td>add命令：ALTER TABLE Students ADD sex CHAR(1), ADD age INT;</td><td>插入字段，用逗号隔开</td></tr><tr><td>drop命令：ALTER TABLE Students DROP stu_num;</td><td>删除字段</td></tr><tr><td>modify命令：ALTER TABLE Students MODIFY age CHAR(3);</td><td>修改字段类型</td></tr><tr><td>change：ALTER TABLE Students CHANGE name stu_name CHAR(12);</td><td>修改字段名name =&gt;stu_name，数据类型改为CHAR(12)。</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">-- 通过FIRST在表首插入字段stu_num</span><br><span class="line">ALTER TABLE Students ADD stu_num INT FIRST;</span><br><span class="line"></span><br><span class="line">-- 通过AFTER在字段sex后插入字段height</span><br><span class="line">ALTER TABLE Students ADD height INT AFTER sex;</span><br></pre></td></tr></table></figure><h3 id="3-2-表的查询"><a href="#3-2-表的查询" class="headerlink" title="3.2 表的查询"></a>3.2 表的查询</h3><h3 id="3-2-1-SELECT语句查询和算式运算"><a href="#3-2-1-SELECT语句查询和算式运算" class="headerlink" title="3.2. 1. SELECT语句查询和算式运算"></a>3.2. 1. SELECT语句查询和算式运算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT &lt;字段名&gt;, ……</span><br><span class="line"> FROM &lt;表名&gt;;</span><br><span class="line"><span class="comment">#查询全部字段</span></span><br><span class="line">SELECT *</span><br><span class="line"> FROM &lt;表名&gt;;</span><br></pre></td></tr></table></figure><ol><li>通过AS语句对展示的字段另起别名，这不会修改表内字段的名字：</li><li>设定汉语别名时需要使用双引号（”）括起来，英文字符则不需要:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT  </span><br><span class="line">  product_id AS <span class="string">&quot;产品编号&quot;</span>,</span><br><span class="line">  product_type AS <span class="string">&quot;产品类型&quot;</span>  </span><br><span class="line"> FROM Product;</span><br></pre></td></tr></table></figure><p>可以在SELECT语句中使用计算表达式：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">  product_name,</span><br><span class="line">  sale_price,</span><br><span class="line">  sale_price * <span class="number">2</span> AS <span class="string">&quot;sale_price_x2&quot;</span></span><br><span class="line"> FROM Product;</span><br><span class="line"></span><br><span class="line">-- 结果如下</span><br><span class="line">+--------------+------------+---------------+</span><br><span class="line">| product_name | sale_price | sale_price_x2 |</span><br><span class="line">+--------------+------------+---------------+</span><br><span class="line">| T恤衫        |       <span class="number">1000</span> |          <span class="number">2000</span> |</span><br><span class="line">| 打孔器       |        <span class="number">500</span> |          <span class="number">1000</span> |</span><br><span class="line">| 运动T恤      |       <span class="number">4000</span> |          <span class="number">8000</span> |</span><br><span class="line">| 菜刀         |       <span class="number">3000</span> |          <span class="number">6000</span> |</span><br><span class="line">| 高压锅       |       <span class="number">6800</span> |         <span class="number">13600</span> |</span><br><span class="line">| 叉子         |        <span class="number">500</span> |          <span class="number">1000</span> |</span><br><span class="line">| 擦菜板       |        <span class="number">880</span> |          <span class="number">1760</span> |</span><br><span class="line">| 圆珠笔       |        <span class="number">100</span> |           <span class="number">200</span> |</span><br><span class="line">+--------------+------------+---------------+</span><br></pre></td></tr></table></figure></p><ol><li>常数的查询：SELECT子句中，除了可以写字段外，还可以写常数。即将查询的字段填入常数</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">  <span class="string">&#x27;商品&#x27;</span> AS string,</span><br><span class="line">  <span class="string">&#x27;2009-05-24&#x27;</span> AS date,</span><br><span class="line">  product_id,</span><br><span class="line">  product_name</span><br><span class="line"> FROM Product;</span><br><span class="line"></span><br><span class="line">-- 结果如下</span><br><span class="line">+--------+------------+------------+--------------+</span><br><span class="line">| string | date       | product_id | product_name |</span><br><span class="line">+--------+------------+------------+--------------+</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0001       | T恤衫        |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0002       | 打孔器       |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0003       | 运动T恤      |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0004       | 菜刀         |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0005       | 高压锅       |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0006       | 叉子         |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0007       | 擦菜板       |</span><br><span class="line">| 商品   | <span class="number">2009</span>-05-<span class="number">24</span> | 0008       | 圆珠笔       |</span><br><span class="line">+--------+------------+------------+--------------+</span><br><span class="line"><span class="number">8</span> rows <span class="keyword">in</span> <span class="built_in">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><ol><li>去重：在SELECT语句中使用DISTINCT可以去除重复行。<br>NULL 也被视为一类数据。NULL 存在于多行中时，会被合并为一条NULL 数据。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mysql&gt; SELECT </span><br><span class="line">    -&gt;   DISTINCT regist_date </span><br><span class="line">    -&gt;  FROM Product;</span><br><span class="line"></span><br><span class="line"><span class="comment">#原表regist_date 字段有三个重复的2009-09-20 </span></span><br></pre></td></tr></table></figure><p>多个字段组合删除：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">  DISTINCT product_type, regist_date</span><br><span class="line"> FROM Product;</span><br><span class="line"></span><br><span class="line"><span class="comment">#（删除product_type, regist_date都重复的商品）</span></span><br></pre></td></tr></table></figure></p><h4 id="3-2-6-指定查询和比较运算符"><a href="#3-2-6-指定查询和比较运算符" class="headerlink" title="3.2.6 指定查询和比较运算符"></a>3.2.6 指定查询和比较运算符</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT &lt;字段名&gt;, ……</span><br><span class="line">  FROM &lt;表名&gt;</span><br><span class="line"> WHERE &lt;条件表达式&gt;;</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT product_name</span><br><span class="line">  FROM Product</span><br><span class="line"> WHERE product_type = <span class="string">&#x27;衣服&#x27;</span>;</span><br><span class="line"> -- 结果如下</span><br><span class="line">+--------------+</span><br><span class="line">| product_name |</span><br><span class="line">+--------------+</span><br><span class="line">| T恤衫        |</span><br><span class="line">| 运动T恤      |</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure><ul><li>WHERE 子句中通过使用比较运算符可以组合出各种各样的条件表达式。（&gt;&lt;=等等）不能对NULL使用任何比较运算符，只能通过IS NULL、IS NOT NULL语句来判断</li><li><p>字符串比较：按照字典顺序进行比较，也就是像姓名那样，按照条目在字典中出现的顺序来进行排序。例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;1&#x27;</span>  &lt; <span class="string">&#x27;10&#x27;</span> &lt; <span class="string">&#x27;11&#x27;</span> &lt; <span class="string">&#x27;2&#x27;</span> &lt; <span class="string">&#x27;222&#x27;</span> &lt; <span class="string">&#x27;3&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>也可以使用逻辑运算符AND OR NOT。NULL和任何值做逻辑运算结果都是不确定（第三种真值UNKNOWN）。因此尽量给字段加上NOT NULL的约束。</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT product_type, sale_price</span><br><span class="line">    FROM Product</span><br><span class="line">WHERE product_type = <span class="string">&#x27;厨房用具&#x27;</span> </span><br><span class="line">AND sale_price &gt;= <span class="number">3000</span>;</span><br></pre></td></tr></table></figure><h3 id="3-3-表的复制"><a href="#3-3-表的复制" class="headerlink" title="3.3 表的复制"></a>3.3 表的复制</h3><div class="table-container"><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td>CREATE TABLE Product_COPY1 SELECT * FROM Product;</td><td>从product表复制表Product_COPY1</td></tr><tr><td>CREATE TABLE Product_COPY1 LIKe Product;</td><td>通过LIKE复制表结构</td></tr></tbody></table></div><h2 id="四、Mysql-分组查询"><a href="#四、Mysql-分组查询" class="headerlink" title="四、Mysql-分组查询"></a>四、Mysql-分组查询</h2><h3 id="4-1-聚合函数"><a href="#4-1-聚合函数" class="headerlink" title="4.1 聚合函数"></a>4.1 聚合函数</h3><p>通过 SQL 对数据进行某种操作或计算时需要使用函数。</p><ul><li>COUNT：计算表中的记录数（行数）</li><li>SUM： 计算表中数值列中数据的合计值</li><li>AVG： 计算表中数值列中数据的平均值</li><li>MAX： 求出表中任意列中数据的最大值</li><li>MIN： 求出表中任意列中数据的最小值</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) FROM Product;<span class="comment">#其它的函数均不可以将*作为参数</span></span><br><span class="line">SELECT COUNT(purchase_price) FROM Product;<span class="comment">#字段名为参数，只会计算不包含NULL的行。AVG函数，计算时分母也不会算上NULL行。</span></span><br></pre></td></tr></table></figure><ul><li>MAX/MIN函数几乎适用于所有数据类型的列，包括字符和日期。SUM/AVG函数只适用于数值类型的列。</li><li>在聚合函数删除重复值，DISTINCT必须写在括号中。这是因为必须要在计算行数之前删除 product_type 字段中的重复数据。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(DISTINCT product_type)</span><br><span class="line"> FROM Product;</span><br><span class="line"> </span><br><span class="line">-- 结果如下</span><br><span class="line">+------------------------------+</span><br><span class="line">| COUNT(DISTINCT product_type) |</span><br><span class="line">+------------------------------+</span><br><span class="line">|                            <span class="number">3</span> |</span><br><span class="line">+------------------------------+</span><br></pre></td></tr></table></figure><h3 id="4-2-对表分组"><a href="#4-2-对表分组" class="headerlink" title="4.2 对表分组"></a>4.2 对表分组</h3><p>类似pandas的group by语句，也是group by分组，语法结构如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT &lt;列名<span class="number">1</span>&gt;, &lt;列名<span class="number">2</span>&gt;, &lt;列名<span class="number">3</span>&gt;, ……</span><br><span class="line"> FROM &lt;表名&gt;</span><br><span class="line"> GROUP BY &lt;列名<span class="number">1</span>&gt;, &lt;列名<span class="number">2</span>&gt;, &lt;列名<span class="number">3</span>&gt;, ……;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SELECT product_type, COUNT(*)</span><br><span class="line"> FROM Product</span><br><span class="line"> GROUP BY product_type;</span><br><span class="line"> </span><br><span class="line">-- 结果如下</span><br><span class="line">+--------------+----------+</span><br><span class="line">| product_type | COUNT(*) |</span><br><span class="line">+--------------+----------+</span><br><span class="line">| 衣服         |        <span class="number">2</span> |</span><br><span class="line">| 办公用品     |        <span class="number">2</span> |</span><br><span class="line">| 厨房用具     |        <span class="number">4</span> |</span><br><span class="line">+--------------+----------+</span><br></pre></td></tr></table></figure><ol><li><p>在该语句中，我们首先通过GROUP BY函数对指定的字段product_type进行分组。分组时，product_type字段中具有相同值的行会汇聚到同一组。</p></li><li><p>最后通过COUNT函数，统计不同分组的包含的行数。</p></li><li>NULL的数据会被聚合为一组。</li><li>语句顺序：1. SELECT → 2. FROM → 3. WHERE → 4. GROUP BY</li></ol><p>windows下安装PyMySQL：<code>python -m pip install PyMySQL</code><br>centos安装： <code>pip install PyMySQL</code></p><h2 id="五、redis安装连接"><a href="#五、redis安装连接" class="headerlink" title="五、redis安装连接"></a>五、redis安装连接</h2><p>redis安装地址：<a href="https://redis.io/download">官网</a>，选择安装5.0.14。<br>参考文章<a href="https://blog.csdn.net/username666/article/details/104687598?utm_source=app&amp;app_version=4.20.0&amp;code=app_1562916241&amp;uLinkId=usr1mkqgl919blen">《CentOS 7安装Redis5.0.7》</a>、<a href="https://www.cnblogs.com/heqiuyong/p/10463334.html">《Centos7安装Redis》</a></p><ol><li>解压安装<br>解压：<code>tar -zxvf redis-5.0.14.tar.gz</code><br>进入目录后安装：<code>cd redis-5.0.14</code><br>先运行：<code>make</code><br>然后指定目录安装：<code>make install PREFIX=/usr/local/redis</code><br>第一次只编译了一次，没有redis -cli命令<br>==后来liunx系统时间不对。本地时间2021-12-14，liunx时间是2021-7-20。编译显示创建不完整，也是错误。==<br><img src="https://img-blog.csdnimg.cn/e0dfd02e23ac43b58d0b42c208ecaf3b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ol><ol><li>前台启动</li></ol><p>[root@localhost redis-5.0.3]# cd /usr/local/redis/bin/<br>[root@localhost bin]# ./redis-server<br>启动后如图所示：<br><img src="https://img-blog.csdnimg.cn/c599aad448364629a38ec1e9006bc4ce.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>==前台启动的时候exit没退出直接ctrl+z强行退出，结果其实redis服务一直没退，后面systemctl start redis.service、systemctl status redis.service一直显示redis状态没开，端口被占用。xshell重现连接才好的。==</p><ol><li>后台启动</li></ol><p>从 redis 的源码目录中复制 redis.conf 到 redis 的安装目录<br>[root@localhost bin]# cp /usr/local/redis-5.0.3/redis.conf /usr/local/redis/bin/</p><p>修改安装目录/usr/local/redis/bin/下的redis.conf配置文件：<code>vim redis.conf</code><br>修改以下配置：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#bind 127.0.0.1 # 将这行代码注释，监听所有的ip地址，外网可以访问</span></span><br><span class="line">protected-mode no <span class="comment"># 把yes改成no，允许外网访问</span></span><br><span class="line">daemonize yes <span class="comment"># 把no改成yes，后台运行</span></span><br><span class="line"></span><br><span class="line">./redis-server redis.conf<span class="comment">#此时就可以后台启动了</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/7a7b65420e334069bd3aeca8651aee70.png" alt="在这里插入图片描述"></p><ol><li>设置开机启动<br>添加开机启动服务<code>[root@localhost bin]# vi /etc/systemd/system/redis.service</code></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=redis-server</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"><span class="type">Type</span>=forking</span><br><span class="line">ExecStart=/usr/local/redis/<span class="built_in">bin</span>/redis-server /usr/local/redis/<span class="built_in">bin</span>/redis.conf<span class="comment">#改成自己redis.conf地址，不写的话后面报错</span></span><br><span class="line">PrivateTmp=true</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ol><li>设置开机启动<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost <span class="built_in">bin</span>]<span class="comment"># systemctl daemon-reload</span></span><br><span class="line">[root@localhost <span class="built_in">bin</span>]<span class="comment"># systemctl start redis.service</span></span><br><span class="line">[root@localhost <span class="built_in">bin</span>]<span class="comment"># systemctl enable redis.service</span></span><br></pre></td></tr></table></figure>其它服务操作命令：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">systemctl start redis.service   <span class="comment">#启动redis服务</span></span><br><span class="line">systemctl stop redis.service   <span class="comment">#停止redis服务</span></span><br><span class="line">systemctl restart redis.service   <span class="comment">#重新启动服务</span></span><br><span class="line">systemctl status redis.service   <span class="comment">#查看服务当前状态</span></span><br><span class="line">systemctl enable redis.service   <span class="comment">#设置开机自启动</span></span><br><span class="line">systemctl disable redis.service   <span class="comment">#停止开机自启动</span></span><br></pre></td></tr></table></figure></li><li><p>创建 redis 命令软链接：<code>[root@localhost ~]# ln -s /usr/local/redis/bin/redis-cli /usr/bin/redis</code></p></li><li><p>连接 redis并退出：（按上面走完就ok了。不知道为啥redis -cli不行）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@<span class="number">192</span> <span class="built_in">bin</span>]<span class="comment"># redis</span></span><br><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; ping</span><br><span class="line">PONG</span><br><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; quit</span><br><span class="line">[root@<span class="number">192</span> <span class="built_in">bin</span>]<span class="comment">#</span></span><br></pre></td></tr></table></figure></li><li>redis-cli 未找到命令的一个解决方式</li></ol><p>将redis解压后的安装文件redis-5.0.14下src目录里的redis-cli文件，复制到/usr/local/bin/路径中<br>[root@localhost redis-5.0.14]# cp src/redis-cli  /usr/local/bin/<br>现在就可以执行redis-cli命令了。</p><h2 id="六、redis操作命令"><a href="#六、redis操作命令" class="headerlink" title="六、redis操作命令"></a>六、redis操作命令</h2><p>具体操作参考<a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/2.2%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/docs/2.2.1.3%20Redis%E5%9F%BA%E7%A1%80.md">《Redis基础.md》</a></p><ul><li>EXPIRE key_name seconds  :多少秒后名为key_name的key自动删除。key存在返回1，否则返回0。<br>127.0.0.1:6378&gt; set key1 “value”：string类型的key<br>127.0.0.1:6378&gt;  lpush key2 “value”：list类型key<br>127.0.0.1:6378&gt; SADD key3 “value”：set类型key</li><li>DEL key：删除key，可以是列表</li><li>TYPE key：查看key的类型</li></ul><p>Python调用Redis：<br>windows下安装redis库：<code>python -m pip install redis</code><br>centos下安装：<code>pip install redis</code></p><h2 id="七、MongoDB"><a href="#七、MongoDB" class="headerlink" title="七、MongoDB"></a>七、MongoDB</h2><h3 id="7-1-安装MongoDB"><a href="#7-1-安装MongoDB" class="headerlink" title="7.1 安装MongoDB"></a>7.1 安装MongoDB</h3><p>先安装libcurl 和openssl。</p><ol><li>yum命令出现Loaded plugins: fastestmirror Determining fastest mirrors 在进行yum安装的时候报错。参考<a href="https://blog.csdn.net/Me_find/article/details/110653042?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163948605316780274130394%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163948605316780274130394&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-110653042.pc_search_insert_es_download_v2&amp;utm_term=%E5%B7%B2%E5%8A%A0%E8%BD%BD%E6%8F%92%E4%BB%B6%EF%BC%9Afastestmirror&amp;spm=1018.2226.3001.4187">此文</a>进行修改。</li><li>[Errno 14] curl#6 - “Could not resolve host: mirrors.163.com； Unknown error“。参考<a href="https://blog.csdn.net/N199109/article/details/113175144?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163948646516780261960225%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163948646516780261960225&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-113175144.pc_search_insert_es_download_v2&amp;utm_term=Could%20not%20resolve%20host:%20mirrors.163.com;%20%E6%9C%AA%E7%9F%A5%E7%9A%84%E9%94%99%E8%AF%AF&amp;spm=1018.2226.3001.4187">此文</a></li></ol><p>然后直接<code>yum install libcurl  yum install openssl</code><br>一开始两个显示网站下载太慢尝试其它镜像，第三个网站顺利下载了。</p><p>at /etc/issue <a href="https://blog.csdn.net/shuaigexiaobo/article/details/78030008?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163949014816780265412839%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163949014816780265412839&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-78030008.pc_search_insert_es_download_v2&amp;utm_term=%E6%9F%A5%E7%9C%8Bcentos%E7%89%88%E6%9C%AC&amp;spm=1018.2226.3001.4187">查看centos版本</a>。然后在<a href="https://www.mongodb.com/try/download/community">官网</a>下载。<br><img src="https://img-blog.csdnimg.cn/4d56ad46c01e4f6f98b49408a1f9b75a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>copy链接之后：<code>wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-5.0.5.tgz</code><br>解压：<code>tar -zxvf mongodb-linux-x86_64-rhel70-5.0.5.tgz</code><br>重命名<code>mv mongodb-linux-x86_64-rhel70-5.0.5 mongodb-5</code></p><p>可以将MongoDB 的可执行文件（bin目录下）添加到 PATH 路径中<code>export PATH=/usr/local/mongodb/bin:$PATH</code>（这步貌似是临时的，但是不执行mongod命令无法执行）<br>配置MongoDB，编辑etc下的profile文件，加入一句指令：<code>export PATH=$PATH:/usr/local/mongodb-5/bin</code>。配置完保存，之后将CentOS7关机重启。<br><img src="https://img-blog.csdnimg.cn/0b0c083d51f34046abdd703c9f993b6e.png" alt="在这里插入图片描述"></p><p>创建数据存储目录：<code>mkdir -p /var/lib/mongo</code><br>创建日志目录：<code>mkdir -p /var/log/mongodb</code></p><p>启动mongo服务：<code>mongod --dbpath /var/lib/mongo --logpath /var/log/mongodb/mongod.log --fork</code></p><p>通过 ps ax | grep mongod查看数据库启动情况，如下图表示启动成功：<br><img src="https://img-blog.csdnimg.cn/77391c2abd7f4c608af2b92a3f66c3dd.png" alt="在这里插入图片描述"></p><blockquote><p>配置文件参考<a href="https://blog.csdn.net/qq_43317529/article/details/83033691?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163964441616780264030489%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163964441616780264030489&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-83033691.pc_search_insert_es_download_v2&amp;utm_term=centos%E5%AE%89%E8%A3%85mongodb&amp;spm=1018.2226.3001.4187">帖子1</a>、<a href="https://blog.csdn.net/m0_37027631/article/details/99934697?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=centos%E5%AE%89%E8%A3%85mongodb&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-99934697.pc_search_insert_es_download_v2&amp;spm=1018.2226.3001.4187">帖子2</a></p></blockquote><p>在usr/mongodb目录下新建一个名为mongodb.conf的配置文件，写入如下配置内容（不能有中文）：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">port=<span class="number">27017</span></span><br><span class="line">dbpath=/var/lib/mongo</span><br><span class="line">logpath=/var/log/mongodb</span><br><span class="line">logappend=true</span><br><span class="line">fork=false</span><br><span class="line">maxConns=<span class="number">100</span></span><br><span class="line">noauth=true</span><br><span class="line">journal=true</span><br><span class="line">bind_ip = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><p>保存。然后输入命令启动<code>mongod --config /usr/local/mongodb-5/mongodb.conf</code></p><p>关闭mongodb服务通过ps ax | grep mongod命令查看mongodb运行的id<br><img src="https://img-blog.csdnimg.cn/890bf81dcb7a4899886c3e77f3ec1a4e.png" alt="在这里插入图片描述"><br>然后输入kill -9 进程id，杀死mongodb服务，如上图执行命令<br>保存。然后输入命令启动mongod —config /usr/local/mongodb-5/mongodb.conf</p><h3 id="7-2-Mongodb操作"><a href="#7-2-Mongodb操作" class="headerlink" title="7.2 Mongodb操作"></a>7.2 Mongodb操作</h3><h4 id="7-2-1-数据库"><a href="#7-2-1-数据库" class="headerlink" title="7.2.1 数据库"></a>7.2.1 数据库</h4><p><img src="https://img-blog.csdnimg.cn/9d92e4e368984efdb91a388cefa3b95a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>“show dbs” ：显示所有数据的列表。</li><li>“db” ：显示当前数据库对象或集合。</li><li>“use”：连接到一个指定的数据库。</li><li>use 数据库名：创建数据库。刚创建的数据库 tobytest并不在数据库的列表中，需要插入数据。</li><li>db.tobytest.insert({“name”:”Toby”})：向数据库插入数据<h4 id="7-2-2-创建集合"><a href="#7-2-2-创建集合" class="headerlink" title="7.2.2 创建集合"></a>7.2.2 创建集合</h4></li><li><code>db.createCollection(name, options)</code>：创建集合</li><li><code>show collections 或 show tables</code> :查看集合</li><li><code>db.collection.drop()</code>：删除集合</li></ul><p>name: 要创建的集合名称<br>options: 可选参数, 指定有关内存大小及索引的选项</p><p>例如：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">db.createCollection(<span class="string">&quot;tobycollection&quot;</span>)</span><br><span class="line">db.tobycollection.drop()</span><br></pre></td></tr></table></figure><h4 id="7-2-3-插入文档"><a href="#7-2-3-插入文档" class="headerlink" title="7.2.3  插入文档"></a>7.2.3  插入文档</h4><p><code>db.COLLECTION_NAME.insert(document)</code>：插入文档<br><code>db.COLLECTION_NAME.save(document)</code>：插入文档</p><ul><li>save()：如果 _id 主键存在则更新数据，如果不存在就插入数据。该方法新版本中已废弃，可以使用 db.collection.insertOne() 或 db.collection.replaceOne() 来代替。</li><li>insert(): 若插入的数据主键已经存在，则会抛 org.springframework.dao.DuplicateKeyException 异常，提示主键重复，不保存当前数据。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">db.col.insert(&#123;title:<span class="string">&#x27;Toby MongoDB&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>description:<span class="string">&#x27;this is MongoDB&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>tags:[<span class="string">&#x27;mongodb&#x27;</span>,<span class="string">&#x27;database&#x27;</span>,<span class="string">&#x27;NoSQL&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>likes:<span class="number">1</span></span><br><span class="line"><span class="meta">... </span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#col 是我们的集合名，如果该集合不在该数据库中， MongoDB 会自动创建该集合并插入文档。</span></span><br></pre></td></tr></table></figure><ul><li>更新文档</li><li>删除文档</li><li>查询文档</li><li>排序</li></ul><h3 id="7-3-Python-MongoDB"><a href="#7-3-Python-MongoDB" class="headerlink" title="7.3 Python MongoDB"></a>7.3 Python MongoDB</h3><p>Python 要连接 MongoDB 需要 MongoDB 驱动，这里我们使用 PyMongo 驱动来连接。<br>安装：<code>pip install pymongo</code></p><p>用vim打开一个空白文档，然后把已经复制的代码给粘贴进来，发现它有自动缩进功能，最终导致粘贴的文本一行比一行靠右，看起来乱成一团。比较快的解决办法是，在粘贴文档前，在命令行模式下，输入：<code>:set paste</code><br>编辑完后输入：<code>:set nopaste</code></p><h2 id="八、scrapy"><a href="#八、scrapy" class="headerlink" title="八、scrapy"></a>八、scrapy</h2><p>按照教程写的编辑好那些文件之后执行sh run<em>scrapy_sina.sh报错：<br>参考：[帖子](<a href="https://blog.csdn.net/lmhlmh">https://blog.csdn.net/lmhlmh</a></em>/article/details/107135295?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163950211916780271576675%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163950211916780271576675&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-107135295.pc_search_insert_es_download_v2&amp;utm_term=Unknown%20command:%20crawl&amp;spm=1018.2226.3001.4187)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Scrapy <span class="number">2.5</span><span class="number">.1</span> - no active project</span><br><span class="line">Unknown command: crawl</span><br></pre></td></tr></table></figure><br><code>no active project</code>：说明我的工程有问题。</p><p>于是去看了看目录结构。查了下手册。在使用命令行startproject的时候，会自动生成scrapy.cfg</p><p>问题就出在这里，别人的项目文件中只有代码，没有配置文件，于是自己找了一个配置文件scrapy.cfg<br><img src="https://img-blog.csdnimg.cn/b32e3a99203643f9846fce7c0ed15f9e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ol><li>No module named ‘pymongo‘</li></ol><ul><li>这是因为我的centos有python2和3两个版本。执行py文件的时候要写python3 xx.py。而教程里面都是直接写python。</li><li>改动成python就可以执行python3操作：<ul><li>cd /usr/bin</li><li>备份原路径：mv python python.bak</li><li>python 链接到python3 ：ln -s python3 python</li><li>查看版本：python -V现在显示python3而不是2了</li><li>修改yum配置文件（yum要使用python2才可以运行）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#以下两个配置文件最顶部的\#!/usr/bin/python修改为 #!/usr/bin/python2</span></span><br><span class="line">vim /usr/<span class="built_in">bin</span>/yum</span><br><span class="line">vim /usr/libexec/urlgrabber-ext-down</span><br></pre></td></tr></table></figure>vim向下删除到文档结尾：dG<br>运行 sh run_scrapy_sina.sh显示没有库sinanews，将monitor_news.py的<code>from sinanews.settings import MONGO_HOST, MONGO_PORT, DB_NAME, COLLECTION_NAME</code>前面的sinanews.删除。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;@[toc]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/2.2%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/docs/2.2.1.1%20Mysql%E5%9F%BA%E7%A1%80.md&quot;&gt;Mysql基础.md&lt;/a&gt;&lt;br&gt;本项目来自&lt;a href=&quot;https://github.com/datawhalechina/fun-rec/tree/master/docs/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/2.2%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98&quot;&gt;《fun-rec/docs/第二章 推荐系统实战/2.2新闻推荐系统实战/》&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://share.weiyun.com/u3ZIjZfg&quot;&gt;【交流分享-腾讯微云】&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt; vim /etc/resolv.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#Google&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;nameserver &lt;span class=&quot;number&quot;&gt;8.8&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.8&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;nameserver &lt;span class=&quot;number&quot;&gt;8.8&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.4&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda activate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;mysql命令&lt;/p&gt;</summary>
    
    
    
    <category term="12月组队学习：推荐系统" scheme="https://zhxnlp.github.io/categories/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="推荐系统" scheme="https://zhxnlp.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="数据库" scheme="https://zhxnlp.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="前端开发" scheme="https://zhxnlp.github.io/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>task0：Fun-Rec推荐系统视频讲解</title>
    <link href="https://zhxnlp.github.io/2021/12/13/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task0%EF%BC%9AFun-Rec%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A7%86%E9%A2%91%E8%AE%B2%E8%A7%A3/"/>
    <id>https://zhxnlp.github.io/2021/12/13/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task0%EF%BC%9AFun-Rec%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A7%86%E9%A2%91%E8%AE%B2%E8%A7%A3/</id>
    <published>2021-12-13T09:52:39.000Z</published>
    <updated>2021-12-31T23:24:06.472Z</updated>
    
    <content type="html"><![CDATA[<p>视频讲解地址：<a href="https://datawhale.feishu.cn/minutes/obcnzns778b725r5l535j32o">Task01：熟悉推荐系统基本流程</a><br><a href="http://47.108.56.188:8686/#/recLists">推荐系统项目网页地址</a></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>本项目是离线的推荐系统。即不是实时的通过用户 ID 等信息通过模型实时、动态地获取用户的推荐列表，而是==提前已经把用户的这个推荐列表计算好了，用Redis存到了这个倒排索引表里面。我们线下线上真正要做的事情就是从这个 Redis 里面去拉取就够了。所以这里可能就会存在一个 T + 1 的延迟。（后面一天才会更新前一天的动态数据）==</p><p>推荐系统架构如下图：<br><span id="more"></span><br><img src="https://img-blog.csdnimg.cn/7dd55b57470b4889b907e1b7cea0bb95.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>除了离线offline部分，系统还有online部分，跟后端数据交互。<br>onlineserver：offline得到的的倒排索引数据经过online server处理后才给到online。如果不处理直接拉过来，会产生问题：</p><ul><li>推荐页和热门页挤在一起重复曝光（推荐页看过的热门页又看了一遍）</li><li>同的类别的很多东西会摞在一起，影响体验（同类产品score可能相近）</li><li>新用户需要冷启动策略<br>recsysserver：和onlineserver一样处理后端数据</li></ul><p>offline：得到用户第二天需要展示的推荐列表。</p><ul><li>爬取新物料并处理成物料画像</li><li>获得用户操作的动态信息，更新用户画像（年龄性别、物料侧标签得到的长短期兴趣等）</li><li>根据模型做出推荐列表</li></ul><p>画像处理有两部分：处理新来物料，更新旧物料动态属性。处理完后从Mongo DB 存到Redis（前端新闻展示信息）。如果还是在Mongo DB操作会很卡。<br>热门页：直接更新物料库里所有新闻的热度，做出倒排索引存入Redis。<br>推荐页：用模型得出倒排索引，涉及冷启动问题</p><h2 id="二、网页"><a href="#二、网页" class="headerlink" title="二、网页"></a>二、网页</h2><p>打开<a href="http://47.108.56.188:8686/#/之后按下F12键，然后登陆系统。在network里面有login、user_id等字典信息，是通过json的前后端交互得到的。">http://47.108.56.188:8686/#/之后按下F12键，然后登陆系统。在network里面有login、user_id等字典信息，是通过json的前后端交互得到的。</a><br>code表示前后端处理状态/进度，这里表示登陆成功login success。<br><img src="https://img-blog.csdnimg.cn/b8ef0c7375144a0d9b411452d187529e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>现在跳转到推荐页。rec_list?user_id=zhxscut：表示当前页展示信息。也有状态和json。<br>登陆之后跳转到推荐页也是按设定逻辑前后端交互好的。<br><img src="https://img-blog.csdnimg.cn/f5bc5d79199d420fa77ac37ad41811f7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>data点开之后是一个数组，每个数组点开是一条新闻。包括类别、时间、标题、收藏、喜欢、阅读次数、url等。<br><img src="https://img-blog.csdnimg.cn/4ef2b5364bcf49c6bfa11a12530bc86e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/916adedbb25f4d1b8fef12006497639d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>左侧推荐页面展示10条数据，往下拉的时候不够了不够展示了，再往下拉的时候他会重新请求重新去后端拉 10 个数据，右侧显示几个新的rec_list。<br>点击热门页，右侧显示hot_list?user_id=zhxscut：。<br>打开一篇新闻，会有news_detail?和一个action。点击喜欢或者收藏会有一个action。<br><img src="https://img-blog.csdnimg.cn/a80a2ba84cfd4449be881a60f24b02ce.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>前后端交互核心：通过url链接。前端展示的 URL （这个项目的网址）和我们后端资源访问的 URL （推荐新闻页面）是不一样的</p><h2 id="三、项目代码解读"><a href="#三、项目代码解读" class="headerlink" title="三、项目代码解读"></a>三、项目代码解读</h2><h3 id="3-1-前端url"><a href="#3-1-前端url" class="headerlink" title="3.1 前端url"></a>3.1 前端url</h3><p><img src="https://img-blog.csdnimg.cn/35b7d1fc8b024bf6afb52ae91857aa6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>router.js：路由，path：‘/’就是根目录，默认是signin界面。对应的是componence下面的signin.vue页面,展示的样式、处理逻辑等。<br><img src="https://img-blog.csdnimg.cn/44dbec30656441d483551bd666dd8b45.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>除了登陆页面还有注册、hot_list、rec_list页面等。</p><p>前后端交互的时候，我们可以大框架都可以看不懂，但是我们可以。但至少要知道比如说我推荐什么东西之后，一些我要搞的东西可能有些变化。那我们应该去看哪块的代码，他怎么是写的，我应该怎么改，大概需要了解一下这个</p><p><img src="https://img-blog.csdnimg.cn/9a0b9612f6214c1b8a6efe1983bcd58e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>上图表示post后端请求验证用户名和密码，登陆后跳转到推荐页。<br>状态码：<br><img src="https://img-blog.csdnimg.cn/3039bde92f3642f58b9dbe50de19d2cd.png" alt="在这里插入图片描述"></p><h3 id="3-2-后端url"><a href="#3-2-后端url" class="headerlink" title="3.2 后端url"></a>3.2 后端url</h3><p><img src="https://img-blog.csdnimg.cn/4c53729673c84bdfa5c5fb4e5d55619b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>后端url通过get list 方法获取，即通过 get 请求去后端拉取数据。<br>get 请求它就是这种格式：一个问号后面加上一些参数，比如说用户名或者是年龄性别之类的，当然还有 post 的请求。<br>get 请求：前端往后端发东西的时候会把一些具体的参数写在这个 URL 里面。</p><p>flask ： Python 的一个 web 框架，做20w用户的网站是没问题的，再多就不行了。<br><img src="https://img-blog.csdnimg.cn/56648b9d7b6a4c748efbcc201fa5f0c2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>server：app.route是一个路由，装饰器，第一个参数是url，第二个参数是get请求（对应下面的arg.get）。然后绑定一个具体的函数rec_list。<br>绑定的函数通过 flask 路由，根据当前的这个 URL 实现它要处理的具体逻辑，比如说现在是获取推荐列表（上一步reclist.vue里面的推荐列表）</p><p>try：<br>    rec_new_list=：表示从后端Redis拿取数据，即onlineserver、recsysserver后端请求这块。</p><p>signin没有url，不用get请求，而是post请求直接request.get_data。</p><h2 id="四、推荐系统流程"><a href="#四、推荐系统流程" class="headerlink" title="四、推荐系统流程"></a>四、推荐系统流程</h2><h3 id="4-1-物料处理"><a href="#4-1-物料处理" class="headerlink" title="4.1 物料处理"></a>4.1 物料处理</h3><p>物料处理自动化主要体现在使用 clone Tab 然后把整个链路给连串起来了：<br><img src="https://img-blog.csdnimg.cn/8ce84dd9357840a3bf380a9e1c61adbc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>上图表示：每天0点会运行很多命令。</p><ul><li>爬虫获取物料，代码在materials下的news_scrapy。爬取完存在mongoDB。</li><li>写入日志</li><li>离线处理物料（新闻物料和用户画像）</li><li>处理完写入日志</li><li>通过算法策略等用处理后物料生成排序列表，存入Redis</li><li>再写入日志。<br>整个过程就串起来了。</li></ul><p><img src="https://img-blog.csdnimg.cn/d5398c3bab1040e6b7a6e9635fba29d5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>终端输入mongo，进入后输入show dbs，显示所有数据库。<br>第一个是存推荐系统相关的物料画像、用户画像。<br>输入use SinaNews切换数据库、show collections显示数据。<br>通过以下命令查看爬取的新闻：<br><img src="https://img-blog.csdnimg.cn/b50bc5cb5707436c8b96bea51cff5037.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>每天爬取的数据会存盘一次，方便回溯。</p><h3 id="4-2-物料处理"><a href="#4-2-物料处理" class="headerlink" title="4.2 物料处理"></a>4.2 物料处理</h3><p>爬取的物料通过下面的sh文件，运行里面的三个py代码，更新画像和更新Redis。第三个py文件没有排序列表，而是新闻的详细信息。<br><img src="https://img-blog.csdnimg.cn/33ff79550d8547dea94218577908e27d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="4-2-1-更新物料画像，代码如下："><a href="#4-2-1-更新物料画像，代码如下：" class="headerlink" title="4.2.1 更新物料画像，代码如下："></a>4.2.1 更新物料画像，代码如下：</h4><p><img src="https://img-blog.csdnimg.cn/86d84fd455c84809b4f8bfd265c1ba25.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>更新存入mongo是为了备份，避免Redis数据被清空。</p><p>mongo NewsRecSys下面有三个集合。Feature集合是特征画像（物料池），只存了一份（方便处理，也可以分布式存储），包含每天用户交互的一些新闻产生的动态特征。<br>第二个部分update_news_items()代码如下：（点击函数跳转）<br><img src="https://img-blog.csdnimg.cn/ca5611c0c224475e899880b24850c8e3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>遍历今天爬去的所有数据（SinaNews），去重，初始化等生成特征画像，存入物料池（比如点击收藏开始都是0，热度是1000，逐渐衰减）<br>以上对应：<br><img src="https://img-blog.csdnimg.cn/bebbbf63bba442f88b25d89d6127c2e5.png" alt="在这里插入图片描述"></p><h4 id="4-2-2-当天新闻动态画像更新"><a href="#4-2-2-当天新闻动态画像更新" class="headerlink" title="4.2.2 当天新闻动态画像更新"></a>4.2.2 当天新闻动态画像更新</h4><ul><li>用户浏览新闻时，会有点击、收藏、喜欢等操作。这个是需要实时反馈的，但是又不可能每次都去MongoDB 那个物料库里面去把他拉回来展示。把新闻展示的物料分成了两部分，一部分是静态的，一部分是动态的，都是存在 Redis 里面。</li><li>静态：标题、类别、详情页</li><li>动态：阅读次数、喜欢次数和收藏次数。</li><li>用户在前端交互完之后，立马修改动态的信息，并且再拉回来再展示。此时物料池并没有更新</li><li>Redis 在清除缓存前，需要遍历动态画像，更新到物料画像池。</li></ul><p>动态信息更新到物料池代码：<br><img src="https://img-blog.csdnimg.cn/018c45ebe4d9446b8cfcece9e97e4513.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/b21498be38cd47c1ae173921256a000f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>dao_config：注释了每个数据库存了什么。<br><img src="https://img-blog.csdnimg.cn/361ad3949e8c4829a2eb4826c3030895.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>上面显示动态数据在2号库，静态在1号库。redis就是一个k-v系统。<br><img src="https://img-blog.csdnimg.cn/76823696fd5d43b7b1c3e3d5e2c7bb47.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>通过get得到静态信息。<br>==通过新闻 ID get对应的静态信息和动态信息，拼接起来送到这个前端展示==</p><p>redis-cli —raw参数设置可以看中文，否则有些信息显示乱码。redis-cli 无空格。2号库动态信息展示如图：<br><img src="https://img-blog.csdnimg.cn/7fd2bb930b6e4dc7babe6a4cf16206a8.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;视频讲解地址：&lt;a href=&quot;https://datawhale.feishu.cn/minutes/obcnzns778b725r5l535j32o&quot;&gt;Task01：熟悉推荐系统基本流程&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://47.108.56.188:8686/#/recLists&quot;&gt;推荐系统项目网页地址&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h2&gt;&lt;p&gt;本项目是离线的推荐系统。即不是实时的通过用户 ID 等信息通过模型实时、动态地获取用户的推荐列表，而是==提前已经把用户的这个推荐列表计算好了，用Redis存到了这个倒排索引表里面。我们线下线上真正要做的事情就是从这个 Redis 里面去拉取就够了。所以这里可能就会存在一个 T + 1 的延迟。（后面一天才会更新前一天的动态数据）==&lt;/p&gt;
&lt;p&gt;推荐系统架构如下图：&lt;br&gt;</summary>
    
    
    
    <category term="12月组队学习：推荐系统" scheme="https://zhxnlp.github.io/categories/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="推荐系统" scheme="https://zhxnlp.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="数据库" scheme="https://zhxnlp.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="前端开发" scheme="https://zhxnlp.github.io/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记8：transformers</title>
    <link href="https://zhxnlp.github.io/2021/12/06/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08%EF%BC%9Atransformer%E6%80%BB%E7%BB%93%EF%BC%881%EF%BC%89/"/>
    <id>https://zhxnlp.github.io/2021/12/06/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08%EF%BC%9Atransformer%E6%80%BB%E7%BB%93%EF%BC%881%EF%BC%89/</id>
    <published>2021-12-05T16:03:13.000Z</published>
    <updated>2022-01-02T20:47:22.740Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、attention"><a href="#一、attention" class="headerlink" title="一、attention"></a>一、attention</h2><h3 id="1-1循环神经网络的不足："><a href="#1-1循环神经网络的不足：" class="headerlink" title="1.1循环神经网络的不足："></a>1.1循环神经网络的不足：</h3><ul><li>长距离衰减问题</li><li>解码阶段，越靠后的内容，翻译效果越差</li><li>解码阶段缺乏对编码阶段各个词的直接利用<h3 id="1-2-attention在机器翻译的优点"><a href="#1-2-attention在机器翻译的优点" class="headerlink" title="1.2 attention在机器翻译的优点"></a>1.2 attention在机器翻译的优点</h3></li></ul><ol><li>使用全部token信息而非最后时刻的context信息。由此在解码时每时刻可以计算attention权重，让输出对输入进行聚焦的能力，找到此时刻解码时最该注意的词。</li><li>attention的计算是序列各tokens的v向量和attention权重加权求和，每个词关注到所有词，一步到位，不存在长距离衰减</li><li>可以关注到不同位置的词语，而且使用多头和多层注意力、加入FFNN，表达能力更强。<span id="more"></span><h3 id="1-3-self-Attention和循环神经网络对比"><a href="#1-3-self-Attention和循环神经网络对比" class="headerlink" title="1.3 self Attention和循环神经网络对比"></a>1.3 self Attention和循环神经网络对比</h3>LSTM:非词袋模型，含有顺序信息，无法解决长距离依赖，无法并行，没有注意力机制<br>Self Attention：词袋模型，不含位置信息，没有长距离依赖，可以并行，有注意力机制。<h3 id="1-4为什么求内积之后除以-sqrt-d"><a href="#1-4为什么求内积之后除以-sqrt-d" class="headerlink" title="1.4为什么求内积之后除以$\sqrt{d}$"></a>1.4为什么求内积之后除以$\sqrt{d}$</h3>&#8195;&#8195;上面计算相似度s=<q,k>时，s要除以$\sqrt(d_{key})$(Key 向量的长度）。原因是词向量embedding维度过高时，s过大，softmax函数会进入饱和区。&lt;/font&gt;例如：<br>&#8195;&#8195;对于两个d维向量q,k，假设它们都采样自“均值为0、方差为1”的分布。Attention是内积后softmax，主要设计的运算是$e^{q⋅k}$，我们可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而$e^{-3\sqrt{d}}$比较小，softmax函数进入饱和区。这样会有两个影响：</li></ol><ul><li>带来严重的梯度消失问题，导致训练效果差。</li><li>softmax之后，归一化后计算出来的结果a要么趋近于1要么趋近于0，Attention的分布非常接近一个one hot分布了，加权求和退化成胜者全拿，则解码时只关注注意力最高的（attention模型还是希望别的词也有权重）</li></ul><p>相应地，解决方法就有两个:（参考苏剑林<a href="https://kexue.fm/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>）</p><ul><li>像NTK参数化那样，在内积之后除以$\sqrt{d}$，使q⋅k的方差变为1，对应$e^3$,$e^{−3}$都不至于过大过小，这也是常规的Transformer如BERT里边的Self Attention的做法。对公式s=<q,k>进行优化：（q和k求内积，所以其实key和q的向量长度一样。）<script type="math/tex; mode=display">s=\frac{<q,k>}{\sqrt{d_{key}}}</script></li><li>另外就是不除以$\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。<h3 id="1-5-slef-attention过程"><a href="#1-5-slef-attention过程" class="headerlink" title="1.5 slef-attention过程"></a>1.5 slef-attention过程</h3><img src="https://img-blog.csdnimg.cn/369d5ca872984b4bbe1b5a6eb0eb084e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="slef-attention过程"><br>一个粗略的类比是把它看作是在一个文件柜里面搜索<br>向量     |含义<br>———— | ——-<br>Query   |一个==便签==，上面写着你正在研究的主题<br>Key  | 柜子里的文件夹的==标签==<br>Value  |文件夹里面的内容</li></ul><p>&#8195;&#8195;首先将==主题便签==与==标签==匹配，会为每个文件夹产生一个分数（attention score）。然后取出匹配的那些文件夹里面的内容 Value 向量。最后我们将每个 Value 向量和分数加权求和，就得到 Self Attention 的输出。</p><h3 id="1-6-多头注意力可视化"><a href="#1-6-多头注意力可视化" class="headerlink" title="1.6 多头注意力可视化"></a>1.6 多头注意力可视化</h3><p>下面以head=8举例说明如下：</p><ol><li>输入 X 和8组权重矩阵$W^Q$, $W^K$ $W^V$相乘，得到 8 组 Q, K, V 矩阵。进行attention计算，得到 8 组 Z 矩阵（特就是head）</li><li>把8组矩阵拼接起来，乘以权重矩阵$W^O$，得到最终的矩阵 Z。这个矩阵包含了所有 attention heads（注意力头） 的信息。</li><li>矩阵Z会输入到 FFNN (Feed orward Neural Network)层。（前馈神经网络层接收的也是 1 个矩阵，而不是8个。其中每行的向量表示一个词）</li></ol><p>多头注意力结果串联在一起维度可能比较高，所以通过$W^{O}$进行一次线性变换，实现降维和各头信息融合的目的，得到最终结果。&lt;/font &gt;<br><img src="https://img-blog.csdnimg.cn/cb223fd81b044e83b39fd710b9cd2c3d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&#8195;&#8195;在前面的讲解中，我们的 K、Q、V 矩阵的序列长度都是一样的。但是在实际中，K、V 矩阵的序列长度是一样的（加权求和），而 Q 矩阵的序列长度可以不一样。<br>&#8195;&#8195;这种情况发生在：在解码器部分的Encoder-Decoder Attention层中，Q 矩阵是来自解码器下层，而 K、V 矩阵则是来自编码器的输出。&lt;/font &gt;</p><h2 id="二、transformers"><a href="#二、transformers" class="headerlink" title="二、transformers"></a>二、transformers</h2><h3 id="2-1-自注意力模型的缺点及transformer的提出"><a href="#2-1-自注意力模型的缺点及transformer的提出" class="headerlink" title="2.1 自注意力模型的缺点及transformer的提出"></a>2.1 自注意力模型的缺点及transformer的提出</h3><p>自注意力模型有如下问题:.</p><ol><li>在计算自注意力时,没有考虑输入的位置信息,因此无法对序列进行建模;.</li><li>输入向量 T ,同时承担了Q、K、V三种角色,导致其不容易学习;</li><li>只考虑了两个输入序列单元之间的关系,无法建模多个输入序列单元之间更复杂的关系;</li><li>自注意力计算结果互斥,无法同时关注多个输入</li></ol><p>解决如下：</p><ul><li>加入位置编码信息，具体使用sin/cos函数，将一个位置索引值映射到一个 d 维向量上。尝试用位置嵌入（可学习的Position Embeddings(cite)）来代替固定的位置编码，结果发现两种方法产生了几乎相同的效果。于是我们选择了正弦版本，因为它可以使模型外推到，比训练集序列更长的序列。</li><li>输入X经过三个不同参数矩阵映射为不同的向量矩阵QKV（线性变换）</li><li>多层自注意力，建模多个输入序列不同单元的高阶信息。加入FFNN层通过非线性变换、前后两个linear层增强语义信息。通过残差连接和norm，使模型学习的更快。残差连接解决深层网络退化问题，norm使训练数据分布更加稳定，增强网络稳定性，加快收敛速度；使异常值不那么异常，减少过拟合。</li><li>多头自注意力：学习不同语义空间下的语义信息，表达能力更强，，可以进行多语义匹配，也相当于多个卷积核提取不同类型的特征。解决注意力互斥问题。 <h3 id="2-2-模型具体结构"><a href="#2-2-模型具体结构" class="headerlink" title="2.2 模型具体结构"></a>2.2 模型具体结构</h3>可以看出每一层都是计算之后（attention计算或者linear计算）dropout，再Add，再Norm。唯一不同是FFNN层第一层linear计算后激活，再dropout，再linear第二层。<h4 id="2-2-1-Encoder-Layer"><a href="#2-2-1-Encoder-Layer" class="headerlink" title="2.2.1 Encoder Layer"></a>2.2.1 Encoder Layer</h4>Self-Attention模型的作用是提取语义级别的信息（不存在长距离依赖），而FFNN是在各个时序上对特征进行非线性变换，提高网络表达能力。</li></ul><p><img src="https://img-blog.csdnimg.cn/953b03a12dcf4a7b82e15408bb6ab508.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>encoder层前向传播代码表示为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line"><span class="comment">#attention层：</span></span><br><span class="line">        src = positional_encoding(src, src.shape[-<span class="number">1</span>])</span><br><span class="line">        src2 = self.self_attn(src, src, src, attn_mask=src_mask, </span><br><span class="line">        key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src = self.norm1(src)</span><br><span class="line">        <span class="comment">#FFNN层：</span></span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        src = self.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-Transformer-layer组成Encoder"><a href="#2-2-2-Transformer-layer组成Encoder" class="headerlink" title="2.2.2 Transformer layer组成Encoder"></a>2.2.2 Transformer layer组成Encoder</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        encoder_layer（必备）</span></span><br><span class="line"><span class="string">        num_layers： encoder_layer的层数（必备）</span></span><br><span class="line"><span class="string">        norm: 归一化的选择（可选）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.randn((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layer = encoder_layer</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        output = positional_encoding(src, src.shape[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            output = self.layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 例子</span></span><br><span class="line">encoder_layer = TransformerEncoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_encoder = TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">src = torch.randn((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = transformer_encoder(src)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([10, 32, 512])</span></span><br></pre></td></tr></table></figure><h4 id="2-2-3-TransformerDecoderLayer"><a href="#2-2-3-TransformerDecoderLayer" class="headerlink" title="2.2.3 TransformerDecoderLayer"></a>2.2.3 TransformerDecoderLayer</h4><ul><li>相比encoder多了Encoder-Decoder Attention层，用来帮解码器把注意力集中到输入序列的合适位置。其个decoder block的Encoder-Decoder Attention层输入k、v值都是encoder最后层的输出memory。<ul><li>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词，即屏蔽掉未来时刻的信息。具体是用mask下三角矩阵实现的，它会将我们想要屏蔽的单元格设置为负无穷大或者一个非常大的负数。&lt;/font&gt;先正常通过qk计算attention score，再乘以mask矩阵。这样进行softmax计算时，屏蔽位置的attention权重为0。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">               memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">       <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">       参数：</span></span><br><span class="line"><span class="string">           tgt: 目标语言序列（必备）</span></span><br><span class="line"><span class="string">           memory: 从最后一个encoder_layer跑出的句子（必备）</span></span><br><span class="line"><span class="string">           tgt_mask: 目标语言序列的mask（可选）</span></span><br><span class="line"><span class="string">           memory_mask（可选）</span></span><br><span class="line"><span class="string">           tgt_key_padding_mask（可选）</span></span><br><span class="line"><span class="string">           memory_key_padding_mask（可选）</span></span><br><span class="line"><span class="string">       &#x27;&#x27;&#x27;</span></span><br><span class="line">       <span class="comment">#1.self-attention层</span></span><br><span class="line">       tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,</span><br><span class="line">                             key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">       tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">       tgt = self.norm1(tgt)</span><br><span class="line">       <span class="comment">#Encoder-Decoder Attention层</span></span><br><span class="line">       tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,</span><br><span class="line">                                  key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">       tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">       tgt = self.norm2(tgt)</span><br><span class="line">       <span class="comment">#3.FFNN层</span></span><br><span class="line">       tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))</span><br><span class="line">       tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">       tgt = self.norm3(tgt)</span><br><span class="line">       <span class="keyword">return</span> tgt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可爱的小例子</span></span><br><span class="line">decoder_layer = nn.TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">memory = torch.randn((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.randn((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = decoder_layer(tgt, memory)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure><h4 id="2-2-4-TransformerDecoderLayer组成Decoder"><a href="#2-2-4-TransformerDecoderLayer组成Decoder" class="headerlink" title="2.2.4 TransformerDecoderLayer组成Decoder"></a>2.2.4 TransformerDecoderLayer组成Decoder</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        output = tgt</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            output = self.layer(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可爱的小例子</span></span><br><span class="line">decoder_layer =TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_decoder = TransformerDecoder(decoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">memory = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">tgt = torch.rand(<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">out = transformer_decoder(tgt, memory)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5-Transformer"><a href="#2-2-5-Transformer" class="headerlink" title="2.2.5 Transformer"></a>2.2.5 Transformer</h4><p><img src="https://img-blog.csdnimg.cn/54303ee411f5480697043101334f35ff.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        d_model: 词嵌入的维度（必备）（Default=512）</span></span><br><span class="line"><span class="string">        nhead: 多头注意力中平行头的数目（必备）（Default=8）</span></span><br><span class="line"><span class="string">        num_encoder_layers:编码层层数（Default=8）</span></span><br><span class="line"><span class="string">        num_decoder_layers:解码层层数（Default=8）</span></span><br><span class="line"><span class="string">        dim_feedforward: 全连接层的神经元的数目，又称经过此层输入的维度（Default = 2048）</span></span><br><span class="line"><span class="string">        dropout: dropout的概率（Default = 0.1）</span></span><br><span class="line"><span class="string">        activation: 两个线性层中间的激活函数，默认relu或gelu</span></span><br><span class="line"><span class="string">        custom_encoder: 自定义encoder（Default=None）</span></span><br><span class="line"><span class="string">        custom_decoder: 自定义decoder（Default=None）</span></span><br><span class="line"><span class="string">        lay_norm_eps: layer normalization中的微小量，防止分母为0（Default = 1e-5）</span></span><br><span class="line"><span class="string">        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)（Default：False）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 activation = F.relu, custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.encoder = custom_encoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first)</span><br><span class="line">            encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.decoder = custom_decoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first)</span><br><span class="line">            decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.nhead = nhead</span><br><span class="line"></span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, tgt: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            src: 源语言序列（送入Encoder）（必备）</span></span><br><span class="line"><span class="string">            tgt: 目标语言序列（送入Decoder）（必备）</span></span><br><span class="line"><span class="string">            src_mask: （可选)</span></span><br><span class="line"><span class="string">            tgt_mask: （可选）</span></span><br><span class="line"><span class="string">            memory_mask: （可选）</span></span><br><span class="line"><span class="string">            src_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        形状：</span></span><br><span class="line"><span class="string">            - src: shape:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: shape:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: shape:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: shape:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: shape:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: shape:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: shape:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: shape:`(N, S)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [src/tgt/memory]_mask确保有些位置不被看到，如做decode的时候，只能看该位置及其以前的，而不能看后面的。</span></span><br><span class="line"><span class="string">            若为ByteTensor，非0的位置会被忽略不做注意力；若为BoolTensor，True对应的位置会被忽略；</span></span><br><span class="line"><span class="string">            若为数值，则会直接加到attn_weights</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [src/tgt/memory]_key_padding_mask 使得key里面的某些元素不参与attention计算，三种情况同上</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - output: shape:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        注意：</span></span><br><span class="line"><span class="string">            src和tgt的最后一维需要等于d_model，batch的那一维需要相等</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        例子:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_square_subsequent_mask</span>(<span class="params">self, sz: <span class="built_in">int</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;产生关于序列的mask，被遮住的区域赋值`-inf`，未被遮住的区域赋值为`0`&#x27;&#x27;&#x27;</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.<span class="built_in">float</span>().masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class="number">1</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;用正态分布初始化参数&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                xavier_uniform_(p)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 小例子</span></span><br><span class="line">transformer_model = Transformer(nhead=<span class="number">16</span>, num_encoder_layers=<span class="number">12</span>)</span><br><span class="line">src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = transformer_model(src, tgt)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure><p>总结一下，其实经过位置编码，多头注意力，Encoder Layer和Decoder Layer形状不会变的，而Encoder和Decoder分别与src和tgt形状一致</p><p><img src="https://img-blog.csdnimg.cn/5eb80e29d6994914bbcf1d87b443aedd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b80d230b08314554b1e37a76c24ce7d2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/0de6fb81505b4454b11e817d02099362.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>2.2.3  最后的线性层和 Softmax 层<br>Decoder 最终的输出是一个向量，其中每个元素是浮点数。输出向量经过线性层（普通的全连接神经网络）映射为一个更长的向量，这个向量称为 logits 向量。<br><img src="https://img-blog.csdnimg.cn/5f3bd119d54d47f0888c24c92f5ad07a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>三、GPT-2</p><ul><li>BERT全称是“Bidirectional Encoder Representation from Transformers“，即双向Transformer解码器。</li><li>“自回归（auto-regression）”：这类模型的实际工作方式是，在产生每个 token 之后，将这个 token 添加到输入的序列中，形成一个新序列。然后这个新序列成为模型在下一个时间步的输入，这种做法可以使得 RNN 非常有效。</li><li>gpt2的训练方式是生成文本，类似解码。bert是用masker-ML训练，是提取特征建立语言模型。</li><li>GPT-2 能够处理 1024 个 token。GPT-2 和传统的语言模型一样，一次输出一个 token。但若是一直选择模型建议的单词，它有时会陷入重复的循环之中，唯一的出路就是点击第二个或者第三个建议的单词。GPT-2 有一个 top-k 参数，可以用来选择top-1之外的其他词。</li><li>GPT-2 的每一层都保留了它自己对第一个 token 的解释，而且会在预测第二个 token 时使用它，但是不会反过来根据后面的token重新计算前面的token。<h3 id="3-2-模型输出流程"><a href="#3-2-模型输出流程" class="headerlink" title="3.2 模型输出流程"></a>3.2 模型输出流程</h3><img src="https://img-blog.csdnimg.cn/44e0d2b8065946c98d6cb62d7c041be9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>输入一共会经过四个矩阵（QKV多头矩阵、多头结果拼接转换矩阵W0，两层全连接的矩阵）<br>W0：经过一个线性映射得到想要的维度，随后输入全连接网络。</li></ul><p><img src="https://img-blog.csdnimg.cn/9b611c0e51174b91b41a668679d8c8b2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>输入token embedding向量序列流过decoder得到的输出向量和嵌入矩阵相乘，得到选词概率logits。<br>类似于根据输出向量和词表词向量计算相似度（相乘得到）。</p><h3 id="3-3-灾难性遗忘"><a href="#3-3-灾难性遗忘" class="headerlink" title="3.3 灾难性遗忘"></a>3.3 灾难性遗忘</h3><p>  为了进一步提升精调后模型的通用性以及收敛速度,可以在下游任务精调时加入一定权重的预训练任务损失。这样做是为了缓解在下游任务精调的过程中出现灾难性遗忘( Catastrophic Forgetting )问题。<br>  因为在下游任务精调过程中, GPT 的训练目标是优化下游任务数据上的效果,更强调特殊性。因此,势必会对预训练阶段学习的通用知识产生部分的覆盖或擦除,丢失一定的通用性。通过结合下游任务精调损失和预训练任务损失,可以有效地缓解灾难性遗忘问题,在优化下游任务效果的同时保留一定的通用性。<br>损失函数=精调任务损失+λ \lambdaλ预训练任务损失<br>一般设置λ = 0.5 \lambda=0.5λ=0.5，因为在精调下游任务时，主要目的还是优化有标注数据集的效果，即优化精调任务损失。预训练任务损失的加入只是为了提升精调模型的通用性，其重要程度不及精调任务损失。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、attention&quot;&gt;&lt;a href=&quot;#一、attention&quot; class=&quot;headerlink&quot; title=&quot;一、attention&quot;&gt;&lt;/a&gt;一、attention&lt;/h2&gt;&lt;h3 id=&quot;1-1循环神经网络的不足：&quot;&gt;&lt;a href=&quot;#1-1循环神经网络的不足：&quot; class=&quot;headerlink&quot; title=&quot;1.1循环神经网络的不足：&quot;&gt;&lt;/a&gt;1.1循环神经网络的不足：&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;长距离衰减问题&lt;/li&gt;
&lt;li&gt;解码阶段，越靠后的内容，翻译效果越差&lt;/li&gt;
&lt;li&gt;解码阶段缺乏对编码阶段各个词的直接利用&lt;h3 id=&quot;1-2-attention在机器翻译的优点&quot;&gt;&lt;a href=&quot;#1-2-attention在机器翻译的优点&quot; class=&quot;headerlink&quot; title=&quot;1.2 attention在机器翻译的优点&quot;&gt;&lt;/a&gt;1.2 attention在机器翻译的优点&lt;/h3&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;使用全部token信息而非最后时刻的context信息。由此在解码时每时刻可以计算attention权重，让输出对输入进行聚焦的能力，找到此时刻解码时最该注意的词。&lt;/li&gt;
&lt;li&gt;attention的计算是序列各tokens的v向量和attention权重加权求和，每个词关注到所有词，一步到位，不存在长距离衰减&lt;/li&gt;
&lt;li&gt;可以关注到不同位置的词语，而且使用多头和多层注意力、加入FFNN，表达能力更强。</summary>
    
    
    
    <category term="8月组队学习：nlp之transformers" scheme="https://zhxnlp.github.io/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/"/>
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="transformers" scheme="https://zhxnlp.github.io/tags/transformers/"/>
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>集成学习3： XGBoost&amp;LightGBM</title>
    <link href="https://zhxnlp.github.io/2021/12/04/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A03%EF%BC%9A%20XGBoost&amp;LightGBM/"/>
    <id>https://zhxnlp.github.io/2021/12/04/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A03%EF%BC%9A%20XGBoost&amp;LightGBM/</id>
    <published>2021-12-04T13:13:48.000Z</published>
    <updated>2022-01-02T21:06:22.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、XGBoost"><a href="#一、XGBoost" class="headerlink" title="一、XGBoost"></a>一、XGBoost</h2><blockquote><p><a href="https://xgboost.readthedocs.io/en/latest/python/python_intro.html">XGBoost官方文档</a></p><h3 id="1-1-XGBoost原理及构建"><a href="#1-1-XGBoost原理及构建" class="headerlink" title="1.1 XGBoost原理及构建"></a>1.1 XGBoost原理及构建</h3><p>XGBoost本质上还是一个GBDT，是一个优化的分布式梯度增强库，旨在实现高效，灵活和便携。Xgboost以CART决策树为子模型，通过Gradient Tree Boosting实现多棵CART树的集成学习，得到最终模型。</p></blockquote><p>XGBoost的最终模型构建：<br><img src="https://img-blog.csdnimg.cn/d87b34dc7f8747f4b0cd74dcea1eb70d.png" alt="在这里插入图片描述"><br>因此，目标函数的构建为：                                </p><script type="math/tex; mode=display">\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)</script><span id="more"></span>                                    <p>其中，$\sum<em>{i} l(\hat{y}</em>{i}, y<em>{i})$为loss function，$\sum</em>{k} \Omega\left(f_{k}\right)$为正则化项。    </p><p>(2) 叠加式的训练(Additive Training)：       </p><p>给定样本$x_i$，$\hat{y}_i^{(0)} = 0$(初始预测)，$\hat{y}_i^{(1)} = \hat{y}_i^{(0)} + f_1(x_i)$，$\hat{y}_i^{(2)} = \hat{y}_i^{(0)} + f_1(x_i) + f_2(x_i) = \hat{y}_i^{(1)} + f_2(x_i)$…….以此类推，可以得到：<script type="math/tex">\hat{y}_i^{(K)} = \hat{y}_i^{(K-1)} + f_K(x_i)</script> 其中，$\hat{y}_i^{(K-1)}$ 为前K-1棵树的预测结果，$f_K(x_i)$ 为第K棵树的预测结果。<br>因此，目标函数可以分解为：                                        </p><script type="math/tex; mode=display">\mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathrm{x}_{i}\right)\right)+\sum_{k} \Omega\left(f_{k}\right)</script><p>由于正则化项也可以分解为前K-1棵树的复杂度加第K棵树的复杂度，因此：<script type="math/tex">\mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathrm{x}_{i}\right)\right)+\sum_{k=1} ^{K-1}\Omega\left(f_{k}\right)+\Omega\left(f_{K}\right)</script>由于$\sum<em>{k=1} ^{K-1}\Omega\left(f</em>{k}\right)$在模型构建到第K棵树的时候已经固定，无法改变，因此是一个已知的常数，可以在最优化的时候省去，故：                     </p><script type="math/tex; mode=display">\mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathrm{x}_{i}\right)\right)+\Omega\left(f_{K}\right)</script><p>(3) 使用泰勒级数<strong>近似</strong>目标函数：                                      </p><script type="math/tex; mode=display">\mathcal{L}^{(K)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(K-1)}\right)+g_{i} f_{K}\left(\mathrm{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathrm{x}_{i}\right)\right]+\Omega\left(f_{K}\right)</script><p>其中，$g<em>{i}=\partial</em>{\hat{y}(t-1)} l\left(y<em>{i}, \hat{y}^{(t-1)}\right)$和$h</em>{i}=\partial<em>{\hat{y}^{(t-1)}}^{2} l\left(y</em>{i}, \hat{y}^{(t-1)}\right)$<br>在这里，我们补充下泰勒级数的相关知识：<br>在数学中，泰勒级数（英语：Taylor series）用无限项连加式——级数来表示一个函数，这些相加的项由函数在某一点的导数求得。具体的形式如下：                          </p><script type="math/tex; mode=display">f(x)=\frac{f\left(x_{0}\right)}{0 !}+\frac{f^{\prime}\left(x_{0}\right)}{1 !}\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2 !}\left(x-x_{0}\right)^{2}+\ldots+\frac{f^{(n)}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+......</script><p>由于$\sum<em>{i=1}^{n}l\left(y</em>{i}, \hat{y}^{(K-1)}\right)$在模型构建到第K棵树的时候已经固定，无法改变，因此是一个已知的常数，可以在最优化的时候省去，故：                               </p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(K)}=\sum_{i=1}^{n}\left[g_{i} f_{K}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{K}\right)</script><p>(4) 如何定义一棵树：<br>为了说明如何定义一棵树的问题，我们需要定义几个概念：</p><ul><li>第一个概念是样本所在的节点位置$q(x)$</li><li>第二个概念是有哪些样本落在节点j上$I<em>{j}=\left{i \mid q\left(\mathbf{x}</em>{i}\right)=j\right}$</li><li>第三个概念是每个结点的预测值$w_{q(x)}$</li><li>第四个概念是模型复杂度$\Omega\left(f<em>{K}\right)$，它可以由叶子节点的个数以及节点函数值来构建，则：$\Omega\left(f</em>{K}\right) = \gamma T+\frac{1}{2} \lambda \sum<em>{j=1}^{T} w</em>{j}^{2}$。如下图的例子：<br><img src="https://img-blog.csdnimg.cn/f2eccb86e4634309ab1fd8841cd25bfe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><p>$q(x_1) = 1,q(x_2) = 3,q(x_3) = 1,q(x_4) = 2,q(x_5) = 3$<br>$I_1 = {1,3},I_2 = {4},I_3 = {2,5}$，$w = (15,12,20)$<br>因此，目标函数用以上符号替代后：                                      </p><script type="math/tex; mode=display">\begin{aligned}\tilde{\mathcal{L}}^{(K)} &=\sum_{i=1}^{n}\left[g_{i} f_{K}\left(\mathrm{x}_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(\mathrm{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T\end{aligned}</script><p>由于我们的目标就是最小化目标函数，现在的目标函数化简为一个关于w的二次函数：<script type="math/tex">\tilde{\mathcal{L}}^{(K)}=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T</script>根据二次函数求极值的公式：$y=ax^2 bx c$求极值，对称轴在$x=-\frac{b}{2 a}$，极值为$y=\frac{4 a c-b^{2}}{4 a}$，因此：                                       </p><script type="math/tex; mode=display">w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}</script><p>以及</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(K)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T</script><p>(5) 如何寻找树的形状：<br>不难发现，刚刚的讨论都是基于树的形状已经确定了计算$w$和$L$，但是实际上我们需要像学习决策树一样找到树的形状。因此，我们借助决策树学习的方式，使用目标函数的变化来作为分裂节点的标准。我们使用一个例子来说明：<br><img src="https://img-blog.csdnimg.cn/b7e9e79cd9f84537acaeabcb0d3c056e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-N7UR4bz6-1638620761777)(./17.png)\]"></p><p>例子中有8个样本，分裂方式如下，因此:                                    </p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(old)} = -\frac{1}{2}[\frac{(g_7 + g_8)^2}{H_7+H_8 + \lambda} + \frac{(g_1 +...+ g_6)^2}{H_1+...+H_6 + \lambda}] + 2\gamma \\\tilde{\mathcal{L}}^{(new)} = -\frac{1}{2}[\frac{(g_7 + g_8)^2}{H_7+H_8 + \lambda} + \frac{(g_1 +...+ g_3)^2}{H_1+...+H_3 + \lambda} + \frac{(g_4 +...+ g_6)^2}{H_4+...+H_6 + \lambda}] + 3\gamma\\\tilde{\mathcal{L}}^{(old)} - \tilde{\mathcal{L}}^{(new)} = \frac{1}{2}[ \frac{(g_1 +...+ g_3)^2}{H_1+...+H_3 + \lambda} + \frac{(g_4 +...+ g_6)^2}{H_4+...+H_6 + \lambda} - \frac{(g_1+...+g_6)^2}{h_1+...+h_6+\lambda}] - \gamma</script><p>因此，从上面的例子看出：分割节点的标准为$max{\tilde{\mathcal{L}}^{(old)} - \tilde{\mathcal{L}}^{(new)} }$，即：                               </p><script type="math/tex; mode=display">\mathcal{L}_{\text {split }}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma</script><h3 id="1-2-精确贪心分裂算法"><a href="#1-2-精确贪心分裂算法" class="headerlink" title="1.2 精确贪心分裂算法"></a>1.2 精确贪心分裂算法</h3><ul><li>生成新树的过程中，最基本的操作是节点分裂。节点分裂中最重 要的环节是找到最优特征及最优切分点。</li><li>精确贪心算法：首先找到所有的候 选特征及所有的候选切分点, 求其 $\mathcal{L}<em>{\text {split }}$, 然后 选择使$\mathcal{L}</em>{\mathrm{split}}$ 最大的特征及 对应切分点作为最优特征和最优切分点。节点分裂时只选择当前最优的分裂策略, 而非全局最优的分裂策略。</li><li>精确贪心算法的计算过程如下所示：<br><img src="https://img-blog.csdnimg.cn/514f06dee037441fa765f0b9cab7f428.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="1-3-基于直方图的近似算法："><a href="#1-3-基于直方图的近似算法：" class="headerlink" title="1.3 基于直方图的近似算法："></a>1.3 基于直方图的近似算法：</h3></li><li>精确贪心算法优点：它计算了所有特征、所有切分点的收益, 并从中选择了最优的, 从而保证模型能比较好地拟合了训练数据。</li><li>精确贪心算法缺点：当数据不能完全加载到内存时非常低效。算法在计算过程中需要不断在内存与磁盘之间进行数据交换，非常耗时, 并且在分布式环境中面临同样的问题</li></ul><p>基于直方图的近似算法，可以更高效地选 择最优特征及切分点。主要思想是：</p><ol><li>对某一特征寻找最优切分点时，首先对该特征的所有切分点按分位数 (如百分位) 分桶, 得到一个候选切分点集。</li><li>特征的每一个切分点都可以分到对应的分桶，对每个桶计算特征统计G和H得到直方图, G为该桶内所有样本一阶特征统计g之和, H为该桶内所有样本二阶特征统计h之和</li><li>选择所有候选特征及候选切分点中对应桶的特征统计收益最大的作为最优特征及最优切分点</li></ol><p>基于直方图的近似算法的计算过程如下所示：<br>1) 对于每个特征 $k=1,2, \cdots, m,$ 按分位数对特征 $k$ 分桶 $\Theta,$ 可得候选切分点, $S<em>{k}=\left{S</em>{k 1}, S<em>{k 2}, \cdots, S</em>{k l}\right}^{1}$<br>2) 对于每个特征 $k=1,2, \cdots, m,$ 有：                           </p><script type="math/tex; mode=display">\begin{array}{l}G_{k v} \leftarrow=\sum_{j \in\left\{j \mid s_{k, v} \geq \mathbf{x}_{j k}>s_{k, v-1\;}\right\}} g_{j} \\H_{k v} \leftarrow=\sum_{j \in\left\{j \mid s_{k, v} \geq \mathbf{x}_{j k}>s_{k, v-1\;}\right\}} h_{j}\end{array}</script><p>3) 类似精确贪心算法，依据梯度统计找到最大增益的候选切分点。</p><p>下面用一个例子说明基于直方图的近似算法：<br>假设有一个年龄特征，其特征的取值为18、19、21、31、36、37、55、57，我们需要使用近似算法找到年龄这个特征的最佳分裂点：<br><img src="https://img-blog.csdnimg.cn/3788b0ba826d4cd9b470dea1837bfd48.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>近似算法实现了两种候选切分点的构建策略：全局策略和本地策略。</p><ul><li>全局策略是在树构建的初始阶段对每一个特征确定一个候选切分点的集合, 并在该树每一层的节点分裂中均采用此集合计算收益, 整个过程候选切分点集合不改变。全局策略需要更细的分桶才能达到本地策略的精确度, 但全局策略在选取候选切分点集合时比本地策略更简单。</li><li>本地策略则是在每一次节点分裂时均重新确定候选切分点。</li><li>在XGBoost系统中, 用户可以根据需求自由选择使用==精确贪心算法、近似算法全局策略、近似算法本地策略==, 算法均可通过参数进行配置。<h3 id="1-4-XGBoost代码讲解"><a href="#1-4-XGBoost代码讲解" class="headerlink" title="1.4 XGBoost代码讲解"></a>1.4 XGBoost代码讲解</h3></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># XGBoost原生工具库的上手：</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb  <span class="comment"># 引入工具库</span></span><br><span class="line"><span class="comment"># read in data</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">&#x27;demo/data/agaricus.txt.train&#x27;</span>)   <span class="comment"># XGBoost的专属数据格式，但是也可以用dataframe或者ndarray</span></span><br><span class="line">dtest = xgb.DMatrix(<span class="string">&#x27;demo/data/agaricus.txt.test&#x27;</span>)  <span class="comment"># # XGBoost的专属数据格式，但是也可以用dataframe或者ndarray</span></span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line">param = &#123;<span class="string">&#x27;max_depth&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;eta&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;objective&#x27;</span>:<span class="string">&#x27;binary:logistic&#x27;</span> &#125;    <span class="comment"># 设置XGB的参数，使用字典形式传入</span></span><br><span class="line">num_round = <span class="number">2</span>     <span class="comment"># 使用线程数</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)   <span class="comment"># 训练</span></span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">preds = bst.predict(dtest)   <span class="comment"># 预测</span></span><br></pre></td></tr></table></figure><p>XGBoost的参数设置(括号内的名称为sklearn接口对应的参数名字):</p><blockquote><p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/luanpeng825485697/article/details/79907149">推荐博客</a>：<br><a href="https://link.zhihu.com/?target=https://xgboost.readthedocs.io/en/latest/parameter.html">推荐官方文档</a></p></blockquote><h4 id="1-4-1-XGBoost的参数"><a href="#1-4-1-XGBoost的参数" class="headerlink" title="1.4.1 XGBoost的参数"></a>1.4.1 XGBoost的参数</h4><p><strong>XGBoost的参数分为三种：</strong></p><ol><li>通用参数：（两种类型的booster，因为tree的性能比线性回归好得多，因此我们很少用线性回归。）<ul><li>booster:使用哪个弱学习器训练，默认gbtree，可选gbtree，gblinear 或dart</li><li>nthread：用于运行XGBoost的并行线程数，默认为最大可用线程数</li><li>verbosity：打印消息的详细程度。有效值为0（静默），1（警告），2（信息），3（调试）。</li></ul><ol><li><strong>Tree Booster的参数：</strong><ul><li>eta（learning_rate）：learning_rate，在更新中使用步长收缩以防止过度拟合，默认= 0.3，范围：[0,1]；典型值一般设置为：0.01-0.2</li><li>gamma（min_split_loss）：默认= 0，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要平衡。范围：[0，∞]</li><li>max_depth：默认= 6，一棵树的最大深度。增加此值将使模型更复杂，并且更可能过度拟合。范围：[0，∞]</li><li>min_child_weight：默认值= 1，如果新分裂的节点的样本权重和小于min_child_weight则停止分裂 。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合。范围：[0，∞]</li><li>max_delta_step：默认= 0，允许每个叶子输出的最大增量步长。如果将该值设置为0，则表示没有约束。如果将其设置为正值，则可以帮助使更新步骤更加保守。通常不需要此参数，但是当类极度不平衡时，它可能有助于逻辑回归。将其设置为1-10的值可能有助于控制更新。范围：[0，∞]</li><li>subsample：默认值= 1，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择一半的样本作为训练集。范围：（0,1]</li><li>sampling_method：默认= uniform，用于对训练实例进行采样的方法。<ul><li>uniform：每个训练实例的选择概率均等。通常将subsample&gt; = 0.5 设置 为良好的效果。</li><li>gradient_based：每个训练实例的选择概率与规则化的梯度绝对值成正比，具体来说就是$\sqrt{g^2+\lambda h^2}$，subsample可以设置为低至0.1，而不会损失模型精度。</li></ul></li><li>colsample_bytree：默认= 1，列采样率，也就是特征采样率。范围为（0，1]</li><li>lambda（reg_lambda）：默认=1，L2正则化权重项。增加此值将使模型更加保守。</li><li>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。</li><li>tree_method：默认=auto，XGBoost中使用的树构建算法。<ul><li>auto：使用启发式选择最快的方法。<ul><li>对于小型数据集，exact将使用精确贪婪（）。</li><li>对于较大的数据集，approx将选择近似算法（）。它建议尝试hist，gpu_hist，用大量的数据可能更高的性能。（gpu_hist）支持。external memory外部存储器。</li></ul></li><li>exact：精确的贪婪算法。枚举所有拆分的候选点。</li><li>approx：使用分位数和梯度直方图的近似贪婪算法。</li><li>hist：更快的直方图优化的近似贪婪算法。（LightGBM也是使用直方图算法）</li><li>gpu_hist：GPU hist算法的实现。</li></ul></li><li>scale_pos_weight:控制正负权重的平衡，这对于不平衡的类别很有用。Kaggle竞赛一般设置sum(negative instances) / sum(positive instances)，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。</li><li>num_parallel_tree：默认=1，每次迭代期间构造的并行树的数量。此选项用于支持增强型随机森林。</li><li>monotone_constraints：可变单调性的约束，在某些情况下，如果有非常强烈的先验信念认为真实的关系具有一定的质量，则可以使用约束条件来提高模型的预测性能。（例如params_constrained[‘monotone_constraints’] = “(1,-1)”，(1,-1)我们告诉XGBoost对第一个预测变量施加增加的约束，对第二个预测变量施加减小的约束。）</li></ul></li><li><strong>Linear Booster的参数：</strong><ul><li>lambda（reg_lambda）：默认= 0，L2正则化权重项。增加此值将使模型更加保守。归一化为训练示例数。</li><li>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。归一化为训练示例数。</li><li>updater：默认= shotgun。<ul><li>shotgun：基于shotgun算法的平行坐标下降算法。使用“ hogwild”并行性，因此每次运行都产生不确定的解决方案。</li><li>coord_descent：普通坐标下降算法。同样是多线程的，但仍会产生确定性的解决方案。</li></ul></li><li>feature_selector：默认= cyclic。特征选择和排序方法<ul><li>cyclic：通过每次循环一个特征来实现的。</li><li>shuffle：类似于cyclic，但是在每次更新之前都有随机的特征变换。</li><li>random：一个随机(有放回)特征选择器。</li><li>greedy：选择梯度最大的特征。（贪婪选择）</li><li>thrifty：近似贪婪特征选择（近似于greedy）</li></ul></li><li>top_k：要选择的最重要特征数（在greedy和thrifty内）</li></ul></li></ol></li><li><p>任务参数（这个参数用来控制理想的优化目标和每一步结果的度量方法。）</p><ul><li>objective：默认=reg:squarederror，表示最小平方误差。<ul><li><strong>reg:squarederror,最小平方误差。</strong></li><li><strong>reg:squaredlogerror,对数平方损失。$\frac{1}{2}[log(pred+1)-log(label+1)]^2$</strong></li><li><strong>reg:logistic,逻辑回归</strong></li><li>reg:pseudohubererror,使用伪Huber损失进行回归，这是绝对损失的两倍可微选择。</li><li><strong>binary:logistic,二元分类的逻辑回归，输出概率。</strong></li><li>binary:logitraw：用于二进制分类的逻辑回归，逻辑转换之前的输出得分。</li><li><strong>binary:hinge：二进制分类的铰链损失。这使预测为0或1，而不是产生概率。（SVM就是铰链损失函数）</strong></li><li>count:poisson –计数数据的泊松回归，泊松分布的输出平均值。</li><li>survival:cox：针对正确的生存时间数据进行Cox回归（负值被视为正确的生存时间）。</li><li>survival:aft：用于检查生存时间数据的加速故障时间模型。</li><li>aft_loss_distribution：survival:aft和aft-nloglik度量标准使用的概率密度函数。</li><li><strong>multi:softmax：设置XGBoost以使用softmax目标进行多类分类，还需要设置num_class（类数）</strong></li><li><strong>multi:softprob：与softmax相同，但输出向量，可以进一步重整为矩阵。结果包含属于每个类别的每个数据点的预测概率。</strong></li><li>rank:pairwise：使用LambdaMART进行成对排名，从而使成对损失最小化。</li><li>rank:ndcg：使用LambdaMART进行列表式排名，使标准化折让累积收益（NDCG）最大化。</li><li>rank:map：使用LambdaMART进行列表平均排名，使平均平均精度（MAP）最大化。</li><li>reg:gamma：使用对数链接进行伽马回归。输出是伽马分布的平均值。</li><li>reg:tweedie：使用对数链接进行Tweedie回归。</li><li>自定义损失函数和评价指标：<a href="https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html">https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html</a></li></ul></li><li>eval_metric：验证数据的评估指标，将根据目标分配默认指标（回归均方根，分类误差，排名的平均平均精度），用户可以添加多个评估指标<ul><li><strong>rmse，均方根误差；</strong>  <strong>rmsle：均方根对数误差；</strong>  mae：平均绝对误差；mphe：平均伪Huber错误；<strong>logloss：负对数似然；</strong> <strong>error：二进制分类错误率；</strong></li><li><strong>merror：多类分类错误率；</strong> <strong>mlogloss：多类logloss；</strong> <strong>auc：曲线下面积；</strong> aucpr：PR曲线下的面积；ndcg：归一化累计折扣；map：平均精度；</li></ul></li><li>seed ：随机数种子，[默认= 0]。</li></ul></li><li><p>命令行参数（这里不说了，因为很少用命令行控制台版本）</p><h4 id="1-4-2-XGBoost的调参说明："><a href="#1-4-2-XGBoost的调参说明：" class="headerlink" title="1.4.2 XGBoost的调参说明："></a>1.4.2 XGBoost的调参说明：</h4><p>参数调优的一般步骤:</p><ul><li>确定学习速率和提升参数调优的初始值</li><li>max_depth 和 min_child_weight 参数调优</li><li>gamma参数调优</li><li>subsample 和 colsample_bytree 参数优</li><li>正则化参数alpha调优</li><li>降低学习速率和使用更多的决策树</li></ul></li></ol><p>具体的api请查看：<a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html">https://xgboost.readthedocs.io/en/latest/python/python_api.html</a><br>推荐github：<a href="https://github.com/dmlc/xgboost/tree/master/demo/guide-pytho">https://github.com/dmlc/xgboost/tree/master/demo/guide-pytho</a></p><h2 id="1-5-XGBoost案例"><a href="#1-5-XGBoost案例" class="headerlink" title="1.5 XGBoost案例"></a>1.5 XGBoost案例</h2><p>请查看datawhale<a href="https://github.com/zhxnlp/ensemble-learning/blob/main/CH4-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bboosting/Boosting2.ipynb">《集成学习Boosting》</a></p><h2 id="二、LightGBM算法"><a href="#二、LightGBM算法" class="headerlink" title="二、LightGBM算法"></a>二、LightGBM算法</h2><h3 id="2-1-LightGBM算法的改进"><a href="#2-1-LightGBM算法的改进" class="headerlink" title="2.1 LightGBM算法的改进"></a>2.1 LightGBM算法的改进</h3><p>ightGBM也是像XGBoost一样，是一类集成算法，他跟XGBoost总体来说是一样的，算法本质上与Xgboost没有出入，只是在XGBoost的基础上进行了优化：</p><ul><li>优化速度和内存使用<ul><li>降低了计算每个分割增益的成本。<ul><li>使用直方图减法进一步提高速度。</li><li>减少内存使用。</li><li>减少并行学习的计算成本。</li></ul></li></ul></li><li>稀疏优化<ul><li>用离散的bin替换连续的值。如果#bins较小，则可以使用较小的数据类型（例如uint8_t）来存储训练数据 。 </li><li>无需存储其他信息即可对特征数值进行预排序  。</li></ul></li><li>精度优化  <ul><li>使用叶子数为导向的决策树建立算法而不是树的深度导向。</li><li>分类特征的编码方式的优化</li><li>通信网络的优化</li><li>并行学习的优化</li><li>GPU支持</li></ul></li></ul><p>LightGBM的优点：<br>　　1）更快的训练效率<br>　　2）低内存使用<br>　　3）更高的准确率<br>　　4）支持并行化学习</p><h3 id="2-2-LightGBM参数"><a href="#2-2-LightGBM参数" class="headerlink" title="2.2 LightGBM参数"></a>2.2 LightGBM参数</h3><blockquote><p>LightGBM参数说明： <a href="https://lightgbm.apachecn.org/#/docs/6">推荐文档1</a>、<a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">推荐文档2</a><br>LightGBM与网格搜索结合调参，<a href="https://blog.csdn.net/u012735708/article/details/83749703">参考代码</a>：</p></blockquote><p><strong>1.核心参数：</strong>（括号内名称是别名）  </p><ul><li>objective（objective，app ，application）：默认regression，用于设置损失函数<ul><li>回归问题：<ul><li>L2损失：regression（regression_l2，l2，mean_squared_error，mse，l2_root，root_mean_squared_error，rmse）</li><li>L1损失：regression_l1（l1, mean_absolute_error, mae）</li><li>其他损失：huber，fair，poisson，quantile，mape，gamma，tweedie</li></ul></li><li>二分类问题：二进制对数损失分类（或逻辑回归）：binary</li><li>多类别分类：<ul><li>softmax目标函数： multiclass（softmax）</li><li>One-vs-All 目标函数：multiclassova（multiclass_ova，ova，ovr）</li></ul></li><li>交叉熵：<ul><li>用于交叉熵的目标函数（具有可选的线性权重）：cross_entropy（xentropy）</li><li>交叉熵的替代参数化：cross_entropy_lambda（xentlambda） </li></ul></li></ul></li><li>boosting ：默认gbdt，设置提升类型，选项有gbdt，rf，dart，goss，别名：boosting_type，boost<ul><li>gbdt（gbrt）:传统的梯度提升决策树</li><li>rf（random_forest）：随机森林</li><li>dart：多个加性回归树的DROPOUT方法 Dropouts meet Multiple Additive Regression Trees，参见：<a href="https://arxiv.org/abs/1505.01866">https://arxiv.org/abs/1505.01866</a></li><li>goss：基于梯度的单边采样 Gradient-based One-Side Sampling   </li></ul></li><li>data（train，train_data，train_data_file，data_filename）：用于训练的数据或数据file</li><li>valid （test，valid_data，valid_data_file，test_data，test_data_file，valid_filenames）：验证/测试数据的路径，LightGBM将输出这些数据的指标</li><li>num_iterations：默认=100，类型= INT</li><li>n_estimators：提升迭代次数，<strong>LightGBM构造用于多类分类问题的树num_class * num_iterations</strong></li><li>learning_rate（shrinkage_rate，eta） ：收缩率，默认=0.1</li><li>num_leaves（num_leaf，max_leaves，max_leaf） ：默认=31，一棵树上的最大叶子数</li><li>tree_learner （tree，tree_type，tree_learner_type）：默认=serial，可选：serial，feature，data，voting<ul><li>serial：单台机器的 tree learner</li><li>feature：特征并行的 tree learner</li><li>data：数据并行的 tree learner</li><li>voting：投票并行的 tree learner</li></ul></li><li>num_threads（num_thread, nthread）：LightGBM 的线程数，为了更快的速度, 将此设置为真正的 CPU 内核数, 而不是线程的数量 (大多数 CPU 使用超线程来使每个 CPU 内核生成 2 个线程)，当你的数据集小的时候不要将它设置的过大 (比如, 当数据集有 10,000 行时不要使用 64 线程)，对于并行学习, 不应该使用全部的 CPU 内核, 因为这会导致网络性能不佳。  </li><li>device（device_type）：默认cpu，为树学习选择设备, 你可以使用 GPU 来获得更快的学习速度，可选cpu, gpu。</li><li>seed （random_seed，random_state）：与其他种子相比，该种子具有较低的优先级，这意味着如果您明确设置其他种子，它将被覆盖。</li></ul><p><strong>2.用于控制模型学习过程的参数：</strong></p><ul><li>max_depth：限制树模型的最大深度. 这可以在 #data 小的情况下防止过拟合. 树仍然可以通过 leaf-wise 生长。</li><li>min_data_in_leaf： 默认=20，一个叶子上数据的最小数量. 可以用来处理过拟合。</li><li>min_sum_hessian_in_leaf（min_sum_hessian_per_leaf, min_sum_hessian, min_hessian）：默认=1e-3，一个叶子上的最小 hessian 和. 类似于 min_data_in_leaf, 可以用来处理过拟合.</li><li>feature_fraction：default=1.0，如果 feature_fraction 小于 1.0, LightGBM 将会在每次迭代中随机选择部分特征. 例如, 如果设置为 0.8, 将会在每棵树训练之前选择 80% 的特征，可以用来加速训练，可以用来处理过拟合。</li><li>feature_fraction_seed：默认=2，feature_fraction 的随机数种子。</li><li>bagging_fraction（sub_row, subsample）：默认=1，不进行重采样的情况下随机选择部分数据</li><li>bagging_freq（subsample_freq）：bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging</li><li>bagging_seed（bagging_fraction_seed） ：默认=3，bagging 随机数种子。</li><li>early_stopping_round（early_stopping_rounds, early_stopping）：默认=0，如果一个验证集的度量在 early_stopping_round 循环中没有提升, 将停止训练</li><li>lambda_l1（reg_alpha）：L1正则化系数</li><li>lambda_l2（reg_lambda）：L2正则化系数</li><li>min_split_gain（min_gain_to_split）：执行切分的最小增益，默认=0.</li><li>cat_smooth：默认=10，用于分类特征，可以降低噪声在分类特征中的影响, 尤其是对数据很少的类别</li></ul><p><strong>3.度量参数：</strong></p><ul><li>metric：default={l2 for regression}, {binary_logloss for binary classification}, {ndcg for lambdarank}, type=multi-enum, options=l1, l2, ndcg, auc, binary_logloss, binary_error …<ul><li>l1, absolute loss, alias=mean_absolute_error, mae</li><li>l2, square loss, alias=mean_squared_error, mse</li><li>l2_root, root square loss, alias=root_mean_squared_error, rmse</li><li>quantile, Quantile regression</li><li>huber, Huber loss</li><li>fair, Fair loss</li><li>poisson, Poisson regression</li><li>ndcg, NDCG</li><li>map, MAP</li><li>auc, AUC</li><li>binary_logloss, log loss</li><li>binary_error, 样本: 0 的正确分类, 1 错误分类</li><li>multi_logloss, mulit-class 损失日志分类</li><li>multi_error, error rate for mulit-class 出错率分类</li><li>xentropy, cross-entropy (与可选的线性权重), alias=cross_entropy</li><li>xentlambda, “intensity-weighted” 交叉熵, alias=cross_entropy_lambda</li><li>kldiv, Kullback-Leibler divergence, alias=kullback_leibler</li><li>支持多指标, 使用 , 分隔</li></ul></li><li>train_metric（training_metric, is_training_metric）：默认=False，如果你需要输出训练的度量结果则设置 true  </li></ul><p><strong>4.GPU 参数：</strong></p><ul><li>gpu_device_id：default为-1, 这个default意味着选定平台上的设备。     <h3 id="2-3-LightGBM与网格搜索结合调参"><a href="#2-3-LightGBM与网格搜索结合调参" class="headerlink" title="2.3 LightGBM与网格搜索结合调参"></a>2.3 LightGBM与网格搜索结合调参</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"> </span><br><span class="line">canceData=load_breast_cancer()</span><br><span class="line">X=canceData.data</span><br><span class="line">y=canceData.target</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=<span class="number">0</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">### 数据转换</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据转换&#x27;</span>)</span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=<span class="literal">False</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">### 设置初始参数--不含交叉验证参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;设置参数&#x27;</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">          <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;nthread&#x27;</span>:<span class="number">4</span>,</span><br><span class="line">          <span class="string">&#x27;learning_rate&#x27;</span>:<span class="number">0.1</span></span><br><span class="line">          &#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">### 交叉验证(调参)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;交叉验证&#x27;</span>)</span><br><span class="line">max_auc = <span class="built_in">float</span>(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">best_params = &#123;&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;调参1：提高准确率&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> num_leaves <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">100</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> max_depth <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">8</span>,<span class="number">1</span>):</span><br><span class="line">        params[<span class="string">&#x27;num_leaves&#x27;</span>] = num_leaves</span><br><span class="line">        params[<span class="string">&#x27;max_depth&#x27;</span>] = max_depth</span><br><span class="line"> </span><br><span class="line">        cv_results = lgb.cv(</span><br><span class="line">                            params,</span><br><span class="line">                            lgb_train,</span><br><span class="line">                            seed=<span class="number">1</span>,</span><br><span class="line">                            nfold=<span class="number">5</span>,</span><br><span class="line">                            metrics=[<span class="string">&#x27;auc&#x27;</span>],</span><br><span class="line">                            early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">                            verbose_eval=<span class="literal">True</span></span><br><span class="line">                            )</span><br><span class="line">            </span><br><span class="line">        mean_auc = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).<span class="built_in">max</span>()</span><br><span class="line">        boost_rounds = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).idxmax()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> mean_auc &gt;= max_auc:</span><br><span class="line">            max_auc = mean_auc</span><br><span class="line">            best_params[<span class="string">&#x27;num_leaves&#x27;</span>] = num_leaves</span><br><span class="line">            best_params[<span class="string">&#x27;max_depth&#x27;</span>] = max_depth</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;num_leaves&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;max_depth&#x27;</span> <span class="keyword">in</span> best_params.keys():          </span><br><span class="line">    params[<span class="string">&#x27;num_leaves&#x27;</span>] = best_params[<span class="string">&#x27;num_leaves&#x27;</span>]</span><br><span class="line">    params[<span class="string">&#x27;max_depth&#x27;</span>] = best_params[<span class="string">&#x27;max_depth&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 过拟合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;调参2：降低过拟合&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> max_bin <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">256</span>,<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> min_data_in_leaf <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">102</span>,<span class="number">10</span>):</span><br><span class="line">            params[<span class="string">&#x27;max_bin&#x27;</span>] = max_bin</span><br><span class="line">            params[<span class="string">&#x27;min_data_in_leaf&#x27;</span>] = min_data_in_leaf</span><br><span class="line">            </span><br><span class="line">            cv_results = lgb.cv(</span><br><span class="line">                                params,</span><br><span class="line">                                lgb_train,</span><br><span class="line">                                seed=<span class="number">1</span>,</span><br><span class="line">                                nfold=<span class="number">5</span>,</span><br><span class="line">                                metrics=[<span class="string">&#x27;auc&#x27;</span>],</span><br><span class="line">                                early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">                                verbose_eval=<span class="literal">True</span></span><br><span class="line">                                )</span><br><span class="line">                    </span><br><span class="line">            mean_auc = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).<span class="built_in">max</span>()</span><br><span class="line">            boost_rounds = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).idxmax()</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> mean_auc &gt;= max_auc:</span><br><span class="line">                max_auc = mean_auc</span><br><span class="line">                best_params[<span class="string">&#x27;max_bin&#x27;</span>]= max_bin</span><br><span class="line">                best_params[<span class="string">&#x27;min_data_in_leaf&#x27;</span>] = min_data_in_leaf</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;max_bin&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;min_data_in_leaf&#x27;</span> <span class="keyword">in</span> best_params.keys():</span><br><span class="line">    params[<span class="string">&#x27;min_data_in_leaf&#x27;</span>] = best_params[<span class="string">&#x27;min_data_in_leaf&#x27;</span>]</span><br><span class="line">    params[<span class="string">&#x27;max_bin&#x27;</span>] = best_params[<span class="string">&#x27;max_bin&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;调参3：降低过拟合&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> feature_fraction <span class="keyword">in</span> [<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line">    <span class="keyword">for</span> bagging_fraction <span class="keyword">in</span> [<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line">        <span class="keyword">for</span> bagging_freq <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">50</span>,<span class="number">5</span>):</span><br><span class="line">            params[<span class="string">&#x27;feature_fraction&#x27;</span>] = feature_fraction</span><br><span class="line">            params[<span class="string">&#x27;bagging_fraction&#x27;</span>] = bagging_fraction</span><br><span class="line">            params[<span class="string">&#x27;bagging_freq&#x27;</span>] = bagging_freq</span><br><span class="line">            </span><br><span class="line">            cv_results = lgb.cv(</span><br><span class="line">                                params,</span><br><span class="line">                                lgb_train,</span><br><span class="line">                                seed=<span class="number">1</span>,</span><br><span class="line">                                nfold=<span class="number">5</span>,</span><br><span class="line">                                metrics=[<span class="string">&#x27;auc&#x27;</span>],</span><br><span class="line">                                early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">                                verbose_eval=<span class="literal">True</span></span><br><span class="line">                                )</span><br><span class="line">                    </span><br><span class="line">            mean_auc = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).<span class="built_in">max</span>()</span><br><span class="line">            boost_rounds = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).idxmax()</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> mean_auc &gt;= max_auc:</span><br><span class="line">                max_auc=mean_auc</span><br><span class="line">                best_params[<span class="string">&#x27;feature_fraction&#x27;</span>] = feature_fraction</span><br><span class="line">                best_params[<span class="string">&#x27;bagging_fraction&#x27;</span>] = bagging_fraction</span><br><span class="line">                best_params[<span class="string">&#x27;bagging_freq&#x27;</span>] = bagging_freq</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;feature_fraction&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;bagging_fraction&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;bagging_freq&#x27;</span> <span class="keyword">in</span> best_params.keys():</span><br><span class="line">    params[<span class="string">&#x27;feature_fraction&#x27;</span>] = best_params[<span class="string">&#x27;feature_fraction&#x27;</span>]</span><br><span class="line">    params[<span class="string">&#x27;bagging_fraction&#x27;</span>] = best_params[<span class="string">&#x27;bagging_fraction&#x27;</span>]</span><br><span class="line">    params[<span class="string">&#x27;bagging_freq&#x27;</span>] = best_params[<span class="string">&#x27;bagging_freq&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;调参4：降低过拟合&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> lambda_l1 <span class="keyword">in</span> [<span class="number">1e-5</span>,<span class="number">1e-3</span>,<span class="number">1e-1</span>,<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line">    <span class="keyword">for</span> lambda_l2 <span class="keyword">in</span> [<span class="number">1e-5</span>,<span class="number">1e-3</span>,<span class="number">1e-1</span>,<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.4</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line">        params[<span class="string">&#x27;lambda_l1&#x27;</span>] = lambda_l1</span><br><span class="line">        params[<span class="string">&#x27;lambda_l2&#x27;</span>] = lambda_l2</span><br><span class="line">        cv_results = lgb.cv(</span><br><span class="line">                            params,</span><br><span class="line">                            lgb_train,</span><br><span class="line">                            seed=<span class="number">1</span>,</span><br><span class="line">                            nfold=<span class="number">5</span>,</span><br><span class="line">                            metrics=[<span class="string">&#x27;auc&#x27;</span>],</span><br><span class="line">                            early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">                            verbose_eval=<span class="literal">True</span></span><br><span class="line">                            )</span><br><span class="line">                </span><br><span class="line">        mean_auc = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).<span class="built_in">max</span>()</span><br><span class="line">        boost_rounds = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).idxmax()</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> mean_auc &gt;= max_auc:</span><br><span class="line">            max_auc=mean_auc</span><br><span class="line">            best_params[<span class="string">&#x27;lambda_l1&#x27;</span>] = lambda_l1</span><br><span class="line">            best_params[<span class="string">&#x27;lambda_l2&#x27;</span>] = lambda_l2</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;lambda_l1&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;lambda_l2&#x27;</span> <span class="keyword">in</span> best_params.keys():</span><br><span class="line">    params[<span class="string">&#x27;lambda_l1&#x27;</span>] = best_params[<span class="string">&#x27;lambda_l1&#x27;</span>]</span><br><span class="line">    params[<span class="string">&#x27;lambda_l2&#x27;</span>] = best_params[<span class="string">&#x27;lambda_l2&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;调参5：降低过拟合2&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> min_split_gain <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line">    params[<span class="string">&#x27;min_split_gain&#x27;</span>] = min_split_gain</span><br><span class="line">    </span><br><span class="line">    cv_results = lgb.cv(</span><br><span class="line">                        params,</span><br><span class="line">                        lgb_train,</span><br><span class="line">                        seed=<span class="number">1</span>,</span><br><span class="line">                        nfold=<span class="number">5</span>,</span><br><span class="line">                        metrics=[<span class="string">&#x27;auc&#x27;</span>],</span><br><span class="line">                        early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">                        verbose_eval=<span class="literal">True</span></span><br><span class="line">                        )</span><br><span class="line">            </span><br><span class="line">    mean_auc = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).<span class="built_in">max</span>()</span><br><span class="line">    boost_rounds = pd.Series(cv_results[<span class="string">&#x27;auc-mean&#x27;</span>]).idxmax()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> mean_auc &gt;= max_auc:</span><br><span class="line">        max_auc=mean_auc</span><br><span class="line">        </span><br><span class="line">        best_params[<span class="string">&#x27;min_split_gain&#x27;</span>] = min_split_gain</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;min_split_gain&#x27;</span> <span class="keyword">in</span> best_params.keys():</span><br><span class="line">    params[<span class="string">&#x27;min_split_gain&#x27;</span>] = best_params[<span class="string">&#x27;min_split_gain&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(best_params)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line"><span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">30</span>,</span><br><span class="line"><span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.1</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"><span class="string">&#x27;max_bin&#x27;</span>: <span class="number">255</span>,</span><br><span class="line"><span class="string">&#x27;max_depth&#x27;</span>: <span class="number">4</span>,</span><br><span class="line"><span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">81</span>,</span><br><span class="line"><span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.1</span>,</span><br><span class="line"><span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、XGBoost&quot;&gt;&lt;a href=&quot;#一、XGBoost&quot; class=&quot;headerlink&quot; title=&quot;一、XGBoost&quot;&gt;&lt;/a&gt;一、XGBoost&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/python/python_intro.html&quot;&gt;XGBoost官方文档&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;1-1-XGBoost原理及构建&quot;&gt;&lt;a href=&quot;#1-1-XGBoost原理及构建&quot; class=&quot;headerlink&quot; title=&quot;1.1 XGBoost原理及构建&quot;&gt;&lt;/a&gt;1.1 XGBoost原理及构建&lt;/h3&gt;&lt;p&gt;XGBoost本质上还是一个GBDT，是一个优化的分布式梯度增强库，旨在实现高效，灵活和便携。Xgboost以CART决策树为子模型，通过Gradient Tree Boosting实现多棵CART树的集成学习，得到最终模型。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;XGBoost的最终模型构建：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/d87b34dc7f8747f4b0cd74dcea1eb70d.png&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;br&gt;因此，目标函数的构建为：                                &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)&lt;/script&gt;</summary>
    
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Boosting" scheme="https://zhxnlp.github.io/tags/Boosting/"/>
    
    <category term="XGBoost" scheme="https://zhxnlp.github.io/tags/XGBoost/"/>
    
    <category term="LightGBM" scheme="https://zhxnlp.github.io/tags/LightGBM/"/>
    
  </entry>
  
  <entry>
    <title>集成学习2：Boosting算法：Adaboost&amp;GBDT</title>
    <link href="https://zhxnlp.github.io/2021/12/03/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A02%EF%BC%9ABoosting%E7%AE%97%E6%B3%95%EF%BC%9AAdaboost&amp;GBDT/"/>
    <id>https://zhxnlp.github.io/2021/12/03/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A02%EF%BC%9ABoosting%E7%AE%97%E6%B3%95%EF%BC%9AAdaboost&amp;GBDT/</id>
    <published>2021-12-02T22:49:40.000Z</published>
    <updated>2022-01-02T20:49:08.001Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、-Boosting算法原理"><a href="#一、-Boosting算法原理" class="headerlink" title="一、 Boosting算法原理"></a>一、 Boosting算法原理</h2><ul><li>Bagging：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。</li><li>Bagging主要通过降低方差的方式减少预测误差&lt;/font&gt;</li><li>Boosting：使用同一组数据集进行反复学习，得到一系列简单模型，然后组合这些模型构成一个预测性能十分强大的机器学习模型。</li><li>Boosting通过不断减少偏差的形式提高最终的预测效果，与Bagging有着本质的不同。</li></ul><p>在概率近似正确（PAC）学习的框架下：</p><ol><li>弱学习：识别准确率略高于1/2（即准确率仅比随机猜测略高的学习算法）</li><li>强学习：识别准确率很高并能在多项式时间内完成的学习算法</li><li>强可学习和弱可学习是等价的，弱可学习算法，能提升至强可学习算法<span id="more"></span></li></ol><ul><li>提升方法：从弱学习算法出发，反复学习，得到一系列弱分类器(又称为基本分类器)，再通过一定的形式去组合这些弱分类器构成一个强分类器。而弱可学习算法比强可学习算法容易得多。</li><li>大多数的Boosting方法都是通过改变训练数据集的概率分布(训练数据不同样本的权值)，针对不同概率分布的数据调用弱分类算法学习一系列的弱分类器。</li></ul><p>Boosting方法关键点：</p><ol><li>每一轮学习应该如何改变数据的概率分布</li><li>如何将各个弱分类器组合起来</li></ol><h2 id="二、-Adaboost算法"><a href="#二、-Adaboost算法" class="headerlink" title="二、 Adaboost算法"></a>二、 Adaboost算法</h2><h3 id="2-1-Adaboost算法原理"><a href="#2-1-Adaboost算法原理" class="headerlink" title="2.1 Adaboost算法原理"></a>2.1 Adaboost算法原理</h3><p>Adaboost解决上述的两个问题的方式是：</p><ol><li>提高那些被前一轮分类器错误分类的样本的权重&lt;/font&gt;，而降低那些被正确分类的样本的权重。错误分类样本权重的增大而在后一轮的训练中“备受关注”。</li><li>各个弱分类器通过采取加权多数表决的方式组合&lt;/font&gt;。分类错误率低的弱分类器权重高，分类错误率较大的弱分类器权重低。</li><li><p>Adaboost等Boosting模型增加了计算的复杂度和计算成本，用降低偏差的方式去减少总误差，但是过程中引入了方差，可能出现过拟合</p></li><li><p>Boosting方式无法做到现在流行的并行计算的方式进行训练，因为每一步迭代都要基于上一部的基本分类器。</p></li><li>Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</li></ol><p>Adaboost算法：</p><ul><li>输入：二分类的训练数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$，特征$x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，类别$y</em>{i} \in \mathcal{Y}={-1,+1}$，$\mathcal{X}$是特征空间，$\mathcal{Y}$是类别集合，其中每个样本点由特征与类别组成。</li><li>输出：最终分类器$G(x)$。</li></ul><p>(1) 初始化训练集样本的权值分布：<script type="math/tex">D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N</script>其中权值是均匀分布，使得第一次没有先验信息的条件下每个样本在基本分类器的学习中作用一样。</p><p>(2) 对于学习轮次m=1,2,…,M</p><ol><li>使用具有权值分布$D<em>m$的训练数据集进行学习，得到基本分类器：$G</em>{m}(x): \mathcal{X} \rightarrow{-1,+1}$</li><li>计算$G<em>m(x)$在训练集上的分类误差率$$e</em>{m}=\sum<em>{i=1}^{N} P\left(G</em>{m}\left(x<em>{i}\right) \neq y</em>{i}\right)=\sum<em>{i=1}^{N} w</em>{m i} I\left(G<em>{m}\left(x</em>{i}\right) \neq y<em>{i}\right)$$<br>$w</em>{m i}$代表了在$G_m(x)$中分类错误的样本权重和，这点直接说明了权重分布$D_m$与$G_m(x)$的分类错误率$e_m$有直接关系。</li><li>计算$G<em>m(x)$的系数$\alpha</em>{m}=\frac{1}{2} \log \frac{1-e<em>{m}}{e</em>{m}}$，这里的log是自然对数ln<ul><li>$\alpha_{m}$表示了$G_m(x)$在最终分类器的重要性程度。</li><li>当$e<em>{m} \leqslant \frac{1}{2}$时，$\alpha</em>{m} \geqslant 0$，并且$\alpha_m$随着$e_m$的减少而增大，因此分类错误率越小的基本分类器在最终分类器的作用越大！                       </li></ul></li><li><p>更新训练数据集的权重分布</p><script type="math/tex; mode=display">D_{m+1}=(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N})</script><script type="math/tex; mode=display">w_{m+1, i}=\left\{\begin{array}{ll}\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}\end{array}\right.</script><script type="math/tex; mode=display">Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)</script><p>这里的$Z<em>m$是规范化因子，使得$D</em>{m+1}$成为概率分布。</p><p>从上式可以看到：被基本分类器$G<em>m(x)$错误分类的样本的权重扩大，被正确分类的样本权重减少，二者相比相差$\mathrm{e}^{2 \alpha</em>{m}}=\frac{1-e<em>{m}}{e</em>{m}}$倍。     </p></li></ol><p>(3) 构建基本分类器的线性组合$f(x)=\sum<em>{m=1}^{M} \alpha</em>{m} G_{m}(x)$，得到最终的分类器                       </p><script type="math/tex; mode=display">\begin{aligned}G(x) &=\operatorname{sign}(f(x)) \\&=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)\end{aligned}</script><script type="math/tex; mode=display">sign(x)=\begin{cases}1 & \text{ if } x\geqslant 0 \\ -1 & \text{ if } x< 0 \end{cases}</script><p>线性组合$f(x)$实现了将M个基本分类器的加权表决，系数$\alpha_m$标志了基本分类器$G_m(x)$的重要性，值得注意的是：所有的$\alpha_m$之和不为1。$f(x)$的符号决定了样本x属于哪一类,其绝对值表示分类的确信度。</p><p>简单来说：计算M个基本分类器，每个分类器的错误率、模型权重及样本权重</p><ol><li>均匀初始化样本权重$D_{1}$</li><li>对于轮次m，针对当前权重$D<em>{m}$ 学习分类器 $G</em>{m}(x)$，并计算其分类错误率$e_{m}$。</li><li>计算分类器$G<em>m(x)$的权重系数$\alpha</em>{m}=\frac{1}{2} \log \frac{1-e<em>{m}}{e</em>{m}}$&lt;/font&gt;。$e<em>{m} \leqslant \frac{1}{2}$时，$\alpha</em>{m} \geqslant 0$，并且$\alpha_m$随着$e_m$的减少而增大，因此分类错误率越小的基本分类器在最终分类器的作用越大！</li><li>更新权重分布 <script type="math/tex">w_{m+1, i}=\left\{\begin{array}{ll}\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}\end{array}\right.</script><br>一般来说$\alpha<em>{m} \geqslant 0，e^0=1$。被基本分类器$G_m(x)$错误分类的样本的权重扩大，被正确分类的样本权重减少。$e</em>{m}$减小，$\alpha<em>{m}$增大，${w</em>{m+1, i}}$增大。即错误率越低的分类器，分类器权重和错误样本权重都越大，感觉上越准确准确的分类器学习力度越大。</li><li>基本分类器加权组合表决</li><li>总结：Adaboost不改变训练数据，而是改变其权值分布，使每一轮的基学习器学习不同权重分布的样本集，最后加权组合表决。</li></ol><h3 id="2-2-Adaboost算法举例"><a href="#2-2-Adaboost算法举例" class="headerlink" title="2.2 Adaboost算法举例"></a>2.2 Adaboost算法举例</h3><p>下面，我们使用一组简单的数据来手动计算Adaboost算法的过程：(例子来源<a href="http://www.csie.edu.tw">http://www.csie.edu.tw</a>)                                                               </p><p>训练数据如下表，假设基本分类器的形式是一个分割$x<v$或$x>v$表示，阈值v由该基本分类器在训练数据集上分类错误率$e_m$最低确定。&lt;/font&gt;                                                </p><script type="math/tex; mode=display">\begin{array}{ccccccccccc}\hline \text { 序号 } & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\hline x & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\y & 1 & 1 & 1 & -1 & -1 & -1 & 1 & 1 & 1 & -1 \\\hline\end{array}</script><p>解：<br>初始化样本权值分布</p><script type="math/tex; mode=display">\begin{aligned}D_{1} &=\left(w_{11}, w_{12}, \cdots, w_{110}\right) \\w_{1 i} &=0.1, \quad i=1,2, \cdots, 10\end{aligned}</script><ol><li><p>对m=1:                      </p><ul><li>在权值分布$D_1$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=2.5时分类误差率最低，那么基本分类器为：<script type="math/tex; mode=display">G_{1}(x)=\left\{\begin{array}{ll}1, & x<2.5 \\-1, & x>2.5\end{array}\right.</script></li><li>样本7.8.9分错，$G<em>1(x)$在训练数据集上的误差率为$e</em>{1}=P\left(G<em>{1}\left(x</em>{i}\right) \neq y_{i}\right)=0.1*3=0.3$。                                        </li><li>计算$G<em>1(x)$的系数：$\alpha</em>{1}=\frac{1}{2} \log \frac{1-e<em>{1}}{e</em>{1}}=0.4236$               </li><li>更新训练数据的权值分布：                  <script type="math/tex; mode=display">\begin{aligned}D_{2}=&\left(w_{21}, \cdots, w_{2 i}, \cdots, w_{210}\right) \\w_{2 i}=& \frac{w_{1 i}}{Z_{1}} \exp \left(-\alpha_{1} y_{i} G_{1}\left(x_{i}\right)\right), \quad i=1,2, \cdots, 10 \\D_{2}=&(0.07143,0.07143,0.07143,0.07143,0.07143,0.07143,\\&0.16667,0.16667,0.16667,0.07143) \\f_{1}(x) &=0.4236 G_{1}(x)=array([ 0.4236,  0.4236,  0.4236, -0.4236, -0.4236, -0.4236, -0.4236,  -0.4236, -0.4236, -0.4236])\end{aligned}</script>权重为[0.07143×7,0.16667×3]</li></ul></li><li><p>对于m=2：                   </p><ul><li>在权值分布$D_2$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=8.5时分类误差率最低，那么基本分类器为：                  <script type="math/tex; mode=display">G_{2}(x)=\left\{\begin{array}{ll}1, & x<8.5 \\-1, & x>8.5\end{array}\right.</script></li><li>样本4.5.6分错，$G_2(x)$在训练数据集上的误差率为$e_2 = 0.07143*3=0.2143$                    </li><li>计算$G_2(x)$的系数：$\alpha_2 = 0.6496$                        </li><li>更新训练数据的权值分布：                  <script type="math/tex; mode=display">\begin{aligned}D_{3}=&(0.0455,0.0455,0.0455,0.1667,0.1667,0.1667\\&0.1060,0.1060,0.1060,0.0455) \\f_{2}(x) &=0.4236 G_{1}(x)+0.6496 G_{2}(x)=array([ 1.0732,  1.0732,  1.0732,  0.226 ,  0.226 ,  0.226 ,  0.226 ,   0.226 ,  0.226 , -1.0732])\end{aligned}</script>权重为[0.00455×4,0.1060×3,0.16667×3]。</li></ul></li><li>对m=3：                          <ul><li>在权值分布$D_3$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=5.5时分类误差率最低，那么基本分类器为：                     <script type="math/tex; mode=display">G_{3}(x)=\left\{\begin{array}{ll}1, & x>5.5 \\-1, & x<5.5\end{array}\right.</script></li><li>样本1.2.3.10分错，$G_3(x)$在训练数据集上的误差率为$e_3 =0.0455*4= 0.1820$                       </li><li>计算$G_3(x)$的系数：$\alpha_3 = 0.7514$                                 </li><li>更新训练数据的权值分布：<br>$D_{4}=(0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)$                       <script type="math/tex; mode=display">f_{3}(x)=0.4236 G_{1}(x)+0.6496 G_{2}(x)+0.7514 G_{3}(x)=array([ 0.3218,  0.3218,  0.3218, -0.5254, -0.5254, -0.5254,  0.9774,   0.9774,  0.9774, -0.3218])</script>   分类器$\operatorname{sign}\left[f<em>{3}(x)\right]$在训练数据集上的误分类点的个数为0。<br>最终分类器为：$G(x)=\operatorname{sign}\left[f</em>{3}(x)\right]=\operatorname{sign}\left[0.4236 G<em>{1}(x)+0.6496 G</em>{2}(x)+0.7514 G_{3}(x)\right]$</li></ul></li></ol><p>==可以看到每次样本权重和都为1，分类器错误率越来越低，分类器权重越来越高。最终分类器结果就是基分类器结果乘以其权重的加和==</p><h3 id="2-3-Adaboos代码举例"><a href="#2-3-Adaboos代码举例" class="headerlink" title="2.3 Adaboos代码举例"></a>2.3 Adaboos代码举例</h3><p>数据集：CI的机器学习库里的<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data">葡萄酒数据集</a>，该数据集包含了178个样本和13个特征，从不同的角度对不同的化学特性进行描述，最终预测红酒属于哪一个类别。(案例来源《python机器学习(第二版》)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 引入数据科学相关工具包：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据：         </span></span><br><span class="line">wine = pd.read_csv(<span class="string">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;</span>,header=<span class="literal">None</span>)</span><br><span class="line">wine.columns = [<span class="string">&#x27;Class label&#x27;</span>, <span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Malic acid&#x27;</span>, <span class="string">&#x27;Ash&#x27;</span>, <span class="string">&#x27;Alcalinity of ash&#x27;</span>,<span class="string">&#x27;Magnesium&#x27;</span>, <span class="string">&#x27;Total phenols&#x27;</span>,<span class="string">&#x27;Flavanoids&#x27;</span>, <span class="string">&#x27;Nonflavanoid phenols&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;Proanthocyanins&#x27;</span>,<span class="string">&#x27;Color intensity&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>,<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>,<span class="string">&#x27;Proline&#x27;</span>]</span><br><span class="line"><span class="comment"># 数据查看：</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Class labels&quot;</span>,np.unique(wine[<span class="string">&quot;Class label&quot;</span>]))</span><br><span class="line">wine.head()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/8e3e3b19c1f04d1b9257419b42646479.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>下面对数据做简单解读：</p><p>Class label：分类标签<br>Alcohol：酒精<br>Malic acid：苹果酸<br>Ash：灰<br>Alcalinity of ash：灰的碱度<br>Magnesium：镁<br>Total phenols：总酚<br>Flavanoids：黄酮类化合物<br>Nonflavanoid phenols：非黄烷类酚类<br>Proanthocyanins：原花青素<br>Color intensity：色彩强度<br>Hue：色调<br>OD280/OD315 of diluted wines：稀释酒OD280 OD350<br>Proline：脯氨酸</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 仅仅考虑2，3类葡萄酒，去除1类</span></span><br><span class="line">wine = wine[wine[<span class="string">&#x27;Class label&#x27;</span>] != <span class="number">1</span>]</span><br><span class="line">y = wine[<span class="string">&#x27;Class label&#x27;</span>].values</span><br><span class="line">X = wine[[<span class="string">&#x27;Alcohol&#x27;</span>,<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>]].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分类标签变成二进制编码：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">y = le.fit_transform(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按8：2分割训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1</span>,stratify=y)  <span class="comment"># stratify参数代表了按照y的类别等比例抽样</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用单一决策树建模</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">tree = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>,random_state=<span class="number">1</span>,max_depth=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">tree = tree.fit(X_train,y_train)</span><br><span class="line">y_train_pred = tree.predict(X_train)</span><br><span class="line">y_test_pred = tree.predict(X_test)</span><br><span class="line">tree_train = accuracy_score(y_train,y_train_pred)</span><br><span class="line">tree_test = accuracy_score(y_test,y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Decision tree train/test accuracies %.3f/%.3f&#x27;</span> % (tree_train,tree_test))</span><br><span class="line"></span><br><span class="line">Decision tree train/test accuracies <span class="number">0.916</span>/<span class="number">0.875</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用sklearn实现Adaboost(基分类器为决策树)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">AdaBoostClassifier相关参数：</span></span><br><span class="line"><span class="string">base_estimator：基本分类器，默认为DecisionTreeClassifier(max_depth=1)</span></span><br><span class="line"><span class="string">n_estimators：终止迭代的次数</span></span><br><span class="line"><span class="string">learning_rate：学习率</span></span><br><span class="line"><span class="string">algorithm：训练的相关算法，&#123;&#x27;SAMME&#x27;，&#x27;SAMME.R&#x27;&#125;，默认=&#x27;SAMME.R&#x27;</span></span><br><span class="line"><span class="string">random_state：随机种子</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">ada = AdaBoostClassifier(base_estimator=tree,n_estimators=<span class="number">500</span>,learning_rate=<span class="number">0.1</span>,random_state=<span class="number">1</span>)</span><br><span class="line">ada = ada.fit(X_train,y_train)</span><br><span class="line">y_train_pred = ada.predict(X_train)</span><br><span class="line">y_test_pred = ada.predict(X_test)</span><br><span class="line">ada_train = accuracy_score(y_train,y_train_pred)</span><br><span class="line">ada_test = accuracy_score(y_test,y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Adaboost train/test accuracies %.3f/%.3f&#x27;</span> % (ada_train,ada_test))</span><br><span class="line"></span><br><span class="line">Adaboost train/test accuracies <span class="number">1.000</span>/<span class="number">0.917</span></span><br></pre></td></tr></table></figure><p>结果分析：单层决策树似乎对训练数据欠拟合，而Adaboost模型正确地预测了训练数据的所有分类标签，而且与单层决策树相比，Adaboost的测试性能也略有提高。然而，为什么模型在训练集和测试集的性能相差这么大呢？我们使用图像来简单说明下这个道理！</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出单层决策树与Adaboost的决策边界：</span></span><br><span class="line">x_min = X_train[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">x_max = X_train[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min = X_train[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">y_max = X_train[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.1</span>),np.arange(y_min, y_max, <span class="number">0.1</span>))</span><br><span class="line">f, axarr = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>,sharex=<span class="string">&#x27;col&#x27;</span>,sharey=<span class="string">&#x27;row&#x27;</span>,figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="number">0</span>, <span class="number">1</span>],[tree, ada],[<span class="string">&#x27;Decision tree&#x27;</span>, <span class="string">&#x27;Adaboost&#x27;</span>]):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx].contourf(xx, yy, Z, alpha=<span class="number">0.3</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">0</span>, <span class="number">0</span>],X_train[y_train==<span class="number">0</span>, <span class="number">1</span>],c=<span class="string">&#x27;blue&#x27;</span>, marker=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">1</span>, <span class="number">0</span>],X_train[y_train==<span class="number">1</span>, <span class="number">1</span>],c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    axarr[idx].set_title(tt)</span><br><span class="line">axarr[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;Alcohol&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.text(<span class="number">0</span>, -<span class="number">0.2</span>,s=<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>,ha=<span class="string">&#x27;center&#x27;</span>,va=<span class="string">&#x27;center&#x27;</span>,fontsize=<span class="number">12</span>,transform=axarr[<span class="number">1</span>].transAxes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/e7ee7c34eacb4218b8e52011424f4055.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>从上面的决策边界图可以看到：</p><ul><li>Adaboost模型的决策边界比单层决策树的决策边界要复杂的多。也就是说，Adaboost试图用增加模型复杂度而降低偏差的方式去减少总误差，但是过程中引入了方差，可能出现过拟合</li><li>与单个分类器相比，Adaboost等Boosting模型增加了计算的复杂度，在实践中需要仔细思考是否愿意为预测性能的相对改善而增加计算成本</li><li>Boosting方式无法做到现在流行的并行计算的方式进行训练，因为每一步迭代都要基于上一部的基本分类器。</li></ul><h2 id="三、-前向分步算法"><a href="#三、-前向分步算法" class="headerlink" title="三、 前向分步算法"></a>三、 前向分步算法</h2><p>Adaboost的算法内容：通过计算M个基本分类器，每个分类器的错误率、样本权重以及模型权重。我们可以认为：Adaboost每次学习单一分类器以及单一分类器的参数(权重)。<br>抽象出Adaboost算法的整体框架逻辑，构建集成学习的一个非常重要的框架——前向分步算法，有了这个框架，我们不仅可以解决分类问题，也可以解决回归问题。</p><h3 id="3-1加法模型"><a href="#3-1加法模型" class="headerlink" title="3.1加法模型"></a>3.1加法模型</h3><p>在Adaboost模型中，我们把每个基本分类器合成一个复杂分类器的方法是每个基本分类器的加权和，即：$f(x)=\sum<em>{m=1}^{M} \beta</em>{m} b\left(x ; \gamma<em>{m}\right)$，其中，$b\left(x ; \gamma</em>{m}\right)$为即基本分类器，$\gamma<em>{m}$为基本分类器的参数，$\beta_m$为基本分类器的权重，显然这与第二章所学的加法模型。为什么这么说呢？大家把$b(x ; \gamma</em>{m})$看成是即函数即可。<br>在给定训练数据以及损失函数$L(y, f(x))$的条件下，学习加法模型$f(x)$就是：                        </p><script type="math/tex; mode=display">\min _{\beta_{m}, \gamma_{m}} \sum_{i=1}^{N} L\left(y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i} ; \gamma_{m}\right)\right)</script><p>通常这是一个复杂的优化问题，很难通过简单的凸优化的相关知识进行解决。前向分步算法可以用来求解这种方式的问题，它的基本思路是：因为学习的是加法模型，如果从前向后，每一步只优化一个基函数及其系数，逐步逼近目标函数，那么就可以降低优化的复杂度。具体而言，每一步只需要优化：                    </p><script type="math/tex; mode=display">\min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, \beta b\left(x_{i} ; \gamma\right)\right)</script><h3 id="3-2-前向分步算法"><a href="#3-2-前向分步算法" class="headerlink" title="3.2 前向分步算法"></a>3.2 前向分步算法</h3><p>给定数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$，$x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，$y</em>{i} \in \mathcal{Y}={+1,-1}$。损失函数$L(y, f(x))$，基函数集合${b(x ; \gamma)}$，我们需要输出加法模型$f(x)$。                         </p><ul><li>初始化：$f_{0}(x)=0$                           </li><li>对m = 1,2,…,M:                     <ul><li>(a) 极小化损失函数：<script type="math/tex; mode=display">\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)</script>得到参数$\beta<em>{m}$与$\gamma</em>{m}$                                           </li><li>(b) 更新：                          <script type="math/tex; mode=display">f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left(x ; \gamma_{m}\right)</script></li></ul></li><li>得到加法模型：                           <script type="math/tex; mode=display">f(x)=f_{M}(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)</script></li></ul><p>这样，前向分步算法将同时求解从m=1到M的所有参数$\beta<em>{m}$，$\gamma</em>{m}$的优化问题简化为逐次求解各个$\beta<em>{m}$，$\gamma</em>{m}$的问题。                           </p><h3 id="3-3-前向分步算法与Adaboost的关系"><a href="#3-3-前向分步算法与Adaboost的关系" class="headerlink" title="3.3 前向分步算法与Adaboost的关系"></a>3.3 前向分步算法与Adaboost的关系</h3><p>由于这里不是我们的重点，我们主要阐述这里的结论，不做相关证明，具体的证明见李航老师的《统计学习方法》第八章的3.2节。Adaboost算法是前向分步算法的特例，Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</p><h2 id="四、梯度提升决策树-GBDT"><a href="#四、梯度提升决策树-GBDT" class="headerlink" title="四、梯度提升决策树(GBDT)"></a>四、梯度提升决策树(GBDT)</h2><ul><li>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树。GBDT使用的决策树是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树</li><li>CART假设决策树都是二叉树，内部节点特征取值为“是”和“否”，等价于递归二分每个特征。对回归树用平方误差最小化准则（回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适），对分类树用基尼系数最小化准则，进行特征选择生成二叉树</li><li><p>回归问题没有分类错误率可言，，用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题</p><h3 id="4-1-Decision-Tree：CART回归树"><a href="#4-1-Decision-Tree：CART回归树" class="headerlink" title="4.1 Decision Tree：CART回归树"></a>4.1 Decision Tree：CART回归树</h3><p>最小二乘回归树生成算法见《统计学习方法》P82，算法5.5：<br><img src="https://img-blog.csdnimg.cn/15618f981d2f4b29a6d81ffceca8cfdb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="4-2-回归提升树算法"><a href="#4-2-回归提升树算法" class="headerlink" title="4.2 回归提升树算法"></a>4.2 回归提升树算法</h3><p>提升法：加法模型+前向分步算法。<br>提升树：以决策树为基函数（基本分类器）的提升方法，可表示为决策树的加法模型。<br>决策树：分类问题是二叉分类树，回归问题是二叉回归树</p></li><li><p>Adaboost：加法模型+前向分步算法的分类树模型</p></li><li>GBDT：     加法模型+前向分步算法的回归树模型</li></ul><p>分类误差率</p><ul><li>Adaboost算法：使用了分类错误率修正样本权重以及计算每个基本分类器的权重</li><li>GBDT：回归问题没有分类错误率可言，，用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题</li></ul><p>根据以上两点得到回归问题提升树算法：</p><p>输入：数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}, x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y</em>{i} \in \mathcal{Y} \subseteq \mathbf{R}$<br>输出：最终的提升树$f_{M}(x)$                             </p><ul><li>初始化$f_0(x) = 0$                        </li><li>对m = 1,2,…,M：                  <ul><li>计算每个样本的残差:$r<em>{m i}=y</em>{i}-f<em>{m-1}\left(x</em>{i}\right), \quad i=1,2, \cdots, N$                                    </li><li>拟合残差$r<em>{mi}$学习一棵回归树，得到$T\left(x ; \Theta</em>{m}\right)$                        </li><li>更新$f<em>{m}(x)=f</em>{m-1}(x)+T\left(x ; \Theta_{m}\right)$</li></ul></li><li>得到最终的回归问题的提升树：$f<em>{M}(x)=\sum</em>{m=1}^{M} T\left(x ; \Theta_{m}\right)$                         </li></ul><p>下面我们用一个实际的案例来使用这个算法：(案例来源：李航老师《统计学习方法》P168，此处省略)                                                             </p><h3 id="4-3-梯度提升决策树算法-GBDT"><a href="#4-3-梯度提升决策树算法-GBDT" class="headerlink" title="4.3 梯度提升决策树算法(GBDT)"></a>4.3 梯度提升决策树算法(GBDT)</h3><p> GBDT：利用损失函数的负梯度作为回归问题提升树算法中的残差的近似值，拟合回归树。</p><ul><li>提升树利用加法模型和前向分步算法实现学习的过程，当损失函数为平方损失和指数损失时，每一步优化是相当简单的，也就是我们前面探讨的提升树算法和Adaboost算法。对于一般的损失函数而言，往往每一步的优化不是那么容易</li><li>针对这一问题，Freidman提出了梯度提升算法(gradient boosting)，利用损失函数的负梯度在当前模型的值$-\left[\frac{\partial L\left(y, f\left(x<em>{i}\right)\right)}{\partial f\left(x</em>{i}\right)}\right]<em>{f(x)=f</em>{m-1}(x)}$作为回归问题提升树算法中的残差的近似值，拟合回归树。<strong>与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例。</strong></li></ul><p>以下开始具体介绍梯度提升算法：<br>输入训练数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}, x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y</em>{i} \in \mathcal{Y} \subseteq \mathbf{R}$和损失函数$L(y, f(x))$，输出回归树$\hat{f}(x)$                              </p><ul><li>初始化$f<em>{0}(x)=\arg \min </em>{c} \sum<em>{i=1}^{N} L\left(y</em>{i}, c\right)$                     </li><li>对于m=1,2,…,M：                   <ul><li>对i = 1,2,…,N计算：$r<em>{m i}=-\left[\frac{\partial L\left(y</em>{i}, f\left(x<em>{i}\right)\right)}{\partial f\left(x</em>{i}\right)}\right]<em>{f(x)=f</em>{m-1}(x)}$                </li><li>对$r<em>{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R</em>{m j}, j=1,2, \cdots, J$                           </li><li>对j=1,2,…J，计算：$c<em>{m j}=\arg \min </em>{c} \sum<em>{x</em>{i} \in R<em>{m j}} L\left(y</em>{i}, f<em>{m-1}\left(x</em>{i}\right)+c\right)$                      </li><li>更新$f<em>{m}(x)=f</em>{m-1}(x)+\sum<em>{j=1}^{J} c</em>{m j} I\left(x \in R_{m j}\right)$                    </li></ul></li><li>得到回归树：$\hat{f}(x)=f<em>{M}(x)=\sum</em>{m=1}^{M} \sum<em>{j=1}^{J} c</em>{m j} I\left(x \in R_{m j}\right)$</li></ul><p>下面，我们来使用一个具体的案例来说明GBDT是如何运作的(<a href="https://blog.csdn.net/zpalyq110/article/details/79527653">案例来源</a> )：<br>下面的表格是数据：<br><img src="https://img-blog.csdnimg.cn/14f12b8bf27d43ad90e52d67258a25b0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-hBCnWMyY-1638484890166)(./6.png)\]"></p><p>学习率：learning_rate=0.1，迭代次数：n_trees=5，树的深度：max_depth=3<br>平方损失的负梯度为：</p><script type="math/tex; mode=display">-\left[\frac{\left.\partial L\left(y, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{t-1}(x)}=y-f\left(x_{i}\right)</script><p>$c=(1.1+1.3+1.7+1.8)/4=1.475，f_{0}(x)=c=1.475$<br><img src="https://img-blog.csdnimg.cn/3319d5746aed4188b03e57d37cb53cca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt=""></p><p>学习决策树，分裂结点：<br><img src="https://img-blog.csdnimg.cn/3c1ff3f51eb7480e99248dd3fb4dba73.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/b9bcefd41c2342d99935c6070e6fbaa7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/69b7a5687482439a8025ba693cb33558.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>对于右节点，只有2，3两个样本，那么根据下表我们选择年龄30进行划分：<br><img src="https://img-blog.csdnimg.cn/7d63133f988e46648ccaa30269007385.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/cce3670868d54729933a6769d75f6a29.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-DgdpHG6v-1638484890170)(./13.png)\]"></p><p>因此根据$\Upsilon<em>{j 1}=\underbrace{\arg \min }</em>{\Upsilon} \sum<em>{x</em>{i} \in R<em>{j 1}} L\left(y</em>{i}, f<em>{0}\left(x</em>{i}\right)+\Upsilon\right)$：                                </p><script type="math/tex; mode=display">\begin{array}{l}\left(x_{0} \in R_{11}\right), \quad \Upsilon_{11}=-0.375 \\\left(x_{1} \in R_{21}\right), \quad \Upsilon_{21}=-0.175 \\\left(x_{2} \in R_{31}\right), \quad \Upsilon_{31}=0.225 \\\left(x_{3} \in R_{41}\right), \quad \Upsilon_{41}=0.325\end{array}</script><p>这里其实和上面初始化学习器是一个道理，平方损失，求导，令导数等于零，化简之后得到每个叶子节点的参数$\Upsilon$,其实就是标签值的均值。<br>最后得到五轮迭代：<br><img src="https://img-blog.csdnimg.cn/ba161ee53e4b487fa7509dca014222c8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>最后的强学习器为：$f(x)=f<em>{5}(x)=f</em>{0}(x)+\sum<em>{m=1}^{5} \sum</em>{j=1}^{4} \Upsilon<em>{j m} I\left(x \in R</em>{j m}\right)$。<br>其中：</p><script type="math/tex; mode=display">\begin{array}{ll}f_{0}(x)=1.475 & f_{2}(x)=0.0205 \\f_{3}(x)=0.1823 & f_{4}(x)=0.1640 \\f_{5}(x)=0.1476\end{array}</script><p>预测结果为：                       </p><script type="math/tex; mode=display">f(x)=1.475+0.1 *(0.2250+0.2025+0.1823+0.164+0.1476)=1.56714</script><p>为什么要用学习率呢？这是Shrinkage的思想，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。    </p><h3 id="4-3-GBDT代码示例"><a href="#4-3-GBDT代码示例" class="headerlink" title="4.3 GBDT代码示例"></a>4.3 GBDT代码示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">下面我们来使用sklearn来使用GBDT</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_friedman1</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">GradientBoostingRegressor参数解释：</span></span><br><span class="line"><span class="string">loss：&#123;‘ls’, ‘lad’, ‘huber’, ‘quantile’&#125;, default=’ls’：‘ls’ 指最小二乘回归. ‘lad’ (最小绝对偏差) 是仅基于输入变量的顺序信息的高度鲁棒的损失函数。. ‘huber’ 是两者的结合. ‘quantile’允许分位数回归（用于alpha指定分位数）</span></span><br><span class="line"><span class="string">learning_rate：学习率缩小了每棵树的贡献learning_rate。在learning_rate和n_estimators之间需要权衡。</span></span><br><span class="line"><span class="string">n_estimators：要执行的提升次数。</span></span><br><span class="line"><span class="string">subsample：用于拟合各个基础学习者的样本比例。如果小于1.0，则将导致随机梯度增强。subsample与参数n_estimators。选择会导致方差减少和偏差增加。subsample &lt; 1.0</span></span><br><span class="line"><span class="string">criterion：&#123;&#x27;friedman_mse&#x27;，&#x27;mse&#x27;，&#x27;mae&#x27;&#125;，默认=&#x27;friedman_mse&#x27;：“ mse”是均方误差，“ mae”是平均绝对误差。默认值“ friedman_mse”通常是最好的，因为在某些情况下它可以提供更好的近似值。</span></span><br><span class="line"><span class="string">min_samples_split：拆分内部节点所需的最少样本数</span></span><br><span class="line"><span class="string">min_samples_leaf：在叶节点处需要的最小样本数。</span></span><br><span class="line"><span class="string">min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</span></span><br><span class="line"><span class="string">max_depth：各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量的相互作用。</span></span><br><span class="line"><span class="string">min_impurity_decrease：如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂。</span></span><br><span class="line"><span class="string">min_impurity_split：提前停止树木生长的阈值。如果节点的杂质高于阈值，则该节点将分裂</span></span><br><span class="line"><span class="string">max_features&#123;‘auto’, ‘sqrt’, ‘log2’&#125;，int或float：寻找最佳分割时要考虑的功能数量：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为int，则max_features在每个分割处考虑特征。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果“auto”，则max_features=n_features。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果是“ sqrt”，则max_features=sqrt(n_features)。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为“ log2”，则为max_features=log2(n_features)。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果没有，则max_features=n_features。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">X, y = make_friedman1(n_samples=<span class="number">1200</span>, random_state=<span class="number">0</span>, noise=<span class="number">1.0</span>)</span><br><span class="line">X_train, X_test = X[:<span class="number">200</span>], X[<span class="number">200</span>:]</span><br><span class="line">y_train, y_test = y[:<span class="number">200</span>], y[<span class="number">200</span>:]</span><br><span class="line">est = GradientBoostingRegressor(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>, loss=<span class="string">&#x27;ls&#x27;</span>).fit(X_train, y_train)</span><br><span class="line">mean_squared_error(y_test, est.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="number">5.009154859960321</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X, y = make_regression(random_state=<span class="number">0</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, random_state=<span class="number">0</span>)</span><br><span class="line">reg = GradientBoostingRegressor(random_state=<span class="number">0</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="number">0.43848663277068134</span></span><br></pre></td></tr></table></figure><p>GradientBoostingRegressor与GradientBoostingClassifier函数的各个参数的意思！参考文档：<br><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor</a><br><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gra#sklearn.ensemble.GradientBoostingClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gra#sklearn.ensemble.GradientBoostingClassifier</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、-Boosting算法原理&quot;&gt;&lt;a href=&quot;#一、-Boosting算法原理&quot; class=&quot;headerlink&quot; title=&quot;一、 Boosting算法原理&quot;&gt;&lt;/a&gt;一、 Boosting算法原理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Bagging：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。&lt;/li&gt;
&lt;li&gt;Bagging主要通过降低方差的方式减少预测误差&amp;lt;/font&amp;gt;&lt;/li&gt;
&lt;li&gt;Boosting：使用同一组数据集进行反复学习，得到一系列简单模型，然后组合这些模型构成一个预测性能十分强大的机器学习模型。&lt;/li&gt;
&lt;li&gt;Boosting通过不断减少偏差的形式提高最终的预测效果，与Bagging有着本质的不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在概率近似正确（PAC）学习的框架下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;弱学习：识别准确率略高于1/2（即准确率仅比随机猜测略高的学习算法）&lt;/li&gt;
&lt;li&gt;强学习：识别准确率很高并能在多项式时间内完成的学习算法&lt;/li&gt;
&lt;li&gt;强可学习和弱可学习是等价的，弱可学习算法，能提升至强可学习算法</summary>
    
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Boosting" scheme="https://zhxnlp.github.io/tags/Boosting/"/>
    
    <category term="Adaboost" scheme="https://zhxnlp.github.io/tags/Adaboost/"/>
    
    <category term="GBDT" scheme="https://zhxnlp.github.io/tags/GBDT/"/>
    
  </entry>
  
  <entry>
    <title>集成学习1——voting、bagging&amp;stacking</title>
    <link href="https://zhxnlp.github.io/2021/12/02/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94voting%E3%80%81bagging&amp;stacking/"/>
    <id>https://zhxnlp.github.io/2021/12/02/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94voting%E3%80%81bagging&amp;stacking/</id>
    <published>2021-12-01T16:53:25.000Z</published>
    <updated>2022-01-02T20:49:14.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考datawhale课程<a href="https://github.com/datawhalechina/ensemble-learning">《集成学习》</a><br>参考<a href="https://www.cnblogs.com/Christina-Notebook/p/10063146.html">《Stacking方法详解》</a></p></blockquote><p>Bagging思想的实质是：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差</p><h2 id="一、投票法与bagging"><a href="#一、投票法与bagging" class="headerlink" title="一、投票法与bagging"></a>一、投票法与bagging</h2><h3 id="1-1-投票法的原理分析"><a href="#1-1-投票法的原理分析" class="headerlink" title="1.1 投票法的原理分析"></a>1.1 投票法的原理分析</h3><p>投票法是一种遵循少数服从多数原则的集成学习模型，通过多个模型的集成降低方差，从而提高模型的鲁棒性。在理想情况下，投票法的预测效果应当优于任何一个基模型的预测效果。</p><ul><li>回归投票法：预测结果是所有模型预测结果的平均值。</li><li>分类投票法：预测结果是所有模型种出现最多的预测结果。</li></ul><p>分类投票法又可以被划分为硬投票与软投票：</p><ul><li>硬投票：预测结果是所有投票结果最多出现的类。（基模型能预测出清晰的类别标签时）</li><li>软投票：预测结果是所有投票结果中概率加和最大的类。（基模型能预测类别的概率，或可以输出类似于概率的预测分数值，例如支持向量机、k-最近邻和决策树等）<span id="more"></span>相对于硬投票，软投票法考虑到了预测概率这一额外的信息，因此可以得出比硬投票法更加准确的预测结果。</li></ul><p>想要投票法产生较好的结果，基模型需要满足两个条件：</p><ul><li>基模型之间的效果不能差别过大。&lt;/font &gt;当某个基模型相对于其他基模型效果过差时，该模型很可能成为噪声。</li><li>基模型之间同质性较小。&lt;/font &gt;例如在基模型预测效果近似的情况下，基于树模型与线性模型的投票，往往优于两个树模型或两个线性模型。</li></ul><p>  投票法的局限性：所有模型对预测的贡献是一样的。</p><h3 id="1-2-Voting案例分析"><a href="#1-2-Voting案例分析" class="headerlink" title="1.2  Voting案例分析"></a>1.2  Voting案例分析</h3><ul><li>Sklearn中提供了 VotingRegressor 与 VotingClassifier 两个投票方法。这两种模型的操作方式相同，并采用相同的参数。</li><li>使用模型需要提供一个模型列表，列表中每个模型采用Tuple的结构表示，第一个元素代表名称，第二个元素代表模型，需要保证每个模型必须拥有唯一的名称。</li></ul><p>  例如这里，我们定义两个模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">models = [(<span class="string">&#x27;lr&#x27;</span>,LogisticRegression()),(<span class="string">&#x27;svm&#x27;</span>,make_pipeline(StandardScaler(),SVC()))]<span class="comment">#定义Pipeline完成模型预处理工作</span></span><br><span class="line">ensemble = VotingClassifier(estimators=models,voting=<span class="string">&#x27;soft&#x27;</span>)<span class="comment">#voting参数让我们选择软投票或者硬投票</span></span><br></pre></td></tr></table></figure><p>创建一个1000个样本，20个特征的随机数据集：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test classification dataset</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span>():</span></span><br><span class="line">    X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, n_redundant=<span class="number">5</span>, random_state=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># summarize the dataset</span></span><br><span class="line">    <span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure><p>我们使用多个KNN模型作为基模型演示投票法，其中每个模型采用不同的邻居值K参数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get a voting ensemble of models</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_voting</span>():</span></span><br><span class="line">    <span class="comment"># define the base models</span></span><br><span class="line">    models = <span class="built_in">list</span>()</span><br><span class="line">    models.append((<span class="string">&#x27;knn1&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">1</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn3&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">3</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn5&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">5</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn7&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">7</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn9&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">9</span>)))</span><br><span class="line">    <span class="comment"># define the voting ensemble</span></span><br><span class="line">    ensemble = VotingClassifier(estimators=models, voting=<span class="string">&#x27;hard&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> ensemble</span><br></pre></td></tr></table></figure><p>创建一个模型列表来评估投票带来的提升，包括KNN模型配置的每个独立版本和硬投票模型。下面的get_models()函数可以为我们创建模型列表进行评估。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get a list of models to evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_models</span>():</span></span><br><span class="line">    models = <span class="built_in">dict</span>()</span><br><span class="line">    models[<span class="string">&#x27;knn1&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn3&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn5&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn7&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">7</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn9&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">9</span>)</span><br><span class="line">    models[<span class="string">&#x27;hard_voting&#x27;</span>] = get_voting()</span><br><span class="line">    <span class="keyword">return</span> models</span><br></pre></td></tr></table></figure><p>evaluate_model()函数接收一个模型实例，并以分层10倍交叉验证三次重复的分数列表的形式返回。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate a give model using cross-validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score   <span class="comment">#Added by ljq</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_model</span>(<span class="params">model, X, y</span>):</span></span><br><span class="line">    cv = RepeatedStratifiedKFold(n_splits=<span class="number">10</span>, n_repeats=<span class="number">3</span>, random_state=<span class="number">1</span>)</span><br><span class="line">    scores = cross_val_score(model, X, y, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=cv, n_jobs=-<span class="number">1</span>, error_score=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><p>报告每个算法的平均性能，还可以创建一个箱形图和须状图来比较每个算法的精度分数分布。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = get_dataset()</span><br><span class="line"><span class="comment"># get the models to evaluate</span></span><br><span class="line">models = get_models()</span><br><span class="line"><span class="comment"># evaluate the models and store results</span></span><br><span class="line">results, names = <span class="built_in">list</span>(), <span class="built_in">list</span>()</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models.items():</span><br><span class="line">    scores = evaluate_model(model, X, y)</span><br><span class="line">    results.append(scores)</span><br><span class="line">    names.append(name)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&gt;%s %.3f (%.3f)&#x27;</span> % (name, mean(scores), std(scores)))</span><br><span class="line"><span class="comment"># plot model performance for comparison</span></span><br><span class="line">pyplot.boxplot(results, labels=names, showmeans=<span class="literal">True</span>)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>结果如下：</p><blockquote><p>knn1 0.873 (0.030)<br>knn3 0.889 (0.038)<br>knn5 0.895 (0.031)<br>knn7 0.899 (0.035)<br>knn9 0.900 (0.033)<br>hard_voting 0.902 (0.034）</p></blockquote><p><img src="https://img-blog.csdnimg.cn/546a2624da1c48eba1431876e835f8b7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_8,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>显然投票的效果略大于任何一个基模型。</p><p>通过箱形图我们可以看到硬投票方法对交叉验证整体预测结果分布带来的提升。</p><h3 id="1-3-bagging的原理分析"><a href="#1-3-bagging的原理分析" class="headerlink" title="1.3 bagging的原理分析"></a>1.3 bagging的原理分析</h3><p>Voting法希望各个模型之间具有较大的差异性，而在实际操作中的模型却往往是同质的，故考虑是通过不同的采样增加模型的差异性。&lt;/font &gt;</p><p>自助采样(bootstrap)：有放回的从数据集中进行采样。<br>Bagging思想的实质是：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差</p><p>bagging流程：</p><ul><li>随机取出一个训练样本放入采样集合中，再把这个样本放回初始数据集，重复K次采样，最终得到大小为K的样本集合。</li><li>重复以上步骤采样出T个含K个样本的采样集合，然后基于每个采样集合训练出一个基学习器</li><li>将这些基学习器进行结合。<ul><li>预测回归问题：预测值取平均来进行的。</li><li>预测分类问题：预测值投票取多数票</li></ul></li></ul><p>Bagging同样是一种降低方差的技术，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更加明显。&lt;/font &gt;在实际的使用中，加入列采样的Bagging技术对高维小样本往往有神奇的效果。</p><h3 id="1-4-决策树和随机森林"><a href="#1-4-决策树和随机森林" class="headerlink" title="1.4 决策树和随机森林"></a>1.4 决策树和随机森林</h3><p>Sklearn为我们提供了 BaggingRegressor 与 BaggingClassifier 两种Bagging方法的API，默认基模型是决策树模型。</p><ul><li>决策树，它是一种树形结构，树的每个非叶子节点表示对样本在一个特征上的判断，节点下方的分支代表对样本的划分。</li><li>决策树的建立过程是一个对数据不断划分的过程。</li><li>每次划分中，首先要选择用于划分的特征，之后要确定划分的方案（类别/阈值）。我们希望通过划分，决策树的分支节点所包含的样本“纯度”尽可能地高。节点划分过程中所用的指标主要是信息增益和GINI系数。</li><li>选择信息增益最大或者gini指数最小的划分方式</li><li>划分过程直到样本的类别被完全分开，所有特征都已使用，或达到树的最大深度为止。<br><img src="https://img-blog.csdnimg.cn/c74115db96474747b47fd3c5f0f907af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>Bagging的一个典型应用是随机森林。由许多“树”bagging组成的。每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）&lt;/font &gt;。随机森林的示意图如下：<br><img src="https://img-blog.csdnimg.cn/879df8be2f9f4183b72c64ee427b1564.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="1-5-bagging案例分析"><a href="#1-5-bagging案例分析" class="headerlink" title="1.5 bagging案例分析"></a>1.5 bagging案例分析</h3></li></ul><ol><li>创建一个含有1000个样本20维特征的随机分类数据集：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test classification dataset</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, </span><br><span class="line">                           n_redundant=<span class="number">5</span>, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># summarize the dataset</span></span><br><span class="line"><span class="built_in">print</span>(X.shape, y.shape)</span><br><span class="line"></span><br><span class="line">(<span class="number">1000</span>, <span class="number">20</span>) (<span class="number">1000</span>,)</span><br></pre></td></tr></table></figure><ol><li>使用重复的分层k-fold交叉验证来评估该模型，一共重复3次，每次有10个fold。我们将评估该模型在所有重复交叉验证中性能的平均值和标准差。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate bagging algorithm for classification</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mean</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> std</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RepeatedStratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, n_redundant=<span class="number">5</span>, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># define the model</span></span><br><span class="line">model = BaggingClassifier()</span><br><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line">cv = RepeatedStratifiedKFold(n_splits=<span class="number">10</span>, n_repeats=<span class="number">3</span>, random_state=<span class="number">1</span>)</span><br><span class="line">n_scores = cross_val_score(model, X, y, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=cv, n_jobs=-<span class="number">1</span>, error_score=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line"><span class="comment"># report performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.3f (%.3f)&#x27;</span> % (mean(n_scores), std(n_scores)))</span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.851</span> (<span class="number">0.041</span>)</span><br></pre></td></tr></table></figure><p>最终模型的效果是Accuracy: 0.856 标准差0.037</p><h2 id="二、stacking"><a href="#二、stacking" class="headerlink" title="二、stacking"></a>二、stacking</h2><p>stacking被称为“懒人”算法，因为它不需要花费过多时间的调参就可以得到一个效果不错的算法。<br>Stacking集成算法可以理解为一个两层的集成，第一层含有多个基础分类器，把预测的结果(元特征)提供给第二层， 而第二层的分类器通常是逻辑回归，他把一层分类器的结果当做特征做拟合输出预测结果。</p><h3 id="2-1-Blending算法原理"><a href="#2-1-Blending算法原理" class="headerlink" title="2.1 Blending算法原理"></a>2.1 Blending算法原理</h3><p>Blending集成学习方式：</p><ul><li>(1) 将数据划分为训练集和测试集(test_set)，其中训练集需要再次划分为训练集(train_set)和验证集(val_set)；</li><li>(2) 创建第一层的多个模型（同质异质都可），使用train_set进行训练。然后用训练好的模型预测val_set和test_set得到val_predict, test_predict1</li><li>(3) 创建第二层的模型（一般是LR），使用val_predict作为训练集，验证集val_set的标签作为标签训练第二层的模型；</li><li>(4) 使用第二层训练好的模型对第二层测试集test_predict1进行预测&lt;/font &gt;，该结果为整个测试集的结果。<br><img src="https://img-blog.csdnimg.cn/3f2d967828844579b3fae50a043cf085.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="stacking"><br>Blending集成方式的优劣：</li><li>最重要的优点就是实现简单粗暴，没有太多的理论的分析。</li><li>缺点：只使用了一部分数据集作为留出集进行验证，也就是只能用上数据中的一部分，进而容易产生过拟合。<h3 id="2-2-Blending案例"><a href="#2-2-Blending案例" class="headerlink" title="2.2 Blending案例"></a>2.2 Blending案例</h3></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载相关工具包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">data, target = make_blobs(n_samples=<span class="number">10000</span>, centers=<span class="number">2</span>, random_state=<span class="number">1</span>, cluster_std=<span class="number">1.0</span> )</span><br><span class="line"><span class="comment">## 创建训练集和测试集</span></span><br><span class="line">X_train1,X_test,y_train1,y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">## 创建训练集和验证集</span></span><br><span class="line">X_train,X_val,y_train,y_val = train_test_split(X_train1, y_train1, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  设置第一层分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">clfs = [SVC(probability = <span class="literal">True</span>),RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),KNeighborsClassifier()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置第二层分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出第一层的验证集结果与测试集结果</span></span><br><span class="line">val_features = np.zeros((X_val.shape[<span class="number">0</span>],<span class="built_in">len</span>(clfs)))  <span class="comment"># 初始化验证集结果</span></span><br><span class="line">test_features = np.zeros((X_test.shape[<span class="number">0</span>],<span class="built_in">len</span>(clfs)))  <span class="comment"># 初始化测试集结果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    clf.fit(X_train,y_train)</span><br><span class="line">    val_feature = clf.predict_proba(X_val)[:, <span class="number">1</span>]</span><br><span class="line">    test_feature = clf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">    val_features[:,i] = val_feature</span><br><span class="line">    test_features[:,i] = test_feature</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第一层的验证集的结果输入第二层训练第二层分类器</span></span><br><span class="line">lr.fit(val_features,y_val)</span><br><span class="line"><span class="comment"># 输出预测的结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(lr,test_features,y_test,cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">array([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><p>每一折的交叉验证的效果都是非常好的，这个集成学习方法在这个数据集上是十分有效的，不过这个数据集是我们虚拟的，因此大家可以把他用在实际数据上看看效果。</p><h3 id="2-3-Stacking算法原理"><a href="#2-3-Stacking算法原理" class="headerlink" title="2.3 Stacking算法原理"></a>2.3 Stacking算法原理</h3><p>lending在集成的过程中只会用到验证集的数据，对数据实际上是一个很大的浪费，所以考虑交叉验证进行优化。<br><img src="https://img-blog.csdnimg.cn/b6a1d8f36b914374af38219c0d3290b9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ol><li>将所有数据集生成测试集和训练集（假如训练集为10000,测试集为2500行），那么上层会进行5折交叉检验（训练集8000条、验证集2000条）。<br>最后得到5×2000条验证集预测结果(橙色），5×2500条测试集的预测结果。    </li><li>将验证集的预测结果拼接成10000×1的矩阵，标记为$A_1$，测试集预测结果进行加权平均，得到2500×1的矩阵，标记为$B_1$。</li><li><p>对3个基模型进行如上训练，得到了$A_1$、$A_2$、$A_3$、$B_1$、$B_2$、$B_3$六个矩阵。</p></li><li><p>将$A_1$、$A_2$、$A_3$并列为10000×3的矩阵作为training data,$B_1$、$B_2$、$B_3$合并在一起成2500×3的矩阵作为testing  data，5次验证集标签作为标签进行训练。</p></li><li>预测testing  data的结果为最终结果<br><img src="https://img-blog.csdnimg.cn/0bb6ae78d63c4e299f6e33d2138fd64e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-Stacking算法案例"><a href="#2-4-Stacking算法案例" class="headerlink" title="2.4 Stacking算法案例"></a>2.4 Stacking算法案例</h3>sklearn并没有直接对Stacking的方法，因此我们需要下载mlxtend工具包(pip install mlxtend)</li></ol><p>StackingClassifier使用API和参数说明：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">StackingClassifier(classifiers, meta_classifier, use_probas=<span class="literal">False</span>, average_probas=<span class="literal">False</span>, </span><br><span class="line">verbose=<span class="number">0</span>, use_features_in_secondary=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>classifiers : 基分类器，数组形式，[cl1, cl2, cl3]. 每个基分类器的属性被存储在类属性 self.clfs_.</li><li>meta_classifier : 目标分类器，即将前面分类器合起来的分类器</li><li>use_probas : bool (default: False) ，如果设置为True， 那么目标分类器的输入就是前面分类输出的类别概率值而不是类别标签</li><li>average_probas : bool (default: False)，当上一个参数use_probas = True时需设置</li><li>average_probas=True表示所有基分类器输出的概率值需被平均，否则拼接。</li><li>verbose : int, optional (default=0)。用来控制使用过程中的日志输出，当 verbose = 0时，什么也不输出， verbose = 1，输出回归器的序号和名字。verbose = 2，输出详细的参数信息。verbose &gt; 2, 自动将verbose设置为小于2的，verbose -2.</li><li>use_features_in_secondary : bool (default: False). 如果设置为True，那么最终的目标分类器就被基分类器产生的数据和最初的数据集同时训练。如果设置为False，最终的分类器只会使用基分类器产生的数据训练。<h4 id="2-4-1-基分类器预测类别为特征"><a href="#2-4-1-基分类器预测类别为特征" class="headerlink" title="2.4.1 基分类器预测类别为特征"></a>2.4.1 基分类器预测类别为特征</h4>使用基分类器所产生的预测类别作为meta-classifier“特征”的输入数据<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 简单堆叠3折CV分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting from v0.16.0, StackingCVRegressor supports</span></span><br><span class="line"><span class="comment"># `random_state` to get deterministic result.</span></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],  <span class="comment"># 第一层分类器</span></span><br><span class="line">                            meta_classifier=lr,   <span class="comment"># 第二层分类器</span></span><br><span class="line">                            random_state=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3-fold cross validation:\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>,<span class="string">&#x27;StackingClassifier&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>-fold cross validation:</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.93</span> (+/- <span class="number">0.02</span>) [StackingClassifier]</span><br></pre></td></tr></table></figure><p>画出决策边界：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_decision_regions</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> clf, lab, grd <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], </span><br><span class="line">                         [<span class="string">&#x27;KNN&#x27;</span>, </span><br><span class="line">                          <span class="string">&#x27;Random Forest&#x27;</span>, </span><br><span class="line">                          <span class="string">&#x27;Naive Bayes&#x27;</span>,</span><br><span class="line">                          <span class="string">&#x27;StackingCVClassifier&#x27;</span>],</span><br><span class="line">                          itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)):</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(lab)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/5241dd417cbc48b894676aa3e2428e43.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="2-4-2-基分类器类别概率值为特征"><a href="#2-4-2-基分类器类别概率值为特征" class="headerlink" title="2.4.2  基分类器类别概率值为特征"></a>2.4.2  基分类器类别概率值为特征</h4><p>使用第一层所有基分类器所产生的类别概率值作为meta-classfier的输入。需要在StackingClassifier 中增加一个参数设置：use_probas = True。</p><p>另外，还有一个参数设置average_probas = True,那么这些基分类器所产出的概率值将按照列被平均，否则会拼接。</p><ul><li>基分类器1：predictions=[0.2,0.2,0.7]</li><li>基分类器2：predictions=[0.4,0.3,0.8]</li><li>基分类器3：predictions=[0.1,0.4,0.6]</li></ul><ol><li><p>若use_probas = True，average_probas = True，</p><p> 产生的meta-feature 为：[0.233, 0.3, 0.7]</p></li><li><p>若use_probas = True，average_probas = False，</p><p> 产生的meta-feature 为：[0.2,0.2,0.7,0.4,0.3,0.8,0.1,0.4,0.6]</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.使用概率作为元特征</span></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                            use_probas=<span class="literal">True</span>,  <span class="comment"># </span></span><br><span class="line">                            meta_classifier=lr,</span><br><span class="line">                            random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3-fold cross validation:\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], </span><br><span class="line">                      [<span class="string">&#x27;KNN&#x27;</span>, </span><br><span class="line">                       <span class="string">&#x27;Random Forest&#x27;</span>, </span><br><span class="line">                       <span class="string">&#x27;Naive Bayes&#x27;</span>,</span><br><span class="line">                       <span class="string">&#x27;StackingClassifier&#x27;</span>]):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, </span><br><span class="line">                                              cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> </span><br><span class="line">          % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>-fold cross validation:</span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [StackingClassifier]</span><br></pre></td></tr></table></figure><h4 id="2-4-3-基分类器使用部分特征"><a href="#2-4-3-基分类器使用部分特征" class="headerlink" title="2.4.3 基分类器使用部分特征"></a>2.4.3 基分类器使用部分特征</h4><p>赋予不同的基分类器不同的特征。<br>比如：基分类器1训练前半部分的特征，基分类器2训练后半部分的特征。这部分的操作是通过sklearn中的pipelines实现。最终通过StackingClassifier组合起来。而不是给每一个基分类器全部的特征</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.在不同特征子集上运行的分类器的堆叠</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> ColumnSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"><span class="comment">#基分类器1：xgboost</span></span><br><span class="line">pipe1 = make_pipeline(ColumnSelector(cols=(<span class="number">0</span>, <span class="number">2</span>)),</span><br><span class="line">                      XGBClassifier())</span><br><span class="line"><span class="comment">#基分类器2：RandomForest</span></span><br><span class="line">pipe2 = make_pipeline(ColumnSelector(cols=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)),</span><br><span class="line">                      RandomForestClassifier())</span><br><span class="line"></span><br><span class="line">sclf = StackingClassifier(classifiers=[pipe1, pipe2],</span><br><span class="line">                          meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line">sclf.fit(X, y)</span><br></pre></td></tr></table></figure><h4 id="2-4-4-结合网格搜索优化"><a href="#2-4-4-结合网格搜索优化" class="headerlink" title="2.4.4 结合网格搜索优化"></a>2.4.4 结合网格搜索优化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. 堆叠5折CV分类与网格搜索(结合网格搜索调参优化)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing models</span></span><br><span class="line"></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], </span><br><span class="line">                            meta_classifier=lr,</span><br><span class="line">                            random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">params = &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: [<span class="number">1</span>, <span class="number">5</span>],</span><br><span class="line">          <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: [<span class="number">10</span>, <span class="number">50</span>],</span><br><span class="line">          <span class="string">&#x27;meta_classifier__C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">10.0</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># 如果我们打算多次使用回归算法，我们要做的就是在参数网格中添加一个附加的数字后缀，如下所示：</span></span><br><span class="line"><span class="string">sclf = StackingCVClassifier(classifiers=[clf1, clf1, clf2, clf3], </span></span><br><span class="line"><span class="string">                            meta_classifier=lr,</span></span><br><span class="line"><span class="string">                            random_state=RANDOM_SEED)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">params = &#123;&#x27;kneighborsclassifier-1__n_neighbors&#x27;: [1, 5],</span></span><br><span class="line"><span class="string">          &#x27;kneighborsclassifier-2__n_neighbors&#x27;: [1, 5],</span></span><br><span class="line"><span class="string">          &#x27;randomforestclassifier__n_estimators&#x27;: [10, 50],</span></span><br><span class="line"><span class="string">          &#x27;meta_classifier__C&#x27;: [0.1, 10.0]&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">grid = GridSearchCV(estimator=sclf, </span><br><span class="line">                    param_grid=params, </span><br><span class="line">                    cv=<span class="number">5</span>,</span><br><span class="line">                    refit=<span class="literal">True</span>)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line"></span><br><span class="line">cv_keys = (<span class="string">&#x27;mean_test_score&#x27;</span>, <span class="string">&#x27;std_test_score&#x27;</span>, <span class="string">&#x27;params&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> r, _ <span class="keyword">in</span> <span class="built_in">enumerate</span>(grid.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%0.3f +/- %0.2f %r&quot;</span></span><br><span class="line">          % (grid.cv_results_[cv_keys[<span class="number">0</span>]][r],</span><br><span class="line">             grid.cv_results_[cv_keys[<span class="number">1</span>]][r] / <span class="number">2.0</span>,</span><br><span class="line">             grid.cv_results_[cv_keys[<span class="number">2</span>]][r]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Best parameters: %s&#x27;</span> % grid.best_params_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f&#x27;</span> % grid.best_score_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0.947</span> +/- <span class="number">0.03</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.933</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.940</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.940</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line">Best parameters: &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">Accuracy: <span class="number">0.95</span></span><br></pre></td></tr></table></figure><h4 id="2-4-5-绘制ROC曲线"><a href="#2-4-5-绘制ROC曲线" class="headerlink" title="2.4.5 绘制ROC曲线"></a>2.4.5 绘制ROC曲线</h4><ul><li>像其他scikit-learn分类器一样，它StackingCVClassifier具有decision_function可用于绘制ROC曲线的方法。</li><li>请注意，decision_function期望并要求元分类器实现decision_function。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, [<span class="number">0</span>, <span class="number">1</span>]], iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Binarize the output</span></span><br><span class="line">y = label_binarize(y, classes=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">n_classes = y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.33</span>, random_state=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">clf1 =  LogisticRegression()</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = SVC(random_state=RANDOM_SEED)</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                            meta_classifier=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learn to predict each class against the other</span></span><br><span class="line">classifier = OneVsRestClassifier(sclf)</span><br><span class="line">y_score = classifier.fit(X_train, y_train).decision_function(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute micro-average ROC curve and ROC area</span></span><br><span class="line">fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>], _ = roc_curve(y_test.ravel(), y_score.ravel())</span><br><span class="line">roc_auc[<span class="string">&quot;micro&quot;</span>] = auc(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>])</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.plot(fpr[<span class="number">2</span>], tpr[<span class="number">2</span>], color=<span class="string">&#x27;darkorange&#x27;</span>,</span><br><span class="line">         lw=lw, label=<span class="string">&#x27;ROC curve (area = %0.2f)&#x27;</span> % roc_auc[<span class="number">2</span>])</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver operating characteristic example&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://img-blog.csdnimg.cn/ad0f2d5e0ffd4e52905b69e5dd108fca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_9,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-6-Blending与Stacking对比"><a href="#2-4-6-Blending与Stacking对比" class="headerlink" title="2.4.6 Blending与Stacking对比"></a>2.4.6 Blending与Stacking对比</h3>Blending的优点在于：</li><li>比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）</li></ul><p>而缺点在于：</p><ul><li>使用了很少的数据（是划分hold-out作为测试集，并非cv）</li><li>blender可能会过拟合（其实大概率是第一点导致的）</li><li>stacking使用多次的CV会比较稳健</li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;参考datawhale课程&lt;a href=&quot;https://github.com/datawhalechina/ensemble-learning&quot;&gt;《集成学习》&lt;/a&gt;&lt;br&gt;参考&lt;a href=&quot;https://www.cnblogs.com/Christina-Notebook/p/10063146.html&quot;&gt;《Stacking方法详解》&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bagging思想的实质是：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差&lt;/p&gt;
&lt;h2 id=&quot;一、投票法与bagging&quot;&gt;&lt;a href=&quot;#一、投票法与bagging&quot; class=&quot;headerlink&quot; title=&quot;一、投票法与bagging&quot;&gt;&lt;/a&gt;一、投票法与bagging&lt;/h2&gt;&lt;h3 id=&quot;1-1-投票法的原理分析&quot;&gt;&lt;a href=&quot;#1-1-投票法的原理分析&quot; class=&quot;headerlink&quot; title=&quot;1.1 投票法的原理分析&quot;&gt;&lt;/a&gt;1.1 投票法的原理分析&lt;/h3&gt;&lt;p&gt;投票法是一种遵循少数服从多数原则的集成学习模型，通过多个模型的集成降低方差，从而提高模型的鲁棒性。在理想情况下，投票法的预测效果应当优于任何一个基模型的预测效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;回归投票法：预测结果是所有模型预测结果的平均值。&lt;/li&gt;
&lt;li&gt;分类投票法：预测结果是所有模型种出现最多的预测结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分类投票法又可以被划分为硬投票与软投票：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;硬投票：预测结果是所有投票结果最多出现的类。（基模型能预测出清晰的类别标签时）&lt;/li&gt;
&lt;li&gt;软投票：预测结果是所有投票结果中概率加和最大的类。（基模型能预测类别的概率，或可以输出类似于概率的预测分数值，例如支持向量机、k-最近邻和决策树等）</summary>
    
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="集成学习" scheme="https://zhxnlp.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="bagging" scheme="https://zhxnlp.github.io/tags/bagging/"/>
    
    <category term="stacking" scheme="https://zhxnlp.github.io/tags/stacking/"/>
    
    <category term="随机森林" scheme="https://zhxnlp.github.io/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记7：循环神经网络</title>
    <link href="https://zhxnlp.github.io/2021/11/30/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://zhxnlp.github.io/2021/11/30/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-11-29T20:30:21.000Z</published>
    <updated>2022-01-02T20:47:15.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、RNN"><a href="#一、RNN" class="headerlink" title="一、RNN"></a>一、RNN</h2><p>前馈神经网络：信息往一个方向流动。包括MLP和CNN<br>循环神经网络：信息循环流动，网络隐含层输出又作为自身输入&lt;/font&gt;，包括RNN、LSTM、GAN等。</p><h3 id="1-1-RNN模型结构"><a href="#1-1-RNN模型结构" class="headerlink" title="1.1 RNN模型结构"></a>1.1 RNN模型结构</h3><p>RNN模型结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/62ffdeb2b9ec4e2cbcd4edfc85583c24.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_9,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>展开之后相当于堆叠多个共享隐含层参数的前馈神经网络：<br><img src="https://img-blog.csdnimg.cn/e43dbda3e0d24019a13285a6e31086c4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><span id="more"></span><p>其输出为：</p><script type="math/tex; mode=display">\mathbf {h_{t}=tanh(W^{xh}x_{t}+b^{xh}+W^{hh}h_{t-1}+b^{hh})}</script><script type="math/tex; mode=display">\mathbf {y_{n}=softmax(W^{hy}h_{n}+b^{hy})}</script><ul><li>隐含层输入不但与当前时刻输入$x<em>{t}$有关，还与前一时刻隐含层$h</em>{t-1}$有关。每个时刻的输入经过层层递归，对最终输入产生一定影响。</li><li>每个时刻隐含层$h_{t}$包含1~t时刻全部输入信息，所以隐含层也叫记忆单元（Memory）</li><li>每个时刻参数共享（‘循环’的由来）&lt;/font&gt;</li><li>使用tanh激活函数是因为值域（-1,1），能提供的信息比sigmoid、Relu函数丰富。</li><li>变长神经网络只能进行层标准化</li><li>RNN处理时序信息能力很强，可以用于语音处理。NLP等<h3 id="1-2-RNN模型的缺点"><a href="#1-2-RNN模型的缺点" class="headerlink" title="1.2 RNN模型的缺点"></a>1.2 RNN模型的缺点</h3>在前向传播时：<script type="math/tex; mode=display">\mathbf {h_{t}=tanh(W^{xh}x_{t}+b^{xh}+W^{hh}h_{t-1}+b^{hh})}</script>假设最后时刻为t，反向传播求对i时刻的导数为：<script type="math/tex; mode=display">\mathbf {\frac{\partial Loss}{\partial W_{i}^{hh}}=\frac{\partial Loss}{\partial y_{t}^{}}\cdot \frac{\partial y_{t}^{}}{\partial h_{i}}\cdot \frac{\partial h_{i}^{}}{\partial W_{i}^{hh}}}</script><script type="math/tex; mode=display">\mathbf {\frac{\partial h_{i}}{\partial W_{i}^{hh}}=(h_{i-1})^T}</script><script type="math/tex; mode=display">\mathbf {\frac{\partial y_{t}}{\partial h_{i}}=\frac{\partial y_{t}}{\partial h_{t}}\cdot\frac{\partial h_{t}}{\partial h_{i}}=\frac{\partial y_{t}}{\partial h_{t}}\cdot tanh'\cdot\frac{\partial h_{t}}{\partial (h_{t-1})^{T}}\cdot\tanh'\cdot\frac{\partial  h_{t-1}}{\partial  (h_{t-2})^{T}}...\cdot\tanh'\cdot\frac{\partial h_{i+1}}{\partial (h_{i})^{T}}=\frac{\partial y_{t}}{\partial h_{t}}\cdot (tanh')^{t-i}\cdot W^{t-i}}</script>所以最终结果是：<script type="math/tex">\mathbf {\frac{\partial Loss}{\partial W_{i}^{hh}}=\frac{\partial Loss}{\partial y_{t}}\cdot\frac{\partial y_{t}}{\partial h_{t}}\cdot (tanh')^{t-i}\cdot W^{t-i}\cdot(h_{i-1})^T}</script><br>可以看到涉及到矩阵W的连乘。</li></ul><p>线性代数中有：<script type="math/tex">W=P^{-1}\Sigma P</script><br>其中，$E=P^{-1} P$为单位矩阵，$\Sigma$为对角线矩阵，对角线元素为W对应的特征值。即</p><script type="math/tex; mode=display">\Sigma =\begin{bmatrix}\lambda _{1} & ... & 0\\ ... &...  &... \\ ... & ... &\lambda _{m} \end{bmatrix}</script><p>所以有：</p><script type="math/tex; mode=display">W=P^{-1}\Sigma^T P=\Sigma =\begin{bmatrix}\lambda _{1}^T & ... & 0\\ ... &...  &... \\ ... & ... &\lambda _{m} ^T\end{bmatrix}</script><p>所以有：</p><ol><li>矩阵特征值$\lambda _{m}$要么大于1要么小于1。所以t时刻导数要么梯度消失，要么梯度爆炸。而且比DNN更严重。&lt;/font&gt;因为DNN链式求导累乘的各个W是不一样的，有的大有的小，互相还可以抵消影响。而RNN的W全都一样，必然更快的梯度消失或者爆炸。</li><li>$\lambda <em>{m}&gt;1$则$\lambda </em>{m}^T→\infty$，过去信息越来越强，$\lambda <em>{m}＜1$则$\lambda </em>{m}^T→0$，信息原来越弱，传不远。所有时刻W都相同，即所有时刻传递信息的强度都一样，传递的信息无法调整，和当前时刻输入没太大关系。</li><li>为了避免以上问题，序列不能太长。</li><li>无法解决超长依赖问题：例如$h<em>1$传到$h</em>{10}$，$x_1$的信息在中间被多个W和$x_2-x_9$稀释</li><li>递归模型，无法并行计算</li></ol><h2 id="二、长短时记忆网络LSTM"><a href="#二、长短时记忆网络LSTM" class="headerlink" title="二、长短时记忆网络LSTM"></a>二、长短时记忆网络LSTM</h2><p>RNN的缺点是信息经过多个隐含层传递到输出层，会导致信息损失。更本质地，会造成网络参数难以优化。LSTM加入全局信息context，可以解决这一问题。</p><h3 id="2-1-LSTM模型结构"><a href="#2-1-LSTM模型结构" class="headerlink" title="2.1 LSTM模型结构"></a>2.1 LSTM模型结构</h3><ol><li>跨层连接&lt;/font&gt;<br>LSTM首先将隐含层更新方式改为：<script type="math/tex; mode=display">\mathbf {u_{t}=tanh(W^{xh}x_{t}+b^{xh}+W^{hh}h_{t-1}+b^{hh})}</script><script type="math/tex; mode=display">\mathbf {h_{t}=h_{t-1}+u_{t}}</script></li></ol><p>这样可以直接将$h<em>{k}$与$h</em>{t}$相连，实现跨层连接，减小网络层数，使得网络参数更容易被优化。&lt;/font&gt;证明如下：</p><script type="math/tex; mode=display">\mathbf {h_{t}=h_{t-1}+u_{t}=h_{t-2}+u_{t-1}+u_{t}=...=h_{k}+u_{k+1}+u_{k+2}+...+u_{t-1}+u_{t}}</script><ol><li>增加遗忘门 forget gate&lt;/font&gt;<br>上式直接将旧状态$h<em>{t-1}$和新状态$u</em>{t}$相加，没有考虑两种状态对$h<em>{t}$的不同贡献。故计算$h</em>{t-1}$和$u_{t}$的系数，再进行加权求和&lt;/font&gt;<script type="math/tex; mode=display">\mathbf {f_{t}=\sigma(W^{f,xh}x_{t}+b^{f,xh}+W^{f,hh}h_{t-1}+b^{f,hh})}</script><script type="math/tex; mode=display">\mathsf {h_{t}=f_{t}\odot h_{t-1}+(1-f_{t})\odot u_{t}}</script>其中$\sigma$表示sigmoid函数，值域为（0，1）。当$f_{t}$较小时，旧状态贡献也较小，甚至为0，表示遗忘不重要的信息，所以称为遗忘门。</li><li>增加输入门 Input gate&lt;/font&gt;<br>上一步问题是旧状态$h<em>{t-1}$和新状态$u</em>{t}$权重互斥。但是二者可能都很大或者很小。所以需要用独立的系数来调整。即：<script type="math/tex; mode=display">\mathbf {i_{t}=\sigma(W^{i,xh}x_{t}+b^{i,xh}+W^{i,hh}h_{t-1}+b^{i,hh})}</script><script type="math/tex; mode=display">\mathsf {h_{t}=f_{t}\odot h_{t-1}+i_{t}\odot u_{t}}</script>$i<em>{t}$用于控制输入状态$u</em>{t}$对当前状态的贡献，所以称为输入门</li><li>增加输出门output gate&lt;/font&gt;<script type="math/tex; mode=display">\mathbf {o_{t}=\sigma(W^{o,xh}x_{t}+b^{o,xh}+W^{o,hh}h_{t-1}+b^{o,hh})}</script></li><li>综合计算<script type="math/tex; mode=display">\mathbf {u_{t}=tanh(W^{xh}x_{t}+b^{xh}+W^{hh}h_{t-1}+b^{hh})}</script><script type="math/tex; mode=display">\mathbf {f_{t}=\sigma(W^{f,xh}x_{t}+b^{f,xh}+W^{f,hh}h_{t-1}+b^{f,hh})}</script><script type="math/tex; mode=display">\mathbf {i_{t}=\sigma(W^{i,xh}x_{t}+b^{i,xh}+W^{i,hh}h_{t-1}+b^{i,hh})}</script><script type="math/tex; mode=display">\mathbf {c_{t}=f_{t}\odot c_{t-1}+i_{t}\odot u_{t}}</script><script type="math/tex; mode=display">\mathbf {h_{t}=o_{t}\odot tanh(c_{t})}</script><script type="math/tex; mode=display">\mathbf {y_{n}=softmax(W^{hy}h_{n}+b^{hy})}</script></li></ol><ul><li>遗忘门：$f<em>{t}$，是$c</em>{t-1}$的系数，可以过滤上一时刻的记忆信息。否则之前时刻的$c<em>t$完全保留，$c_t$越来越大，$\mathbf {h</em>{t}=o<em>{t}\odot tanh(c</em>{t})}$tanh会马上饱和，无法输入新的信息。</li><li>输入门：$i<em>{t}$，是$u</em>{t}$的系数，可以过滤当前时刻的输入信息。即不会完整传递当前输入信息，可以过滤噪声等</li><li>输出门：$o<em>{t}$，是$tanh(c</em>{t})$的系数，过滤记忆信息。即$c_t$一部分与当前分类有关，部分是与当前分类无关信息，只是用来传递至未来时刻</li><li>三个门控单元，过滤多少记住多少，都跟前一时刻隐含层输出和当前时刻输入有关</li><li>记忆细胞：$c_{t}$，记录了截止当前时刻的重要信息。</li></ul><p>可以看出RNN的输入层隐含层和输出层三层都是共享参数，到了LSTM都变成参数不共享了。</p><h3 id="2-2-双向循环神经网络Bi-LSTM"><a href="#2-2-双向循环神经网络Bi-LSTM" class="headerlink" title="2.2 双向循环神经网络Bi-LSTM"></a>2.2 双向循环神经网络Bi-LSTM</h3><ul><li>解决循环神经网络信息单向流动的问题。（比如一个词的词性与前面的词有关，也与自身及后面的词有关）</li><li>将同一个输入序列分别接入前向和后向两个循环神经网络中，再将两个循环神经网络的隐含层结果拼接在一起，共同接入输出层进行预测。其结构如下：<br><img src="https://img-blog.csdnimg.cn/6baa84330c3d4c429eab020245b35135.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>此外还可以堆叠多个双向循环神经网络。<br>LSTM比起RNN多了最后时刻的记忆细胞，即：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bilstm=nn.LSTM(</span><br><span class="line">        input_size=<span class="number">1024</span>, </span><br><span class="line">        hidden_size=<span class="number">512</span>, </span><br><span class="line">        batch_first=<span class="literal">True</span>,</span><br><span class="line">        num_layers=<span class="number">2</span>,<span class="comment">#堆叠层数</span></span><br><span class="line">        dropout=<span class="number">0.5</span>,  </span><br><span class="line">        bidirectional=<span class="literal">True</span><span class="comment">#双向循环)</span></span><br><span class="line"></span><br><span class="line">hidden, hn = self.rnn(inputs)</span><br><span class="line"><span class="comment">#hidden是各时刻的隐含层，hn为最后时刻隐含层</span></span><br><span class="line">hidden, (hn, cn) = self.lstm(inputs)</span><br><span class="line"><span class="comment">#hidden是各时刻的隐含层，hn, cn为最后时刻隐含层和记忆细胞</span></span><br></pre></td></tr></table></figure><h2 id="三、序列到序列模型"><a href="#三、序列到序列模型" class="headerlink" title="三、序列到序列模型"></a>三、序列到序列模型</h2><p><img src="https://img-blog.csdnimg.cn/64936f4d61b341989d8f8b5774a68e23.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>encoder最后状态的输出输入decoder作为其第一个隐含状态$h_0$。decoder每时刻的输出都会加入下一个时刻的输入序列，一起预测下一时刻的输出，直到预测出End结束。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、RNN&quot;&gt;&lt;a href=&quot;#一、RNN&quot; class=&quot;headerlink&quot; title=&quot;一、RNN&quot;&gt;&lt;/a&gt;一、RNN&lt;/h2&gt;&lt;p&gt;前馈神经网络：信息往一个方向流动。包括MLP和CNN&lt;br&gt;循环神经网络：信息循环流动，网络隐含层输出又作为自身输入&amp;lt;/font&amp;gt;，包括RNN、LSTM、GAN等。&lt;/p&gt;
&lt;h3 id=&quot;1-1-RNN模型结构&quot;&gt;&lt;a href=&quot;#1-1-RNN模型结构&quot; class=&quot;headerlink&quot; title=&quot;1.1 RNN模型结构&quot;&gt;&lt;/a&gt;1.1 RNN模型结构&lt;/h3&gt;&lt;p&gt;RNN模型结构如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/62ffdeb2b9ec4e2cbcd4edfc85583c24.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_9,color_FFFFFF,t_70,g_se,x_16&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
&lt;p&gt;展开之后相当于堆叠多个共享隐含层参数的前馈神经网络：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/e43dbda3e0d24019a13285a6e31086c4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_18,color_FFFFFF,t_70,g_se,x_16&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="LSTM" scheme="https://zhxnlp.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记6：卷积神经网络CNN</title>
    <link href="https://zhxnlp.github.io/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/"/>
    <id>https://zhxnlp.github.io/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/</id>
    <published>2021-11-27T15:15:49.000Z</published>
    <updated>2022-01-02T20:47:10.022Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、CNN模型原理"><a href="#一、CNN模型原理" class="headerlink" title="一、CNN模型原理"></a>一、CNN模型原理</h2><h3 id="1-1-图像"><a href="#1-1-图像" class="headerlink" title="1.1 图像"></a>1.1 图像</h3><ul><li>图像具有平移不变性和旋转不变性。即对图像的平移或者轻微旋转不改变其类别。图像可以用像素点来表示，存储为一个三维矩阵（长×宽×channels）</li><li>黑白图片channels=1，即每个像素点只有灰度值。彩色图像channels=3，每个像素点由RGB三原色组成，对应一个三维向量，值域[0，255]。一般0表示白色，255表示黑色<h3 id="1-2-DNN图像分类的问题"><a href="#1-2-DNN图像分类的问题" class="headerlink" title="1.2 DNN图像分类的问题"></a>1.2 DNN图像分类的问题</h3>如果直接将图像根据各像素点的向量作为图片特征输入模型，例如LR、SVM、DNN等模型进行分类，理论上可行，但是面临以下问题：</li><li>图像的平移旋转、手写数字笔迹的变化等，会造成输入图像特征矩阵的剧烈变化，影响分类结果。即不抗平移旋转等。</li><li>一般图像像素很高，如果直接DNN这样全连接处理，计算量太大，耗时太长；参数太多需要大量训练样本<span id="more"></span><h3 id="1-3-卷积原理"><a href="#1-3-卷积原理" class="headerlink" title="1.3 卷积原理"></a>1.3 卷积原理</h3></li><li>人类认识图片的原理：不纠结图像每个位置具体的像素值，重点是每个区域像素点组成的几何形状，以及几何形状的相对位置和搭配。也就是图像中更抽象的轮廓信息。</li><li>模型需要对平移、旋转、笔迹变化等不敏感且参数较少。</li></ul><p>卷积原理：</p><ul><li><p>CNN通过卷积核对图像的各个子区域进行特征提取，而不是直接从像素上提取特征。子区域称为感受野。</p></li><li><p>卷积运算：图片感受野的像素值与卷积核的像素值进行按位相乘后求和，加上偏置之后过一个激活函数（一般是Relu）得到特征图Feature Map。</p></li><li><p>特征图：卷积结果不再是像素值，而是感受野形状和卷积核的匹配程度，称之为特征图Feature Map。卷积后都是输出特定形状的强度值，与卷积核形状差异过大的感受野输出为0（经过Relu激活），所以卷积核也叫滤波器Filter。</p></li></ul><p>卷积的特点：</p><ul><li><p>使用一个多通道卷积核对多通道图像卷积，结果仍是单通道图像。要想保持多通道结果，就得使用多个卷积核。同一个通道的卷积结果是一类特征（同一个卷积核计算的结果）。</p></li><li><p>卷积核的参数为待学习参数，可以通过模型训练自动得到。</p></li><li>图像识别中有很多复杂的识别任务，如果每个图像对应一个卷积核，那么卷积核会很大，参数过多。另外复杂图像形状各异，但是基本元素种类差不多。所以CNN使用多个不同尺寸的卷积核进行基本形状的提取</li><li>假设图像大小为$N<em>N$矩阵，卷积核的尺寸为$K</em>K$矩阵，图像边缘像素填充:P，卷积的步伐为S，那么经过一层这样的卷积后出来的图像为：W=(N-K+2P)/S+1。当步幅为1，padding=1时，卷积后图像尺寸不变。<h3 id="1-4-池化原理"><a href="#1-4-池化原理" class="headerlink" title="1.4 池化原理"></a>1.4 池化原理</h3>池化层的作用：缩减图像尺寸；克服图像图像平移、轻微旋转的影响；间接增大后续卷积的感受野；降低运算量和参数量</li><li>消除相邻感受野的信息冗余现象（相邻感受野形状差异不大）</li><li>池化操作对子区域内的轻微改变不敏感，所以可以克服图像平移。轻微旋转的影响</li><li>缩减特征图，增大后续卷积操作的感受野，以便后续提取更宏观的特征。（比如经过2×2池化，池化后图像每个像素对应于原图像2×2的感受野，此时在用3×3卷积，那么卷积后每个像素对应于6×6的感受野）</li><li>降低运算量和参数量</li></ul><p>池化的特点：</p><ul><li>只需要设定池化层大小和池化标准（最大池化或均值池化、中位数池化），就可以进行池化计算，没有参数需要学习</li><li>最大池化提取特征能力较强，但是容易被噪声干扰。均值池化相对稳定，对噪声不敏感</li><li>池化在各个通道上独立进行</li><li>池化步长一般会参考感受野尺寸。当二者相等时，池化时没有交集</li></ul><p>池化技巧：<br>&#8195;&#8195;如果有20个卷积层，max池化和ave池化混用，怎么安排？应该是前面的层用最大池化，尽可能的提取特征；后面层用ave池化，减少尺寸抗平移。<br>&#8195;&#8195;因为一开始就用平均池化，把特征平均掉了，就很难恢复了。所以是先提取特征再去噪。训练样本足够多的时候，不太容易被噪声所影响，直接用max池化。（足够的样本可以平均噪声）</p><p>而在不同场景用的池化操作也不一样：</p><ol><li>人脸识别：公司打卡系统 。对特征要求高，需要把每个人的五官特点提取出来（max pool）</li><li>人脸检测：画面是否有人 。要求低，大概轮廓出来即可（ave pool）</li></ol><h3 id="1-5-Flatten"><a href="#1-5-Flatten" class="headerlink" title="1.5 Flatten"></a>1.5 Flatten</h3><ul><li>卷积-池化输出是一个多通道的而为特征，而最后softmax分类，softmax只能作用于一个向量。所以需要对卷积0池化结果进行拉平操作，也就是Flatten。</li><li>Flatten没有参数，例如一个7×7×10的矩阵，会被拉平成490维向量。<br><img src="https://img-blog.csdnimg.cn/a5ef3ebb7f704a0f8fb4dda4dc7c4d7e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="1-6-卷积池化总结"><a href="#1-6-卷积池化总结" class="headerlink" title="1.6 卷积池化总结"></a>1.6 卷积池化总结</h3>CNN使用不同卷积核提取图像中特定的形状特征，配合池化操作进一步缩减图像尺寸，稳定特征。对低级特征进行卷积计算，得到相对高级的特征。所以多层卷积-池化，层层堆叠可以提取复杂特征。<br>需要注意的是：</li><li>一个CNN模型只能处理一种尺寸的图像，所以实际中需要将输入图像全部处理成同一尺寸</li><li>为了防止数值溢出和激活函数饱和造成的梯度消失或者梯度爆炸，输入图像像素值会归一化至[0,1]</li><li>CNN中每个通道代表同一类特征（同一个卷积核计算的结果），所以可以对同一个通道的数值进行批归一化。分别计算n个通道上的n组均值和方差。</li><li>对于28×28×1的图像，如果全连接并保持图像尺寸不变，则参数量为28×28×1×28×28×1=614656个。如果进行3×3卷积并保持尺寸不变，参数量为28×28×1×3×3。可以理解为隐藏层每个神经元只与输入层9个神经元相连，其它连接都被剪枝，各个位置参数共享&lt;/font&gt;（隐藏层还是576神经元，但是前后层都是9*9相连）&lt;/font&gt;所以CNN是通过权重共享和局部感受野（剪枝）对DNN全连接进行简化。<h2 id="二、卷积池化计算"><a href="#二、卷积池化计算" class="headerlink" title="二、卷积池化计算"></a>二、卷积池化计算</h2>一个常见的CNN例子如下图：<br><img src="https://img-blog.csdnimg.cn/9b5798fb03ed4920be2a6222473f626d.png" alt="在这里插入图片描述"><h3 id="2-1-初识卷积"><a href="#2-1-初识卷积" class="headerlink" title="2.1. 初识卷积"></a>2.1. 初识卷积</h3>微积分中卷积的表达式为：<script type="math/tex">S(t) = \int x(t-a)w(a) da</script></li></ul><p>　　　　离散形式是：<script type="math/tex">s(t) = \sum\limits_ax(t-a)w(a)</script></p><p>　　　　这个式子如果用矩阵表示可以为：<script type="math/tex">s(t)=(X*W)(t)</script></p><p>　　　　其中星号表示卷积。</p><p>　　　　如果是二维的卷积，则表示式为：<script type="math/tex">s(i,j)=(X*W)(i,j) = \sum\limits_m \sum\limits_n x(i-m,j-n) w(m,n)</script></p><p>　　　　在CNN中，虽然我们也是说卷积，但是我们的卷积公式和严格意义数学中的定义稍有不同,比如对于二维的卷积，定义为：<script type="math/tex">s(i,j)=(X*W)(i,j) = \sum\limits_m \sum\limits_n x(i+m,j+n) w(m,n)</script></p><p>　　　　这个式子虽然从数学上讲不是严格意义上的卷积，但是大牛们都这么叫了，那么我们也跟着这么叫了。后面讲的CNN的卷积都是指的上面的最后一个式子。</p><p>　　　　其中，我们叫W为我们的卷积核，而X则为我们的输入。如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。但是如果X是多维张量，那么W也是一个多维的张量。</p><h3 id="2-2-CNN中的卷积层"><a href="#2-2-CNN中的卷积层" class="headerlink" title="2.2. CNN中的卷积层"></a>2.2. CNN中的卷积层</h3><p>图像卷积:对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到。</p><p>　　　　举个例子如下:</p><ul><li>输入二维的3x4的矩阵</li><li>卷积核是一个2x2的矩阵</li><li>卷积步长为1（一次移动一个像素来卷积）</li></ul><ol><li>首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。</li><li>我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素</li><li>同样的方法，我们可以得到输出矩阵S的$S<em>{02}，S</em>{10}，S<em>{11}， S</em>{12}$的元素。<br>图像卷积，回想我们的上一节的卷积公式，其实就是对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到。</li></ol><h4 id="2-2-1-二维卷积："><a href="#2-2-1-二维卷积：" class="headerlink" title="2.2.1 二维卷积："></a>2.2.1 二维卷积：</h4><p>举例如下:</p><ul><li>输入二维的3x4的矩阵，卷积核是一个2x2的矩阵。步幅S=1。首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。</li><li>接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素</li><li>同样的方法，我们可以得到输出矩阵S的$S<em>{02}，S</em>{10}，S<em>{11}， S</em>{12}$的元素。<br><img src="https://img-blog.csdnimg.cn/07c3043832834aeeb273b4a6dc3313b9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>最终我们得到卷积输出的矩阵为一个2x3的矩阵S。</li></ul><p>假设图像大小为$N<em>N$矩阵，卷积核的尺寸为$K</em>K$矩阵，图像边缘像素填充:P，卷积的步伐为S，那么经过一层这样的卷积后出来的图像为：W=(N-K+2P)/S+1</p><h4 id="2-2-2-三维卷积"><a href="#2-2-2-三维卷积" class="headerlink" title="2.2.2 三维卷积"></a>2.2.2 三维卷积</h4><p> 输入是多维的情况，在斯坦福大学的cs231n的课程上，有一个<a href="https://cs231n.github.io/convolutional-networks/">动态的例子</a></p><ul><li>输入3个7x7的矩阵。（原输入是3个5x5的矩阵，padding=1）</li><li>卷积核：<ul><li>两个卷积核，卷积核的个数是即模板的个数。</li><li>卷积核也是三维，且最后一维和输入维数第三维channel数一样。&lt;/font&gt;卷积核W0的单个子矩阵维度为3x3，加上最后一维为3，最终是一个3x3x3的张量。</li><li>步幅为2　　　　</li></ul></li><li>张量的卷积：两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏置。</li></ul><p>每个卷积核卷积的结果是一个3x3的矩阵，卷积的结果是一个3x3x2的张量。把上面的卷积过程用数学公式表达出来就是：<script type="math/tex">s(i,j)=(X*W)(i,j) + b = \sum\limits_{k=1}^{n\_in}(X_k*W_k)(i,j) +b</script><br>　　　　其中，$n_in$为输入矩阵的个数，或者是张量的最后一维的维数。$X_k$代表第k个输入矩阵（channel个）。$W_k$代表卷积核的第k个子卷积核矩阵。$s(i,j)$即卷积核$W$对应的输出矩阵的对应位置元素的值。　　　　</p><ul><li>激活:对于卷积后的输出，一般会通过ReLU激活函数，将输出的张量中的小于0的位置对应的元素值都变为0。<h4 id="2-2-3-卷积计算公式"><a href="#2-2-3-卷积计算公式" class="headerlink" title="2.2.3 卷积计算公式"></a>2.2.3 卷积计算公式</h4>&#8195;&#8195;对图像的每个像素进行编号，用$x<em>{i,j}$表示图像的第行第列元素；用$w</em>{m,n}$表示卷积核filter&lt;/font&gt;第m行第n列权重，用$w<em>b$表示filter的偏置项；用$a</em>{i,j}$表示特征图Feature Map&lt;/font&gt;的第i行第j列元素；用$f$表示激活函数(这个例子选择relu函数作为激活函数)。使用下列公式计算卷积：<script type="math/tex; mode=display">a_{i,j}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_{b})</script>&#8195;&#8195;如果卷积前的图像深度为D，那么相应的filter的深度也必须为D。我们扩展一下上式，得到了深度大于1的卷积计算公式：<script type="math/tex; mode=display">a_{d,i,j}=f(\sum_{d=0}^{D-1}\sum_{m=0}^{F-1}\sum_{n=0}^{F-1}w_{d,m,n}x_{d,i+m,j+n}+w_{b})</script>&#8195;&#8195; W2是卷积后Feature Map的宽度；W1是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量。D是深度（卷积核个数）；F是卷积核filter的矩阵维数；$w<em>{d,m,n}$表示filter的第d层第m行第n列权重；$a</em>{d,i,j}$表示Feature Map图像的第d层第i行第j列像素。<br>&#8195;&#8195;每个卷积层可以有多个卷积核filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度(个数)和卷积层的filter个数是相同的。</li></ul><h3 id="2-3-CNN中的池化层"><a href="#2-3-CNN中的池化层" class="headerlink" title="2.3 CNN中的池化层"></a>2.3 CNN中的池化层</h3><ul><li>池化，就是对输入张量的各个子矩阵进行压缩，将输入子矩阵的每nxn个元素变成一个元素，所以需要一个池化标准。</li><li>常见的池化标准有2个，MAX或者是Average。即取对应区域的最大值或者平均值作为池化后的元素值。</li></ul><p>2x2最大池化，步幅为2时，池化操作如下：<br><img src="https://img-blog.csdnimg.cn/821b9e32e4e842519dc6412f1eb8f2c8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-4-CNN前向传播算法"><a href="#2-4-CNN前向传播算法" class="headerlink" title="2.4 CNN前向传播算法"></a>2.4 CNN前向传播算法</h3><p>输入：1个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。</p><p>　　　　输出：CNN模型的输出$a^L$</p><p>　　　　&#8195;&#8195;1) 根据输入层的填充大小P，填充原始图片的边缘，得到输入张量$a^1$。</p><p>　　　　&#8195;&#8195;2）初始化所有隐藏层的参数$W,b$　　</p><p>　　　　&#8195;&#8195;3）for $l$=2 to $L-1$（层数$l$）:</p><p>&#8195;&#8195;&#8195;&#8195;a) 如果第$l$层是卷积层&lt;/font&gt;,则输出为（*表示卷积，而不是矩阵乘法）</p><script type="math/tex; mode=display">a^l= ReLU(z^l) = ReLU(a^{l-1}*W^l +b^l)$$　　&#8195;&#8195;&#8195;&#8195;（这里要定义卷积核个数，卷积核中每个子矩阵大小，填充padding（以下简称P）和填充padding（以下简称P）） 　　　　b) 如果第$l$层是池化层</font>,则输出为：$$a^l= pool(a^{l-1})</script><p> 　　　　 需要定义池化大小和池化标准,池化层没有激活函数</p><p>　　　　　　&#8195;&#8195;&#8195;&#8195;c) 如果第$l$层是全连接层&lt;/font&gt;,则输出为：<script type="math/tex">a^l= \sigma(z^l) = \sigma(W^la^{l-1} +b^l)</script></p><p>　　　　&#8195;&#8195;4)对于输出层第L层&lt;/font&gt;: <script type="math/tex">a^L= softmax(z^L) = softmax(W^La^{L-1} +b^L)</script></p><h3 id="2-5-CNN反向传播算法"><a href="#2-5-CNN反向传播算法" class="headerlink" title="2.5 CNN反向传播算法"></a>2.5 CNN反向传播算法</h3><blockquote><p>参考<a href="https://blog.csdn.net/m0_64375823/article/details/121584188">《卷积神经网络(CNN)》</a></p></blockquote><h2 id="三、深入卷积层"><a href="#三、深入卷积层" class="headerlink" title="三、深入卷积层"></a>三、深入卷积层</h2><h3 id="3-1-1×1卷积"><a href="#3-1-1×1卷积" class="headerlink" title="3.1 1×1卷积"></a>3.1 1×1卷积</h3><p> 1×1卷积作用是改变通道数，降低运算量和参数量。同时增加一次非线性变化，提升网络拟合能力。</p><ul><li>对于一个$28×28×f<em>{1}$的图像，进行$f</em>{2}个1×1$卷积操作，得到$28×28×f<em>{2}$的图像，且参数量仅有$f</em>{1}×f_{2}$（忽略偏置）。<ul><li>$f<em>{1}&gt;f</em>{2}$时起到降维的作用，降低其它卷积操作的运算量。但是降维太厉害会丢失很多信息。</li><li>$f<em>{1}&lt;f</em>{2}$时起到升维作用（增加通道数），可以让后续卷积层提取更加丰富的特征</li></ul></li><li>增加一次非线性变化，提升网络拟合能力。</li></ul><p>所以可以先用1×1卷积改变通道数，再进行后续卷积操作，这个是Depth wise提出的。</p><h3 id="3-2-VGGnet：小尺寸卷积效果好"><a href="#3-2-VGGnet：小尺寸卷积效果好" class="headerlink" title="3.2 VGGnet：小尺寸卷积效果好"></a>3.2 VGGnet：小尺寸卷积效果好</h3><p>卷积的尺寸决定卷积的视野，越大则提取的特征越宏观。但是大尺寸卷积，参数量和运算量都很大，而多个小尺寸卷积可以达到相同的效果，且参数量更小。还可以多次进行激活操作，提高拟合能力。&lt;/font&gt;例如：<br>一个5×5卷积参数量25，可以替换成两个3×3卷积。，参数量为18。每个3×3卷积可以替换成3×1卷积加1×3卷积，参数量为12。</p><h3 id="3-3-inception宽度卷积核和GoogLeNet"><a href="#3-3-inception宽度卷积核和GoogLeNet" class="headerlink" title="3.3 inception宽度卷积核和GoogLeNet"></a>3.3 inception宽度卷积核和GoogLeNet</h3><p><img src="https://img-blog.csdnimg.cn/7517f9b7b00f497cab5f4c57eca04345.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>实际CNN识别中，会遇到识别物体尺寸不一的情况。不同尺寸的物体需要不同尺寸的卷积核来提取特征。如果增加网络深度来处理，会造成：</p><ul><li>训练集不够大，则过拟合</li><li>深层网络容易梯度消失，模型难以优化</li><li>简单堆叠较大的卷积层浪费计算资源</li></ul><p>为了使卷积层适应不同的物体尺寸，一般会在同一层网络中并列使用多种尺寸的卷积，以定位不同尺寸的物体。此时增加了网络宽度，而不会增加其深度。</p><p>2016年google的inception模型首先提出，结构如下：</p><blockquote><p>图片参考：<a href="https://blog.csdn.net/qq_37555071/article/details/108214680">《深入解读GoogLeNet网络结构（附代码实现）》</a><br><img src="https://img-blog.csdnimg.cn/827565c5ccd84d84a06236424462e4c5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></blockquote><ul><li>上图使用三种尺寸卷积核进行特征提取，并列了一个最大池化（抗平移），池化步长为1，不会改变输入尺寸。最后多个同尺寸的卷积结果进行级联，得到新的图像。（并列在一起，增加了通道数）</li><li>多个卷积核级联造成通道数过多，所以可以在卷积前、3×3池化后分别进行1×1卷积进行降维，同时提高网络非线性程度。</li><li>最终输出和输入图像尺寸相同，但是通道数可以不一样。</li></ul><p>多个inception堆叠就是GoogLeNet:<br><img src="https://img-blog.csdnimg.cn/85f04938e7a94fd5a31b34f02f24fe54.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>第一个红色框住的模块叫stem（根源），就是传统的卷积。</li><li>后面九个模块都是inception，再过一个全连接层，最后过softmax进行分类。</li></ul><p>这样会有一个问题：    网络太深造成梯度消失，前面的层基本就没有什么信息了（梯度消失学不到）。所以中间层引入两个辅助分类器，并配以辅助损失函数。防止前层网络信息丢失。&lt;/font &gt;具体地：</p><ul><li>三个黄色和椭圆模块是做三次分类。即在3.6.9层inception时输出做分类。</li><li>三个分类器的任务完全一样，$loss=w<em>{1}loss</em>{1}+ w<em>{2}loss</em>{2}<br>+w<em>{3}loss</em>{3}$。$w_{3}$的系数应该高一些，具体权重可以查。辅助分类器只用来训练，不用于推断</li><li>训练时三个分类器一起训练，使用的时候只用最后一层。最后一个inception使用全局平均池化</li></ul><p>GoogleNet知识点：</p><ol><li>inception</li><li>深层网络可以从中间抽取loss来训练，防止过拟合。</li><li>启发：网络太深，涉及梯度消失时，都可以这样搞：中间层可以抽出loss来一起学习。（shortcut也可以，一个道理，可能还好一点，比较方便）。</li></ol><p>GoogleNet：</p><p><img src="https://img-blog.csdnimg.cn/bc3f9203cabc442da8688cba3f12b10d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="3-4-Depth-wise和Pointwise降低运算量"><a href="#3-4-Depth-wise和Pointwise降低运算量" class="headerlink" title="3.4 Depth wise和Pointwise降低运算量"></a>3.4 Depth wise和Pointwise降低运算量</h3><ul><li>传统卷积：一个卷积核卷积图像的所有通道，参数过多，运算量大。<br><img src="https://img-blog.csdnimg.cn/b79e31d49ba14ab69f17f0fc9817b01a.png" alt="在这里插入图片描述"><br>运算量（忽略偏置）：$28<em>28</em>128<em>3</em>3<em>256=231211008$<br>参数量（忽略偏置）：$128</em>3<em>3</em>256=294912$</li><li>Depth wise卷积：一个卷积核只卷积一个通道。输出图像通道数和输入时不变。缺点是每个通道独立卷积运算，没有利用同一位置上不同通道的信息</li><li>Pointwise卷积：使用多个1×1标准卷积，将Depth wise卷积结果的各通道特征加权求和，得到新的特征图<br><img src="https://img-blog.csdnimg.cn/dac8bfddaf3645e8b109cfa7adad7214.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>运算量（忽略偏置）：$28<em>28</em>128<em>3</em>3+28<em>28</em>128<em>256=26593280$<br>参数量（忽略偏置）：$3</em>3<em>128+128</em>256=33920$<h3 id="3-5-SENet、CBAM特征通道加权卷积"><a href="#3-5-SENet、CBAM特征通道加权卷积" class="headerlink" title="3.5 SENet、CBAM特征通道加权卷积"></a>3.5 SENet、CBAM特征通道加权卷积</h3><h4 id="3-5-1-SENet"><a href="#3-5-1-SENet" class="headerlink" title="3.5.1 SENet"></a>3.5.1 SENet</h4><blockquote><p>可参考<a href="https://blog.csdn.net/qq_41917697/article/details/114100031">《CNN卷积神经网络之SENet及代码》</a><br>SENet：卷积操作中，每个通道对应一类特征。而不同特征对最终任务结果贡献是不一样的，所以考虑调整各通道的权重。</p></blockquote></li></ul><ol><li>SE模块，对各通道中所有数值进行全局平均，此操作称为Squeeze。比如28×28×128的图像，操作后得到128×1的向量。</li><li>此向量输入全连接网络，经过sigmoid输出128维向量，每个维度值域为（0,1），表示各个通道的权重</li><li>在正常卷积中改为各通道加权求和，得到最终结果<br><img src="https://img-blog.csdnimg.cn/323821f98d6f49c1ac526604c3e5090b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ol><ul><li>Squeeze建立channel间的依赖关系；Excitation重新校准特征。二者结合强调有用特征抑制无用特征</li><li>能有效提升模型性能，提高准确率。几乎可以无脑添加到backbone中。根据论文，SE block应该加在Inception block之后，ResNet网络应该加在shortcut之前，将前后对应的通道数对应上即可<h4 id="3-5-2-CBAM"><a href="#3-5-2-CBAM" class="headerlink" title="3.5.2 CBAM"></a>3.5.2 CBAM</h4><blockquote><p>参考<a href="https://blog.csdn.net/qq_36584673/article/details/116088055">《CBAM重点干货和流程详解及Pytorch实现》</a></p><p>除了通道权重，CBAM还考虑空间权重，即：图像中心区域比周围区域更重要，由此设置不同位置的空间权重。CBAM将空间注意力和通道注意力结合起来。</p></blockquote></li></ul><p>Channel attention module：<br><img src="https://img-blog.csdnimg.cn/cad6ce18b4b943ed8dfb452acdd1d01b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>输入特征图F，经过两个并行的最大值池化和平均池化将C×H×W的特征图变成C×1×1的大小</li><li>经过一个共享神经网络Shared MLP(Conv/Linear，ReLU，Conv/Linear)，压缩通道数C/r (reduction=16)，再扩张回C，得到两个激活后的结果。</li><li>最后将二者相加再接一个sigmoid得到权重channel_out，再加权求和。</li></ul><p>此步骤与SENet不同之处是加了一个并行的最大值池化，提取到的高层特征更全面，更丰富。</p><p>Channel attention module：</p><p>将上一步得到的结果通过最大值池化和平均池化分成两个大小为H×W×1的张量，然后通过Concat操作将二者堆叠在一起(C为2)，再通过卷积操作将通道变为1同时保证H和W不变，经过一个sigmoid得到spatial_out，最后spatial_out乘上一步的输入变回C×H×W，完成空间注意力操作</p><p>总结：</p><ul><li>实验表明：通道注意力在空间注意力之前效果更好</li><li>加入CBAM模块不一定会给网络带来性能上的提升，受自身网络还有数据等其他因素影响，甚至会下降。如果网络模型的泛化能力已经很强，而你的数据集不是benchmarks而是自己采集的数据集的话，不建议加入CBAM模块。要根据自己的数据、网络等因素综合考量。<h3 id="3-6-inception几个改进版"><a href="#3-6-inception几个改进版" class="headerlink" title="3.6 inception几个改进版"></a>3.6 inception几个改进版</h3>google对inception进行改造，出现了inception1→inception2→inception3→Xception→inception4→inception ResNetV1→inception →ResNetV2。<h4 id="3-6-1-Inception2"><a href="#3-6-1-Inception2" class="headerlink" title="3.6.1 Inception2"></a>3.6.1 Inception2</h4><img src="https://img-blog.csdnimg.cn/2f0616b8df2448a1bc6da9c944ee866b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>变种A是基础版的5×5改成两个3×3，B是3×3拆成两个，C是拆成的两个并联。</li></ul><p>对卷积核进行了几种改造。但是设计思想都是：<br>1.大量应用1×1卷积核和n×1卷积核（1×3和3×1）<br>2.大量应用小卷积核（没有超过3乘3的）<br>3.并联卷积</p><h4 id="3-6-2-Inception3"><a href="#3-6-2-Inception3" class="headerlink" title="3.6.2 Inception3"></a>3.6.2 Inception3</h4><p>最大贡献：标签平滑防止过拟合</p><ul><li>对于逻辑回归来说，单个样本$loss=-ylogy’-(1-y)log(1-y’)$。y’∈（0,1）是一个闭区间,预测值y’只能无限逼近0和1，但是永远取不到0或1。即单个样本没有极小值。</li><li>这样在拟合的时候随着梯度下降，y’不断向0或1逼近，w会不断增大。而如果标签y=1做平滑改成y=0.97，y’就可以取到这个值，w就不会无限增大，所以避免了过拟合。</li><li>也可以看做对标签适当注入噪声防止过拟合。（LR可以看做二分类的softmax，所以此处也适用）</li><li>加正则项主要是让模型在测试集上的效果尽可能和训练集效果一样好，标签平滑让模本本身有一个好的性能（防止标签打错等噪声）。<h4 id="3-6-3-Xception、inception4"><a href="#3-6-3-Xception、inception4" class="headerlink" title="3.6.3 Xception、inception4"></a>3.6.3 Xception、inception4</h4></li><li>Xception：3×3正常卷积变成Depth wise（上一节讲过）</li><li>inception4是改变了stem<br><img src="https://img-blog.csdnimg.cn/6ea077128fb4406f94ac4a06d886a130.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h4 id="3-6-4-inception-ResNetV1-amp-2"><a href="#3-6-4-inception-ResNetV1-amp-2" class="headerlink" title="3.6.4 inception ResNetV1&amp;2"></a>3.6.4 inception ResNetV1&amp;2</h4><img src="https://img-blog.csdnimg.cn/435581b3b8bc4bf381e1c7867620f024.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>inception ResNetV1&amp;2最主要思想就是与shortcut结合。inception模块的输出和模块输入直接按位相加。即对应channel对应位置的元素相加。这样就要求输出的channel和尺寸要和输出的一致。而一般不池化尺寸可以不变，channel数通过最后一层的1×1卷积核来调整。<br>  至于中间的细节，右侧的构造为啥是这样的，都是试验碰出来的，没必要纠结。</li></ul><h4 id="3-6-5-Resnet-amp-Renext"><a href="#3-6-5-Resnet-amp-Renext" class="headerlink" title="3.6.5 Resnet&amp;Renext"></a>3.6.5 Resnet&amp;Renext</h4><blockquote><p>参考<a href="https://blog.csdn.net/qq_41917697/article/details/115905056">《CNN卷积神经网络之ResNeXt》</a></p></blockquote><p>Resnet是微软何凯明搞出来的（93年的人）。主要也是借鉴shortcut思想，因为网络太深必然会碰到梯度消失的问题。然后就是一堆小卷积核，每两层抄一次近道是试验出来的效果。抄近道就必须保持前后的channel数一致。<br>最重要的部分就是：<br><img src="https://img-blog.csdnimg.cn/0ec2991998ce4df88784a23e188d1423.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_10,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>然后Resnet又借鉴inception把网络都做宽了，就是Renext：</p><p>ResNeXt是ResNet和Inception的结合体，因此你会觉得与InceptionV4有些相似，但却更简洁，同时还提出了一个新的维度： cardinality （基数），在不加深或加宽网络增加参数复杂度的前提下提高准确率，还减少了超参数的数量。<br><img src="https://img-blog.csdnimg.cn/80147e3181fe476b9ab70650be2a8f72.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>进一步进行了等效转换的，采用了分组卷积的方法。<br><img src="https://img-blog.csdnimg.cn/2f910437e1d542d791482b0ef794d1c2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、CNN模型原理&quot;&gt;&lt;a href=&quot;#一、CNN模型原理&quot; class=&quot;headerlink&quot; title=&quot;一、CNN模型原理&quot;&gt;&lt;/a&gt;一、CNN模型原理&lt;/h2&gt;&lt;h3 id=&quot;1-1-图像&quot;&gt;&lt;a href=&quot;#1-1-图像&quot; class=&quot;headerlink&quot; title=&quot;1.1 图像&quot;&gt;&lt;/a&gt;1.1 图像&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图像具有平移不变性和旋转不变性。即对图像的平移或者轻微旋转不改变其类别。图像可以用像素点来表示，存储为一个三维矩阵（长×宽×channels）&lt;/li&gt;
&lt;li&gt;黑白图片channels=1，即每个像素点只有灰度值。彩色图像channels=3，每个像素点由RGB三原色组成，对应一个三维向量，值域[0，255]。一般0表示白色，255表示黑色&lt;h3 id=&quot;1-2-DNN图像分类的问题&quot;&gt;&lt;a href=&quot;#1-2-DNN图像分类的问题&quot; class=&quot;headerlink&quot; title=&quot;1.2 DNN图像分类的问题&quot;&gt;&lt;/a&gt;1.2 DNN图像分类的问题&lt;/h3&gt;如果直接将图像根据各像素点的向量作为图片特征输入模型，例如LR、SVM、DNN等模型进行分类，理论上可行，但是面临以下问题：&lt;/li&gt;
&lt;li&gt;图像的平移旋转、手写数字笔迹的变化等，会造成输入图像特征矩阵的剧烈变化，影响分类结果。即不抗平移旋转等。&lt;/li&gt;
&lt;li&gt;一般图像像素很高，如果直接DNN这样全连接处理，计算量太大，耗时太长；参数太多需要大量训练样本</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="CNN" scheme="https://zhxnlp.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记5：word2vec、FastText原理</title>
    <link href="https://zhxnlp.github.io/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05%EF%BC%9Aword2vec%E5%92%8Cfasttext/"/>
    <id>https://zhxnlp.github.io/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05%EF%BC%9Aword2vec%E5%92%8Cfasttext/</id>
    <published>2021-11-27T10:26:31.000Z</published>
    <updated>2022-01-02T20:47:03.862Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、word2vec"><a href="#一、word2vec" class="headerlink" title="一、word2vec"></a>一、word2vec</h2><p>参考文档<a href="https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80">《word2vec原理和gensim实现》</a>、<a href="https://zhuanlan.zhihu.com/p/114538417">《深入浅出Word2Vec原理解析》</a></p><h3 id="1-1-word2vec为什么-不用现成的DNN模型"><a href="#1-1-word2vec为什么-不用现成的DNN模型" class="headerlink" title="1.1 word2vec为什么 不用现成的DNN模型"></a>1.1 word2vec为什么 不用现成的DNN模型</h3><ol><li>最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，==从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值==。解决办法有两个：霍夫曼树和负采样。</li><li>对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。输入从多个词向量变成了一个词向量</li><li>在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量<span id="more"></span><h3 id="1-2-word2vec两种模型：CBOW和Skip-gram"><a href="#1-2-word2vec两种模型：CBOW和Skip-gram" class="headerlink" title="1.2 word2vec两种模型：CBOW和Skip-gram"></a>1.2 word2vec两种模型：CBOW和Skip-gram</h3>&#8195;&#8195;Word2Vec是轻量级的神经网络，其模型仅仅包括输入层、隐藏层和输出层，模型框架根据输入输出的不同，主要包括CBOW和Skip-gram模型。 </li></ol><ul><li>CBOW的方式是在知道词$w<em>{t}$的上下文$w</em>{t-2}$、$w<em>{t-1}$和$w</em>{t+1}$、$w<em>{t+2}$的情况下预测当前词$w</em>{t}$。</li><li>Skip-gram是在知道了词$w_{t}$的情况下,对词的上下文进行预测，如下图所示：<br><img src="https://img-blog.csdnimg.cn/8486814a2075418a807be2402c23abf1.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></li></ul><h3 id="1-2-word2vec两种优化解法：霍夫曼树和负采样"><a href="#1-2-word2vec两种优化解法：霍夫曼树和负采样" class="headerlink" title="1.2 word2vec两种优化解法：霍夫曼树和负采样"></a>1.2 word2vec两种优化解法：霍夫曼树和负采样</h3><ol><li><p>霍夫曼树解法：</p><ul><li>采用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</li><li>把之前计算所有词的softmax概率变成了查找二叉霍夫曼树。那么我们的softmax概率计算只需要沿着树形结构进行，从根节点一直走到我们的叶子节点的词。将每个节点向左或向右走的概率连乘就是最终预测的概率。训练时只更新对应通路的w，与全连接W相比大大减少。</li><li>因为涉及连乘，每次乘的概率都是小于1，所以越到深层概率越低。所以其实存在一个词与词之间概率不对等的问题。</li><li>霍夫曼编码：由于权重高的叶子节点越靠近根节点，编码值较短。而权重低的叶子节点会远离根节点，编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望==越常用的词拥有更短的编码，查找就更快==。如何编码呢？参见上面提的文档</li></ul></li><li><p>负采样：</p><ul><li>使用霍夫曼树可以提高模型训练的效率。但是如果我们的训练样本里的中心词是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。</li><li>Negative Sampling：word2vec用神经网络解法时，输出是计算V类的概率，其中1类是中心词，概率往大的方向走，剩下一类是V-1个其它词，概率往小的方向走。==真正计算复杂的就是负类别。负采样法就是从V-1个负样本中随机挑几个词做负样本==。每个词被选为负样本的概率和其词频正相关00</li><li>Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，利用这一个正例和neg个负例，我们进行二元逻辑回归，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。二元逻辑回归算法见文档。</li><li>负采样中每个词有两套向量，分别作为输入和预测时使用。</li></ul></li><li>两种解法进行一定优化，牺牲了一定的分类的准确度。比如负采样的负样本是随机选取的，所以相对已经没那么准了。<h4 id="1-2-2-基于Hierarchical-Softmax的CBOW模型算法流程："><a href="#1-2-2-基于Hierarchical-Softmax的CBOW模型算法流程：" class="headerlink" title="1.2.2 基于Hierarchical Softmax的CBOW模型算法流程："></a>1.2.2 基于Hierarchical Softmax的CBOW模型算法流程：</h4></li></ol><ul><li>输入：根据词向量的维度大小M，以及CBOW的上下文大小2c，步长$\eta$，得到训练样本。</li><li>建立霍夫曼树，整体语料的各个词频 决定 huffman树。</li><li>随机初始化所有的模型参数$\theta$，所有的词向量w。这些训练样本所用的huffman树是一棵</li><li>随机梯度上升法，对于训练集中的每一个样本$(context(w), w)$中的每一个词向量$x_i$(共2c个)进行迭代更新。</li><li>如果梯度收敛，则结束梯度迭代，否则回到上一步继续迭代<script type="math/tex; mode=display">h=\sum_{i=1}^{2c} embedding_{i}</script><script type="math/tex; mode=display">y=softmax(d)=softmax(Wh)=\frac{1}{\sum_{i=1}^{V}e^{d_{i}}}\begin{bmatrix}e^{d_{1}}\\ e^{d_{2}}\\ ...\\e^{d_{V}}\end{bmatrix}</script>W为全连接层参数，将词向量维度映射为V维（词表大小），表示预测词的概率。<h4 id="1-2-3-负采样方法"><a href="#1-2-3-负采样方法" class="headerlink" title="1.2.3 负采样方法"></a>1.2.3 负采样方法</h4>如果词汇表的大小为$V$,那么我们就将一段长度为1的线段分成$V$份，每份对应词汇表中的一个词。&lt;/font&gt;高频词对应的线段长，低频词对应的线段短(高频词数量多，分子count就大)。每个词$w$的线段长度&lt;/font&gt;由下式决定：<script type="math/tex">len(w) = \frac{count(w)}{\sum\limits_{u \in vocab} count(u)}</script></li></ul><p>　　　　在word2vec中，分子和分母都取了3/4次幂（经验参数，提高低频词被选取的概率）如下：<script type="math/tex">len(w) = \frac{count(w)^{3/4}}{\sum\limits_{u \in vocab} count(u)^{3/4}}</script></p><p>　　　　在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M &gt;&gt; V$，这样可以保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要==从$M$个位置中采样出$neg$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词==<br><img src="https://img-blog.csdnimg.cn/3753e2f61df74e54878f6726356c0b50.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><strong>在word2vec中，$M$取值默认为$10^8$。</strong><br><img src="https://img-blog.csdnimg.cn/68d66db9b92141d9addd2a4831098b61.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="1-3-总结："><a href="#1-3-总结：" class="headerlink" title="1.3 总结："></a>1.3 总结：</h3><ol><li>one-hot：词表大大时内存不够。且所有词相似度都是一样的没有区别</li><li>word embedding：考虑使用使用神经网络语言模型，通过训练，将每个词都映射到一个较短的词向量上来</li><li>神经网络语言模型的输入输出，有连续词袋模型CBOW(Continuous Bag-of-Words） 和Skip-Gram两种模型。<ul><li>CBOW模型的训练输入是某个中心词的上下文词向量，输出是词表所有词的softmax概率，训练的目标是期望中心词对应的softmax概率最大。</li><li>Skip-Gram模型和CBOW的思路是反着来的，即输入中心词词向量，而输出是中心词对应的上下文词向量。比如窗口大小为4，就是输出softmax概率排前8的8个词。</li></ul></li><li>word2vec有两种解法，霍夫曼树和负采样。负采样用得较多，因为构建霍夫曼树比较麻烦。</li><li>一般来说， Skip-Gram模型比CBOW模型更好，因为：<ul><li>Skip-Gram模型有更多的训练样本。Skip-Gram是一个词预测n个词，而CBOW是n个词预测一个词。</li><li>误差反向更新中，CBOW是中心词误差更新n个周边词，这n个周边词被更新的力度是一样的。而Skip-Gram中，每个周边词都可以根据误差更新中心词，所以Skip-Gram是更细粒度的学习方法。</li><li>Skip-Gram效果更好（默认Skip-Gram模型）但是缺点就是训练次数更多，时间更长。 </li></ul></li></ol><h2 id="二、fasttext"><a href="#二、fasttext" class="headerlink" title="二、fasttext"></a>二、fasttext</h2><h3 id="2-1、简介"><a href="#2-1、简介" class="headerlink" title="2.1、简介"></a>2.1、简介</h3><p>&#8195;&#8195;fasttext是facebook开源的一个词向量与文本分类工具，在2016年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。</p><p>&#8195;&#8195;fastText的核心思想：==将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类==。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。</p><p>这些不同概念被用于两个不同任务：<br>•    有效文本分类 ：有监督学习（短文本）<br>•    学习词向量表征：无监督学习</p><h3 id="2-2-FastText原理"><a href="#2-2-FastText原理" class="headerlink" title="2.2 FastText原理"></a>2.2 FastText原理</h3><p>fastText方法包含三部分，模型架构，层次SoftMax和N-gram特征。用词向量的叠加代表文档向量，全连接之后softmax分类。</p><h4 id="2-2-1-模型架构"><a href="#2-2-1-模型架构" class="headerlink" title="2.2.1 模型架构"></a>2.2.1 模型架构</h4><p>fastText的架构和word2vec中的CBOW的架构类似，因为它们的作者都是Facebook的科学家Tomas Mikolov，而且确实fastText（2016）也算是words2vec（2014）所衍生出来的。<br>Continuous Bog-Of-Words： </p><p><img src="https://img-blog.csdnimg.cn/4e880a15d1b14c82a7f1797676a87bf8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f9891d5be1134c50aded04345f5e8b72.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>==隐藏层就是叠加后的句子（文档）向量==<br>参考<a href="https://zhuanlan.zhihu.com/p/375614469">《理解文本分类利器fastText》</a></p><ul><li>序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 </li><li>fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。</li><li>所以fastText只有CBOW模型，对应fastText.train_supervised 没有model参数。 Word2Vec有两种模型，所以fastText.train_unsupervised可以选择model={cbow, skipgram} ，默认skipgram。</li></ul><h4 id="2-2-2-层次SoftMax"><a href="#2-2-2-层次SoftMax" class="headerlink" title="2.2.2 层次SoftMax"></a>2.2.2 层次SoftMax</h4><p>层次softmax的基本思想是根据类别的频率构造霍夫曼树来代替扁平化的标准softmax。通过层次softmax，获得概率分布的时间复杂度可以从O(N)降至O(logN)。(多分类转成一系列二分类）</p><p>下图为层次softmax的一个具体示例：<br><img src="https://img-blog.csdnimg.cn/267e028268f346a184e211105c3fd76d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/eb9f5beaf0f14ec4b076c663037ad927.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>（见速通一书162页）</p><h4 id="2-2-3-N-gram特征"><a href="#2-2-3-N-gram特征" class="headerlink" title="2.2.3 N-gram特征"></a>2.2.3 N-gram特征</h4><p>&#8195;&#8195; n-gram解决词袋模型没有词序的问题，Hash解决n-gram膨胀问题。最大问题是有Hash冲突，但是实际中问题不大。</p><p>&#8195;&#8195; fastText 本身是词袋模型，为了分类的准确性，所以加入了 N-gram 特征提取词序信息。&lt;/font&gt;“我 爱 她”如果加入 2-Ngram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然啦，为了提高效率，我们需要过滤掉低频的 N-gram。<br>&#8195;&#8195; n-gram的问题是词表会急剧扩大，变为$|V|^n$，没有机器扛得住。所以使用散列法（Hash）对n-gram特征进行压缩。&lt;/font&gt;<br>&#8195;&#8195; Hash：使用Hash函数将字符串映射到某个整数。这样不管n-gram词表有多大，最后整数范围都是函数输出范围（比如4000亿词表。hash函数是对10526取余，最后输出就10526个数值，数值再转成向量）</p><h4 id="2-2-4-subword"><a href="#2-2-4-subword" class="headerlink" title="2.2.4 subword"></a>2.2.4 subword</h4><ul><li>word2vec中每个词都是一个基本信息单元，不可再切分。忽略了词内部特征。fasttext采样子词模型表示词，可以从词的构造上学习词义，解决未登录词的问题。</li><li>fasttext中子词的n-gram长度在minn和maxn之间。如果模型输入是ID之类的特征，子词没有任何意义，应取消子词。即minn=maxn=0。</li><li>中文中子词是两个相邻的字，英文中是词根和词缀。</li></ul><h4 id="2-2-5-fasttext文本分类总结"><a href="#2-2-5-fasttext文本分类总结" class="headerlink" title="2.2.5 fasttext文本分类总结"></a>2.2.5 fasttext文本分类总结</h4><ul><li>一个句子进行分词，每个词进行embedding转换成一个词向量，默认100维。</li><li>每个词按位相加成一个新的100维向量。再过一个全连接矩阵，100行(词向量维度)22列（分类数）</li><li>经过softmax得到每一类的类别概率。</li></ul><h2 id="三、fastText和word2vec对比总结"><a href="#三、fastText和word2vec对比总结" class="headerlink" title="三、fastText和word2vec对比总结"></a>三、fastText和word2vec对比总结</h2><h3 id="3-1-fastText和word2vec的区别"><a href="#3-1-fastText和word2vec的区别" class="headerlink" title="3.1 fastText和word2vec的区别"></a>3.1 fastText和word2vec的区别</h3><p>相似处：<br>1.图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。<br>2.都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。</p><p>不同处：<br>==word2vec用词预测词，而且是词袋模型，没有n-gram。fasttext用文章/句子词向量预测类别，加入了n-gram信息==。所以有：</p><ol><li>模型的输入层：word2vec的输入层，是 context window 内的词；而fasttext 对应的整个sentence的内容，包括word、n-gram、subword。</li><li>模型的输出层：word2vec的输出层，计算某个词的softmax概率最大；而fasttext的输出层对应的是 分类的label；</li><li>两者本质的不同，体现在 h-softmax的使用：<ul><li>word2vec用的负采样或者霍夫曼树解法（计算所有词概率，类别过大）。</li><li>fasttext用的softmsx全连接分类（类别少）</li></ul></li><li>word2vec主要目的的得到词向量，该词向量 最终是在输入层得到（不关注预测的结果准不准，因为霍夫曼树和负采样解法虽然优化了训练速度，但是分类结果没那么准了）。fasttext主要是做分类 ，虽然也会生成一系列的向量，但最终都被抛弃，不会使用。</li><li>word2vec有两种模型cbow和 skipgram，fasttext只有cbow模型。</li><li>word2vec属于监督模型，但是不需要标注样本。fasttext也属于监督模型，但是需要标注样本。</li></ol><h3 id="3-2-小结"><a href="#3-2-小结" class="headerlink" title="3.2 小结"></a>3.2 小结</h3><h4 id="3-2-1-fasttext适用范围"><a href="#3-2-1-fasttext适用范围" class="headerlink" title="3.2.1 fasttext适用范围"></a>3.2.1 fasttext适用范围</h4><p>总的来说，fastText的学习速度比较快，效果还不错。</p><ol><li>==fastText适用与分类类别比较大而且数据集足够多的情况，当分类类别比较小或者数据集比较少的话，很容易过拟合==。</li><li>适用于短文本。因为第一步是多个向量相加，文本越长，高频词越多，最后相加结果越趋于相同。（比如关键词只有那么几个，如果长文本词向量相加，关键词就被淹没了）如果非要用于长文本分类，就先去停用词或者干脆提取关键词（这个软件没有分开计算词的权重）<h4 id="3-2-2-fasttext应用场景"><a href="#3-2-2-fasttext应用场景" class="headerlink" title="3.2.2 fasttext应用场景"></a>3.2.2 fasttext应用场景</h4></li><li>可以完成无监督的词向量的学习，可以学习出来词向量，来保持住词和词之间，相关词之间是一个距离比较近的情况；</li><li>也可以用于有监督学习的文本分类任务，（新闻文本分类，垃圾邮件分类、情感分析中文本情感分析，电商中用户评论的褒贬分析）</li><li>封装的特别好，用了很多加速模块包括多线程实现。非常简单。Keras可以做模型，定制化，很灵活，但是需要自己搭。Fasttext任务单一，用起来方便。<h4 id="3-2-3-fastText优点"><a href="#3-2-3-fastText优点" class="headerlink" title="3.2.3 fastText优点"></a>3.2.3 fastText优点</h4>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</li><li>fastText在保持高精度的情况下加快了训练速度和测试速度</li><li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li><li>fastText两个重要的优化：Hierarchical Softmax、N-gram<br>训练代码中，如果电脑一开始训练就卡了，可以设置线程thread=2。（卡住只能kill进程ps - aux│grep python,kill – 9 1531(进程数）</li></ol><p>fasttext已经嵌入word2vec，可以用它做有监督和无监督（就是word2vec）。涉及到离散特征都可以用fasttext。比如招聘网站预测求职者和职位的匹配度。（求职者和职位分别提取关键词特征，然后用fasttext训练，输出录用和不录用的概率。但是求职者简历写本科就是本科学位，职位要求的本科是指本科及以上。二者还是有些不一样。需要把求职者关键字/标签加P，职位标签加J予以区分。即当数据来源不同纬度时，语义可能不同，前面加一个field予以区分）</p><h2 id="四、用gensim学习word2vec"><a href="#四、用gensim学习word2vec" class="headerlink" title="四、用gensim学习word2vec"></a>四、用gensim学习word2vec</h2><p>参考文档<a href="https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80">《word2vec原理和gensim实现》</a></p><h3 id="4-1-使用技巧"><a href="#4-1-使用技巧" class="headerlink" title="4.1 使用技巧"></a>4.1 使用技巧</h3><ol><li><p>用哪种方法看需求：<br>      1.使用时需要将多个向量相加（文本向量化） 用cbow<br>      2.使用时都是单个词向量使用（找近义词） 用skip-gram<br>     大原则：使用的过程和训练的过程越一致 ，效果一般越好<br>如果实在不知道怎么选，一般来说skip-gram+ns负采样效果好一点点。&lt;/font&gt;</p></li><li><p>同一批词分别进行两次训练，embedding也不在同一语义空间，不同语义空间的向量没有可比性。word2vec不能进行增量更新，有新词只能全量训练，因为语料库变了one-hot也变了，V也变了。&lt;/font&gt;</p></li><li>孤岛效应：有一堆词，明明不相关，训练出来确是显示相似的。<ul><li>某部分词总是一起出现，另一堆词也是一起出现，但是这两堆词互相没有任何交集，虽然在一起训练是一个向量空间，但实际上是两个向量空间。这两堆词互相比较是没有意义的。</li><li>孤岛效应本质是由一些不相关语料或者弱相关语料组成。Word2vec本身不能解决这个问题，这个只能在样本选取上下功夫，让训练样本尽可能相关。==所以各领域自己训练自己的，不要把一堆不相关的东西放到一起训练。几个行业几套词向量==。<h3 id="4-2-推荐系统中的Word2vec"><a href="#4-2-推荐系统中的Word2vec" class="headerlink" title="4.2 推荐系统中的Word2vec"></a>4.2 推荐系统中的Word2vec</h3></li></ul></li></ol><ul><li>word2vec可以计算向量之间的相似度，所以可以在其它领域广泛使用。比如视频分类</li><li>nlp和推荐系统中最大区别是nlp的词向量比较固定，而推荐系统中用户不断推陈出新，用户向量变化很快。</li><li>可以使用Hash技术，将用户ID(如手机设备号）进行hash作为类别。</li><li>将视频ID作为词，用户的点击序列作为句子（一连串视频），用word2vec对点击序列进行训练。最后每个视频ID对应一个embedding，用来计算不同视频的相似度，或者作为视频向量输入后续模型。<h2 id="五、-基于fastText实现文本分类"><a href="#五、-基于fastText实现文本分类" class="headerlink" title="五、 基于fastText实现文本分类"></a>五、 基于fastText实现文本分类</h2></li></ul><p>直接pip安装报错：“Microsoft Visual C++ 14.0 or greater is required”。在<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#fasttext">此页面</a>下载fasttext文件，然后安装：pip install C:\Users\LS\Downloads\fasttext-0.9.2-cp38-cp38-win_amd64.whl</p><p>FastText可以快速的在CPU上进行训练，最好的实践方法就是<a href="https://github.com/facebookresearch/fastText/tree/master/python">github教程</a>，以及<a href="https://fasttext.cc/docs/en/cheatsheet.html">官网教程</a>。</p><h3 id="5-1-fasttext参数："><a href="#5-1-fasttext参数：" class="headerlink" title="5.1 fasttext参数："></a>5.1 fasttext参数：</h3><p>参考官方文档<a href="https://fasttext.cc/docs/en/python-module.html">《Python模块》</a>、<a href="https://zhuanlan.zhihu.com/p/52154254?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《FastText代码详解》</a></p><ul><li>FUNCTIONS<br>  load_model(path)：加载给定文件路径的模型并返回模型对象。<br>  read_args(arg_list, arg_dict, arg_names, default_values)<br>  tokenize(text)：给定一串文本，对其进行标记并返回一个标记列表<br>  ==train_supervised(<em>kargs, **kwargs)：监督训练，样本包含标签，即fasttext==。<br>   train_unsupervised(</em>kargs, **kwargs)：无监督训练，样本没有标签，即word2vec。</li></ul><ul><li>fasttext.train_unsupervised函数：调用此函数学习词向量，即word2vec模型。<ul><li>维度 ( dim ) ：向量维度的大小，defult=100 ，也可以选100-300 。</li><li>子词是包含在最小大小 ( minn ) 和最大大小 ( maxn )之间的单词中的所有子字符串。默认minn=3， maxn=6。</li><li>minn和maxn分别代表subwords的最小长度和最大长度</li><li>bucket表示可容纳的subwords和wordNgrams的数量，可以理解成是它们存放的表，与word存放的表是分开的。</li><li>t表示过滤高频词的阈值，像”the”，”a”这种高频但语义很少的词应该过滤掉。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span>             <span class="comment"># training file path (required)</span></span><br><span class="line">model             <span class="comment"># unsupervised fasttext model &#123;cbow, skipgram&#125; [skipgram]</span></span><br><span class="line">lr                <span class="comment"># 学习率 [0.05]</span></span><br><span class="line">dim               <span class="comment"># 词向量维度 [100]</span></span><br><span class="line">ws                <span class="comment"># 上下文窗口大小 [5]</span></span><br><span class="line">epoch             <span class="comment"># 训练轮数 [5]</span></span><br><span class="line">minCount          <span class="comment"># 最少单词词频，过滤过少的单词 [5]</span></span><br><span class="line">minn              <span class="comment"># min length of char ngram [3]</span></span><br><span class="line">maxn              <span class="comment"># max length of char ngram [6]</span></span><br><span class="line">neg               <span class="comment"># 负采样个数 [5]</span></span><br><span class="line">wordNgrams        <span class="comment"># 词ngram最大长度 [1]</span></span><br><span class="line">loss              <span class="comment"># loss function &#123;ns, hs, softmax, ova&#125;[ns]</span></span><br><span class="line">                  <span class="comment">#（负采样、霍夫曼树、softmax和多分类采用多个二分类计算，即loss one-vs-all） </span></span><br><span class="line">bucket            <span class="comment"># number of buckets，放的是subwords [2000000]</span></span><br><span class="line">thread            <span class="comment"># cpu线程 [number of cpus]</span></span><br><span class="line">lrUpdateRate      <span class="comment"># change the rate of updates for the learning rate，实现阶梯动态学习率 [100]</span></span><br><span class="line">t                 <span class="comment"># sampling threshold，过滤高频词，越大被保留的概率越大 [0.0001]</span></span><br><span class="line">verbose           <span class="comment"># verbose [2]</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><p>train_supervised 参数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span>             <span class="comment"># training file path (required)</span></span><br><span class="line">lr                <span class="comment"># 学习率 [0.05]</span></span><br><span class="line">dim               <span class="comment"># 词向量维度 [100]</span></span><br><span class="line">ws                <span class="comment"># 上下文窗口大小 [5]</span></span><br><span class="line">epoch             <span class="comment"># 训练轮数 [5]</span></span><br><span class="line">minCount          <span class="comment"># 最小词频 [1]</span></span><br><span class="line">minCountLabel     <span class="comment"># minimal number of label occurences [1]</span></span><br><span class="line">minn              <span class="comment"># min length of char ngram [0]</span></span><br><span class="line">maxn              <span class="comment"># max length of char ngram [0]</span></span><br><span class="line">neg               <span class="comment"># 负采样个数 [5]</span></span><br><span class="line">wordNgrams        <span class="comment"># n-gram [1]</span></span><br><span class="line">loss              <span class="comment"># loss function &#123;ns, hs, softmax, ova&#125; [softmax]</span></span><br><span class="line">bucket            <span class="comment"># number of buckets [2000000]</span></span><br><span class="line">thread            <span class="comment"># cpu线程数 [number of cpus]</span></span><br><span class="line">lrUpdateRate      <span class="comment"># change the rate of updates for the learning rate [100]</span></span><br><span class="line">t                 <span class="comment"># sampling threshold [0.0001]</span></span><br><span class="line">label             <span class="comment"># 标签前缀 [&#x27;__label__&#x27;]</span></span><br><span class="line">verbose           <span class="comment"># verbose [2]</span></span><br><span class="line">pretrainedVectors <span class="comment"># 从 (.vec file)加载预训练的词向量，用于监督训练 []</span></span><br></pre></td></tr></table></figure><p>model属性</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_dimension           <span class="comment"># 获取向量（隐藏层）的维度（大小）.这等价于 `dim` 属性           </span></span><br><span class="line">get_input_vector        <span class="comment"># 给定一个索引，得到输入矩阵对应的向量 </span></span><br><span class="line">get_input_matrix        <span class="comment"># 获取模型的完整输入矩阵的副本</span></span><br><span class="line">get_labels              <span class="comment"># 获取字典的整个标签列表，这相当于 `labels` 属性。</span></span><br><span class="line">get_line                <span class="comment"># 将一行文本拆分为单词和标签</span></span><br><span class="line">get_output_matrix       <span class="comment"># 获取模型的完整输出矩阵的副本。</span></span><br><span class="line">get_sentence_vector     <span class="comment"># 给定一个字符串，获得向量表示。这个函数</span></span><br><span class="line">                        <span class="comment"># assumes to be given a single line of text. We split words on</span></span><br><span class="line">                        <span class="comment"># whitespace (space, newline, tab, vertical tab) and the control</span></span><br><span class="line">                        <span class="comment"># characters carriage return, formfeed and the null character.</span></span><br><span class="line">get_subword_id          <span class="comment"># 给定一个subword，获取字典中的词 id hashes to.</span></span><br><span class="line">get_subwords            <span class="comment"># 给定一个词，获取子词及其索引。</span></span><br><span class="line">get_word_id             <span class="comment"># 给定一个词，获取字典中的词 id</span></span><br><span class="line">get_word_vector         <span class="comment"># 获取训练好的词向量。</span></span><br><span class="line">get_words               <span class="comment"># 获取字典的整个单词列表，这相当于 `words` 属性。</span></span><br><span class="line">is_quantized            <span class="comment"># 模型是否已经量化过</span></span><br><span class="line">predict                 <span class="comment"># 给定一个字符串，得到一个标签列表和一个对应概率列表</span></span><br><span class="line">quantize                <span class="comment"># 量化模型，减少模型的大小和内存占用</span></span><br><span class="line">save_model              <span class="comment"># 保存模型</span></span><br><span class="line">test                    <span class="comment"># Evaluate supervised model using file given by path</span></span><br><span class="line">test_label              <span class="comment"># 返回每个标签的准确率和召回率。</span></span><br></pre></td></tr></table></figure><h3 id="5-2-基本使用"><a href="#5-2-基本使用" class="headerlink" title="5.2 基本使用"></a>5.2 基本使用</h3><p>当 fastText 运行时，进度和预计完成时间会显示在您的屏幕上。训练完成后，model变量包含有关训练模型的信息，可用于查询：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_unsupervised(<span class="string">&#x27;data/fil9&#x27;</span>)<span class="comment">#维基百科文件</span></span><br><span class="line">model.words</span><br><span class="line"></span><br><span class="line">[<span class="string">u&#x27;the&#x27;</span>, <span class="string">u&#x27;of&#x27;</span>, <span class="string">u&#x27;one&#x27;</span>, <span class="string">u&#x27;zero&#x27;</span>, <span class="string">u&#x27;and&#x27;</span>, <span class="string">u&#x27;in&#x27;</span>, <span class="string">u&#x27;two&#x27;</span>, <span class="string">u&#x27;a&#x27;</span>, <span class="string">u&#x27;nine&#x27;</span>, <span class="string">u&#x27;to&#x27;</span>, <span class="string">u&#x27;is&#x27;</span>, ...</span><br></pre></td></tr></table></figure><p>==获得词向量==：（它返回词汇表中的所有单词，按频率递减排序。）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.get_word_vector(<span class="string">&quot;the&quot;</span>)</span><br><span class="line">array([-<span class="number">0.03087516</span>,  <span class="number">0.09221972</span>,  <span class="number">0.17660329</span>,  <span class="number">0.17308897</span>,  <span class="number">0.12863874</span>,</span><br><span class="line">        <span class="number">0.13912526</span>, -<span class="number">0.09851588</span>,  <span class="number">0.00739991</span>,  <span class="number">0.37038437</span>, -<span class="number">0.00845221</span>,</span><br><span class="line">        ...</span><br><span class="line">       -<span class="number">0.21184735</span>, -<span class="number">0.05048715</span>, -<span class="number">0.34571868</span>,  <span class="number">0.23765688</span>,  <span class="number">0.23726143</span>],</span><br><span class="line">      dtype=float32)</span><br></pre></td></tr></table></figure><p>==保存模型（二进制），后续加载==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_model(<span class="string">&quot;result/fil9.bin&quot;</span>)</span><br><span class="line">model = fasttext.load_model(<span class="string">&quot;result/fil9.bin&quot;</span>)</span><br></pre></td></tr></table></figure><br>cobw和skipgram：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_unsupervised(<span class="string">&#x27;data/fil9&#x27;</span>, <span class="string">&quot;cbow&quot;</span>)</span><br></pre></td></tr></table></figure><br>==预测结果==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取测试集，预测模型输出</span></span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">10000</span>)</span><br><span class="line">results=[model.predict(x)  <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">results</span><br><span class="line"></span><br><span class="line">[((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.99827653</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__11&#x27;</span>,), array([<span class="number">0.84706676</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__3&#x27;</span>,), array([<span class="number">0.99988556</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.99980879</span>])),</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.9998678</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__1&#x27;</span>,), array([<span class="number">0.87650901</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__3&#x27;</span>,), array([<span class="number">1.00001013</span>])),</span><br><span class="line"> ...]</span><br></pre></td></tr></table></figure><br>所以输出结果是带前缀的标签和分类概率。想只得到类别，可以这样写：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result=[model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">result</span><br><span class="line">[<span class="string">&#x27;2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;11&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;9&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;12&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">...]</span><br></pre></td></tr></table></figure><h3 id="5-3-bin格式词向量转换为vec格式"><a href="#5-3-bin格式词向量转换为vec格式" class="headerlink" title="5.3 bin格式词向量转换为vec格式"></a>5.3 bin格式词向量转换为vec格式</h3><p>参考<a href="https://blog.csdn.net/huyidu/article/details/112712526?utm_source=app&amp;app_version=4.16.0&amp;code=app_1562916241&amp;uLinkId=usr1mkqgl919blen">《fasttext训练的bin格式词向量转换为vec格式词向量》</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载的fasttext预训练词向量都是vec格式的，但fasttext无监督训练后却是bin格式，因此需要进行转换</span></span><br><span class="line"><span class="comment"># 以下代码为fasttext官方推荐：</span></span><br><span class="line"><span class="comment"># 请将以下代码保存在bin_to_vec.py文件中</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, absolute_import, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fasttext <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 整个代码逻辑非常简单</span></span><br><span class="line">    <span class="comment"># 以bin格式的模型为输入参数</span></span><br><span class="line">    <span class="comment"># 按照vec格式进行文本写入</span></span><br><span class="line">    <span class="comment"># 可通过head -5 xxx.vec进行文件查看</span></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=(<span class="string">&quot;Print fasttext .vec file to stdout from .bin file&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;model&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Model to use&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    f = load_model(args.model)</span><br><span class="line">    words = f.get_words()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(<span class="built_in">len</span>(words)) + <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(f.get_dimension()))</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        v = f.get_word_vector(w)</span><br><span class="line">        vstr = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> vi <span class="keyword">in</span> v:</span><br><span class="line">            vstr += <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(vi)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="built_in">print</span>(w + vstr)</span><br><span class="line">        <span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">if</span> e.errno == errno.EPIPE:</span><br><span class="line">                <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开cmd，在bin_to_vec.py路径下执行该命令，生成unsupervised_data.vec</span></span><br><span class="line">python bin_to_vec.py word15000.<span class="built_in">bin</span> &gt; word15000.vec</span><br></pre></td></tr></table></figure><p>==在实践中，我们观察到 skipgram 模型在处理子词信息方面比 cbow 更好==</p><h2 id="六、新闻文本分类——fasttext"><a href="#六、新闻文本分类——fasttext" class="headerlink" title="六、新闻文本分类——fasttext"></a>六、新闻文本分类——fasttext</h2><h3 id="6-1-正常fasttext分类"><a href="#6-1-正常fasttext分类" class="headerlink" title="6.1 正常fasttext分类"></a>6.1 正常fasttext分类</h3><p>单纯的fasttext分类，参数用讨论区默认参数，没有调整。分数0.9151。<br>fasttext训练很快，大概十来分钟吧。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>]=<span class="string">&#x27;__label__&#x27;</span>+train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;./train.csv&#x27;</span>,index=<span class="literal">None</span>,header=<span class="literal">None</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model=fasttext.train_supervised(<span class="string">&#x27;./train.csv&#x27;</span>,lr=<span class="number">1.0</span>,wordNgrams=<span class="number">2</span>, </span><br><span class="line">verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">25</span>,loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">result=[model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">result[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:result&#125;).to_csv(<span class="string">&#x27;fasttext.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><br>最终上传，得分0.9151。<br>调整部分参数后，最终得分0.9358。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=fasttext.train_supervised(<span class="string">&#x27;./train.csv&#x27;</span>,lr=<span class="number">0.8</span>,wordNgrams=<span class="number">3</span>, </span><br><span class="line">verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">25</span>,loss=<span class="string">&quot;softmax&quot;</span>)</span><br></pre></td></tr></table></figure></p><h3 id="6-2-小数据集：word2vec-fasttext-首尾截断"><a href="#6-2-小数据集：word2vec-fasttext-首尾截断" class="headerlink" title="6.2 小数据集：word2vec+fasttext+首尾截断"></a>6.2 小数据集：word2vec+fasttext+首尾截断</h3><p>首先拿15000条数据进行试验，前10000条fasttext训练，后5000条测试，代码见讨论区：<a href="https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.64063dadaDY31g&amp;postId=118255">《Task4 基于深度学习的文本分类1-fastText》</a>（其实就是上面代码改了点数据集）：</p><ol><li>试验正常fasttext效果，f1 score=0.8272<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;../data/train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].iloc[:-<span class="number">5000</span>].to_csv(<span class="string">&#x27;train.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_supervised(<span class="string">&#x27;train.csv&#x27;</span>, lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">25</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">val_pred = [model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure></li><li>试验word2vec+fasttext效果，f1 score=0.8426<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先进行word2vec训练，含全部15000条数据</span></span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train15000.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">model1 = fasttext.train_unsupervised(<span class="string">&#x27;train15000.csv&#x27;</span>, lr=<span class="number">0.1</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">8</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#保存模型转为词向量</span></span><br><span class="line">model1.save_model(<span class="string">&quot;word15000.bin&quot;</span>)</span><br><span class="line"><span class="comment">#cmd命令行执行python bin_to_vec.py result1000.bin &lt; result1000.vec，转换为vec词向量</span></span><br></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#fasttext进行训练，词向量为前一步训练好的词向量，训练数据为10000条</span></span><br><span class="line">model2 = fasttext.train_supervised(<span class="string">&#x27;train.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word15000.vec&#x27;</span>,lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line"><span class="comment">#                                  verbose=2, minCount=1, epoch=16, loss=&quot;hs&quot;)</span></span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">val_pred = [model2.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure><ol><li>试验首尾截断效果，f1 score=0.8222(首尾各50词），0.8304（首尾各100词）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断实验效果</span></span><br><span class="line"><span class="comment">#准备将text文本首尾截断，各取100tokens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slipt2</span>(<span class="params">x</span>):</span></span><br><span class="line">  ls=x.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">  le=<span class="built_in">len</span>(ls)</span><br><span class="line">  <span class="keyword">if</span> le&lt;<span class="number">201</span>:</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(ls[:<span class="number">100</span>]+ls[-<span class="number">100</span>:])</span><br><span class="line">    </span><br><span class="line">trains_df[<span class="string">&#x27;summary&#x27;</span>]=trains_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">train_df[[<span class="string">&#x27;summary&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].iloc[:-<span class="number">5000</span>].to_csv(<span class="string">&#x27;trains_summary10000.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model3 = fasttext.train_supervised(<span class="string">&#x27;trains_summary10000.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word15000.vec&#x27;</span>,lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">16</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">val_pred = [model3.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure></li></ol><h3 id="6-3-全数据集：word2vec-fasttext-首尾截断"><a href="#6-3-全数据集：word2vec-fasttext-首尾截断" class="headerlink" title="6.3 全数据集：word2vec+fasttext+首尾截断"></a>6.3 全数据集：word2vec+fasttext+首尾截断</h3><ol><li>数据处理<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取训练测试集数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train_20w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_df = pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">df=pd.concat([train_df,test_df])</span><br><span class="line">df[[<span class="string">&#x27;text&#x27;</span>]].to_csv(<span class="string">&#x27;train_25w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li>用word2vec进行train+test数据的词向量训练，这一步花了2个小时。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line"></span><br><span class="line">model1 = fasttext.train_unsupervised(<span class="string">&#x27;train_25w.csv&#x27;</span>, lr=<span class="number">0.1</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">8</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">model1.save_model(<span class="string">&quot;word_25w.bin&quot;</span>)</span><br><span class="line"><span class="comment">#cmd下运行python bin_to_vec.py word_25w.bin &gt; word_25w.vec</span></span><br></pre></td></tr></table></figure></li><li>fasttext进行有监督训练，相当于分类微调。最终上传，得分0.9162，吐血。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2=fasttext.train_supervised(<span class="string">&#x27;train_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,lr=<span class="number">0.8</span>, wordNgrams=<span class="number">2</span>, verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">18</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_df = pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test_pred = [model2.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_pred&#125;).to_csv(<span class="string">&#x27;word_fast.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></li><li>接下来进行首尾截断测试：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断进行训练</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[<span class="string">&#x27;summary&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">train_df[[<span class="string">&#x27;summary&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model3 = fasttext.train_supervised(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,lr=<span class="number">0.8</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">18</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">test_df[<span class="string">&#x27;summary&#x27;</span>]=test_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">test_pred = [model3.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;summary&#x27;</span>]]</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_pred&#125;).to_csv(<span class="string">&#x27;word_fast_cut.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>最终得分0.9203，至少证明了长文本分类，数据集够多的时候，进行部分截断比较好。</p><div class="table-container"><table><thead><tr><th>数据量</th><th>fasttext</th><th>word2vec+fasttext</th><th>word2vec+fasttext+首尾截断</th></tr></thead><tbody><tr><td>10000+5000</td><td>0.8272</td><td>0.8426</td><td>0.8304</td></tr><tr><td>20w+5w</td><td>0.9151（没调参）</td><td>0.9162（没调参）</td><td>0.9203（没调参）</td></tr><tr><td>20w+5w</td><td>0.9358（已调参）</td><td></td><td>0.9421（已调参）</td></tr></tbody></table></div><p>截断比不截断高0.4-0.6个点。</p><ol><li>下面是部分调参记录<br>继续首尾截断试验，训练集前19w为悬链数据，最后1w为测试数据。</li></ol><div class="table-container"><table><thead><tr><th>首尾截断</th><th>f1</th><th>loss</th><th>n-gram</th></tr></thead><tbody><tr><td>各30，同时epoch=18，lr=0.8，下同</td><td>0.9190</td><td>hs</td><td>2</td></tr><tr><td>各30</td><td>0.9352</td><td>softmax</td><td>2</td></tr><tr><td>各30</td><td>0.9388</td><td>softmax</td><td>3</td></tr><tr><td>各30</td><td>0.9382</td><td>softmax</td><td>4</td></tr><tr><td>各30</td><td></td><td>softmax</td><td>5</td></tr><tr><td>各30，同时epoch=18，lr=0.5</td><td></td><td>softmax</td><td>4</td></tr><tr><td>各30，同时epoch=27，lr=0.5</td><td></td><td>softmax</td><td>4</td></tr><tr><td>——-</td><td>——-</td><td>——-</td><td>——-</td></tr><tr><td>各50</td><td>0.9192</td><td>hs</td><td>2</td></tr><tr><td>各80</td><td>0.9170</td><td>hs</td><td>2</td></tr><tr><td>各100</td><td>0.9200/0.9184</td><td>hs</td><td>2</td></tr><tr><td>各150</td><td>0.9226</td><td>hs</td><td>2</td></tr><tr><td>各150</td><td>0.9371</td><td>softmax</td><td>2</td></tr><tr><td>各150</td><td>0.9436</td><td>softmax</td><td>3</td></tr><tr><td>各150</td><td>0.9417</td><td>softmax</td><td>4</td></tr><tr><td>各200</td><td>0.9212</td><td>hs</td><td>2</td></tr><tr><td>不截断</td><td>0.9158</td><td>hs</td><td>2</td></tr></tbody></table></div><p>不截断，加和平均的词向量太多，无用信息冲淡了关键信息。<br>fasttext分类的loss必须选择softmex，不需要hs和ng，因为类别少。<br>n-gram中，n增大可以表示一部分词序，有利于文本表征。但是太大的话，词向量和n-gram向量太多，分类效果也不好（参数过多学不好或者是无用信息过多）。</p><p>初步选择以下参数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断各150个词</span></span><br><span class="line">model3=fasttext.train_supervised(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,</span><br><span class="line">lr=<span class="number">0.8</span>,wordNgrams=<span class="number">3</span>,verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">18</span>,loss=<span class="string">&quot;softmax&quot;</span>)</span><br></pre></td></tr></table></figure><p>最终分数f1=0.9421。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、word2vec&quot;&gt;&lt;a href=&quot;#一、word2vec&quot; class=&quot;headerlink&quot; title=&quot;一、word2vec&quot;&gt;&lt;/a&gt;一、word2vec&lt;/h2&gt;&lt;p&gt;参考文档&lt;a href=&quot;https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80&quot;&gt;《word2vec原理和gensim实现》&lt;/a&gt;、&lt;a href=&quot;https://zhuanlan.zhihu.com/p/114538417&quot;&gt;《深入浅出Word2Vec原理解析》&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;1-1-word2vec为什么-不用现成的DNN模型&quot;&gt;&lt;a href=&quot;#1-1-word2vec为什么-不用现成的DNN模型&quot; class=&quot;headerlink&quot; title=&quot;1.1 word2vec为什么 不用现成的DNN模型&quot;&gt;&lt;/a&gt;1.1 word2vec为什么 不用现成的DNN模型&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，==从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值==。解决办法有两个：霍夫曼树和负采样。&lt;/li&gt;
&lt;li&gt;对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。输入从多个词向量变成了一个词向量&lt;/li&gt;
&lt;li&gt;在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="DNN" scheme="https://zhxnlp.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记4：深度学习DNN2</title>
    <link href="https://zhxnlp.github.io/2021/11/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN2/"/>
    <id>https://zhxnlp.github.io/2021/11/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN2/</id>
    <published>2021-11-25T13:31:06.000Z</published>
    <updated>2022-01-02T20:46:56.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、神经网络参数优化器"><a href="#一、神经网络参数优化器" class="headerlink" title="一、神经网络参数优化器"></a>一、神经网络参数优化器</h2><p>参考曹健<a href="https://blog.csdn.net/weixin_45558569/article/details/110728137?spm=1001.2014.3001.5501">《人工智能实践：Tensorflow2.0 》</a><br>深度学习优化算法经历了SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam<br>这样的发展历程。<br><img src="https://img-blog.csdnimg.cn/65f1bb4bf04f4aa8b260f2126b37099a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><span id="more"></span></p><ul><li>上图中f一般指loss</li><li>一阶动量：与梯度相关的函数</li><li>二阶动量：与梯度平方相关的函数</li><li>不同的优化器，实质上只是定义了不同的一阶动量和二阶动量公式</li></ul><h3 id="1-2-SGD（无动量）随机梯度下降。"><a href="#1-2-SGD（无动量）随机梯度下降。" class="headerlink" title="1.2 SGD（无动量）随机梯度下降。"></a>1.2 SGD（无动量）随机梯度下降。</h3><ul><li>最常用的梯度下降方法是随机梯度下降，即随机采集样本来计算梯度。根据统计学知识有：采样数据的平均值为全量数据平均值的无偏估计。</li><li>实际计算出来的SGD在全量梯度附近以一定概率出现，batch_size越大，概率分布的方差越小，SGD梯度就越确定，相当于在全量梯度上注入了噪声。适当的噪声是有益的。</li><li>batch_size越小，计算越快，w更新越频繁，学习越快。但是太小的话，SGD梯度和真实梯度差异过大，而且震荡厉害，不利于学习</li><li>SGD梯度有一定随机性，所以可以逃离鞍点、平坦极小值或尖锐极小值区域</li><li>最初版本是vanilla SGD，没有动量。$m<em>{t}=g</em>{t}，V_{t}=1$（p32—sgd.py）</li></ul><p><img src="https://img-blog.csdnimg.cn/2003ca0870694c979e9728c609832aa6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>每个bacth_size样本的梯度都不一样，甚至前后差异很大，造成梯度震荡。（比如一个很大的正值接一个很大的负值）</li><li><p>神经网络中，输入归一化。但是多层非线性变化后，中间层输入各维度数值差别可能很大。不同方向的敏感度不一样。有的方向更陡，容易震荡。</p></li><li><p>vanilla SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优<br>点</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sgd</span></span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>])</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="1-3-SGDM——引入动量减少震荡"><a href="#1-3-SGDM——引入动量减少震荡" class="headerlink" title="1.3 SGDM——引入动量减少震荡"></a>1.3 SGDM——引入动量减少震荡</h3><ul><li>动量法是一种使梯度向量向相关方向加速变化，抑制震荡，最终实现加速收敛的方法。<ul><li>为了抑制SGD的震荡，SGDM认为 梯度下降过程可以加入惯性。如果前后梯度方向相反，动量对冲，减少震荡。如果前后方向相同，步伐加大，加快学习速度。</li></ul></li></ul><ul><li>SGDM全称是SGD with Momentum，在SGD基础上引入了一阶动量：<br><img src="https://img-blog.csdnimg.cn/441deed8edc541bea8cbe99d30e23bf2.png" alt="在这里插入图片描述"></li><li>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近$1/(1-\beta <em>{1})$个时刻的梯度向量和的平均值。(指数移动平均值大约是过去一段时间的平均值，反映“局部的”参数信息$)。\beta </em>{1}$的经验值为0.9。所以t 时刻的下降方向主要偏向此前累积的下降方向，并略微偏向当前时刻的下降方向。<br><img src="https://img-blog.csdnimg.cn/7d18ec7ad9fa4604b213b4b6d862d374.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>老师书上写<script type="math/tex">W_{t+1}=W_{t}-v_{t}=W_{t}-(\alpha v_{t-1}+\varepsilon g_{t})</script><br>$\alpha、\varepsilon$是超参数。</li><li>SGDM问题1：时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算。</li><li>SGD/SGDM 问题2:会被困在一个局部最优点里</li><li>SGDM 问题3：如果梯度连续多次迭代都是一个方向，剃度一直增大，最后造成梯度爆炸</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sgd-momentun</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(learning_rate * m_w)</span><br><span class="line">b1.assign_sub(learning_rate * m_b)</span><br></pre></td></tr></table></figure><h3 id="1-4-SGD-with-Nesterov-Acceleration"><a href="#1-4-SGD-with-Nesterov-Acceleration" class="headerlink" title="1.4 SGD with Nesterov Acceleration"></a>1.4 SGD with Nesterov Acceleration</h3><ul><li>SGDM：主要看当前梯度方向。计算当前loss对w梯度，再根据历史梯度计算一阶动量$m_{t}$</li><li>NAG：主要看动量累积之后的梯度方向。 计算参数在累积动量之后的更新值，并计算此时梯度&lt;/font&gt;，再将这个梯度带入计算一阶动量$m_{t}$</li><li>主要差别在于是否先对w减去累积动量&lt;/font&gt;<br><img src="https://img-blog.csdnimg.cn/c3d6a6cb8c7243a0b6512790c58135c4.png" alt="在这里插入图片描述"><br>思路如下图：</li></ul><p><img src="https://img-blog.csdnimg.cn/9b5c723108434a3181da8d87ebfe9d09.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>首先，按照原来的更新方向更新一步（棕色线），然后计算该新位置的梯度方向（红色线），然后<br>用这个梯度方向修正最终的更新方向（绿色线）。上图中描述了两步的更新示意图，其中蓝色线是标准momentum更新路径。</p><h3 id="1-5-AdaGrad——累积全部梯度，自适应学习率"><a href="#1-5-AdaGrad——累积全部梯度，自适应学习率" class="headerlink" title="1.5 AdaGrad——累积全部梯度，自适应学习率"></a>1.5 AdaGrad——累积全部梯度，自适应学习率</h3><ul><li>SGD:对所有的参数使用统一的、固定的学习率</li><li>AdaGrad:自适应学习率。对于频繁更新的参数，不希望被单个样本影响太大，我们给它们很小的学习率；对于偶尔出现的参数，希望能多得到一些信息，我们给它较大的学习率。</li><li>另一个解释是：初始时刻W离最优点远，学习率需要设置的大一些。随着学习你的进行，离最优点越近，学习率需要不断减小。</li><li>引入二阶动量——该维度上，所有梯度值的平方和(梯度按位相乘后求和），来度量参数更新频率，用以对学习率进行缩放。（频繁更新的参数、越到学习后期参数也被更新的越多，二阶动量都越大，学习率越小）&lt;/font&gt;<script type="math/tex; mode=display">V_{t}=\sum_{\tau =1}^{t}g_{\tau }^{2}</script><script type="math/tex; mode=display">\eta _{t}=lr\cdot m_{t}/\sqrt{V_{t}}=lr\cdot g_{t}/\sqrt{\sum_{\tau =1}^{t}g_{\tau }^{2}}</script><script type="math/tex; mode=display">W_{t+1}=W_{t}-\eta _{t}</script><img src="https://img-blog.csdnimg.cn/97b9398d2e804171be3368ff83b30b9c.png" alt="在这里插入图片描述"></li><li>优点：AdaGrad 在稀疏数据场景下表现最好。因为对于频繁出现的参数，学习率衰减得快；对于稀疏的参数，学习率衰减得更慢。</li><li>缺点：在实际很多情况下，频繁更新的参数，学习率会很快减至 0 &lt;/font&gt;，导致参数不再更新，训练过程提前结束。（二阶动量呈单调递增，累计从训练开始的梯度)</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adagrad</span></span><br><span class="line">v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure><h3 id="1-6-RMSProp——累积最近时刻梯度"><a href="#1-6-RMSProp——累积最近时刻梯度" class="headerlink" title="1.6 RMSProp——累积最近时刻梯度"></a>1.6 RMSProp——累积最近时刻梯度</h3><ul><li>RMSProp算法的全称叫 Root Mean Square Prop。AdaGrad 的学习率衰减太过激进，考虑改变二阶动量的计算策略：不累计全部梯度，只关注过去某一窗口内的梯度。</li><li>指数移动平均值大约是过去一段时间的平均值，反映“局部的”参数信息，因此我们用这个方法来计算二阶累积动量：（分母会再加一个平滑项，防止为0）<br><img src="https://img-blog.csdnimg.cn/63397961c363425ea85e2f4b700d85e3.png" alt="在这里插入图片描述"><br>对照SGDM的一阶动量公式：<br><img src="https://img-blog.csdnimg.cn/247f69bbc3b8420f8c66818346b35a58.png" alt="在这里插入图片描述"></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RMSProp</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure><h3 id="1-7-Adam"><a href="#1-7-Adam" class="headerlink" title="1.7 Adam"></a>1.7 Adam</h3><p>将SGDM一阶动量和RMSProp二阶动量结合起来，再修正偏差，就是Adam。<br><img src="https://img-blog.csdnimg.cn/ef129fd5ab3f47b4ac9a9667d17969fe.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>一阶动量和二阶动量都是按照指数移动平均值进行计算的。初始化 $m<em>{0}=0,V</em>{0}=0$，在初期，迭<br>代得到的$m<em>{t}、V</em>{t}$会接近于0。我们可以通过偏差修正来解决这一问题：</p><script type="math/tex; mode=display">\widehat{m_{t}}=\frac{m_{t}}{1-\beta _{1}^{t}}</script><script type="math/tex; mode=display">\widehat{V_{t}}=\frac{V_{t}}{1-\beta _{2}^{t}}</script><script type="math/tex; mode=display">\eta _{t}=lr\cdot \widehat{m_{t}}/(\sqrt{\widehat{V_{t}}}+\varepsilon )</script><script type="math/tex; mode=display">W_{t+1}=W_{t}-\eta _{t}</script><ul><li>${1-\beta <em>{1}^{t}}、1-\beta </em>{2}^{t}$的取值范围为（0,1）,可以将开始阶段$m<em>{t}、V</em>{t}$放大至$\widehat{m<em>{t}}、\widehat{V</em>{t}}$。</li><li>随着迭代次数t的增加，$\beta <em>{1}^{t}、\beta </em>{2}^{t}$趋近于0，放大倍数趋近于1，即不再放大$m<em>{t}、V</em>{t}$。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adam</span></span><br><span class="line">m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">w1.assign_sub(learning_rate * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">b1.assign_sub(learning_rate * m_b_correction / tf.sqrt(v_b_correction))</span><br></pre></td></tr></table></figure><h3 id="1-8-悬崖、鞍点问题"><a href="#1-8-悬崖、鞍点问题" class="headerlink" title="1.8 悬崖、鞍点问题"></a>1.8 悬崖、鞍点问题</h3></li><li>高维空间中，一个点各个方向导数为0，只要有一个方向对应的极值和其它方向不一样，该点就是鞍点。鞍点是一个不稳定的点，梯度轻微扰动就可以逃离。</li><li>SGD梯度因为具有一定的随机性，反而可以逃离鞍点</li><li>例如n维空间中，某点各方向导数都为0。假设其中极大或者极小的概率为0.5，则该点为极大或极小值概率均为$0.5^{n}$，反之为鞍点的概率为$1-2*0.5^{n}=1-0.5^{n-1}$。高维空间中鞍点概率几乎为1。因此不必非要找到极小值，只需要loss降到比较低就行了。</li><li>梯度裁剪：悬崖部位loss会突然下降，梯度太大W容易走过头，所以可以梯度裁剪，限制梯度最大值。<h2 id="二、过拟合解决方案"><a href="#二、过拟合解决方案" class="headerlink" title="二、过拟合解决方案"></a>二、过拟合解决方案</h2><h3 id="2-1-正则化"><a href="#2-1-正则化" class="headerlink" title="2.1 正则化"></a>2.1 正则化</h3></li><li>逻辑回归中，损失函数为$Loss+\lambda \left | W \right |or Loss+\lambda \left | W \right |$。在神经网络中则更加灵活。可以为不同层分别设置L1或者L2正则，且系数$\lambda$也可以不同。即各层正则化项可以完全独立。</li><li>L1正则可以对神经网络剪枝&lt;/font &gt;（很多权重趋近于0），网络运行速度可以大大加快。所以对实时要求高的场景可以用L1正则。</li><li>神经网络中可以设置前层的正则化项系数$\lambda$更大，后层小一些。因为前层面对真实物理信号，噪声较大。为了过滤噪声，W应当较小，$\lambda$更大。且后层面对输出，正则化太厉害，影响分类效果。<h3 id="2-2-dropout"><a href="#2-2-dropout" class="headerlink" title="2.2 dropout"></a>2.2 dropout</h3></li></ul><ol><li>Dropout：每次训练时直接随机去除部分神经元。可以达到剪枝的目的，生成新的子网络。所以本质也是一种正则化，类比于L1正则。各层可以独立设置Dropout</li><li>dropout相当于训练大量子网络，预测时使用全部神经元，等同于集成学习，增加泛化能力。</li><li>降低模型复杂度和各神经元之间的依赖程度，降低模型对某一特征的过度依赖，从而降低过拟合</li></ol><ul><li>子网络可以是海量的，大量子网络可能没有训练或者就训练一次，但是由于大部分神经元是一样的，少部分才被去除，所以大部分自网络高度相关。而且训练一个子网络，等于主网络大部分参数也得到了更新</li><li>dropout是随机去除神经元，所以发生在$d^{k}=W^{k}\cdot a^{k-1}$加权求和的时候，而不是$a^{k-1}=f(d^{l-1}+w_{0}^{k-1})$激活之后。</li><li>训练时dropout概率为p，即只有（1-p)个神经元进行计算。则预测时结果要乘以（1-p）</li><li>跟正则化系数一样，dropout应该是前层设置的高。前层噪声多，需要过滤。后层主要用来精确拟合，dropout过高，影响预测结果。</li><li>dropout可以取代正则，例如训练100个神经元，dropout=0.2；效果好于训练80个神经元。<h3 id="2-3-Batch-Normalization"><a href="#2-3-Batch-Normalization" class="headerlink" title="2.3 Batch Normalization"></a>2.3 Batch Normalization</h3>神经网络学习隐含条件：输入数据有一定的规律，符合一定的概率分布，模型就是学习输入分布和类别之间的映射关系。但是神经网络中，各层输入分布并不稳定：</li></ul><ol><li>SGD训练时，随机选取样本，造成分布的差异，即$\mu /\sigma$都不一样</li><li>前层权重W的变化造成后层输入的变化，进而导致后层最优的参数W变化。即使后层参数已接近最优，也要重新学习，造成网络不稳定。后层分布累积了前层所有的W，所以不同轮次输入分布差异很大，学习速度会很慢</li></ol><p>即训练样本数据本身是符合一定的概率分布的，但是因为以上两个原因，造成每个batch训练时概率分布差异很大。</p><p>对于一个batch的数据，第l层第m个神经元来说：</p><ul><li>计算神经元在一个batch样本的输入$d_{i,m}^{l}$的均值和方差，将数据分布转为标准正态分布<script type="math/tex; mode=display">\mu _{m}^{l}=\frac{1}{batch-size}\sum_{i\epsilon batch }d_{i,m}^{l}</script></li><li>通过两个待学习参数$\beta<em>{1} ,\gamma </em>{1}$，将标准正态分布调整至$N(\beta<em>{1} ,\gamma </em>{1})$。</li></ul><p>BN的作用：</p><ul><li>使输入数据对应的概率分布保持不变（不是数值不变）。无论训练集数据如何选择，前层网络参数如何变化，各层接受的输入分布在不同的训练阶段都是一致的，各层最优参数稳定，提高了收敛速度</li><li>不容易受极端数据影响，所以可以提高学习率，使收敛速度进一步提高。</li><li>降低对参数初始化的依赖程度</li><li>对参数进行正则化，提高了模型泛化能力</li></ul><p>其它注意点：</p><ul><li>batch_size不宜过低，否则均值方差估计不准</li><li>预测时没有批量的概念，都是对单个样本进行预测，无法进行BN操作计算$\mu /\sigma$ 。所以要保存训练中计算的$\mu /\sigma$ ，预测时使用它们进行无偏估计（公式中t表示当前迭代次数，T为总迭代次数)<script type="math/tex; mode=display">\mu =\frac{1}{T}\sum_{t=1}^{T}\mu_{t}</script><script type="math/tex; mode=display">\sigma  =\sqrt{\frac{1}{T-1}\sum_{t=1}^{T}\sigma _{t}^{2} }</script></li><li>BN的位置在激活函数之前。因为将输入转换为正态分布$N(\beta<em>{1} ,\gamma </em>{1})$的前提是输入数据本身符合正态分布。而神经网络中激活函数的值域往往有限，造成激活后的输出不会符合正态分布。这样BN之后也不会符合正态分布。<h3 id="2-4-Layer-Normalization"><a href="#2-4-Layer-Normalization" class="headerlink" title="2.4 Layer Normalization"></a>2.4 Layer Normalization</h3></li><li>Batch Normalization：在某一个神经元，所有batch个样本上进行归一化，即归一化不同样本的同一特征</li><li>Layer Normalization：对某一个样本，同一层全部神经元上进行归一化，即归一化一个样本所有特征</li><li>BN和LN都可以用的时候BN一般更好，因为不同数据，同一特征得到的归一化结果更不容易造成信息损失。LN会造成神经元耦合。</li><li>batch_size过小的场合，或者RNN、LSTM、Attention等变长神经网络一般使用LN。（变长神经网络虽然填充到同一长度，但是pad部分是没有意义的，进行特征维度归一化也没用实际意义。NLP任务中一个序列的所有token都是同一语义空间，进行LN归一化有实际意义）</li></ul><p>对于l层第i个样本有：（l层共有m=1、2、……$M^{l}$个神经元）</p><script type="math/tex; mode=display">\mu _{i}^{l}=\frac{1}{M^{l}}\sum_{m=1 }^{M^{l}}d_{i,m}^{l}</script><p>计算出$\mu /\sigma$后，和BN一样将数据归一化为$N(\beta<em>{1} ,\gamma </em>{1})$分布。</p><p>综合之前的讲解：<br>model.train()的作用是启用 Batch Normalization 和 Dropout。model.eval()的作用是不启用 Batch Normalization 和 Dropout。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、神经网络参数优化器&quot;&gt;&lt;a href=&quot;#一、神经网络参数优化器&quot; class=&quot;headerlink&quot; title=&quot;一、神经网络参数优化器&quot;&gt;&lt;/a&gt;一、神经网络参数优化器&lt;/h2&gt;&lt;p&gt;参考曹健&lt;a href=&quot;https://blog.csdn.net/weixin_45558569/article/details/110728137?spm=1001.2014.3001.5501&quot;&gt;《人工智能实践：Tensorflow2.0 》&lt;/a&gt;&lt;br&gt;深度学习优化算法经历了SGD -&amp;gt; SGDM -&amp;gt; NAG -&amp;gt;AdaGrad -&amp;gt; AdaDelta -&amp;gt; Adam -&amp;gt; Nadam&lt;br&gt;这样的发展历程。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/65f1bb4bf04f4aa8b260f2126b37099a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;br&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="DNN" scheme="https://zhxnlp.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记3：深度学习DNN</title>
    <link href="https://zhxnlp.github.io/2021/11/23/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN/"/>
    <id>https://zhxnlp.github.io/2021/11/23/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN/</id>
    <published>2021-11-23T12:04:11.000Z</published>
    <updated>2022-01-02T20:45:53.216Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、BP神经网络"><a href="#一、BP神经网络" class="headerlink" title="一、BP神经网络"></a>一、BP神经网络</h2><h3 id="1-1-为何要引出BP神经网络"><a href="#1-1-为何要引出BP神经网络" class="headerlink" title="1.1 为何要引出BP神经网络"></a>1.1 为何要引出BP神经网络</h3><ol><li>逻辑回归对于如今越来越复杂的任务效果越来越差，主要是难以处理线性不可分的数据，LR处理线性不可分，一般是特征变换和特征组合，将低维空间线性不可分的数据在高维空间中线性可分</li><li>改良方式有几种，本质上都是对原始输入特征做文章。但都是针对特定场景设计。如果实际场景中特征组合在设计之外，模型无能为力<ul><li>人工组合高维特征，将特征升维至高维空间。但是会耗费较多人力，而且需要对业务理解很深</li><li>自动交叉二阶特征，例如FM模型。缺点是只能进行二阶交叉</li><li>SVM+核方法：可以将特征投影到高维空间。缺点是核函数种类有限，升维具有局限性，运算量巨大。<span id="more"></span>3.构建BP神经网络（也叫MLP多层感知器），用线性变换+非线性函数激活的方式进行特征变换。以分类为目的进行学习的时候，网络中的参数会以分类正确为目的自行调整，也就是自动提取合适的特征。&lt;/font &gt;（回归问题，去掉最后一层的激活函数就行。输出层激活函数只是为了结果有概率意义）神经网络最大的惊喜就是自动组合特征。<h3 id="1-2-BP神经网络基本原理"><a href="#1-2-BP神经网络基本原理" class="headerlink" title="1.2 BP神经网络基本原理"></a>1.2 BP神经网络基本原理</h3></li></ul></li></ol><ul><li>MLP网络中，每个节点都接收前一层所有节点的信号的加权和，累加后进行激活，再传入下一层的节点。这个过程和动物神经元细胞传递信号的过程类似，所以叫神经网络，各节点称为神经元。</li><li>每层神经元个数称为神经网络的宽度，层数称为神经网络的深度。所以多层神经网络称为深度神经网络DNN。</li><li>神经元数据过多会造成过拟合和运算量过大。层中神经元过多，相当于该层转换后的特征维度过高，会造成维数灾难。</li><li>DNN仍然使用交叉熵损失函数。<h3 id="1-3-神经网络的多分类"><a href="#1-3-神经网络的多分类" class="headerlink" title="1.3 神经网络的多分类"></a>1.3 神经网络的多分类</h3></li><li>机器学习模型相对简单，参数较少。可以通过训练N个二分类模型来完成多分类。而深度学习中，模型参数较多，训练多个模型不实际。而且多分类任务的前层特征变换是一致的，没必要训练多个模型。一般是输出层采用softmax函数来完成。</li><li>softmax做多分类只适用于类别互斥且和为1的情况。如果和不为1，可以加一个其它类。不互斥时不能保证分母能归一化。<h3 id="1-4-二分类使用softmax还是sigmoid好？"><a href="#1-4-二分类使用softmax还是sigmoid好？" class="headerlink" title="1.4 二分类使用softmax还是sigmoid好？"></a>1.4 二分类使用softmax还是sigmoid好？</h3></li></ul><p>参考<a href="https://blog.csdn.net/qq_56591814/article/details/120876730">《速通8-DNN神经网络学习笔记》</a></p><ul><li>softmax等于分别学习w1和w2，而sigmoid等于学这两个的差值就行了。sigmoid是softmax在二分类上的特例。二分类时sigmoid更好。因为我们只关注w1和w2的差值，但是不关心其具体的值。</li><li>softmax的运算量很大，因为要考虑别的概率值。一般只在神经网络最后一层用（多分类时）。中间层神经元各算各的，不需要考虑别的w数值，所以中间层不需要softmax函数。</li><li>softmax函数分子：通过指数函数，将实数输出映射到零到正无穷。softmax函数分母：将所有结果相加，进行归一化。</li><li>softmax和sigmoid一样有饱和区，x变化几乎不会引起softmax输出的变化，而且饱和区导数几乎为0，无法有效学习。所以需要合适的初始化，控制前层输出的值域。</li><li>两个函数的代码实现如下图<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"> </span><br><span class="line">sigmoid_inputs = np.arange(-<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">sigmoid_outputs=sigmoid(sigmoid_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Input :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(sigmoid_inputs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Output :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(sigmoid_outputs))</span><br><span class="line"> </span><br><span class="line">plt.plot(sigmoid_inputs,sigmoid_outputs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sigmoid Inputs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sigmoid Outputs&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    orig_shape=x.shape</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(x.shape)&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="comment">#Matrix</span></span><br><span class="line">        <span class="comment">#shift max whithin each row</span></span><br><span class="line">        constant_shift=np.<span class="built_in">max</span>(x,axis=<span class="number">1</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        x-=constant_shift</span><br><span class="line">        x=np.exp(x)</span><br><span class="line">        normlize=np.<span class="built_in">sum</span>(x,axis=<span class="number">1</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        x/=normlize</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#vector</span></span><br><span class="line">        constant_shift=np.<span class="built_in">max</span>(x)</span><br><span class="line">        x-=constant_shift</span><br><span class="line">        x=np.exp(x)</span><br><span class="line">        normlize=np.<span class="built_in">sum</span>(x)</span><br><span class="line">        x/=normlize</span><br><span class="line">    <span class="keyword">assert</span> x.shape==orig_shape</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line">softmax_inputs = np.arange(-<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">softmax_outputs=softmax(softmax_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Input :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(softmax_inputs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Output :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(softmax_outputs))</span><br><span class="line"><span class="comment"># 画图像</span></span><br><span class="line">plt.plot(softmax_inputs,softmax_outputs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Softmax Inputs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Softmax Outputs&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="1-5-梯度下降和链式求导"><a href="#1-5-梯度下降和链式求导" class="headerlink" title="1.5 梯度下降和链式求导"></a>1.5 梯度下降和链式求导</h3><h3 id="1-6度量学习"><a href="#1-6度量学习" class="headerlink" title="1.6度量学习"></a>1.6度量学习</h3><h2 id="二、矩阵求导术"><a href="#二、矩阵求导术" class="headerlink" title="二、矩阵求导术"></a>二、矩阵求导术</h2>线性代数参考<a href="https://blog.csdn.net/qq_16555103/article/details/84838943">《线性代数一（基本概念）》</a><h3 id="2-1-标量对向量求导"><a href="#2-1-标量对向量求导" class="headerlink" title="2.1 标量对向量求导"></a>2.1 标量对向量求导</h3>例如标量z对向量X求导，就是看X各元素变化对z的影响。所以结果是z对X各元素求导，结果也是一个同尺寸向量。<h3 id="2-2-向量对向量求导"><a href="#2-2-向量对向量求导" class="headerlink" title="2.2 向量对向量求导"></a>2.2 向量对向量求导</h3>列向量对行向量求导，结果是一个矩阵，方便后面进行链式求导中的导数连乘。&lt;/font&gt;（向量既可以写成列的形式，也可以写成行的形式。列向量可以直接对列向量求导，但是维数太多不便于操作，而且没法进行连乘，一般没人这么干。）</li><li>m×1维列向量$y$对n×1维列向量$x$求导，等于$y$的每个元素$y_{i}$分别对$x$求导，得到m×1维导数。而导数每个元素都是标量对向量求导，结果是n×1维列向量。所以最终结果是mn×1维的列向量。行向量亦然。</li><li>神经网络中一般都用列向量。，如果列向量$y$对列向量$x$求导，结果太长不便于计算。一般是$y$对$x$的转置求导，即$\frac{\partial y}{\partial x^{T}}$，最后结果是一个m×n的矩阵。<h3 id="2-3-标量对矩阵的矩阵"><a href="#2-3-标量对矩阵的矩阵" class="headerlink" title="2.3 标量对矩阵的矩阵"></a>2.3 标量对矩阵的矩阵</h3>向量可以看成某一维为1的矩阵（行为1或列为1），同理，标量对m×n维矩阵求导，是对矩阵中每个元素求导，结果也是一个m×n维的结果。<h3 id="2-4-向量求导及链式法则"><a href="#2-4-向量求导及链式法则" class="headerlink" title="2.4 向量求导及链式法则"></a>2.4 向量求导及链式法则</h3>对于<script type="math/tex">x_{3}=Wx_{2}</script><br>展开之后有：<script type="math/tex; mode=display">\begin{pmatrix}x_{31}\\ x_{32}\\ ...\\ x_{3m}\end{pmatrix}=\begin{pmatrix}m_{11} & m_{12} &... &m _{1n} \\ m_{21} &m _{22} &...  &m_{2n} \\ ...& ... & ... &... \\ m_{m1}&m_{m2}  &... & m_{mn} \end{pmatrix}\times \begin{pmatrix}x_{21}\\ x_{22}\\ ...\\ x_{2n}\end{pmatrix}</script>所以$\frac{\partial x<em>{3}}{\partial x</em>{2}^{T}}=W$，$\frac{\partial x<em>{3}}{\partial W}=x</em>{2}^{T}$</li><li>即之前列向量对列向量转置求导，推出结果是mn的矩阵，但是各元素的具体值不知道。这里直接求出，具体值为矩阵W</li><li>求导要一直盯着尺寸看。</li></ul><p>公式1——标量对向量求导链式法则：<br>向量$x<em>{1}…x</em>{n}$为神经网络各层的输入，最终输出结果是标量z。存在依赖关系：$x<em>{1}\rightarrow x</em>{2}\rightarrow x<em>{3}…\rightarrow x</em>{n}\rightarrow z$，则有：</p><script type="math/tex; mode=display">\frac{\partial z}{\partial x_{1}}=(\frac{\partial x_{n} }{\partial x_{n-1}^{T}}\cdot \frac{\partial x_{n-1} }{\partial x_{n-2}^{T}}...\frac{\partial x_{2} }{\partial x_{1}^{T}})^{T}\frac{\partial z}{\partial x_{n}}</script><ul><li>假如$x<em>{n}、x</em>{n-1}、x_{n-2}$分别是m、n、k维的列向量，则上式右边第一项分别是m×n和n×k的矩阵，这两个矩阵才有相乘的可能，结果是m×k的矩阵。所以必须是列向量对列向量的转置求导</li><li>假如$x<em>{1}$是h维列向量，则括号内最终结果是m×h维矩阵，$\frac{\partial z}{\partial x</em>{n}}$结果是m维列向量，无法直接相乘，所以括号内的矩阵必须转置。</li></ul><p>公式2——标量对矩阵求导链式法则：<br>W为矩阵，$x$和$y$是向量，有$y=Wx$。且标量$z=f(y)$，则：</p><script type="math/tex; mode=display">\frac{\partial z}{\partial W}=\frac{\partial z }{\partial y}\cdot x^{T}</script><p>参照上面写的：$\frac{\partial x<em>{3}}{\partial W}=x</em>{2}^{T}$</p><h3 id="2-5-BP反向传播"><a href="#2-5-BP反向传播" class="headerlink" title="2.5 BP反向传播"></a>2.5 BP反向传播</h3><p>Loss对任意层$W^k$求导有：</p><script type="math/tex; mode=display">\frac{\partial Loss}{\partial W^{k}}=\frac{\partial Loss}{\partial d_{j}^{L}}\cdot \frac{\partial d_{j}^{L}}{\partial W^{k}}</script><p>对于一个L层的神经网络，j表示L层第j维数据（也就是第j个神经元）<br>第一项$\frac{\partial Loss}{\partial d<em>{j}^{L}}=y</em>{j}’-y_{j}$，是一个标量。(即loss对最后一层某个神经元导数为一个标量，推导见P119）<br>又因为：</p><script type="math/tex; mode=display">d^{k}=W^{k}\cdot a^{k-1}</script><script type="math/tex; mode=display">a^{k-1}=f(d^{l-1}+w_{0}^{k-1})</script><p>后一项是标量对矩阵求导，根据公式2有： </p><script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial W^{k}}= \frac{\partial d_{j}^{L}}{\partial d^{k}}\cdot (a^{k-1})^{T}</script><blockquote><p>将$d_{j}^{L}$看做是由向量$d^{L}$映射成标量</p></blockquote><p>上式右边第一项是标量对向量求导，根据公式1有：</p><script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial d^{k}}=\frac{\partial d_{j}^{L}}{\partial a^{L-1}}\cdot (\frac{\partial a^{L-1}}{\partial (d^{L-1})^{T}}\cdot \frac{\partial d^{L-1}}{\partial (a^{L-2})^{T}}...\frac{\partial d^{k+1}}{\partial (a^{k})^{T}}\cdot \frac{\partial a^{k}}{\partial (d^{k})^{T}})^T</script><p>依次求三项导数有：</p><script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial a^{L-1}}=W^L</script><script type="math/tex; mode=display">\frac{\partial a^{m}}{\partial (d^{m})^{T}}=f'(d^m)</script><script type="math/tex; mode=display">\frac{\partial d^{m+1}}{\partial (a^{m})^{T}}=W^{m+1}</script><p>将以上结果联合起来就是：</p><script type="math/tex; mode=display">\frac{\partial Loss}{\partial W^{k}}=\frac{\partial Loss}{\partial d_{j}^{L}}\cdot \frac{\partial d_{j}^{L}}{\partial W^{k}}=(y'_{j}-y_{j})\cdot (a^{k-1})^{T}\cdot W^{L}\cdot f'(d^{L-1})\cdot  W^{L-1}\cdot f'(d^{L-2})... W^{k+1}\cdot f'(d^{k})</script><p>第m层激活函数得导数有：</p><script type="math/tex; mode=display">a^{m}=f(d^{m}+w_{0}^{m})</script><p>展开后为：</p><script type="math/tex; mode=display">\begin{pmatrix}a_{1}\\ a_{2}\\ ...\\ a_{M}\end{pmatrix}=\begin{pmatrix}f(d_{1}+w_{0}^{1})\\ f(d_{2}+w_{0}^{2})\\ ...\\ f(d_{M})+w_{0}^{M}\end{pmatrix}</script><p>其实是省去了上标层数m，其中第m层共有M个神经元。相当于向量d数乘之后得到向量a，每个元素按位操作。$a<em>{1}=f(d</em>{1}+w<em>{0}^{1})$,$a</em>{1}$只和$d_{1}$有关，和向量d其它元素无关，对d其它分量结果为0。</p><script type="math/tex; mode=display">\frac{\partial a^{m}}{\partial (d^{m})^{T}}=f'(d^m)</script><p>列向量对行向量求导，结果是一个矩阵。所以有：</p><script type="math/tex; mode=display">f'(d^m)=\begin{pmatrix}f'(d_{1}+w_{0}^{1}) &0  &...  &0 \\  0& f'(d_{2}+w_{0}^{2}) & ... & 0\\ 0 &...  & ... &... \\ 0& 0 & ... & f'(d_{M}+w_{0}^{M})\end{pmatrix}</script><ul><li>如果激活函数f的导数f’&gt;1，经过多次连乘，最后结果会非常大，即梯度爆炸。会产生震荡甚至溢出。</li><li>如果f’&lt;1，梯度消失，W几乎不会更新。<h3 id="2-5-激活函数及其导数"><a href="#2-5-激活函数及其导数" class="headerlink" title="2.5 激活函数及其导数"></a>2.5 激活函数及其导数</h3></li></ul><ol><li>sigmoid函数：$sigmoid(d)=\frac{1}{1+e^{-d}}$导数为：<br>$f’=f(1-f)=0.25-(f-0.5)^2$,后一项非负，当f=0.5时有最大值0.25。所以其值域为（0，0.25）</li><li><p>softmax函数及其导数，参考<a href="https://blog.csdn.net/cassiePython/article/details/80089760">《Softmax函数及其导数》</a><br><img src="https://img-blog.csdnimg.cn/7b1f7e77f53045e0bf79b29e00d463d7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>relu函数：$f(x)=max(0,x)$。其导数w为：</p><script type="math/tex; mode=display">f'(x)=\left\{\begin{matrix}0 ,x<0\\ 1,x>0\end{matrix}\right.</script></li><li>Tanh函数<script type="math/tex; mode=display">f(d)=Tanh(d)=\frac{e^{d}-e^{-d}}{e^{d}+e^{-d}}=sigmoid(2d-1)</script>导数为：<script type="math/tex">\frac{\partial a}{\partial d}=1-f^{2}</script><br>Tanh值域（-1，1)，导数值域（0,1）。<h2 id="三、神经网络调优"><a href="#三、神经网络调优" class="headerlink" title="三、神经网络调优"></a>三、神经网络调优</h2>海量的数据和强大的算力为深度学习的发展提供了条件。但是也带来一些新的问题：</li></ol><ul><li>网络太深带来梯度消失和梯度爆炸</li><li>损失函数太复杂，有大量的极小点和鞍点，如果学习方法不合适，损失函数可能无法降低</li><li>参数过多造成过拟合<h3 id="3-1-激活函数得选型"><a href="#3-1-激活函数得选型" class="headerlink" title="3.1 激活函数得选型"></a>3.1 激活函数得选型</h3>合适的激活函数需要满足的条件</li></ul><ol><li>零均值输出：例如tanh，零均值输出，有正有负。W各个维度更新方向可以不同，避免单向更新学习慢的问题（各维度只能同增或者同减，走Z字路线。比如loss极小值在左下方，只能先左后下）这一点softmax、sigmoid、relu都不满足，都是非负的</li><li>适当的线性。激活函数对输入的变化不宜过于激烈，否则输入轻微变化造成输出剧烈变化，稳定性不好。softmax、sigmoid、relu、tanh都有近似线性的区域可以满足</li><li>导数不宜过大或过小，否则梯度消失或者爆炸。这也是relu成为神经网络首选的原因。tanh导数是0-1，比sigmoid好一点。</li><li>导数的单调性。避免导数有正有负造成学习震荡，例如三角函数</li><li>有值域的控制。避免多次激活后输出太大。例如$y=x^2$</li><li>没有超参数，计算简单。Maxout、PReLu函数有超参数，设置本身依赖经验。而且ReLu计算足够简单<h3 id="3-2-Relu激活函数及其变体"><a href="#3-2-Relu激活函数及其变体" class="headerlink" title="3.2 Relu激活函数及其变体"></a>3.2 Relu激活函数及其变体</h3>参考<a href="https://blog.csdn.net/Sophia_11/article/details/103998468?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163769303916780357222745%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163769303916780357222745&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-7-103998468.first_rank_v2_pc_rank_v29&amp;utm_term=gelu&amp;spm=1018.2226.3001.4187">《从ReLU到GELU，一文概览神经网络的激活函数》</a><br>线性整流函数Relu:<script type="math/tex">f(x)=max(0,x)</script></li></ol><ul><li>d≥0时，f(d)=d，导数为1，不存在梯度消失或者梯度爆炸，也不存在饱和区</li><li>d＜0时，f(d)=0，梯度为0 ，神经元死亡<br>梯度消失和激活函数饱和困扰业界多年，直到RELU的出现。</li></ul><p>神经元真死和假死：<br>真死：</p><ul><li>无论w和x各维度如何变化，恒有$d<em>{j}^{(l)}=w</em>{j}^{(l)}a^{(l-1)}$≤0，神经元死亡。（l表示神经网络层数，j表示某层中神经元节点j）。$a^{(l-1)}≥0$为上一层神经网络的输出。</li><li>所以是$w<em>{j}^{(l)}$各元素都是很大的负数时，恒有d≤0。这是参数初始化错误或者lr过大导致权重$w</em>{j}^{(l)}$更新过大造成的。因此用Relu做激活函数，lr不宜过大，权重初始化要合理</li></ul><p>假死：饱和形式之一</p><ul><li>恰巧造成$w<em>{j}^{(l)}a</em>{j}^{(l-1)}≤0$，重新训练时有可能会恢复正常。神经元大多数时是假死。</li><li>适当假死可以提升神经网络效果：<ul><li>如果没有神经元假死，$a_{j}^{(l-1)}≥0$，f(d)=d，Relu作为激活函数就是纯线性的，失去意义。</li><li>假死神经元就是对某些输入特征不进行输出，达到特征选择的作用（门控决策）</li></ul></li></ul><p>LRelu： d＜0时，f(d)=k，k为超参数，在0-1之间<br>PRelu： d＜0时，f(d)=k，k是可学习的参数。<br>Maxout：$w<em>{j}^{(l)}(1)$到$w</em>{j}^{(l)}(n)$有多个参数，分别和上一层输出相乘，即每个神经元节点j有多个输入，最终输出选最大值。(P133-134)</p><h3 id="3-3-高斯误差线性单元激活函数gelu"><a href="#3-3-高斯误差线性单元激活函数gelu" class="headerlink" title="3.3 高斯误差线性单元激活函数gelu"></a>3.3 高斯误差线性单元激活函数gelu</h3><blockquote><p>参考<a href="https://mp.weixin.qq.com/s/XqPKsM8yq8rbFFvvJKCtkQ">《超越ReLU却鲜为人知，3年后被挖掘》</a><br>参考<a href="https://arxiv.org/pdf/1606.08415.pdf">《GELU 论文》</a></p></blockquote><h4 id="3-3-1-GELU概述"><a href="#3-3-1-GELU概述" class="headerlink" title="3.3.1 GELU概述"></a>3.3.1 GELU概述</h4><p>BERT、RoBERTa、ALBERT 等目前业内顶尖的 NLP 模型都使用了这种激活函数。另外，在 OpenAI 声名远播的无监督预训练模型 GPT-2 中，研究人员在所有编码器模块中都使用了 GELU 激活函数。在计算机视觉、自然语言处理和自动语音识别等任务上，使用 GELU 激活函数比使用ReLU 或 ELU 效果更好。</p><p>随着网络深度的不断增加，利用 Sigmoid 激活函数来训练被证实不如非平滑、低概率性的 ReLU 有效（Nair &amp; Hinton, 2010），因为 ReLU 基于输入信号做出门控决策。</p><p>深度学习中为了解决过拟合，会随机正则化（如在隐层中加入噪声）或采用 dropout 机制。这两个选择是和激活函数割裂的。非线性和 dropout 共同决定了神经元的输出，而随机正则化在执行时与输入无关。</p><p>由此提出高斯误差线性单元（Gaussian Error Linear Unit，GELU）。GELU 与随机正则化有关，因为它是自适应 Dropout 的修正预期（Ba &amp; Frey, 2013）&lt;/font &gt;。这表明神经元输出的概率性更高。</p><h4 id="3-3-2-GELU数学表示"><a href="#3-3-2-GELU数学表示" class="headerlink" title="3.3.2 GELU数学表示"></a>3.3.2 GELU数学表示</h4><p>Dropout、ReLU 等机制都希望将「不重要」的激活信息规整为零。即对于输入的值，我们根据它的情况乘上 1 或 0。&lt;/font &gt;或者说，对输入x乘上一个伯努利分布 Bernoulli(Φ(x))，其中Φ(x) = P(X ≤ x)。（x服从于标准正态分布 N(0, 1)）</p><p>对于一部分Φ(x)，它直接乘以输入 x，而对于另一部分 (1 − Φ(x))，它们需要归零。随着 x 的降低，它被归零的概率会升高。对于 ReLU 来说，这个界限就是 0。</p><p>我们经常希望神经网络具有确定性决策，这种想法催生了 GELU 激活函数的诞生。具体来说可以表示为：Φ(x) × Ix + (1 − Φ(x)) × 0x = xΦ(x)。可以理解为，不太严格地说，上面这个表达式可以按当前输入 x 比其它输入大多少来缩放 x。&lt;/font &gt;</p><p>高斯概率分布函数通常根据损失函数计算，因此研究者定义高斯误差线性单元（GELU）为：</p><script type="math/tex; mode=display">GELU(x)=xP(X\leqslant x)=x\Phi (x)</script><p>上面这个函数是无法直接计算的，因此可以通过另外的方法来逼近这样的激活函数，研究者得出来的表达式为：</p><script type="math/tex; mode=display">GELU(x)=0.5x(1+tanh[\sqrt{\frac{2}{\pi }}(x+0.044175x^{3})])</script><p>或：<script type="math/tex">GELU(x)=x\sigma (1.702x)</script><br>其中 σ() 是标准的 sigmoid 函数<br><img src="https://img-blog.csdnimg.cn/adbc2fffd2e04be5adf7ccc5702ecd40.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>当 x 大于 0 时，输出为 x；但 x=0 到 x=1 的区间除外，这时曲线更偏向于 y 轴。<br>没能找到该函数的导数，所以我使用了 WolframAlpha 来微分这个函数。结果如下：<br><img src="https://img-blog.csdnimg.cn/338d92546ede439195600449445c66a2.png" alt="在这里插入图片描述"><br>微分的GELU函数：<br><img src="https://img-blog.csdnimg.cn/0c8f7ea9572e4068946a53279375f94c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>GELU 的近似实现方式有两种，借助 tanh() 和借助σ()。我们在 GPT-2 的官方代码中也发现，更多研究者采用了 tanh() 的实现方式尽管它看起来要比 xσ(1.702x) 复杂很多。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GPT-2 的 GELU 实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span>(<span class="params">x</span>):</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0.5</span>*x*(<span class="number">1</span>+tf.tanh(np.sqrt(<span class="number">2</span>/np.pi)*(x+<span class="number">0.044715</span>*tf.<span class="built_in">pow</span>(x, <span class="number">3</span>))))</span><br></pre></td></tr></table></figure><h3 id="3-4-Xavier权重初始化"><a href="#3-4-Xavier权重初始化" class="headerlink" title="3.4 Xavier权重初始化"></a>3.4 Xavier权重初始化</h3><p>参考《苏神文章解析（6篇）》<br><a href="https://kexue.fm/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a><br><a href="https://kexue.fm/archives/7180">《从几何视角来理解模型参数的初始化策略》</a></p><ul><li>如果两个神经元参数完全相同，则梯度也相同，更新幅度一致，最后输出也相同。看上去是两个神经元，但其实相当于就一个，是互相冗余的。所以 初始化W矩阵时要破坏其对称性，独立均匀分布的随机初始化就可以做到。（正态分布采样容易集中在均值附近，可能造成权重对称）均匀分布有两个参数$\mu/ \sigma$ ，均值和方差。</li><li>真实场景中，特征都是客观事实，一般都是＞0。如果W矩阵各元素都是＞0，则relu函数不起作用。都＜0，神经元都死了。所以 W矩阵各元素需要有正有负，所以一般均匀分布选择均值为0。$\sigma$ 越大，w采样到大值的可能性越大,即$W<em>{i,j}^{l}\propto \sigma </em>{i,j}^{l}$</li><li>从均值为0、方差为1/m的随机分布中独立重复采样，这就是Xavier初始化（无激活函数时）relu做激活函数时，方差为2/m</li><li>NTK参数化：均值为0、方差为1的随机分布来初始化，但是将输出结果除以$\sqrt{m}$。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的量级大致都是相同的O(1)级别，让我们更平等地处理每一个参数，于是我们可以设置较大的学习率。</li><li>对于$d<em>{j}^{(l)}=\sum</em>{i=1}^{M(l-1)}w<em>{i,j}^{(l)}a</em>{i}^{(l-1)}$，M(l-1)表示上一层神经元个数。$d<em>{j}^{(l)}$不宜过大,$W</em>{i,j}^{l}\propto \frac{1}{M^{l-1}},\sigma _{i,j}^{l}\propto \frac{1}{M^{l-1}}$&lt;/font &gt;。否则relu前向传播时，输出会层层放大而溢出。softmax、sigmoid、tanh进入饱和区，导数基本为0。<br>其它推导见P137</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、BP神经网络&quot;&gt;&lt;a href=&quot;#一、BP神经网络&quot; class=&quot;headerlink&quot; title=&quot;一、BP神经网络&quot;&gt;&lt;/a&gt;一、BP神经网络&lt;/h2&gt;&lt;h3 id=&quot;1-1-为何要引出BP神经网络&quot;&gt;&lt;a href=&quot;#1-1-为何要引出BP神经网络&quot; class=&quot;headerlink&quot; title=&quot;1.1 为何要引出BP神经网络&quot;&gt;&lt;/a&gt;1.1 为何要引出BP神经网络&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;逻辑回归对于如今越来越复杂的任务效果越来越差，主要是难以处理线性不可分的数据，LR处理线性不可分，一般是特征变换和特征组合，将低维空间线性不可分的数据在高维空间中线性可分&lt;/li&gt;
&lt;li&gt;改良方式有几种，本质上都是对原始输入特征做文章。但都是针对特定场景设计。如果实际场景中特征组合在设计之外，模型无能为力&lt;ul&gt;
&lt;li&gt;人工组合高维特征，将特征升维至高维空间。但是会耗费较多人力，而且需要对业务理解很深&lt;/li&gt;
&lt;li&gt;自动交叉二阶特征，例如FM模型。缺点是只能进行二阶交叉&lt;/li&gt;
&lt;li&gt;SVM+核方法：可以将特征投影到高维空间。缺点是核函数种类有限，升维具有局限性，运算量巨大。</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://zhxnlp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://zhxnlp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="DNN" scheme="https://zhxnlp.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记2：线性回归、决策树、聚类</title>
    <link href="https://zhxnlp.github.io/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E8%81%9A%E7%B1%BB/"/>
    <id>https://zhxnlp.github.io/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E8%81%9A%E7%B1%BB/</id>
    <published>2021-11-22T12:33:20.000Z</published>
    <updated>2022-01-02T20:44:54.014Z</updated>
    
    <content type="html"><![CDATA[<h2 id="学习笔记2——线性回归、决策树、聚类"><a href="#学习笔记2——线性回归、决策树、聚类" class="headerlink" title="学习笔记2——线性回归、决策树、聚类"></a>学习笔记2——线性回归、决策树、聚类</h2><h2 id="一、-线性回归"><a href="#一、-线性回归" class="headerlink" title="一、.线性回归"></a>一、.线性回归</h2><pre><code>fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数。normalize : 布尔型参数，若为True，则X在回归前进行归一化。可选参数。默认值为False。copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为True。n_jobs : 整型参数，表示用于计算的作业数量；若为-1，则用所有的CPU。可选参数。默认为1positive=False#当设置为&#39;True&#39;时，强制系数为正。这选项仅支持密集阵列。rint(model.coef_)#打印线性方程中的wprint(model.intercept_)#打印w0 就是线性方程中的截距b</code></pre><span id="more"></span><h4 id="3-sklearn-metrics"><a href="#3-sklearn-metrics" class="headerlink" title="3.sklearn.metrics"></a>3.sklearn.metrics</h4><p>模块包括评分函数、性能指标和成对度量和距离计算。<br>F1-score:    2<em>(P</em>R)/(P+R)。<a href="https://www.cnblogs.com/techengin/p/8962024.html">参考《sklearn中 F1-micro 与 F1-macro区别和计算原理》</a><br>导入：from sklearn import metrics<br>分类指标</p><pre><code>accuracy_score(y_true, y_pre)#精度log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)交叉熵损失函数auc(x, y, reorder=False)ROC曲线下的面积;较大的AUC代表了较好的performance。AUC：roc_auc_score(y_true, y_score, average=‘macro’, sample_weight=None)f1_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) F1值precision_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’,) 查准率recall_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) 查全率roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)计算ROC曲线的横纵坐标值，TPR，FPRTPR = TP/(TP+FN) = recall(真正例率，敏感度)FPR = FP/(FP+TN)(假正例率，1-特异性)classification_report(y_true, y_pred)#分类结果分析汇总</code></pre><p>f1_score中关于参数average的用法描述:<br>‘micro’:Calculate metrics globally by counting the total true positives, false negatives and false positives.</p><p>‘micro’:通过先计算总体的TP，FN和FP的数量，再计算F1</p><p>‘macro’:Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p><p>‘macro’:分布计算每个类别的F1，然后做平均（各类别F1的权重相同）<br>回归指标</p><pre><code>explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)回归方差(反应自变量与因变量之间的相关程度)mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)平均绝对误差MAEmean_squared_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #均方差MSEmedian_absolute_error(y_true, y_pred)中值绝对误差r2_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #R平方值</code></pre><h4 id="4-PolynomialFeatures构建特征"><a href="#4-PolynomialFeatures构建特征" class="headerlink" title="4.PolynomialFeatures构建特征"></a>4.PolynomialFeatures构建特征</h4><p><a href="https://blog.csdn.net/tiange_xiao/article/details/79755793">https://blog.csdn.net/tiange_xiao/article/details/79755793</a><br>使用sklearn.preprocessing.PolynomialFeatures来进行特征的构造。<br>degree：控制多项式的度<br>interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2。<br>include_bias：默认为True。如果为True的话，那么就会有上面的 1那一项。</p><h4 id="5-机器学习中的random-state参数"><a href="#5-机器学习中的random-state参数" class="headerlink" title="5.机器学习中的random_state参数"></a>5.机器学习中的random_state参数</h4><p>原文链接：<a href="https://blog.csdn.net/ytomc/article/details/113437926">https://blog.csdn.net/ytomc/article/details/113437926</a></p><pre><code>1、在构建模型时：    forest = RandomForestClassifier(n_estimators=100, random_state=0)    forest.fit(X_train, y_train)2、在生成数据集时：    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)3、在拆分数据集为训练集、测试集时：    X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)    参数test_size：如果是浮点数，在0-1之间，表示test set的样本占比；如果是整数的话就表示test set样本数量。</code></pre><p>例如1中，每次构建的模型是不同的。<br>例如2中，每次生成的数据集是不同的。<br>例如3中，每次拆分出的训练集、测试集是不同的。</p><h4 id="5-Solver-lbfgs-supports-only-“l2”-or-“none”-penalties-got-l1-penalty-解决办法"><a href="#5-Solver-lbfgs-supports-only-“l2”-or-“none”-penalties-got-l1-penalty-解决办法" class="headerlink" title="5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法"></a>5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法</h4><p>在用以下代码建立逻辑回归模型的时候<br>lr = LogisticRegression(C = c_param, penalty = ‘l1’)，正则化惩罚选择’L1’报错。<br>LogisticRegression的参数如下：</p><pre><code>LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, l1_ratio=None, max_iter=100,multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l1&#39;,random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0,warm_start=False)</code></pre><p>我们看solver参数，这个参数定义的是分类器，‘newton-cg’，‘sag’和‘lbfgs’等solvers仅支持‘L2’regularization，‘liblinear’ solver同时支持‘L1’、‘L2’regularization，若dual=Ture，则仅支持L2 penalty。<br>决定惩罚项选择的有2个参数：dual和solver，如果要选L1范数，dual必须是False，solver必须是liblinear<br>因此，我们只需将solver=’liblinear’参数添加进去即可</p><pre><code>lr = LogisticRegression(C = c_param, penalty = ‘l1’,solver=‘liblinear’)</code></pre><p>原文链接：<a href="https://blog.csdn.net/kakak_/article/details/104923634">https://blog.csdn.net/kakak_/article/details/104923634</a></p><h2 id="二、决策树和随机森林"><a href="#二、决策树和随机森林" class="headerlink" title="二、决策树和随机森林"></a>二、决策树和随机森林</h2><p>随机森林算法详解：<a href="https://zhuanlan.zhihu.com/p/139510947">https://zhuanlan.zhihu.com/p/139510947</a><br>&#8195;&#8195;GBDT、XGBOOST、LGBM都是以决策树为积木搭建出来的。<br>&#8195;&#8195;随机森林就是决策树们基于bagging集成学习思想搭建起来的。算法实现思路非常简单，只需要记住一句口诀：抽等量样本，选几个特征，构建多棵树。</p><h4 id="2-1随机森林的随机性："><a href="#2-1随机森林的随机性：" class="headerlink" title="2.1随机森林的随机性："></a>2.1随机森林的随机性：</h4><p>&#8195;&#8195;数据集的随机选取、每棵树所使用特征的随机选取。以上两个3.1随机性使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。<br>1）抽等量样本<br>&#8195;&#8195;抽样方式一般是有放回的抽样，也就是说，在训练某棵树的时候，这一次被抽到的样本会被放回数据集中，下一次还可能被抽到，因此，原训练集中有的样本会多次被抽出用来训练，而有的样本可能不会被使用到。<br>但是不用担心有的样本没有用到，只要训练的树的棵数足够多，大多数训练样本总会被取到的。有极少量的样本成为漏网之鱼也不用担心，后边我们会筛选他们出来用来测试模型。</p><p>2）选几个特征（“max_features”）<br>&#8195;&#8195;在训练某棵树的时候，会随机选择一部分特征用来训练。这样做的目的就是让不同的树重点关注不同的特征。</p><p>3）构建多棵树（“n_estimators”）<br>&#8195;&#8195;最终的结果由每棵决策树综合给出：如果是分类问题，所有树投票决定；如果是回归问题，各个树得到的结果加权平均。（每个树的结果是叶节点的均值，预测房价，就是样本输入模型分到某个节点，这个叶节点所以=有房子价格的均值。一棵树m个叶节点，就只有m个输出）所以随机森林做回归比较少。</p><h4 id="2-2优缺点："><a href="#2-2优缺点：" class="headerlink" title="2.2优缺点："></a>2.2优缺点：</h4><p>1）实现简单，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；<br>2）相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；<br>3）能直接特征很多的高维数据，因为在训练过程中依旧会从这些特征中随机选取部分特征用来训练；<br>4）相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；<br>5）训练完成后可以给出特征重要性。当然，这个优点主要来源于决策树。因为决策树在训练过程中会计算熵或者是基尼系数，越往树的根部，特征越重要。<br>2、缺点<br>1）在噪声过大的分类和处理回归问题时还是容易过拟合；<br>2）相比于单一决策树，它的随机性让我们难以对模型进行解释。</p><h4 id="2-3调参"><a href="#2-3调参" class="headerlink" title="2.3调参"></a>2.3调参</h4><p>1、用于调参的参数：<br>max_features（最大特征数）: 这个参数用来训练每棵树时需要考虑的最大特征个数，超过限制个数的特征都会被舍弃，默认为auto。可填入的值有：int值，float（特征总数目的百分比），“auto”/“sqrt”（总特征个数开平方取整）,“log2”（总特征个数取对数取整）。默认值为总特征个数开平方取整。值得一提的是，这个参数在决策树中也有但是不重要，因为其默认为None，即有多少特征用多少特征。为什么要设置这样一个参数呢？原因如下：考虑到训练模型会产生多棵树，如果在训练每棵树的时候都用到所有特征，以来导致运算量加大，二来每棵树训练出来也可能千篇一律，没有太多侧重，所以，设置这个参数，使训练每棵树的时候只随机用到部分特征，在减少整体运算的同时还可以让每棵树更注重自己选到的特征。</p><p>n_estimators：随机森林生成树的个数，默认为100。</p><p>2、控制样本抽样参数：<br>bootstrap：每次构建树是不是采用有放回样本的方式(bootstrap samples)抽取数据集。可选参数：True和False，默认为True。<br>oob_score：是否使用袋外数据来评估模型，默认为False。<br>boostrap和 oob_score两个参数一般要配合使用。如果boostrap是False，那么每次训练时都用整个数据集训练，如果boostrap是True，那么就会产生袋外数据。<br>选择criterion参数（决策树划分标准）<br>和决策树一样，这个参数只有两个参数 ‘entropy’（熵） 和 ‘gini’（基尼系数）可选，默认为gini</p><p>有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能被忽略，一般来说，每一次抽样，某个样本被抽到的概率是 1/n ，所以不被抽到的概率就是 1-1/n ,所以n个样本都不被抽到的概率就是：<br>用洛必达法则化简，可以得到这个概率收敛于(1/e)，约等于0.37。<br>因此，如果数据量足够大的时候，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。<br>为了这些数据不被浪费，我们也可以把他们用来作为集成算法的测试集。也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。</p><h3 id="三、聚类"><a href="#三、聚类" class="headerlink" title="三、聚类"></a>三、聚类</h3><p>见笔记<br>聚类是指试图将相似的数据点分组到人工确定的组或簇中</p><ol><li>聚类的基本思想：对于给定的M个样本的数据集，给定聚类（簇）的个数K（K&lt;M），初始化每个样本所属的类别，再根据一定的规则不断地迭代并重新划分数据集的类别（改变样本与簇的类别关系），使得每一次的划分都比上一次的划分要好。</li><li>聚类算法有很多种，主要分为划分聚类（KMeans）、密度聚类（DBSCAN）和谱聚类等三种聚类。<h4 id="3-2-kmeans"><a href="#3-2-kmeans" class="headerlink" title="3.2 kmeans"></a>3.2 kmeans</h4></li><li>KMeans算法的思想：对于给定的M个样本的数据集（无标签），给定聚类（簇）的个数K（K&lt;M），初始化每个样本所属的类别，再根据距离的不同，将每个样本分配到距离最近的中心点的簇中，然后再对迭代完成的每个簇更新中心点位置（改变样本与簇的类别关系），直到达到终止条件为止。</li><li>KMeans算法的终止条件：1）迭代次数    2）簇中心点变化率    3）最小平方误差值</li><li><p>KMeans算法的优点：理解简单容易，凸聚类的效果不错，效率高；对于服从高斯分布的数据集效果相当好。</p><ol><li>KMeans算法的缺点：</li></ol></li></ol><ul><li>对初始点敏感（局部最优）</li><li>被异常点影响（异常点影响聚类中心值，聚类边界点有可能被分到另一类，异常点很难清洗）</li><li>某些场景中心点缺乏物理意义（例如分类性别以0和1表示，中心点可能是小数）</li><li>数值问题（例如以人的身高体重做聚类，不同单位数值差别很大。例如身高以m为单位基本差别很小，以g为单位差别很大，此时基本由体重主导了，所以每一维要做归一化。可以让方差为1或者都除以最大值）</li><li>K值不好选择<br>K-means本身有很多问题，因为没有数据标注，是没有办法的办法</li></ul><p>伪代码：</p><p>初始化K个中心点（一般随机选择）：</p><pre><code>当中心点变化大于给定值或者小于迭代次数时：    对于数据集中的每个点：        对于每个中心点：            计算数据点到中心点的距离        将数据点分配到距离最近的簇中    对于每个簇，计算簇中所有数据点的平均值，更新簇中心点的位置返回最终的簇中心点和对应的簇</code></pre><h4 id="3-3-DBSCAN聚类算法"><a href="#3-3-DBSCAN聚类算法" class="headerlink" title="3.3 DBSCAN聚类算法"></a>3.3 DBSCAN聚类算法</h4><p>DBSCAN的算法思想：用一个点附近的邻域内的数据点的个数来衡量该点所在空间的密度。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;学习笔记2——线性回归、决策树、聚类&quot;&gt;&lt;a href=&quot;#学习笔记2——线性回归、决策树、聚类&quot; class=&quot;headerlink&quot; title=&quot;学习笔记2——线性回归、决策树、聚类&quot;&gt;&lt;/a&gt;学习笔记2——线性回归、决策树、聚类&lt;/h2&gt;&lt;h2 id=&quot;一、-线性回归&quot;&gt;&lt;a href=&quot;#一、-线性回归&quot; class=&quot;headerlink&quot; title=&quot;一、.线性回归&quot;&gt;&lt;/a&gt;一、.线性回归&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数。
normalize : 布尔型参数，若为True，则X在回归前进行归一化。可选参数。默认值为False。
copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为True。
n_jobs : 整型参数，表示用于计算的作业数量；若为-1，则用所有的CPU。可选参数。默认为1
positive=False#当设置为&amp;#39;True&amp;#39;时，强制系数为正。这选项仅支持密集阵列。

rint(model.coef_)#打印线性方程中的w
print(model.intercept_)#打印w0 就是线性方程中的截距b
&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://zhxnlp.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="机器学习" scheme="https://zhxnlp.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="https://zhxnlp.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    <category term="决策树" scheme="https://zhxnlp.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    <category term="聚类" scheme="https://zhxnlp.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard</title>
    <link href="https://zhxnlp.github.io/2021/11/18/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9Ann.Module%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E3%80%81TensorBoard/"/>
    <id>https://zhxnlp.github.io/2021/11/18/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9Ann.Module%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E3%80%81TensorBoard/</id>
    <published>2021-11-17T16:39:41.000Z</published>
    <updated>2022-01-02T19:49:38.403Z</updated>
    
    <content type="html"><![CDATA[<p>@[toc]<br><a href="https://zhuanlan.zhihu.com/p/265394674">《PyTorch 学习笔记汇总（完结撒花）》</a></p><h2 id="一、nn-Module"><a href="#一、nn-Module" class="headerlink" title="一、nn.Module"></a>一、nn.Module</h2><h3 id="1-1-nn-Module的调用"><a href="#1-1-nn-Module的调用" class="headerlink" title="1.1 nn.Module的调用"></a>1.1 nn.Module的调用</h3><p>pytorch通过继承nn.Module类，定义子模块的实例化和前向传播，实现深度学习模型的搭建。其构建代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *kargs</span>):</span> <span class="comment"># 定义类的初始化函数，...是用户的传入参数</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()<span class="comment">#调用父类nn.Module的初始化方法</span></span><br><span class="line">        ... <span class="comment"># 根据传入的参数来定义子模块</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *kargs</span>):</span> <span class="comment"># 定义前向计算的输入参数，...一般是张量或者其他的参数</span></span><br><span class="line">        ret = ... <span class="comment"># 根据传入的张量和子模块计算返回张量</span></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p><ul><li><strong>init\</strong>方法初始化整个模型</li><li>super(Model, self).<strong>init\</strong>():调用父类nn.Module的初始化方法，初始化必要的变量和参数</li><li>定义前向传播模块</li></ul><span id="more"></span><h3 id="1-2-线性回归的实现"><a href="#1-2-线性回归的实现" class="headerlink" title="1.2 线性回归的实现"></a>1.2 线性回归的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ndim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.ndim = ndim<span class="comment">#输入的特征数</span></span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.randn(ndim, <span class="number">1</span>)) <span class="comment"># 定义权重</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>)) <span class="comment"># 定义偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 定义线性模型 y = Wx + b</span></span><br><span class="line">        <span class="keyword">return</span> x.mm(self.weight) + self.bias</span><br></pre></td></tr></table></figure><ul><li>定义权重和偏置self.weight和self.bias。采用标准正态分布torch.randn进行初始化。</li><li>self.weight和self.bias是模型的参数，使用nn.Parameter包装，表示将这些初始化的张量转换为模型的参数。只有参数才可以进行优化（被优化器访问到）</li></ul><p>实例化方法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm = LinearModel(<span class="number">5</span>) <span class="comment"># 定义线性回归模型，特征数为5</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">5</span>) <span class="comment"># 定义随机输入，迷你批次大小为4</span></span><br><span class="line">lm(x) <span class="comment"># 得到每个迷你批次的输出</span></span><br></pre></td></tr></table></figure><ol><li>使用model.named_parameters()或者model.parameters()获取模型参数的生成器。区别是前者包含参数名和对应的张量值，后者只含有张量值。</li><li>优化器optimzer直接接受参数生成器作为参数，反向传播时根据梯度来优化生成器里的所有张量。</li><li>model.train()的作用是启用 Batch Normalization 和 Dropout。model.eval()的作用是不启用 Batch Normalization 和 Dropout。</li><li>named_buffers和buffers获取张量的缓存（不参与梯度传播但是会被更新的参数，例如BN的均值和方差）register_buffers可以加入这种张量</li><li>使用apply递归地对子模块进行函数应用（可以是匿名函数lambda）</li></ol><p>对于model.train()和model.eval()用法和区别进一步可以参考：<a href="https://zhuanlan.zhihu.com/p/357075502?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《Pytorch：model.train()和model.eval()用法和区别》</a></p><p>对于上面定义的线性模型来举例:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm.named_parameters() <span class="comment"># 获取模型参数（带名字）的生成器</span></span><br><span class="line"><span class="comment">#&lt;generator object Module.named_parameters at 0x00000279A1809510&gt;</span></span><br><span class="line"><span class="built_in">list</span>(lm.named_parameters()) <span class="comment"># 转换生成器为列表</span></span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;weight&#x27;</span>,</span><br><span class="line">  Parameter containing:</span><br><span class="line">  tensor([[-<span class="number">1.0407</span>],</span><br><span class="line">          [ <span class="number">0.0427</span>],</span><br><span class="line">          [ <span class="number">0.4069</span>],</span><br><span class="line">          [-<span class="number">0.7064</span>],</span><br><span class="line">          [-<span class="number">1.1938</span>]], requires_grad=<span class="literal">True</span>)),</span><br><span class="line"> (<span class="string">&#x27;bias&#x27;</span>,</span><br><span class="line">  Parameter containing:</span><br><span class="line">  tensor([-<span class="number">0.7493</span>], requires_grad=<span class="literal">True</span>))]</span><br><span class="line">  </span><br><span class="line">lm.parameters() <span class="comment"># 获取模型参数（不带名字）的生成器</span></span><br><span class="line"><span class="built_in">list</span>(lm.parameters()) <span class="comment"># 转换生成器为列表</span></span><br><span class="line"></span><br><span class="line">[Parameter containing:</span><br><span class="line"> tensor([[-<span class="number">1.0407</span>],</span><br><span class="line">         [ <span class="number">0.0427</span>],</span><br><span class="line">         [ <span class="number">0.4069</span>],</span><br><span class="line">         [-<span class="number">0.7064</span>],</span><br><span class="line">         [-<span class="number">1.1938</span>]], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> Parameter containing:</span><br><span class="line"> tensor([-<span class="number">0.7493</span>], requires_grad=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line">lm.cuda()<span class="comment">#模型参数转到GPU上</span></span><br><span class="line"><span class="built_in">list</span>(lm.parameters()) <span class="comment"># 转换生成器为列表</span></span><br></pre></td></tr></table></figure></p><ul><li><p>model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是在训练中随机去除神经元，用一部分网络连接来训练更新参数。如果被删除的神经元（叉号）是唯一促成正确结果的神经元。一旦我们移除了被删除的神经元，它就迫使其他神经元训练和学习如何在没有被删除神经元的情况下保持准确。这种dropout提高了最终测试的性能，但它对训练期间的性能产生了负面影响，因为网络是不全的</p></li><li><p>在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变（model.eval()时，框架会自动把BN和Dropout固定住，不会取平均，直接使用在训练阶段已经学出的mean和var值）</p></li></ul><h2 id="二、损失函数"><a href="#二、损失函数" class="headerlink" title="二、损失函数"></a>二、损失函数</h2><p>pytorch损失函数有两种形式：</p><ul><li>torch.nn.functional调用的函数形式&lt;/font&gt;.传入神经网络预测值和目标值来计算损失函数</li><li>torch.nn库里面的模块形式&lt;/font&gt;。新建模块的实例，调用模块化方法计算<br>最后输出的是标量，对一个批次的损失函数的值有两种归约方式：求和和求均值。</li></ul><ol><li>回归问题一般调用torch.nn.MSEloss模块。使用默认参数创建实例，输出的是损失函数对一个batch的均值。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">mse = nn.MSELoss() <span class="comment"># 初始化平方损失函数模块</span></span><br><span class="line"><span class="comment">#class torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#x27;mean&#x27;)</span></span><br><span class="line">t1 = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机生成张量t1</span></span><br><span class="line">tensor([ <span class="number">0.6582</span>,  <span class="number">0.0529</span>, -<span class="number">0.9693</span>, -<span class="number">0.9313</span>, -<span class="number">0.7288</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">t2 = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机生成张量t2</span></span><br><span class="line">tensor([ <span class="number">0.8095</span>, -<span class="number">0.3384</span>, -<span class="number">0.9510</span>,  <span class="number">0.1581</span>, -<span class="number">0.1863</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mse(t1, t2) <span class="comment"># 计算张量t1和t2之间的平方损失函数</span></span><br><span class="line">tensor(<span class="number">0.3315</span>, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure><ol><li>二分类问题:<ul><li>使用  torch.nn.BCELoss&lt;/font&gt;二分类交叉熵损失函数。输出的是损失函数的均值。接受两个张量。前一个是正分类标签的概率值（预测值必须经过 <font color='deeppink'> nn.Sigmoid()</font>输出概率），后者是二分类标签的目标数据值（1是正分类）。两个都必须是浮点类型。</li><li>torch.nn.BCEWithLogitsLoss&lt;/font&gt;：自动在损失函数内部实现sigmoid函数的功能，可以增加计算的稳定性。因为概率接近0或1的时候，二分类交叉熵损失函数接受的对数部分容易接近无穷大，造成数值不稳定。使用torch.nn.BCEWithLogitsLoss可以避免此种情况</li></ul></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1s = torch.sigmoid(t1)</span><br><span class="line">t2 = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">5</span>, )).<span class="built_in">float</span>() <span class="comment"># 随机生成0，1的整数序列，并转换为浮点数</span></span><br><span class="line">bce=torch.nn.BCELoss()</span><br><span class="line">(t1s, t2) <span class="comment"># 计算二分类的交叉熵</span></span><br><span class="line">bce_logits = nn.BCEWithLogitsLoss() <span class="comment"># 使用交叉熵对数损失函数</span></span><br><span class="line">bce_logits(t1, t2) <span class="comment"># 计算二分类的交叉熵，可以发现和前面的结果一致</span></span><br></pre></td></tr></table></figure><ol><li>多分类问题<ul><li>torch.nn.NLLLoss&lt;/font&gt;:负对数损失函数，计算之前预测值必须经过softmax函数输出概率值（<font color='deeppink'> torch.nn.functional.log_softmax或torch.nn.LogSoftmax(dim=dim)函数</font>）</li><li>torch.nn.CrossEntropyLoss&lt;/font&gt;:交叉熵损失函数，内部已经整合softmax输出概率，不需要再另外对预测值进行softmax计算。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">N=<span class="number">10</span> <span class="comment"># 定义分类数目</span></span><br><span class="line">t1 = torch.randn(<span class="number">5</span>, N, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机产生预测张量</span></span><br><span class="line">t2 = torch.randint(<span class="number">0</span>, N, (<span class="number">5</span>, )) <span class="comment"># 随机产生目标张量</span></span><br><span class="line">t1s = torch.nn.functional.log_softmax(t1, -<span class="number">1</span>) <span class="comment"># 计算预测张量的LogSoftmax</span></span><br><span class="line">nll = nn.NLLLoss() <span class="comment"># 定义NLL损失函数</span></span><br><span class="line">nll(t1s, t2) <span class="comment"># 计算损失函数</span></span><br><span class="line">ce = nn.CrossEntropyLoss() <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">ce(t1, t2) <span class="comment"># 计算损失函数，可以发现和NLL损失函数的结果一致</span></span><br></pre></td></tr></table></figure><h2 id="三、优化器"><a href="#三、优化器" class="headerlink" title="三、优化器"></a>三、优化器</h2><blockquote><p>完整文档参考：<a href="https://pytorch.org/docs/stable/optim.html">《torch.optim 》</a></p><h3 id="3-1-SGD优化器"><a href="#3-1-SGD优化器" class="headerlink" title="3.1.  SGD优化器"></a>3.1.  SGD优化器</h3><p>以波士顿房价问题举例，构建SGD优化器。第一个参数是模型的参数生成器（lm.parameters()调用），第二个参数是学习率。训练时通过 optim.step()进行优化计算。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">lm = LinearModel(<span class="number">13</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(lm.parameters(), lr=<span class="number">1e-6</span>) <span class="comment"># 定义优化器</span></span><br><span class="line">data = torch.tensor(boston[<span class="string">&quot;data&quot;</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">target = torch.tensor(boston[<span class="string">&quot;target&quot;</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    predict = lm(data) <span class="comment"># 输出模型预测结果</span></span><br><span class="line">    loss = criterion(predict, target) <span class="comment"># 输出损失函数</span></span><br><span class="line">    <span class="keyword">if</span> step <span class="keyword">and</span> step % <span class="number">1000</span> == <span class="number">0</span> :</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loss: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(loss.item()))</span><br><span class="line">    optim.zero_grad() <span class="comment"># 清零梯度</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optim.step()</span><br></pre></td></tr></table></figure></blockquote></li></ul></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.SGD(params,lr=&lt;required parameter&gt;,momentum=<span class="number">0</span>,</span><br><span class="line">    dampening=<span class="number">0</span>,weight_decay=<span class="number">0</span>,nesterov=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#momentum：动量因子</span></span><br><span class="line"><span class="comment">#dampening：动量抑制因子</span></span><br><span class="line"><span class="comment">#nesterov：设为True时使用nesterov动量</span></span><br></pre></td></tr></table></figure><h3 id="3-2-Adagrad优化器"><a href="#3-2-Adagrad优化器" class="headerlink" title="3.2 Adagrad优化器"></a>3.2 Adagrad优化器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.Adagrad(</span><br><span class="line">    params,lr=<span class="number">0.01</span>,lr_decay=<span class="number">0</span>,weight_decay=<span class="number">0</span>,</span><br><span class="line">    initial_accumulator_value=<span class="number">0</span>,eps=<span class="number">1e-10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#lr_decay：学习率衰减速率</span></span><br><span class="line"><span class="comment">#weight_decay：权重衰减</span></span><br><span class="line"><span class="comment">#initial_accumulator_value：梯度初始累加值</span></span><br></pre></td></tr></table></figure><h3 id="3-3-分层学习率"><a href="#3-3-分层学习率" class="headerlink" title="3.3 分层学习率"></a>3.3 分层学习率</h3><p>对不同参数指定不同的学习率：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: model.base.parameters()&#125;,</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>这意味着model.base的参数将使用 的默认学习率1e-2， model.classifier的参数将使用 的学习率1e-3，0.9所有参数将使用动量 。</p><h3 id="3-4-学习率调度器torch-optim-lr-scheduler"><a href="#3-4-学习率调度器torch-optim-lr-scheduler" class="headerlink" title="3.4 学习率调度器torch.optim.lr_scheduler"></a>3.4 学习率调度器torch.optim.lr_scheduler</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment">#没经过30的个迭代周期，学习率降为原来的0.1倍。每个epoch之后学习率都会衰减。</span></span><br><span class="line"><span class="keyword">or</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>大多数学习率调度器都可以称为背靠背（也称为链式调度器）。结果是每个调度器都被一个接一个地应用于前一个调度器获得的学习率。</p><p>例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = [Parameter(torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>))]</span><br><span class="line">optimizer = SGD(model, <span class="number">0.1</span>)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler2 = MultiStepLR(optimizer, milestones=[<span class="number">30</span>,<span class="number">80</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    scheduler1.step()</span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure><h2 id="四-、数据加载torch-utils-data"><a href="#四-、数据加载torch-utils-data" class="headerlink" title="四 、数据加载torch.utils.data"></a>四 、数据加载torch.utils.data</h2><blockquote><p>本节也可以参考<a href="https://blog.csdn.net/qq_56591814/article/details/120467968">《编写transformers的自定义pytorch训练循环（Dataset和DataLoader解析和实例代码）》</a></p></blockquote><h3 id="4-1-DataLoader参数"><a href="#4-1-DataLoader参数" class="headerlink" title="4.1 DataLoader参数"></a>4.1 DataLoader参数</h3><p>PyTorch 数据加载实用程序的核心是torch.utils.data.DataLoader 类。它代表一个 Python 可迭代的数据集，支持：</p><ul><li>map类型和可迭代类型数据集</li><li>自定义数据加载顺序</li><li>自动batching</li><li>单进程和多进程数据加载</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span> ，num_workers=<span class="number">4</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">False</span>，num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>下面看看dataloader代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 worker_init_fn=<span class="literal">None</span>,*, prefetch_factor=<span class="number">2</span>,persistent_workers=<span class="literal">False</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">dataset</span> = <span class="title">dataset</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">batch_size</span> = <span class="title">batch_size</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">num_workers</span> = <span class="title">num_workers</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">collate_fn</span> = <span class="title">collate_fn</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">pin_memory</span> = <span class="title">pin_memory</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">drop_last</span> = <span class="title">drop_last</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">timeout</span> = <span class="title">timeout</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">worker_init_fn</span> = <span class="title">worker_init_fn</span></span></span><br><span class="line"><span class="function"></span></span><br></pre></td></tr></table></figure></p><ul><li>dataset:Dataset类，PyTorch已有的数据读取接口，决定数据从哪里读取及如何读取；</li><li>batch_size：批大小；默认1</li><li>num_works:是否多进程读取数据；默认0使用主进程来导入数据。大于0则多进程导入数据，加快数据导入速度</li><li>shuffle：每个epoch是否乱序；默认False。输入数据的顺序打乱，是为了使数据更有独立性，但如果数据是有序列特征的，就不要设置成True了。一般shuffle训练集即可。</li><li>drop_last:当样本数不能被batchsize整除时，是否舍弃最后一批数据；</li><li>collate_fn:将得到的数据整理成一个batch。默认设置是False。如果设置成True，系统会在返回前会将张量数据（Tensors）复制到CUDA内存中。</li><li>batch_sampler，批量采样，和batch_size、shuffle等参数是互斥的，一般采用默认None。batch_sampler，但每次返回的是一批数据的索引（注意：不是数据），应该是每次输入网络的数据是随机采样模式，这样能使数据更具有独立性质。所以，它和一捆一捆按顺序输入，数据洗牌，数据采样，等模式是不兼容的。</li><li>sampler，默认False。根据定义的策略从数据集中采样输入。如果定义采样规则，则洗牌（shuffle）设置必须为False。</li><li>pin_memory，内存寄存，默认为False。在数据返回前，是否将数据复制到CUDA内存中。</li><li>timeout，是用来设置数据读取的超时时间的，但超过这个时间还没读取到数据的话就会报错。</li><li>worker_init_fn（数据类型 callable），子进程导入模式，默认为Noun。在数据导入前和步长结束后，根据工作子进程的ID逐个按顺序导入数据。</li></ul><p>想用随机抽取的模式加载输入，可以设置 sampler 或 batch_sampler。如何定义抽样规则，可以看sampler.py脚本，或者这篇帖子：<a href="https://blog.csdn.net/aiwanghuan5017/article/details/102147809">《一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系》</a></p><h3 id="4-2-两种数据集类型"><a href="#4-2-两种数据集类型" class="headerlink" title="4.2 两种数据集类型"></a>4.2 两种数据集类型</h3><p>DataLoader 构造函数最重要的参数是dataset，它表示要从中加载数据的数据集对象。PyTorch 支持两种不同类型的数据集：</p><ul><li>map-style datasets：映射类型数据集。每个数据有一个对应的索引，通过输入具体的索引，就可以得到对应的数据</li></ul><p>其构造方法如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># index: 数据缩索引（整数，范围为0到数据数目-1）</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># 返回数据张量</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 返回数据的数目</span></span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>主要重写两个方法：</p><ul><li><strong>getitem\</strong>:python内置的操作符方法，对应索引操作符[]。通过输入整数索引，返回具体某一条数据。具体的内部逻辑根据数据集类型决定</li><li><strong>len\</strong>：返回数据总数</li></ul><p>更具体的可以参考<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">《torch.utils.data.Dataset》</a></p><ul><li>iterable-style datasets：可迭代数据集：实现<strong>iter</strong>()协议的子类的实例。不需要<strong>getitem\</strong>和<strong>len\</strong>方法，其实类似python的迭代器</li><li>不同于映射，索引之间相互独立。多线程载入时，多线程独立分配索引。迭代中索引右前后关系，需要考虑如何分割数据。</li><li>这种类型的数据集特别适用于随机读取代价高昂甚至不可能的情况，以及批量大小取决于获取的数据的情况。</li><li>在调用iter(dataset)时可以返回从数据库、远程服务器甚至实时生成的日志中读取的数据流</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, start, end</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyIterableDataset).__init__()</span><br><span class="line">            <span class="keyword">assert</span> end &gt; start, \</span><br><span class="line"><span class="string">&quot;this example code only works with end &gt;= start&quot;</span></span><br><span class="line">            self.start = start</span><br><span class="line">            self.end = end</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">        <span class="keyword">if</span> worker_info <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 单进程数据载入</span></span><br><span class="line">            iter_start = self.start</span><br><span class="line">            iter_end = self.end</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 多进程，分割数据</span></span><br><span class="line">           <span class="comment">#根据不同工作进程序号worker_id，设置不同进程数据迭代器取值范围。保证不同进程获取不同的迭代器。</span></span><br><span class="line">            per_worker = <span class="built_in">int</span>(math.ceil((self.end - self.start) \</span><br><span class="line">                            / <span class="built_in">float</span>(worker_info.num_workers)))</span><br><span class="line">            worker_id = worker_info.<span class="built_in">id</span></span><br><span class="line">            iter_start = self.start + worker_id * per_worker</span><br><span class="line">            iter_end = <span class="built_in">min</span>(iter_start + per_worker, self.end)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(iter_start, iter_end))</span><br></pre></td></tr></table></figure><p>更多详细信息，请参阅<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset">IterableDataset</a></p><h3 id="4-3-数据加载顺序和-Sampler"><a href="#4-3-数据加载顺序和-Sampler" class="headerlink" title="4.3 数据加载顺序和 Sampler"></a>4.3 数据加载顺序和 Sampler</h3><ul><li>对于iterable-style datasets，数据加载顺序完全由用户定义的 iterable 控制。这允许更容易地实现块读取和动态批量大小（例如，通过每次产生批量样本）。</li><li>map 类型数据，torch.utils.data.Sampler 类用于指定数据加载中使用的索引/键的序列。它们表示数据集索引上的可迭代对象。例如，在随机梯度下降 (SGD) 的常见情况下，Sampler可以随机排列索引列表并一次产生一个，或者为小批量 SGD 产生少量索引。</li></ul><p>将根据shufflea的参数自动构建顺序或混洗采样器DataLoader。或者，用户可以使用该sampler参数来指定一个自定义Sampler对象，该对象每次都会生成下一个要获取的索引/键。</p><p>一次Sampler生成批量索引列表的自定义可以作为batch_sampler参数传递。也可以通过batch_size和 drop_last参数启用自动批处理。</p><h3 id="4-4-批处理和collate-fn"><a href="#4-4-批处理和collate-fn" class="headerlink" title="4.4 批处理和collate_fn"></a>4.4 批处理和collate_fn</h3><p>经由参数 batch_size，drop_last和batch_sampler，DataLoader支持批处理数据<br>当启用自动批处理时，每次都会使用数据样本列表调用 collat​​e_fn。预计将输入样本整理成一个批次，以便从数据加载器迭代器中产生。</p><p>例如，如果每个数据样本由一个 3 通道图像和一个完整的类标签组成，即数据集的每个元素返回一个元组 (image, class_index)，则默认 collat​​e_fn 将此类元组的列表整理成单个元组一个批处理图像张量和一个批处理类标签张量。特别是，默认 collat​​e_fn 具有以下属性：</p><ul><li><p>它总是预先添加一个新维度作为批次维度。</p></li><li><p>它会自动将 NumPy 数组和 Python 数值转换为 PyTorch 张量。</p></li><li><p>它保留了数据结构，例如，如果每个样本是一个字典，它输出一个具有相同键集但批量张量作为值的字典（如果值不能转换为张量，则为列表）。列表 s、元组 s、namedtuple s 等也是如此。</p></li></ul><p>用户可以使用自定义 collat​​e_fn 来实现自定义批处理，例如，沿着除第一个维度之外的维度进行整理，填充各种长度的序列，或添加对自定义数据类型的支持。</p><h2 id="五、模型的保存和加载"><a href="#五、模型的保存和加载" class="headerlink" title="五、模型的保存和加载"></a>五、模型的保存和加载</h2><h3 id="5-1-模块、张量的序列化和反序列化"><a href="#5-1-模块、张量的序列化和反序列化" class="headerlink" title="5.1 模块、张量的序列化和反序列化"></a>5.1 模块、张量的序列化和反序列化</h3><ul><li>PyTorch模块和张量本质是torch.nn.Module和torch.tensor类的实例。PyTorch自带了一系列方法， 可以将这些类的实例转化成字成串&lt;/font &gt;。所以这些实例可以通过Python序列化方法进行序列化和反序列化。</li><li>张量的序列化： 本质上是把张量的信息，包括数据类型和存储位置、以及携带的数据，转换为字符串，然后使用Python自带的文件IO函数进行存储&lt;/font &gt;。当然也是这个过程是可逆的。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(obj, f, pickle_module=pickle, pickle_protocol=<span class="number">2</span>)</span><br><span class="line">torch.load(f, map_location=<span class="literal">None</span>, pickle_module=pickle, **pickle_load_args)</span><br></pre></td></tr></table></figure><ul><li>torch.save参数<ol><li>pytorch中可以被序列化的对象，包括模型和张量</li><li>存储文件路径</li><li>序列化的库，默认pickle</li><li>pickle协议，版本0-4</li></ol></li><li><p>torch.load函数</p><ol><li>文件路径</li><li>张量存储位置的映射（默认CPU，也可以是GPU）</li><li><p>pickle参数，和save时一样。</p><p>如果模型保存在GPU中，而加载的当前计算机没有GPU，或者GPU设备号不对，可以使用map_location=’cpu’。</p></li></ol></li></ul><p>PyTorch默认有两种模型保存方式：</p><ul><li>保存模型的实例</li><li>保存模型的状态字典state_dict：state_dict包含模型所有参数名和对应的张量，通过调用load_state_dict可以获取当前模型的状态字典,载入模型参数。<h3 id="5-2-state-dict保存模型参数"><a href="#5-2-state-dict保存模型参数" class="headerlink" title="5.2 state_dict保存模型参数"></a>5.2 state_dict保存模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure></li><li>保存模型状态字典state_dict ：只保存模型学习到的参数，与模块关联较小，即不依赖版本。</li><li>PyTorch 中最常见的模型保存使‘.pt’或者是‘.pth’作为模型文件扩展名</li><li>在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 层为评<br>估模式。如果不这么做，可能导致 模型推断结果不一致</li></ul><h3 id="5-2-保存-加载完整模型"><a href="#5-2-保存-加载完整模型" class="headerlink" title="5.2 保存/加载完整模型"></a>5.2 保存/加载完整模型</h3><p>以 Python `pickle 模块的方式来保存模型。这种方法的缺点是：</p><ul><li>序列化数据受 限于某种特殊的类而且需要确切的字典结构。当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。</li><li>PyTorch模块的实现依赖于具体的版本。所依一个版本保存的模块序列化文件，在另一个版本可能无法载入。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br><span class="line"><span class="comment"># 模型类必须在此之前被定义</span></span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h3 id="5-3-Checkpoint-用于推理-继续训练"><a href="#5-3-Checkpoint-用于推理-继续训练" class="headerlink" title="5.3  Checkpoint 用于推理/继续训练"></a>5.3  Checkpoint 用于推理/继续训练</h3><ul><li>在训练时，不仅要保存模型相关的信息，还要保存优化器相关的信息。因为可能要从检查点出发，继续训练。所以可以保存优化器本身的状态字典，存储包括当前学习率、调度器等信息。</li><li>最新记录的训练损失，外部的 torch.nn.Embedding 层等等都可以保存。</li><li>PyTorch 中常见的保存checkpoint 是使用 .tar 文件扩展名。</li><li>要加载项目，首先需要初始化模型和优化器，然后使用 torch.load() 来加载本地字典</li></ul><p>一个模型的检查点代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line"><span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line"><span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line"><span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">...</span><br><span class="line">&#125;, PATH)</span><br></pre></td></tr></table></figure><br>加载</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">model.<span class="built_in">eval</span>()<span class="comment">#或model.train()</span></span><br></pre></td></tr></table></figure><p>或者是：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">save_info = &#123; <span class="comment"># 保存的信息</span></span><br><span class="line">    <span class="string">&quot;iter_num&quot;</span>: iter_num,  <span class="comment"># 迭代步数 </span></span><br><span class="line">    <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict(), <span class="comment"># 优化器的状态字典</span></span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model.state_dict(), <span class="comment"># 模型的状态字典</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 保存信息</span></span><br><span class="line">torch.save(save_info, save_path)</span><br><span class="line"><span class="comment"># 载入信息</span></span><br><span class="line">save_info = torch.load(save_path)</span><br><span class="line">optimizer.load_state_dict(save_info[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">model.load_state_dict(sae_info[<span class="string">&quot;model&quot;</span>])</span><br></pre></td></tr></table></figure><h3 id="5-4-在一个文件中保存多个模型"><a href="#5-4-在一个文件中保存多个模型" class="headerlink" title="5.4 在一个文件中保存多个模型"></a>5.4 在一个文件中保存多个模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line"><span class="string">&#x27;modelA_state_dict&#x27;</span>: modelA.state_dict(),</span><br><span class="line"><span class="string">&#x27;modelB_state_dict&#x27;</span>: modelB.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizerA_state_dict&#x27;</span>: optimizerA.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizerB_state_dict&#x27;</span>: optimizerB.state_dict(),</span><br><span class="line">...</span><br><span class="line">&#125;, PATH)</span><br></pre></td></tr></table></figure><p>加载<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">modelA = TheModelAClass(*args, **kwargs)</span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)</span><br><span class="line">optimizerA = TheOptimizerAClass(*args, **kwargs)</span><br><span class="line">optimizerB = TheOptimizerBClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">modelA.load_state_dict(checkpoint[<span class="string">&#x27;modelA_state_dict&#x27;</span>])</span><br><span class="line">modelB.load_state_dict(checkpoint[<span class="string">&#x27;modelB_state_dict&#x27;</span>])</span><br><span class="line">optimizerA.load_state_dict(checkpoint[<span class="string">&#x27;optimizerA_state_dict&#x27;</span>])</span><br><span class="line">optimizerB.load_state_dict(checkpoint[<span class="string">&#x27;optimizerB_state_dict&#x27;</span>])</span><br><span class="line">modelA.<span class="built_in">eval</span>()</span><br><span class="line">modelB.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><br>当保存一个模型由多个 torch.nn.Modules 组成时，例如GAN(对抗生成网络)、sequence-to-<br>sequence (序列到序列模型), 或者是多个模 型融合, 可以采用与保存常规检查点相同的方法。<br>换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，可以通 过简单地<br>将它们附加到字典的方式来保存任何其他项目，这样有助于恢复训练。</p><h2 id="六、TensorBoard的安装和使用"><a href="#六、TensorBoard的安装和使用" class="headerlink" title="六、TensorBoard的安装和使用"></a>六、TensorBoard的安装和使用</h2><blockquote><p>pip install tensorflow-tensorboard<br>pip install tensorboard<br>安装完之后import tensorboard时报错ImportError: TensorBoard logging requires TensorBoard version 1.15 or above<br>试了几种方法。最后关掉ipynb文件，新建一个ipynb文件复制代码运行就好了。</p></blockquote><h3 id="6-1-TensorBoard用法示例："><a href="#6-1-TensorBoard用法示例：" class="headerlink" title="6.1 TensorBoard用法示例："></a>6.1 TensorBoard用法示例：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义线性回归模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ndim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.ndim = ndim</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.randn(ndim, <span class="number">1</span>)) <span class="comment"># 定义权重</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>)) <span class="comment"># 定义偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 定义线性模型 y = Wx + b</span></span><br><span class="line">        <span class="keyword">return</span> x.mm(self.weight) + self.bias</span><br><span class="line"></span><br><span class="line">boston = load_boston()</span><br><span class="line">lm = LinearModel(<span class="number">13</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(lm.parameters(), lr=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">data = torch.tensor(boston[<span class="string">&quot;data&quot;</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">target = torch.tensor(boston[<span class="string">&quot;target&quot;</span>], dtype=torch.float32)</span><br><span class="line">writer = SummaryWriter() <span class="comment"># 构造摘要生成器，定义TensorBoard输出类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    predict = lm(data)</span><br><span class="line">    loss = criterion(predict, target)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;Loss/train&quot;</span>, loss, step) <span class="comment"># 输出损失函数</span></span><br><span class="line">    writer.add_histogram(<span class="string">&quot;Param/weight&quot;</span>, lm.weight, step) <span class="comment"># 输出权重直方图</span></span><br><span class="line">    writer.add_histogram(<span class="string">&quot;Param/bias&quot;</span>, lm.bias, step) <span class="comment"># 输出偏置直方图</span></span><br><span class="line">    <span class="keyword">if</span> step <span class="keyword">and</span> step % <span class="number">1000</span> == <span class="number">0</span> :</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loss: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(loss.item()))</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>训练完之后，在当前目录下面会生成一个文件夹runs。runs下面还有一个文件夹（名字和训练时间、主机名称有关）</p><ul><li>from torch.utils.tensorboard import SummaryWriter是从tensorboard构造一个摘要写入器SummaryWriter。实例化之后调用实例化方法添加要写入摘要的张量信息。</li><li>add_scalar：添加标量数据，比如loss、acc等</li><li>add_histogram：添加直方图</li><li>add_graph()：创建Graphs，Graphs中存放了网络结构</li><li>运行tensorboard-logdir./run命令，启动tensorboard服务器。默认端口6006。访问<a href="http://127.0.0.1:6006可以看到tensorboard网页界面。">http://127.0.0.1:6006可以看到tensorboard网页界面。</a></li></ul><h3 id="6-2-具体函数"><a href="#6-2-具体函数" class="headerlink" title="6.2 具体函数"></a>6.2 具体函数</h3><h4 id="6-2-1-SummaryWriter"><a href="#6-2-1-SummaryWriter" class="headerlink" title="6.2.1 SummaryWriter"></a>6.2.1 SummaryWriter</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(log_dir=<span class="literal">None</span>, comment=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">   purge_step=<span class="literal">None</span>, max_queue=<span class="number">10</span>, flush_secs=<span class="number">120</span>, filename_suffix=<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>log_dir：tensorboard文件的存放路径，默认是创建runs文件夹</li><li>flush_secs：表示写入tensorboard文件的时间间隔</li><li>purge_step:可视化数据不是实时写入，而是有个队列。积累的数据超过队列限制的时候，触发数据文件写入。如果写入的可视化数据崩溃，purge_step步数之后的数据将会被舍弃</li><li>max_queue:写入磁盘之前内存中最多可以保留的事件（数据）的数量</li><li>filaname_suffix:可视化数据文件的后缀，默认为空字符串</li></ul><h4 id="6-2-2-add-scalar-和add-scalars"><a href="#6-2-2-add-scalar-和add-scalars" class="headerlink" title="6.2.2  add_scalar()和add_scalars()"></a>6.2.2  add_scalar()和add_scalars()</h4><ul><li>add_scalar()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalar(tag, scalar_value, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></li></ul><p>于在tensorboard中加入loss，其中常用参数有：</p><pre><code>- tag：不同图表的标签，如下图所示的Train_loss。- scalar_value：标签的值，浮点数- global_step：当前迭代步数，标签的x轴坐标- walltime：迭代时间函数。如果不传入，方法内部使用time.time()返回一个浮点数代表时间</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.add_scalar(<span class="string">&#x27;Train_loss&#x27;</span>, loss, (epoch*epoch_size + iteration))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/ee5a72c90ce742c981ba80b6c5cb4877.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>add_scalars()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalars(main_tag, tag_scalar_dict, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>和上一个方法类似，通过传入一个主标签（main_tag），然后传入键值对是标签和标量值的一个字典（tag_scalar_dict），对每个标量值进行显示。</li></ul><h4 id="6-2-3-add-histogram"><a href="#6-2-3-add-histogram" class="headerlink" title="6.2.3 add_histogram()"></a>6.2.3 add_histogram()</h4><p>显示张量分量的直方图和对应的分布<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_histogram(tag, values, global_step=<span class="literal">None</span>, bins=<span class="string">&#x27;tensorflow&#x27;</span>, walltime=<span class="literal">None</span>, max_bins=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p><ul><li>bins：产生直方图的方法，可以是tensorflow、auto、fd</li><li>max_bins:最大直方图分段数<h4 id="6-2-4-add-graph"><a href="#6-2-4-add-graph" class="headerlink" title="6.2.4 add_graph"></a>6.2.4 add_graph</h4>传入pytorch模块及输入，显示模块对应的计算图</li><li>model：pytorch模型</li><li>input_to_model：pytorch模型的输入</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> Cuda:</span><br><span class="line">    graph_inputs = torch.from_numpy(np.random.rand(<span class="number">1</span>,<span class="number">3</span>,input_shape[<span class="number">0</span>],input_shape[<span class="number">1</span>])).<span class="built_in">type</span>(torch.FloatTensor).cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    graph_inputs = torch.from_numpy(np.random.rand(<span class="number">1</span>,<span class="number">3</span>,input_shape[<span class="number">0</span>],input_shape[<span class="number">1</span>])).<span class="built_in">type</span>(torch.FloatTensor)</span><br><span class="line">writer.add_graph(model, (graph_inputs,))</span><br></pre></td></tr></table></figure><h4 id="6-2-5-add-pr-curve"><a href="#6-2-5-add-pr-curve" class="headerlink" title="6.2.5 add_pr_curve"></a>6.2.5 add_pr_curve</h4><p>显示准确率-召回率曲线（Prediction-Recall Curve）。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_pr_curve(tag, labels, predictions, global_step=<span class="literal">None</span>, num_thresholds=<span class="number">127</span>,</span><br><span class="line">    weights=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p><ul><li>labels：目标值</li><li>predictions：预测值</li><li>num_thresholds：曲线中间插值点数</li><li>weights：每个点的权重<h4 id="6-2-6-tensorboard-—logdir"><a href="#6-2-6-tensorboard-—logdir" class="headerlink" title="6.2.6 tensorboard —logdir="></a>6.2.6 tensorboard —logdir=</h4>完成tensorboard文件的生成后，可在命令行调用该文件，tensorboard网址。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#打开cmd命令</span></span><br><span class="line">tensorboard --logdir=.\Chapter2\runs --bind_all</span><br><span class="line"><span class="comment">#TensorBoard 2.2.2 at http://DESKTOP-OHLNREI:6006/ (Press CTRL+C to quit)</span></span><br></pre></td></tr></table></figure><h4 id="6-2-7-add-image、add-vide、add-audio、add-text"><a href="#6-2-7-add-image、add-vide、add-audio、add-text" class="headerlink" title="6.2.7  add_image、add_vide、add_audio、add_text"></a>6.2.7  add_image、add_vide、add_audio、add_text</h4><h3 id="6-3-tensorboard界面简介"><a href="#6-3-tensorboard界面简介" class="headerlink" title="6.3 tensorboard界面简介"></a>6.3 tensorboard界面简介</h3><p>右上方三个依次是：</p><ul><li>SCALARS：损失函数图像</li><li>DISTRIBUTIONS：权重分布（随时间）</li><li>HISTOGRAMS：权重直方图分布</li></ul><p><img src="https://img-blog.csdnimg.cn/39cf2f69f92948a1ab248ff51adb72f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="DISTRIBUTIONS"><br><img src="https://img-blog.csdnimg.cn/f9f9756f47954534a608cc9c6b6ba73b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="HISTOGRAMS"><br>权重分布和直方图应该是随着训练一直变化，直到分布稳定。如果一直没有变化，可能模型结构有问题或者反向传播有问题。</p><p>Scalars：这个面板是最常用的面板，主要用于将神经网络训练过程中的acc（训练集准确率）val_acc（验证集准确率），loss（损失值），weight（权重）等等变化情况绘制成折线图。</p><ul><li>Ignore outlines in chart scaling（忽略图表缩放中的轮廓），可以消除离散值</li><li>data downloadlinks：显示数据下载链接，用来下载图片</li><li>smoothing：图像的曲线平滑程度，值越大越平滑。每个mini-batch的loss不一定下降，smoothing越大时，代表平均的mini-batch越多。</li><li>Horizontal Axis：水平轴表示方式。<ul><li>STEP：表示迭代次数</li><li>RELATIVE：表示按照训练集和测试集的相对值</li><li>WALL：表示按照时间。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;@[toc]&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/265394674&quot;&gt;《PyTorch 学习笔记汇总（完结撒花）》&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;一、nn-Module&quot;&gt;&lt;a href=&quot;#一、nn-Module&quot; class=&quot;headerlink&quot; title=&quot;一、nn.Module&quot;&gt;&lt;/a&gt;一、nn.Module&lt;/h2&gt;&lt;h3 id=&quot;1-1-nn-Module的调用&quot;&gt;&lt;a href=&quot;#1-1-nn-Module的调用&quot; class=&quot;headerlink&quot; title=&quot;1.1 nn.Module的调用&quot;&gt;&lt;/a&gt;1.1 nn.Module的调用&lt;/h3&gt;&lt;p&gt;pytorch通过继承nn.Module类，定义子模块的实例化和前向传播，实现深度学习模型的搭建。其构建代码如下：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Model&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;nn.Module&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, *kargs&lt;/span&gt;):&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 定义类的初始化函数，...是用户的传入参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;super&lt;/span&gt;(Model, self).__init__()&lt;span class=&quot;comment&quot;&gt;#调用父类nn.Module的初始化方法&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ... &lt;span class=&quot;comment&quot;&gt;# 根据传入的参数来定义子模块&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, *kargs&lt;/span&gt;):&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 定义前向计算的输入参数，...一般是张量或者其他的参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ret = ... &lt;span class=&quot;comment&quot;&gt;# 根据传入的张量和子模块计算返回张量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; ret&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;init\&lt;/strong&gt;方法初始化整个模型&lt;/li&gt;
&lt;li&gt;super(Model, self).&lt;strong&gt;init\&lt;/strong&gt;():调用父类nn.Module的初始化方法，初始化必要的变量和参数&lt;/li&gt;
&lt;li&gt;定义前向传播模块&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="10月组队学习：Pytorch" scheme="https://zhxnlp.github.io/categories/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9APytorch/"/>
    
    
    <category term="Pytorch" scheme="https://zhxnlp.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python类与对象</title>
    <link href="https://zhxnlp.github.io/2021/11/17/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%E3%80%81%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
    <id>https://zhxnlp.github.io/2021/11/17/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%E3%80%81%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</id>
    <published>2021-11-17T15:58:12.000Z</published>
    <updated>2022-01-02T19:49:24.268Z</updated>
    
    <content type="html"><![CDATA[<p>@[toc]</p><h2 id="一、python的类与对象"><a href="#一、python的类与对象" class="headerlink" title="一、python的类与对象"></a>一、python的类与对象</h2><p>参考<a href="https://github.com/datawhalechina/team-learning-program/blob/master/PythonLanguage/13.%20%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1.md">《datawhale——PythonLanguage》</a>、<a href="https://www.runoob.com/python3/python3-class.html">《Python3 面向对象》</a></p><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><p>对象 = 属性 + 方法<br>对象是类的实例。换句话说，类主要定义对象的结构，然后我们以类为模板创建对象。类不但包含方法定义，而且还包含所有实例共享的数据</p><ul><li>类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。</li><li>对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。</li><li>方法：类中定义的函数&lt;/font&gt;。</li><li>类属性：类里面方法外面定义的变量称为类属性。&lt;/font&gt;类属性所属于类对象并且多个实例对象之间共享同一个类属性，说白了就是类属性所有的通过该类实例化的对象都能共享。</li></ul><span id="more"></span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>():</span></span><br><span class="line">    a = <span class="number">0</span>  <span class="comment"># 类属性</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, xx</span>):</span></span><br><span class="line">        <span class="comment"># 使用类属性可以通过 （类名.类属性）调用。</span></span><br><span class="line">        A.a = xx</span><br></pre></td></tr></table></figure><ul><li><p>局部变量：定义在方法中的变量，只作用于当前实例的类。&lt;/font&gt;</p></li><li><p>实例属性/变量：在类的声明中，属性是用变量来表示的。 实例属性和具体的某个实例对象有关系，只能在自己的对象里面使用，其他的对象不能直接使用，不同实例对象的实例属性不共享。实例变量用 self 修饰，因为self是谁调用，它的值就属于该对象。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 类名():</span></span><br><span class="line">    __init__(self)：</span><br><span class="line">        self.name = xx <span class="comment">#实例属性</span></span><br></pre></td></tr></table></figure></li><li>数据成员：类变量或者实例变量用于处理类及其实例对象的相关的数据。</li><li>方法重写：改写从父类继承的方法，满足子类的需求</li><li>继承：派生类（derived class）继承基类（base class）的字段和方法。所以子类自动共享父类之间数据和方法&lt;/font&gt;</li><li>多态：不同对象对同一方法响应不同的行动</li><li>实例化：创建一个类的实例，类的具体对象。</li></ul><h3 id="1-2-主要知识点"><a href="#1-2-主要知识点" class="headerlink" title="1.2 主要知识点"></a>1.2 主要知识点</h3><h4 id="1-2-1-类属性和实例属性区别"><a href="#1-2-1-类属性和实例属性区别" class="headerlink" title="1.2.1 类属性和实例属性区别"></a>1.2.1 类属性和实例属性区别</h4><ul><li>类属性：类外面，可以通过实例对象.类属性和类名.类属性进行调用。类里面，通过self.类属性和类名.类属性进行调用。</li><li>实例属性 ：类外面，可以通过实例对象.实例属性调用。类里面，通过self.实例属性调用。</li><li>实例属性就相当于局部变量。出了这个类或者这个类的实例对象，就没有作用了。</li><li>类属性就相当于类里面的全局变量，可以和这个类的所有实例对象共享。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建类对象</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    class_attr = <span class="number">100</span>  <span class="comment"># 类属性</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.sl_attr = <span class="number">100</span>  <span class="comment"># 实例属性</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;类对象.类属性的值:&#x27;</span>, Test.class_attr)  <span class="comment"># 调用类属性</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;self.类属性的值&#x27;</span>, self.class_attr)  <span class="comment"># 相当于把类属性 变成实例属性</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;self.实例属性的值&#x27;</span>, self.sl_attr)  <span class="comment"># 调用实例属性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = Test()</span><br><span class="line">a.func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类对象.类属性的值: 100</span></span><br><span class="line"><span class="comment"># self.类属性的值 100</span></span><br><span class="line"><span class="comment"># self.实例属性的值 100</span></span><br><span class="line"></span><br><span class="line">b = Test()</span><br><span class="line">b.func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类对象.类属性的值: 100</span></span><br><span class="line"><span class="comment"># self.类属性的值 100</span></span><br><span class="line"><span class="comment"># self.实例属性的值 100</span></span><br><span class="line"></span><br><span class="line">a.class_attr = <span class="number">200</span></span><br><span class="line">a.sl_attr = <span class="number">200</span></span><br><span class="line">a.func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类对象.类属性的值: 100</span></span><br><span class="line"><span class="comment"># self.类属性的值 200</span></span><br><span class="line"><span class="comment"># self.实例属性的值 200</span></span><br><span class="line"></span><br><span class="line">b.func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类对象.类属性的值: 100</span></span><br><span class="line"><span class="comment"># self.类属性的值 100</span></span><br><span class="line"><span class="comment"># self.实例属性的值 100</span></span><br><span class="line"></span><br><span class="line">Test.class_attr = <span class="number">300</span></span><br><span class="line">a.func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类对象.类属性的值: 300</span></span><br><span class="line"><span class="comment"># self.类属性的值 200</span></span><br><span class="line"><span class="comment"># self.实例属性的值 200</span></span><br><span class="line"></span><br><span class="line">b.func()</span><br><span class="line"><span class="comment"># 类对象.类属性的值: 300</span></span><br><span class="line"><span class="comment"># self.类属性的值 300</span></span><br><span class="line"><span class="comment"># self.实例属性的值 100</span></span><br></pre></td></tr></table></figure><p>注意：属性与方法名相同，属性会覆盖方法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">x</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;x_man&#x27;</span>)</span><br><span class="line"></span><br><span class="line">aa = A()</span><br><span class="line">aa.x()  <span class="comment"># x_man</span></span><br><span class="line">aa.x = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(aa.x)  <span class="comment"># 1</span></span><br><span class="line">aa.x()</span><br><span class="line"><span class="comment"># TypeError: &#x27;int&#x27; object is not callable</span></span><br></pre></td></tr></table></figure><h4 id="1-2-2-self-是什么？"><a href="#1-2-2-self-是什么？" class="headerlink" title="1.2.2 self 是什么？"></a>1.2.2 self 是什么？</h4><p>Python 的 self 相当于 C++ 的 this 指针。<br>在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self, 且为第一个参数，self 代表的是类的实例。&lt;/font&gt;（对应于该实例，即该对象本身）在调用方法时，我们无需明确提供与参数 self 相对应的参数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ball</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setName</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kick</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我叫%s,该死的，谁踢我...&quot;</span> % self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = Ball()</span><br><span class="line">a.setName(<span class="string">&quot;球A&quot;</span>)</span><br><span class="line">b = Ball()</span><br><span class="line">b.setName(<span class="string">&quot;球B&quot;</span>)</span><br><span class="line">a.kick()</span><br><span class="line"><span class="comment"># 我叫球A,该死的，谁踢我...</span></span><br><span class="line">b.kick()</span><br><span class="line"><span class="comment"># 我叫球B,该死的，谁踢我...</span></span><br></pre></td></tr></table></figure><h4 id="1-2-3-Python-的魔法方法init"><a href="#1-2-3-Python-的魔法方法init" class="headerlink" title="1.2.3  Python 的魔法方法init"></a>1.2.3  Python 的魔法方法<strong>init</strong></h4><p>类有一个名为<strong>init</strong>(self[, param1, param2…])的魔法方法，该方法在类实例化时会自动调用。&lt;/font&gt;</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ball</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kick</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我叫%s,该死的，谁踢我...&quot;</span> % self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = Ball(<span class="string">&quot;球A&quot;</span>)</span><br><span class="line">b = Ball(<span class="string">&quot;球B&quot;</span>)</span><br><span class="line">a.kick()</span><br><span class="line"><span class="comment"># 我叫球A,该死的，谁踢我...</span></span><br><span class="line">b.kick()</span><br><span class="line"><span class="comment"># 我叫球B,该死的，谁踢我...</span></span><br></pre></td></tr></table></figure><h4 id="1-2-4-继承和覆盖（super-init-）"><a href="#1-2-4-继承和覆盖（super-init-）" class="headerlink" title="1.2.4 继承和覆盖（super()._init_()）"></a>1.2.4 继承和覆盖（super()._<em>init_</em>()）</h4><p>派生类（derived class）继承基类（base class）的字段和方法。如果子类中定义与父类同名的方法或属性，则会自动覆盖父类对应的方法或属性。多继承时基类方法名相同，子类没有指定时，使用靠前的父类的方法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 类定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">people</span>:</span></span><br><span class="line">    <span class="comment"># 定义基本属性</span></span><br><span class="line">    name = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    age = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 定义私有属性,私有属性在类外部无法直接进行访问</span></span><br><span class="line">    __weight = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n, a, w</span>):</span></span><br><span class="line">        self.name = n</span><br><span class="line">        self.age = a</span><br><span class="line">        self.__weight = w</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 说: 我 %d 岁。&quot;</span> % (self.name, self.age))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单继承示例</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">student</span>(<span class="params">people</span>):</span></span><br><span class="line">    grade = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n, a, w, g</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构函数</span></span><br><span class="line">        people.__init__(self, n, a, w)</span><br><span class="line">        <span class="comment">#super().__init__()也可以，表示继承父类的init操作</span></span><br><span class="line">        self.grade = g</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 覆写父类的方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 说: 我 %d 岁了，我在读 %d 年级&quot;</span> % (self.name, self.age, self.grade))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s = student(<span class="string">&#x27;小马的程序人生&#x27;</span>, <span class="number">10</span>, <span class="number">60</span>, <span class="number">3</span>)</span><br><span class="line">s.speak()</span><br><span class="line"><span class="comment"># 小马的程序人生 说: 我 10 岁了，我在读 3 年级</span></span><br></pre></td></tr></table></figure><p>注意：如果上面的程序去掉：people.<strong>init</strong>(self, n, a, w)&lt;/font&gt;，则输出： 说: 我 0 岁了，我在读 3 年级，因为子类的构造方法把父类的构造方法覆盖了。如果继承的方法中含有父类init的属性但是没有在子类中实现，会报错。此时可以</p><ul><li>调用未绑定的父类方法base class.<strong>init</strong>(self,*kargs)</li><li>使用super函数super()._<em>init_</em>()</li></ul><p>==多继承时需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，Python 从左至右搜索，调用靠前的类的此方法。==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 另一个类，多重继承之前的准备</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Speaker</span>:</span></span><br><span class="line">    topic = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    name = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n, t</span>):</span></span><br><span class="line">        self.name = n</span><br><span class="line">        self.topic = t</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我叫 %s，我是一个演说家，我演讲的主题是 %s&quot;</span> % (self.name, self.topic))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多重继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sample01</span>(<span class="params">Speaker, Student</span>):</span></span><br><span class="line">    a = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n, a, w, g, t</span>):</span></span><br><span class="line">        Student.__init__(self, n, a, w, g)</span><br><span class="line">        Speaker.__init__(self, n, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法名同，默认调用的是在括号中排前地父类的方法</span></span><br><span class="line">test = Sample01(<span class="string">&quot;Tim&quot;</span>, <span class="number">25</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="string">&quot;Python&quot;</span>)</span><br><span class="line">test.speak()  </span><br><span class="line"><span class="comment"># 我叫 Tim，我是一个演说家，我演讲的主题是 Python</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sample02</span>(<span class="params">Student, Speaker</span>):</span></span><br><span class="line">    a = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n, a, w, g, t</span>):</span></span><br><span class="line">        Student.__init__(self, n, a, w, g)</span><br><span class="line">        Speaker.__init__(self, n, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法名同，默认调用的是在括号中排前地父类的方法</span></span><br><span class="line">test = Sample02(<span class="string">&quot;Tim&quot;</span>, <span class="number">25</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="string">&quot;Python&quot;</span>)</span><br><span class="line">test.speak()  </span><br><span class="line"><span class="comment"># Tim 说: 我 25 岁了，我在读 4 年级</span></span><br></pre></td></tr></table></figure><h4 id="1-2-5公有和私有"><a href="#1-2-5公有和私有" class="headerlink" title="1.2.5公有和私有"></a>1.2.5公有和私有</h4><p>变量名或函数名前加上“__”两个下划线，那么这个函数或变量就会为私有的了。</p><ul><li>私有属性在类外部无法直接进行访问。</li><li>私有方法只能在类的内部调用 ，不能在类的外部调用<br>【例子】类的私有属性实例<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JustCounter</span>:</span></span><br><span class="line">    __secretCount = <span class="number">0</span>  <span class="comment"># 私有变量</span></span><br><span class="line">    publicCount = <span class="number">0</span>  <span class="comment"># 公开变量</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__secretCount += <span class="number">1</span></span><br><span class="line">        self.publicCount += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(self.__secretCount)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">counter = JustCounter()</span><br><span class="line">counter.count()  <span class="comment"># 1</span></span><br><span class="line">counter.count()  <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(counter.publicCount)  <span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(counter._JustCounter__secretCount)  <span class="comment"># 2 Python的私有为伪私有</span></span><br><span class="line"><span class="built_in">print</span>(counter.__secretCount)  <span class="comment">## 报错，实例不能访问私有变量</span></span><br><span class="line"><span class="comment"># AttributeError: &#x27;JustCounter&#x27; object has no attribute &#x27;__secretCount&#x27;</span></span><br></pre></td></tr></table></figure>【例子】类的私有方法实例<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Site</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, url</span>):</span></span><br><span class="line">        self.name = name  <span class="comment"># public</span></span><br><span class="line">        self.__url = url  <span class="comment"># private</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">who</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;name  : &#x27;</span>, self.name)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;url : &#x27;</span>, self.__url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__foo</span>(<span class="params">self</span>):</span>  <span class="comment"># 私有方法</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;这是私有方法&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">self</span>):</span>  <span class="comment"># 公共方法</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;这是公共方法&#x27;</span>)</span><br><span class="line">        self.__foo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = Site(<span class="string">&#x27;老马的程序人生&#x27;</span>, <span class="string">&#x27;https://blog.csdn.net/LSGO_MYP&#x27;</span>)</span><br><span class="line">x.who()</span><br><span class="line"><span class="comment"># name  :  老马的程序人生</span></span><br><span class="line"><span class="comment"># url :  https://blog.csdn.net/LSGO_MYP</span></span><br><span class="line"></span><br><span class="line">x.foo()</span><br><span class="line"><span class="comment"># 这是公共方法</span></span><br><span class="line"><span class="comment"># 这是私有方法</span></span><br><span class="line">x.__foo()<span class="comment">#报错，外部不能使用私有方法</span></span><br><span class="line"><span class="comment"># AttributeError: &#x27;Site&#x27; object has no attribute &#x27;__foo&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="1-3魔法方法"><a href="#1-3魔法方法" class="headerlink" title="1.3魔法方法"></a>1.3魔法方法</h3>魔法方法总是被双下划线包围，例如<strong>init</strong>。魔法方法的“魔力”体现在它们总能够在适当的时候被自动调用。<br>魔法方法的第一个参数应为cls（类方法） 或者self（实例方法）：</li><li>cls：代表一个类的名称</li><li>self：代表一个实例对象的名称</li></ul><h4 id="1-3-1-init-self-…"><a href="#1-3-1-init-self-…" class="headerlink" title="1.3.1 init\(self[, …])"></a>1.3.1 <strong>init\</strong>(self[, …])</h4><p><strong>init\</strong>(self[, …]) :构造器，当一个实例被创建的时候调用的初始化方法</p><h4 id="1-3-2-new-cls-…"><a href="#1-3-2-new-cls-…" class="headerlink" title="1.3.2 new\(cls[, …])"></a>1.3.2 <strong>new\</strong>(cls[, …])</h4><p><strong>new</strong>方法主要是当你继承一些不可变的 class 时（比如int, str, tuple）， 提供给你一个自定义这些类的实例化过程的途径。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">在一个对象实例化的时候所调用的第一个方法，在调用__init__初始化前，先调用__new__。</span><br><span class="line">__new__至少要有一个参数cls，代表要实例化的类，此参数在实例化时由 Python 解释器自动提供，后面的参数直接传递给__init__。</span><br><span class="line">__new__对当前类进行了实例化，并将实例返回，传给__init__的self。</span><br><span class="line">但是，执行了__new__，并不一定会进入__init__，只有__new__返回了当前类cls的实例，当前类的__init__才会进入。</span><br></pre></td></tr></table></figure></p><h4 id="1-3-3-del-self"><a href="#1-3-3-del-self" class="headerlink" title="1.3.3 del\(self)"></a>1.3.3 <strong>del\</strong>(self)</h4><ul><li>析构器，当一个对象将要被系统回收之时调用的方法。</li></ul><p>大部分时候，Python 的 ARC(自动引用计数，用来回收对象所占用的空间） 都能准确、高效地回收系统中的每个对象。但如果系统中出现循环引用的情况，比如对象 a 持有一个实例变量引用对象 b，而对象 b 又持有一个实例变量引用对象 a，此时两个对象的引用计数都是 1，而实际上程序已经不再有变量引用它们，系统应该回收它们，此时 Python 的垃圾回收器就可能没那么快，要等专门的循环垃圾回收器（Cyclic Garbage Collector）来检测并回收这种引用循环。</p><h4 id="1-3-4-算术运算符、反算术运算符、增量赋值运算符、一元运算符"><a href="#1-3-4-算术运算符、反算术运算符、增量赋值运算符、一元运算符" class="headerlink" title="1.3.4 算术运算符、反算术运算符、增量赋值运算符、一元运算符"></a>1.3.4 算术运算符、反算术运算符、增量赋值运算符、一元运算符</h4><p>例如以下运算符。详见<a href="https://github.com/datawhalechina/team-learning-program/blob/master/PythonLanguage/14.%20%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95.md">《PythonLanguage/14. 魔法方法.md》</a><br>常见+- */和+= -=等等<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__add__(self, other)定义加法的行为：+</span><br><span class="line">__sub__(self, other)定义减法的行为：-</span><br><span class="line"></span><br><span class="line">一元运算符</span><br><span class="line">__neg__(self)定义正号的行为：+x</span><br><span class="line">__pos__(self)定义负号的行为：-x</span><br><span class="line">__abs__(self)定义当被<span class="built_in">abs</span>()调用时的行为</span><br><span class="line">__invert__(self)定义按位求反的行为：~x</span><br></pre></td></tr></table></figure></p><h4 id="1-3-5-属性访问"><a href="#1-3-5-属性访问" class="headerlink" title="1.3.5 属性访问"></a>1.3.5 属性访问</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__getattr__(self, name): 定义当用户试图获取一个不存在的属性时的行为。</span><br><span class="line">__getattribute__(self, name)：定义当该类的属性被访问时的行为（先调用该方法，查看是否存在该属性，若不存在，接着去调用__getattr__）。</span><br><span class="line">__setattr__(self, name, value)：定义当一个属性被设置时的行为。</span><br><span class="line">__delattr__(self, name)：定义当一个属性被删除时的行为。</span><br></pre></td></tr></table></figure><h4 id="1-3-6-定制序列len-和getitem"><a href="#1-3-6-定制序列len-和getitem" class="headerlink" title="1.3.6 定制序列len()和getitem()"></a>1.3.6 定制序列<strong>len</strong>()和<strong>getitem</strong>()</h4><p>容器类型的协议</p><ul><li>如果说你希望定制的容器是不可变的话，你只需要定义<strong>len</strong>()和<strong>getitem</strong>()方法。</li><li>如果你希望定制的容器是可变的话，除了<strong>len</strong>()和<strong>getitem</strong>()方法，你还需要定义<strong>setitem</strong>()和<strong>delitem</strong>()两个方法。</li><li><strong>len\</strong>(self)定义当被len()调用时的行为（返回容器中元素的个数）。</li><li><strong>getitem\</strong>(self, key)定义获取容器中元素的行为，相当于self[key]。</li><li><strong>setitem\</strong>(self, key, value)定义设置容器中指定元素的行为，相当于self[key] = value。</li><li><strong>delitem\</strong>(self, key)定义删除容器中指定元素的行为，相当于del self[key]。</li></ul><p>【例子】编写一个不可改变的自定义列表，要求记录列表中每个元素被访问的次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountList</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.values = [x <span class="keyword">for</span> x <span class="keyword">in</span> args]</span><br><span class="line">        self.count = &#123;&#125;.fromkeys(<span class="built_in">range</span>(<span class="built_in">len</span>(self.values)), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.values)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        self.count[item] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.values[item]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = CountList(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)</span><br><span class="line">c2 = CountList(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(c1[<span class="number">1</span>])  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(c2[<span class="number">2</span>])  <span class="comment"># 6</span></span><br><span class="line"><span class="built_in">print</span>(c1[<span class="number">1</span>] + c2[<span class="number">1</span>])  <span class="comment"># 7</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c1.count)</span><br><span class="line"><span class="comment"># &#123;0: 0, 1: 2, 2: 0, 3: 0, 4: 0&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c2.count)</span><br><span class="line"><span class="comment"># &#123;0: 0, 1: 1, 2: 1, 3: 0, 4: 0&#125;</span></span><br></pre></td></tr></table></figure><p>【例子】编写一个可改变的自定义列表，要求记录列表中每个元素被访问的次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountList</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.values = [x <span class="keyword">for</span> x <span class="keyword">in</span> args]</span><br><span class="line">        self.count = &#123;&#125;.fromkeys(<span class="built_in">range</span>(<span class="built_in">len</span>(self.values)), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.values)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        self.count[item] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.values[item]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">        self.values[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__delitem__</span>(<span class="params">self, key</span>):</span></span><br><span class="line">        <span class="keyword">del</span> self.values[key]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.values)):</span><br><span class="line">            <span class="keyword">if</span> i &gt;= key:</span><br><span class="line">                self.count[i] = self.count[i + <span class="number">1</span>]</span><br><span class="line">        self.count.pop(<span class="built_in">len</span>(self.values))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = CountList(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)</span><br><span class="line">c2 = CountList(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(c1[<span class="number">1</span>])  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(c2[<span class="number">2</span>])  <span class="comment"># 6</span></span><br><span class="line">c2[<span class="number">2</span>] = <span class="number">12</span></span><br><span class="line"><span class="built_in">print</span>(c1[<span class="number">1</span>] + c2[<span class="number">2</span>])  <span class="comment"># 15</span></span><br><span class="line"><span class="built_in">print</span>(c1.count)</span><br><span class="line"><span class="comment"># &#123;0: 0, 1: 2, 2: 0, 3: 0, 4: 0&#125;</span></span><br><span class="line"><span class="built_in">print</span>(c2.count)</span><br><span class="line"><span class="comment"># &#123;0: 0, 1: 0, 2: 2, 3: 0, 4: 0&#125;</span></span><br><span class="line"><span class="keyword">del</span> c1[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(c1.count)</span><br><span class="line"><span class="comment"># &#123;0: 0, 1: 0, 2: 0, 3: 0&#125;</span></span><br></pre></td></tr></table></figure><h4 id="1-3-7-迭代器iter-与-next"><a href="#1-3-7-迭代器iter-与-next" class="headerlink" title="1.3.7 迭代器iter\() 与 next\()"></a>1.3.7 迭代器<strong>iter\</strong>() 与 <strong>next\</strong>()</h4><ul><li>迭代是 Python 最强大的功能之一，是访问集合元素的一种方式。字符串，列表或元组对象都可用于创建迭代器。</li><li>迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。<ul><li>迭代器有两个基本的方法：iter() 和 next()。</li></ul></li></ul><p>下面两个例子返回的结果是一样的<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">string = <span class="string">&#x27;lsgogroup&#x27;</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> string:</span><br><span class="line">    <span class="built_in">print</span>(c)</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">iter</span>(string):</span><br><span class="line">    <span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">links = &#123;<span class="string">&#x27;B&#x27;</span>: <span class="string">&#x27;百度&#x27;</span>, <span class="string">&#x27;A&#x27;</span>: <span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;T&#x27;</span>: <span class="string">&#x27;腾讯&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> links:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s -&gt; %s&#x27;</span> % (each, links[each]))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> <span class="built_in">iter</span>(links):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s -&gt; %s&#x27;</span> % (each, links[each]))</span><br></pre></td></tr></table></figure><ul><li>iter(object) 函数用来生成迭代器。</li><li>next(iterator[, default]) 返回迭代器的下一个项目。iterator表示可迭代对象，default — 可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发 StopIteration 异常。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">links = &#123;<span class="string">&#x27;B&#x27;</span>: <span class="string">&#x27;百度&#x27;</span>, <span class="string">&#x27;A&#x27;</span>: <span class="string">&#x27;阿里&#x27;</span>, <span class="string">&#x27;T&#x27;</span>: <span class="string">&#x27;腾讯&#x27;</span>&#125;</span><br><span class="line">it = <span class="built_in">iter</span>(links)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(it))  <span class="comment"># B</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(it))  <span class="comment"># A</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(it))  <span class="comment"># T</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(it))  <span class="comment"># StopIteration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        each = <span class="built_in">next</span>(it)</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(each)</span><br><span class="line"></span><br><span class="line"><span class="comment"># B</span></span><br><span class="line"><span class="comment"># A</span></span><br><span class="line"><span class="comment"># T</span></span><br></pre></td></tr></table></figure><p>把一个类作为一个迭代器使用需要在类中实现两个魔法方法 <strong>iter\</strong>() 与 <strong>next\</strong>() 。</p><p>把一个类作为一个迭代器使用需要在类中实现两个魔法方法 <strong>iter</strong>() 与 <strong>next</strong>() 。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__iter__(self)定义当迭代容器中的元素的行为，返回一个特殊的迭代器对象， 这个迭代器对象实现了</span><br><span class="line">__next__() 方法并通过 StopIteration 异常标识迭代的完成。</span><br><span class="line"> </span><br><span class="line">__next__() 返回下一个迭代器对象。</span><br><span class="line"></span><br><span class="line">StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 __next__() 方法中</span><br><span class="line">我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fibs</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n=<span class="number">10</span></span>):</span></span><br><span class="line">        self.a = <span class="number">0</span></span><br><span class="line">        self.b = <span class="number">1</span></span><br><span class="line">        self.n = n</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.a, self.b = self.b, self.a + self.b</span><br><span class="line">        <span class="keyword">if</span> self.a &gt; self.n:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">return</span> self.a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fibs = Fibs(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> fibs:</span><br><span class="line">    <span class="built_in">print</span>(each, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 1 2 3 5 8 13 21 34 55 89</span></span><br></pre></td></tr></table></figure><h3 id="1-4-扩展知识点"><a href="#1-4-扩展知识点" class="headerlink" title="1.4 扩展知识点"></a>1.4 扩展知识点</h3><h4 id="1-4-1-组合"><a href="#1-4-1-组合" class="headerlink" title="1.4.1 组合"></a>1.4.1 组合</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Turtle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.num = x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fish</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.num = x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pool</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        self.turtle = Turtle(x)</span><br><span class="line">        self.fish = Fish(y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_num</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;水池里面有乌龟%s只，小鱼%s条&quot;</span> % (self.turtle.num, self.fish.num))</span><br><span class="line">        </span><br><span class="line">p = Pool(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">p.print_num()</span><br><span class="line"><span class="comment"># 水池里面有乌龟2只，小鱼3条</span></span><br></pre></td></tr></table></figure><h4 id="1-4-2类的专有方法和内置函数"><a href="#1-4-2类的专有方法和内置函数" class="headerlink" title="1.4.2类的专有方法和内置函数"></a>1.4.2类的专有方法和内置函数</h4><p>具体见<a href="https://github.com/datawhalechina/team-learning-program/blob/master/PythonLanguage/13.%20%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1.md">《datawhale——PythonLanguage》</a><br>类的专有方法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__init__ : 构造函数，在生成对象时调用</span><br><span class="line">__del__ : 析构函数，释放对象时使用</span><br><span class="line">__repr__ : 打印，转换</span><br><span class="line">__setitem__ : 按照索引赋值</span><br><span class="line">__getitem__: 按照索引获取值</span><br><span class="line">__len__: 获得长度</span><br><span class="line">__call__: 函数调用</span><br></pre></td></tr></table></figure><p>内置函数：</p><ul><li>issubclass(class, classinfo) 方法用于判断参数 class 是否是类型参数 classinfo 的子类。一个类被认为是其自身的子类。</li><li>isinstance(object, classinfo) 方法用于判断一个对象是否是一个已知的类型，类似type()。</li><li>hasattr(object, name)用于判断对象是否包含对应的属性。</li><li>getattr(object, name[, default])用于返回一个对象属性值。</li><li>setattr(object, name, value)对应函数 getattr()，用于设置属性值，该属性不一定是存在的。</li><li>delattr(object, name)用于删除属性。</li><li>class property([fget[, fset[, fdel[, doc]]]])用于在新式类中返回属性值</li></ul><h2 id="二、-基础函数"><a href="#二、-基础函数" class="headerlink" title="二、 基础函数"></a>二、 基础函数</h2><h3 id="2-1-Python5个内建高阶函数（map、reduce、filter、sorted-sort、zip）"><a href="#2-1-Python5个内建高阶函数（map、reduce、filter、sorted-sort、zip）" class="headerlink" title="2.1 Python5个内建高阶函数（map、reduce、filter、sorted/sort、zip）"></a>2.1 Python5个内建高阶函数（map、reduce、filter、sorted/sort、zip）</h3><p>参考<a href="https://github.com/datawhalechina/team-learning-program/blob/master/PythonLanguage/Python%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.md">《PythonLanguage/Python高阶函数使用总结.md》</a><br><img src="https://img-blog.csdnimg.cn/f2c08f549cb548b796f96222c0154e55.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/98622336d11f4aedba819c9ee2232977.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/afd9fd38372e4851bca74f05444bb3f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/bd8a8a20a4ab45429a97c0beddef245a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-2-正则表达式"><a href="#2-2-正则表达式" class="headerlink" title="2.2 正则表达式"></a>2.2 正则表达式</h3><p>参考<a href="https://www.runoob.com/python3/python3-reg-expressions.html">《Python3 正则表达式》</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;@[toc]&lt;/p&gt;
&lt;h2 id=&quot;一、python的类与对象&quot;&gt;&lt;a href=&quot;#一、python的类与对象&quot; class=&quot;headerlink&quot; title=&quot;一、python的类与对象&quot;&gt;&lt;/a&gt;一、python的类与对象&lt;/h2&gt;&lt;p&gt;参考&lt;a href=&quot;https://github.com/datawhalechina/team-learning-program/blob/master/PythonLanguage/13.%20%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1.md&quot;&gt;《datawhale——PythonLanguage》&lt;/a&gt;、&lt;a href=&quot;https://www.runoob.com/python3/python3-class.html&quot;&gt;《Python3 面向对象》&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;1-1-基本概念&quot;&gt;&lt;a href=&quot;#1-1-基本概念&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念&quot;&gt;&lt;/a&gt;1.1 基本概念&lt;/h3&gt;&lt;p&gt;对象 = 属性 + 方法&lt;br&gt;对象是类的实例。换句话说，类主要定义对象的结构，然后我们以类为模板创建对象。类不但包含方法定义，而且还包含所有实例共享的数据&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。&lt;/li&gt;
&lt;li&gt;对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。&lt;/li&gt;
&lt;li&gt;方法：类中定义的函数&amp;lt;/font&amp;gt;。&lt;/li&gt;
&lt;li&gt;类属性：类里面方法外面定义的变量称为类属性。&amp;lt;/font&amp;gt;类属性所属于类对象并且多个实例对象之间共享同一个类属性，说白了就是类属性所有的通过该类实例化的对象都能共享。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="10月组队学习：Pytorch" scheme="https://zhxnlp.github.io/categories/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9APytorch/"/>
    
    
    <category term="python" scheme="https://zhxnlp.github.io/tags/python/"/>
    
  </entry>
  
</feed>
