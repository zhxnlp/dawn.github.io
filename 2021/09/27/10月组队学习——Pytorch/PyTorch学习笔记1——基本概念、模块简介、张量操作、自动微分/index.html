<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分"><meta name="keywords" content="Pytorch"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">一、基础介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1PyTorch-%E7%AE%80%E4%BB%8B%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">1.1PyTorch 简介：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E9%9D%99%E6%80%81%E5%9B%BE%E5%92%8C%E5%8A%A8%E6%80%81%E5%9B%BE"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 静态图和动态图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-pytorch%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 pytorch主要模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-%E5%BC%A0%E9%87%8F"><span class="toc-number">2.</span> <span class="toc-text">二、 张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="toc-number">2.1.</span> <span class="toc-text">2.1.张量的创建方式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%BC%A0%E9%87%8F%E7%B1%BB%E5%9E%8B%E5%92%8C%E7%BB%B4%E5%BA%A6"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 张量类型和维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 张量的存储设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-number">2.4.</span> <span class="toc-text">2. 4 索引和切片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%87%BD%E6%95%B0%E8%BF%90%E7%AE%97%E5%92%8C%E6%9E%81%E5%80%BC%E6%8E%92%E5%BA%8Fsort"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 函数运算和极值排序sort</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95-%E5%92%8C%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BC%A9%E5%B9%B6einsum"><span class="toc-number">2.6.</span> <span class="toc-text">2.6 矩阵乘法@和张量的缩并einsum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%8B%BC%E6%8E%A5%E5%92%8C%E5%88%86%E5%89%B2split"><span class="toc-number">2.7.</span> <span class="toc-text">2.7 张量的拼接和分割split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-%E5%BC%A0%E9%87%8F%E6%89%A9%E5%A2%9E-unsqueeze-%E3%80%81%E5%8E%8B%E7%BC%A9-squeeze-%E5%92%8C%E5%B9%BF%E6%92%AD"><span class="toc-number">2.8.</span> <span class="toc-text">2.8 张量扩增(unsqueeze)、压缩(squeeze)和广播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-PyTorch-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">三. PyTorch 自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-autograd-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E5%92%8C%E5%86%BB%E7%BB%93%E5%8F%82%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 autograd 自动求导和冻结参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%9B%85%E5%85%8B%E6%AF%94%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 雅克比向量积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 计算图</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-27</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9APytorch/">10月组队学习：Pytorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">4.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 17 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>@[toc]</p>
<blockquote>
<p>推荐文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/265394674">《PyTorch 学习笔记汇总（完结撒花）》</a></p>
<h2 id="一、基础介绍"><a href="#一、基础介绍" class="headerlink" title="一、基础介绍"></a>一、基础介绍</h2><h3 id="1-1PyTorch-简介："><a href="#1-1PyTorch-简介：" class="headerlink" title="1.1PyTorch 简介："></a>1.1PyTorch 简介：</h3><ul>
<li>Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高。</li>
<li>PyTorch是一个基于Torch的Python开源机器学习库，提供了两个高级功能： <ul>
<li>具有强大的GPU加速的张量计算（如Numpy） </li>
<li>包含自动求导系统的深度神经网络</li>
<li>PyTorch，通过反向求导技术，可以让你零延迟地任意改变神经网络的行为，而且其实现速度快</li>
<li>底层代码易于理解 +命令式体验 +自定义扩展</li>
<li>缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势。例如目前PyTorch还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量等</li>
</ul>
</li>
</ul>
</blockquote>
<span id="more"></span>
<h3 id="1-2-静态图和动态图"><a href="#1-2-静态图和动态图" class="headerlink" title="1.2 静态图和动态图"></a>1.2 静态图和动态图</h3><p>为了能够计算权重梯度和数据梯度,神经网络需记录运算的过程,并构建出计算图。</p>
<ol>
<li>静态图：tensorflow和caffe。先构建模型对应的静态图，再输入张量。执行引擎会根据输入的张量进行计算,最后输出深度学习模型的计算结果。<ul>
<li>静态图的前向和反向传播路径在计算前已经被构建,所以是已知的。计算图在实际发生计算之前已经存在</li>
<li>执行引擎可以在计算之前对计算图进行优化,比如删除冗余的运算合并两个运算操作等</li>
<li>执行效率较高：不用每次计算都重新构建计算图,减少了计算图构建的时间消耗</li>
<li>不够灵活：因为静态计算图在构建完成之后不能修改，使用条件控制(比如循环和判断语句)会不大方便</li>
<li>代码调试较慢：构建时只能检查静态参数，如输入输出形状。执行时的问题无法在构件图时预先排查 </li>
<li>==计算图中直接集成了优化器==，求出权重张量梯度，直接执行优化器的计算图，更新权重的张量值</li>
</ul>
</li>
<li>动态图：在计算过程中逐步构建计算图。牺牲执行效率但是更灵活<ul>
<li>反向传播路径只有在构建完计算图时才能获得</li>
<li>条件控制语句很简单</li>
<li>调试方便：可以实时输出模型的中间张量</li>
<li>优化器绑定在权重张量上：反向传播后，优化器根据绑定的梯度长量更新权重张量。</li>
<li>强大的可扩展性。例如自由定制张量计算、CPU/GPU异构计算、并行计算环境、设置不同模型层的学习率等。</li>
</ul>
</li>
</ol>
<h3 id="1-3-pytorch主要模块"><a href="#1-3-pytorch主要模块" class="headerlink" title="1.3 pytorch主要模块"></a>1.3 pytorch主要模块</h3><p>下面介绍主要模块。具体都可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">官方文档</a>。</p>
<ol>
<li>torch模块：包含激活函数&lt;/font&gt;和主要的张量操作</li>
<li>torch.Tensor模块：定义了张量的数据类型（整型、浮点型等）另外张量的某个类方法会返回新的张量，==如果方法后缀带下划线，就会修改张量本身==。比如Tensor.add是当前张量和别的张量做加法，返回新的张量。如果是ensor.add_就是将加和的张量结果赋值给当前张量。</li>
<li>torch.cuda:定义了CUDA运算相关的函数。如检查CUDA是否可用及序号，清除其缓存、设置GPU计算流stream等</li>
<li>torch.nn：神经网络模块化的核心，包括卷积神经网络nn.ConvNd和全连接层（线性层）nn.Linear等，以及一系列的损失函数&lt;/font&gt;。</li>
<li>torch,nn.functional:定义神经网络相关的函数，例如卷积函数、池化函数、log_softmax函数等部分激活函数。torch.nn模块一般会调用torch.nn.functional的函数。</li>
<li>torch.nn.init:权重初始化模块。包括均匀初始化torch.nn.init.uniform<em>和正态分布归一化torch.nn.init.normal</em>。（_表示直接修改原张量的数值并返回）</li>
<li>torch.optim：定义一系列优化器，如optim.SGD、optim.Adam、optim.AdamW等。以及学习率调度器torch.optim.lr_scheduler。并可以实现多种学习率衰减方法等。具体参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">官方教程</a>。</li>
<li>torch.autograd：自动微分算法模块。定义一系列自动微分函数，例如torch.autograd.backward反向传播函数和torch.autograd.grad求导函数（一个标量张量对另一个张量求导）。以及设置不求导部分。</li>
<li>torch.distributed：分布式计算模块。设定并行运算环境</li>
<li>torch.distributions：强化学习等需要的策略梯度法（概率采样计算图）  无法直接对离散采样结果求导，这个模块可以解决这个问题</li>
<li>torch.hub：提供一系列预训练模型给用户使用。torch.hub.list获取模型的checkpoint，torch.hub.load来加载对应模型。</li>
<li>torch.random：保存和设置随机数生成器。manual_seed设置随机数种子，initial_seed设置程序初始化种子。set_rng_state设置当前随机数生成器状态，get_rng_state获取前随机数生成器状态。设置统一的随机数种子，可以测试不同神经网络的表现，方便进行调试。</li>
<li>torch.jit：动态图转静态图，保存后被其他前端支持（C++等）。关联的还有torch.onnx（深度学习模型描述文件，用于和其它深度学习框架进行模型交换）<br>除此之外还有一些辅助模块：</li>
</ol>
<ul>
<li>torch.utils.benchmark：记录深度学习模型中各模块运行时间，通过优化运行时间，来优化模型性能</li>
<li>torch.utils.checkpoint：以计算时间换空间，优化模型性能。因为反向传播时，需要保存中间数据，大大增加内存消耗。此模块可以记录中间数据计算过程，然后丢弃中间数据，用的时候再重新计算。这样可以提高batch_size，使模型性能和优化更稳定。</li>
<li>torch.utils.data：主要是Dataset和DataLoader。</li>
<li>torch.utils.tensorboard：pytorch对tensorboard的数据可视化支持工具。显示模型训练过程中的<br>损失函数和张量权重的直方图，以及中间输出的文本、视频等。方便调试程序。<h2 id="二、-张量"><a href="#二、-张量" class="headerlink" title="二、 张量"></a>二、 张量</h2>pytorch提供专门的torch.Tensor类，根据张量的数据格式和存储设备（CPU/GPU）来存储张量。<br>Tensors 类似于 NumPy 的 ndarrays ，同时 Tensors 可以使用 GPU 进行计算。<br>详细的张量操作参考：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#tensor-creation-ops">张量创建和运算： torch</a><h3 id="2-1-张量的创建方式："><a href="#2-1-张量的创建方式：" class="headerlink" title="2.1.张量的创建方式："></a>2.1.张量的创建方式：</h3></li>
<li>python列表、ndarray数组转为张量<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1.</span>, -<span class="number">1.</span>], [<span class="number">1.</span>, -<span class="number">1.</span>]])<span class="comment">#python列表转为张量，子列表长度必须一致</span></span><br><span class="line">torch.tensor(np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]))<span class="comment">#ndarray数组转为张量</span></span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure></li>
<li>利用函数创建张量<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br></pre></td></tr></table></figure></li>
<li>常见的构造Tensor的函数：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right">函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">Tensor(sizes)</td>
<td>基础构造函数</td>
</tr>
<tr>
<td style="text-align:right">tensor(data)</td>
<td>类似于np.array</td>
</tr>
<tr>
<td style="text-align:right">ones(sizes)</td>
<td>全1</td>
</tr>
<tr>
<td style="text-align:right">zeros(sizes)</td>
<td>全0</td>
</tr>
<tr>
<td style="text-align:right">eye(sizes)</td>
<td>对角为1，其余为0</td>
</tr>
<tr>
<td style="text-align:right">arange(s,e,step)</td>
<td>从s到e，步长为step</td>
</tr>
<tr>
<td style="text-align:right">linspace(s,e,steps)</td>
<td>从s到e，均匀分成step份</td>
</tr>
<tr>
<td style="text-align:right">randn(sizes)</td>
<td>标准正态分布</td>
</tr>
<tr>
<td style="text-align:right">rand（size）</td>
<td>[0,1)j均匀分布</td>
</tr>
<tr>
<td style="text-align:right">normal(mean,std)</td>
<td>正态分布</td>
</tr>
<tr>
<td style="text-align:right">uniform(from,to)</td>
<td>/均匀分布 </td>
</tr>
<tr>
<td style="text-align:right">randint(a,b,(sizes))</td>
<td>从a到b形状为size的整数张量</td>
</tr>
<tr>
<td style="text-align:right">randperm(m)</td>
<td>随机排列</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>创建类似形状的张量：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">torch.zeros_like(t)<span class="comment">#zeros还可以换成其它构造函数ones、randdeng</span></span><br><span class="line"><span class="comment">#如果t是整型，构造函数生成浮点型会报错</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-张量类型和维度"><a href="#2-2-张量类型和维度" class="headerlink" title="2.2 张量类型和维度"></a>2.2 张量类型和维度</h3><ul>
<li>访问dtype属性可以查看张量的类型。shape属性可以查看张量的形状<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1.</span>, -<span class="number">1.</span>], [<span class="number">1.</span>, -<span class="number">1.</span>]])</span><br><span class="line"><span class="built_in">print</span>(a.dtype,a.<span class="built_in">type</span>(),a.shape)</span><br><span class="line"></span><br><span class="line">torch.float32 torch.FloatTensor torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></li>
<li>pytorch不同数据类型之间可以用to转换，或者.int()方法<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#浮点型转整型</span></span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>).to(torch.<span class="built_in">int</span>)</span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>).<span class="built_in">int</span>()</span><br></pre></td></tr></table></figure></li>
<li>张量的维度<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=torch.randn(<span class="number">3</span>,<span class="number">4</span>).to(torch.<span class="built_in">int</span>)</span><br><span class="line">t.nelement()<span class="comment">#获取元素总数</span></span><br><span class="line">t.ndimension()<span class="comment">#获取张量维度</span></span><br><span class="line">t.shape<span class="comment">#张量形状</span></span><br></pre></td></tr></table></figure></li>
<li>改变张量的维度可以用view方法，指定n-1维，最后一维写-1</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.view(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">t.view(-<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">t.view(<span class="number">12</span>)<span class="comment">#tensor([0, 0, 0, 0, -1, 1, 0, 2, 0, 2, 0, 0], dtype=torch.int32)</span></span><br></pre></td></tr></table></figure>
<p>另外还有reshape和contiguous方法。</p>
<h3 id="2-3-张量的存储设备"><a href="#2-3-张量的存储设备" class="headerlink" title="2.3 张量的存储设备"></a>2.3 张量的存储设备</h3><p>两个张量只有在同一设备上才可以运算（CPU或者同一个GPU）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nvidia-smi<span class="comment">#可以查看GPU的信息</span></span><br><span class="line">!nvidia-smi<span class="comment">#colab上命令是这个</span></span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>,device=<span class="string">&#x27;cuda:0&#x27;</span>).device<span class="comment">#在0号cuda上创建张量，查看张量存储设备</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>,device=<span class="string">&#x27;cuda:0&#x27;</span>).cpu().device<span class="comment">#cuda 0上的张量复制到CPU上</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>,device=<span class="string">&#x27;cuda:0&#x27;</span>).cuda(<span class="number">1</span>）</span><br><span class="line">torch.randn(<span class="number">3</span>,<span class="number">3</span>,device=<span class="string">&#x27;cuda:0&#x27;</span>).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI <span class="number">495.44</span>       Driver Version: <span class="number">460.32</span><span class="number">.03</span>    CUDA Version: <span class="number">11.2</span>     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   <span class="number">0</span>  Tesla P100-PCIE...  Off  | <span class="number">00000000</span>:<span class="number">00</span>:<span class="number">04.0</span> Off |                    <span class="number">0</span> |</span><br><span class="line">| N/A   47C    P0    28W / 250W |      0MiB / 16280MiB |      <span class="number">0</span>%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   <span class="type">Type</span>   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h3 id="2-4-索引和切片"><a href="#2-4-索引和切片" class="headerlink" title="2. 4 索引和切片"></a>2. 4 索引和切片</h3><p>等同numpy的操作。如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t[:,<span class="number">1</span>:-<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">t&gt;<span class="number">0</span><span class="comment">#得到一个掩码矩阵</span></span><br><span class="line">t[t&gt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>筛选出t中大于0的元素，最终得到一个一维向量<br>如果不想改变原张量的数值，可以先用clone得到张量的副本，再进行索引和切片的赋值操作。</p>
<h3 id="2-5-函数运算和极值排序sort"><a href="#2-5-函数运算和极值排序sort" class="headerlink" title="2.5 函数运算和极值排序sort"></a>2.5 函数运算和极值排序sort</h3><p>所有运算符、操作符见文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#tensor-creation-ops">《Creation Ops》</a><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.mean()<span class="comment">#对所有维度求均值</span></span><br><span class="line">t.mean(<span class="number">0</span>)<span class="comment">#对第0维元素求均值</span></span><br><span class="line">t.mean([<span class="number">0</span>,<span class="number">1</span>])<span class="comment">#对0,1两维元素求均值</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>argmax和argmin可以根据传入的维度，求的该维度极大极小值对应的序号。</li>
<li>max和min会得到一个元组，包括极值位置和极值。</li>
<li>sort默认从小到大排序。从大到小需要设置descending=True。需要传入排序的维度，返回排序后的张量和各元素在原始张量的位置</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=torch.randint(<span class="number">1</span>,<span class="number">100</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">20</span>, <span class="number">95</span>,  <span class="number">9</span>, <span class="number">94</span>],</span><br><span class="line">        [<span class="number">97</span>, <span class="number">61</span>, <span class="number">80</span>, <span class="number">67</span>],</span><br><span class="line">        [<span class="number">76</span>, <span class="number">66</span>, <span class="number">64</span>, <span class="number">65</span>]])</span><br><span class="line">        </span><br><span class="line">t.<span class="built_in">max</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">97</span>, <span class="number">95</span>, <span class="number">80</span>, <span class="number">94</span>]),</span><br><span class="line">indices=tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">t.argmax(<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">t.sort(-<span class="number">1</span>,descending=<span class="literal">True</span>)</span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values=tensor([[<span class="number">95</span>, <span class="number">94</span>, <span class="number">20</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">97</span>, <span class="number">80</span>, <span class="number">67</span>, <span class="number">61</span>],</span><br><span class="line">        [<span class="number">76</span>, <span class="number">66</span>, <span class="number">65</span>, <span class="number">64</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">1</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure>
<p>==函数后面加下划线是原地操作，改变被调用的张量的值==</p>
<h3 id="2-6-矩阵乘法-和张量的缩并einsum"><a href="#2-6-矩阵乘法-和张量的缩并einsum" class="headerlink" title="2.6 矩阵乘法@和张量的缩并einsum"></a>2.6 矩阵乘法@和张量的缩并einsum</h3><p>矩阵乘法可以用a.mm(b)或者torch.mm(a,b)或者a@b三种形式。<br>==还是a@b最好用==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=torch.randint(<span class="number">1</span>,<span class="number">100</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">q=torch.randint(<span class="number">1</span>,<span class="number">100</span>,(<span class="number">4</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#三种写法都是得到3×3的矩阵</span></span><br><span class="line">t.mm(q)</span><br><span class="line">torch.mm(t,q)</span><br><span class="line">t@q</span><br><span class="line">tensor([[<span class="number">12151</span>,  <span class="number">9911</span>,  <span class="number">8876</span>],</span><br><span class="line">        [<span class="number">16752</span>, <span class="number">18098</span>, <span class="number">15618</span>],</span><br><span class="line">        [<span class="number">14844</span>, <span class="number">15675</span>, <span class="number">13614</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>==一个batch矩阵的乘法，需要用bmm函数==。即两个批次的矩阵乘法，是沿着批次方向分别对两个矩阵做乘法，最后将矩阵组合在一起。</li>
<li>比如一个b×m×k的矩阵和一个b×k×n的矩阵，做张量相乘，得到b×m×n的张量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment"># 随机产生张量</span></span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">a.bmm(c) <span class="comment"># 批次矩阵乘法的结果</span></span><br><span class="line">torch.bmm(a,c)</span><br><span class="line">a@b</span><br></pre></td></tr></table></figure>
<p>如果是3维以上张量的乘积，称为缩并。需要用到爱因斯坦求和约定。对应函数为torch.einsum。</p>
<h3 id="2-7-张量的拼接和分割split"><a href="#2-7-张量的拼接和分割split" class="headerlink" title="2.7 张量的拼接和分割split"></a>2.7 张量的拼接和分割split</h3><p>torch.stack:传入张量列表和维度，将张量沿此维度进行堆叠（新建一个维度来堆叠）<br>torch.cat:传入张量列表和维度，将张量沿此维度进行堆叠<br>两个都是拼接张量，torch.stack会新建一个维度来拼接，后者维度预先存在，沿着此维度堆叠就行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.randn(<span class="number">3</span>,<span class="number">4</span>) <span class="comment"># 随机产生三个张量</span></span><br><span class="line">t2 = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t3 = torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"> </span><br><span class="line">torch.stack([t1,t2,t3], -<span class="number">1</span>).shape<span class="comment"># 沿着最后一个维度做堆叠，返回大小为3×4×3的张量</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line">torch.cat([t1,t2,t3], -<span class="number">1</span>).shape <span class="comment"># 沿着最后一个维度做拼接，返回大小为3×14的张量</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">12</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>torch.split函数，有三个参数。将张量沿着指定维度进行分割。<br>第二个参数可以是整数n或者列表list。前者表示这个维度等分成n份（最后一份可以是剩余的）。或者表示分成列表元素值来分割。</p>
<p>torch.chunk函数和slpit函数类似<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randint(<span class="number">1</span>, <span class="number">10</span>,(<span class="number">3</span>,<span class="number">6</span>)) <span class="comment"># 随机产生一个3×6的张量</span></span><br><span class="line">tensor([[<span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>]])</span><br><span class="line">------------------------------------------------------------------------------        </span><br><span class="line">t.split([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], -<span class="number">1</span>) <span class="comment"># 把张量沿着最后一个维度分割为三个张量</span></span><br><span class="line">(tensor([[<span class="number">8</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">5</span>]]),</span><br><span class="line"> tensor([[<span class="number">9</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">5</span>]]),</span><br><span class="line"> tensor([[<span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>]]))</span><br><span class="line">------------------------------------------------------------------------------         </span><br><span class="line">t.split(<span class="number">3</span>, -<span class="number">1</span>) <span class="comment"># 把张量沿着最后一个维度分割，分割大小为3，输出的张量大小均为3×3</span></span><br><span class="line">(tensor([[<span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">2</span>, <span class="number">5</span>]]),</span><br><span class="line"> tensor([[<span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>]]))</span><br><span class="line">         </span><br><span class="line">t.chunk(<span class="number">3</span>, -<span class="number">1</span>) <span class="comment"># 把张量沿着最后一个维度分割为三个张量，大小均为3×2</span></span><br><span class="line">(tensor([[<span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">2</span>]]),</span><br><span class="line"> tensor([[<span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">7</span>]]),</span><br><span class="line"> tensor([[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">7</span>]]))</span><br><span class="line">​</span><br></pre></td></tr></table></figure></p>
<h3 id="2-8-张量扩增-unsqueeze-、压缩-squeeze-和广播"><a href="#2-8-张量扩增-unsqueeze-、压缩-squeeze-和广播" class="headerlink" title="2.8 张量扩增(unsqueeze)、压缩(squeeze)和广播"></a>2.8 张量扩增(unsqueeze)、压缩(squeeze)和广播</h3><ul>
<li>张量可以任意扩增一个维度大小为1 的维度，数据不变。反过来这些维度大小为1的维度也可以压缩掉。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.rand(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 随机生成一个张量</span></span><br><span class="line"></span><br><span class="line">t.unsqueeze(-<span class="number">1</span>).shape <span class="comment"># 扩增最后一个维度</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">t.unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">1</span>).shape  <span class="comment"># 继续扩增一个维度</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">t = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>) <span class="comment"># 随机生成一个张量，有两个维度大小为1</span></span><br><span class="line">t.squeeze().shape <span class="comment"># 压缩所有大小为1的维度</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>两个不同维度的张量做四则运算，需要先把维度数目少的张量扩增到和另一个一致（unsqueeze方法），再进行运算。运算时，将扩增的维度进行复制，到最后维度一致再运算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>) </span><br><span class="line">t2 = torch.randn(<span class="number">3</span>,<span class="number">5</span>) </span><br><span class="line">t2 = t2.unsqueeze(<span class="number">1</span>) <span class="comment"># 张量2的形状变为3×1×5</span></span><br><span class="line"><span class="built_in">print</span>(t2)</span><br><span class="line">tensor([[[ <span class="number">0.7188</span>, -<span class="number">1.1053</span>, -<span class="number">0.1161</span>, -<span class="number">2.2889</span>, -<span class="number">0.8046</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.1434</span>, -<span class="number">2.8369</span>, -<span class="number">1.5712</span>,  <span class="number">1.1490</span>,  <span class="number">0.7161</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.8259</span>,  <span class="number">1.8744</span>, -<span class="number">0.7918</span>, -<span class="number">0.4208</span>,  <span class="number">1.6935</span>]]])</span><br><span class="line">        </span><br><span class="line">t3 = t1 + t2 <span class="comment">#将t2沿着第二个维度复制4次，最后形状为(3,4,5) </span></span><br><span class="line"><span class="built_in">print</span>(t3)</span><br><span class="line">tensor([[[ <span class="number">1.6212</span>, -<span class="number">1.0232</span>,  <span class="number">1.9735</span>, -<span class="number">2.3579</span>, -<span class="number">2.8416</span>],</span><br><span class="line">         [ <span class="number">1.3389</span>, -<span class="number">0.7377</span>, -<span class="number">0.8453</span>, -<span class="number">2.2385</span>, -<span class="number">1.4370</span>],</span><br><span class="line">         [ <span class="number">1.4433</span>, -<span class="number">1.8982</span>, -<span class="number">0.0669</span>, -<span class="number">2.8503</span>, -<span class="number">1.0240</span>],</span><br><span class="line">         [-<span class="number">0.0498</span>, -<span class="number">2.2708</span>,  <span class="number">0.4583</span>, -<span class="number">0.3370</span>, -<span class="number">2.7074</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.7768</span>, -<span class="number">2.4552</span>,  <span class="number">0.3409</span>, -<span class="number">0.7948</span>,  <span class="number">1.9718</span>],</span><br><span class="line">         [ <span class="number">0.1147</span>, -<span class="number">3.2569</span>, -<span class="number">1.4112</span>,  <span class="number">1.3465</span>,  <span class="number">0.2129</span>],</span><br><span class="line">         [ <span class="number">0.8951</span>, -<span class="number">3.5355</span>, -<span class="number">0.3349</span>,  <span class="number">1.4523</span>,  <span class="number">0.2659</span>],</span><br><span class="line">         [ <span class="number">0.6704</span>, -<span class="number">2.3110</span>, -<span class="number">1.1827</span>,  <span class="number">0.8700</span>,  <span class="number">2.9844</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.3561</span>,  <span class="number">0.7850</span>, -<span class="number">0.9848</span>, -<span class="number">0.8666</span>,  <span class="number">0.0758</span>],</span><br><span class="line">         [-<span class="number">0.1744</span>,  <span class="number">1.3592</span>, -<span class="number">1.7955</span>, -<span class="number">0.0697</span>,  <span class="number">3.8696</span>],</span><br><span class="line">         [-<span class="number">2.5559</span>,  <span class="number">2.6479</span>, -<span class="number">0.1718</span>, -<span class="number">0.2446</span>,  <span class="number">1.7351</span>],</span><br><span class="line">         [ <span class="number">0.5748</span>,  <span class="number">1.2866</span>, -<span class="number">1.3801</span>,  <span class="number">0.0290</span>,  <span class="number">1.0740</span>]]])</span><br></pre></td></tr></table></figure>
<h2 id="三-PyTorch-自动微分"><a href="#三-PyTorch-自动微分" class="headerlink" title="三. PyTorch 自动微分"></a>三. PyTorch 自动微分</h2><h3 id="3-1-autograd-自动求导和冻结参数"><a href="#3-1-autograd-自动求导和冻结参数" class="headerlink" title="3.1 autograd 自动求导和冻结参数"></a>3.1 autograd 自动求导和冻结参数</h3><ol>
<li>autograd 软件包为 Tensors 上的所有操作提供自动微分，是 PyTorch 中所有神经网络的核心。</li>
<li>设置torch.Tensor 类的属性 .requires_grad = True&lt;/font&gt;，则表示该张量会加入到计算图中，作为叶子节点参与计算，自动跟踪针对 tensor的所有操作。计算的中间结果都是requires_grad = True。</li>
<li>每个张量都有一个 grad_fn方法&lt;/font&gt;，保存创建该张量的运算的导数信息、计算图信息。</li>
<li>调用 Tensor.backward() &lt;/font&gt;传入最后一层的神经网络梯度。grad_fn方法的 next.functions属性&lt;/font&gt;，包含连接该张量的其它张量的grad_fn。不断反向传播回溯中间张量计算节点，可以得到所有张量的梯度。该张量的梯度将累积到 .grad 属性 &lt;/font&gt;中。如果Tensor 是标量，则backward()不需要指定任何参数。否则，需要指定一个gradient 参数来指定张量的形状。</li>
<li>with torch.no_grad() &lt;/font&gt;: 包装的代码块部分，停止跟踪历史记录（和使用内存）。</li>
<li>张量绑定的梯度在不清空的情况下会不断累积。可用来一次性求很多batch的累积梯度。</li>
</ol>
<p>Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">[<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>冻结参数</li>
</ul>
<p>在 torch.nn 中，不计算梯度的参数通常称为冻结参数。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分很有用（通过减少自动梯度计算，这会带来一些表现优势）。</p>
<p>例如加载一个预训练的 resnet18 模型，并冻结所有参数，仅修改分类器层以对新标签进行预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结网络的所有参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>假设我们要在具有 10 个标签的新数据集中微调模型。 在 resnet 中，分类器是最后一个线性层model.fc。 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>现在，除了model.fc的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是model.fc的权重和偏差。（torch.no_grad()中的上下文管理器可以使用相同的排除功能。）</p>
<h3 id="3-2-雅克比向量积"><a href="#3-2-雅克比向量积" class="headerlink" title="3.2 雅克比向量积"></a>3.2 雅克比向量积</h3><h3 id="3-3-计算图"><a href="#3-3-计算图" class="headerlink" title="3.3 计算图"></a>3.3 计算图</h3><p>Autograd 在由函数对象组成的有向无环图（DAG）中记录张量、所有已执行的操作（以及由此产生的新张量）。 在此 DAG 中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。</p>
<ol>
<li><p>在正向传播中，Autograd 同时执行两项操作：</p>
<ul>
<li>根据张量和function计算结果张量</li>
<li>在 DAG 中维护操作的梯度函数。</li>
</ul>
</li>
<li><p>当在 DAG 根目录上调用.backward()时，开始回传梯度，然后：</p>
<ul>
<li>从每个.grad_fn计算梯度，将它们累积在各自的张量的.grad属性中</li>
<li>使用链式规则，一直传播到叶子张量。</li>
</ul>
</li>
</ol>
<p>下面是我们示例中 DAG 的直观表示。 在图中，箭头指向前进的方向。 节点代表正向传播中每个操作的反向函数。 蓝色的叶节点代表我们的叶张量a和b：<br><img src="https://img-blog.csdnimg.cn/9c6e2816a72e456bb5fb582f6817fc12.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/09/27/10月组队学习——Pytorch/PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分/">https://zhxnlp.github.io/2021/09/27/10月组队学习——Pytorch/PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/10/01/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/README/"><i class="fa fa-chevron-left">  </i><span>天池-新闻文本分类task0：任务简介</span></a></div><div class="next-post pull-right"><a href="/2021/09/25/huggingface/%E7%BC%96%E5%86%99transformers%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89pytorch%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%EF%BC%88Dataset%E5%92%8CDataLoader%E8%A7%A3%E6%9E%90%E5%92%8C%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%89/"><span>Hugging Face官方文档——编写transformers的自定义pytorch训练循环</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>