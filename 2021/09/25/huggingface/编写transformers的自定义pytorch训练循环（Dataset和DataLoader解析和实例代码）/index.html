<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hugging Face官方文档——编写transformers的自定义pytorch训练循环"><meta name="keywords" content="nlp,transformers,Hugging Face"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>Hugging Face官方文档——编写transformers的自定义pytorch训练循环 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Dataset%E5%92%8CDataLoader%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">一、Dataset和DataLoader加载数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-torch-utils-data"><span class="toc-text">1.torch.utils.data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">2. 加载数据流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Dataset"><span class="toc-text">3. Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-dataloader%E7%B1%BB%E5%8F%8A%E5%85%B6%E5%8F%82%E6%95%B0"><span class="toc-text">4. dataloader类及其参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-dataloader%E5%86%85%E9%83%A8%E5%87%BD%E6%95%B0"><span class="toc-text">5. dataloader内部函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-next%E5%87%BD%E6%95%B0"><span class="toc-text">5.1 next函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-DataLoaderIter%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 DataLoaderIter函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-dataloader%E5%BE%AA%E7%8E%AF"><span class="toc-text">6. dataloader循环</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">二、代码示例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-transformer%E5%8D%95%E5%8F%A5%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%88HF%E6%95%99%E7%A8%8B%EF%BC%89"><span class="toc-text">1. transformer单句文本分类（HF教程）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E4%BD%BF%E7%94%A8Trainer%E8%AE%AD%E7%BB%83"><span class="toc-text">1.1使用Trainer训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%BD%BF%E7%94%A8-PyTorch%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">1.2 使用 PyTorch进行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%8F%A5%E5%AD%90%E5%AF%B9%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%88rte%EF%BC%89%EF%BC%9A"><span class="toc-text">1.3 句子对文本分类（rte）：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E6%9B%B4%E5%A4%9A%E7%A4%BA%E4%BE%8B"><span class="toc-text">1.4 更多示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E4%B8%AD%E6%96%87%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BB%A3%E7%A0%81%E8%B5%8F%E6%9E%90"><span class="toc-text">2. 科大讯飞中文相似度代码赏析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E8%B5%9B%E9%A2%98%E8%A7%A3%E6%9E%90"><span class="toc-text">2.1赛题解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B"><span class="toc-text">2.2 代码实例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-CCF-BDCI-%E5%89%A7%E6%9C%AC%E8%A7%92%E8%89%B2%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB"><span class="toc-text">3. CCF BDCI 剧本角色情感识别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%B5%9B%E4%BA%8B%E8%A7%A3%E6%9E%90"><span class="toc-text">3.1 赛事解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">3.2  代码示例</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Hugging Face官方文档——编写transformers的自定义pytorch训练循环</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/HuggingFace/">HuggingFace</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">6.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 25 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>@[toc]</p>
<h1 id="一、Dataset和DataLoader加载数据集"><a href="#一、Dataset和DataLoader加载数据集" class="headerlink" title="一、Dataset和DataLoader加载数据集"></a>一、Dataset和DataLoader加载数据集</h1><h2 id="1-torch-utils-data"><a href="#1-torch-utils-data" class="headerlink" title="1.torch.utils.data"></a>1.torch.utils.data</h2><p>torch.utils.data主要包括以下三个类： </p>
<ol>
<li>class torch.utils.data.Dataset<br>其他的数据集类必须是torch.utils.data.Dataset的子类,比如说torchvision.ImageFolder. </li>
<li>class torch.utils.data.sampler.Sampler(data_source)<br>参数: data_source (Dataset) – dataset to sample from<br>作用: 创建一个采样器, class torch.utils.data.sampler.Sampler是所有的Sampler的基类, 其中,iter(self)函数来获取一个迭代器,对数据集中元素的索引进行迭代,len(self)方法返回迭代器中包含元素的长度. </li>
<li>class torch.utils.data.DataLoader<span id="more"></span>
<h2 id="2-加载数据流程"><a href="#2-加载数据流程" class="headerlink" title="2. 加载数据流程"></a>2. 加载数据流程</h2>pytorch中加载数据的顺序是：</li>
<li>加载数据，提取出feature和label，并转换成tensor</li>
<li>创建一个dataset对象</li>
<li>创建一个dataloader对象，dataloader类的作用就是实现数据以什么方式输入到什么网络中</li>
<li>循环dataloader对象，将data,label拿到模型中去训练<br>代码一般是这么写的：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义学习集 DataLoader</span></span><br><span class="line">train_data = torch.utils.data.DataLoader(各种设置...) </span><br><span class="line"><span class="comment"># 将数据喂入神经网络进行训练</span></span><br><span class="line"><span class="keyword">for</span> i, (<span class="built_in">input</span>, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data): </span><br><span class="line">    循环代码行......</span><br></pre></td></tr></table></figure>
<h2 id="3-Dataset"><a href="#3-Dataset" class="headerlink" title="3. Dataset"></a>3. Dataset</h2><p>Dataset是我们用的数据集的库，是Pytorch中所有数据集加载类中应该继承的父类。其中父类中的两个私有成员函数必须被重载，否则将会触发错误提示。其中<strong>len</strong>应该返回数据集的大小，而<strong>getitem</strong>应该编写支持数据集索引的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ...    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ...</span><br></pre></td></tr></table></figure>
<p>上面三个方法是最基本的，其中<strong>getitem</strong>是最主要的方法，它规定了如何读取数据。其主要作用是能让该类可以像list一样通过索引值对数据进行访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstDataset</span>(<span class="params">data.Dataset</span>):</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 初始化，定义你用于训练的数据集(文件路径或文件名列表)，以什么比例进行sample（多个数据集的情况），每个epoch训练样本的数目，预处理方法等等</span></span><br><span class="line">        <span class="comment">#也就是在这个模块里，我们所做的工作就是初始化该类的一些基本参数。</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">         <span class="comment">#从文件中读取一个数据（例如，使用numpy.fromfile，PIL.Image.open）。</span></span><br><span class="line">         <span class="comment">#预处理数据（例如torchvision.Transform）。</span></span><br><span class="line">         <span class="comment">#返回数据对（例如图像和标签）。</span></span><br><span class="line">         <span class="comment">#这里需要注意的是，第一步：read one data，是一个data</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 定义为数据集的总大小。</span></span><br></pre></td></tr></table></figure>
<p>图片加载的dataset可以参考帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_33431368/article/details/105463045">《带你详细了解并使用Dataset以及DataLoader》</a><br>人民币二分类参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37388085/article/details/102663166">《pytorch - 数据读取机制中的Dataloader与Dataset》</a></p>
<h2 id="4-dataloader类及其参数"><a href="#4-dataloader类及其参数" class="headerlink" title="4. dataloader类及其参数"></a>4. dataloader类及其参数</h2><p>dataloader类调用torch.utils.Data.DataLoader，实际过程中数据集往往很大，通过DataLoader加载数据集使用mini-batch的时候可以使用多线程并行处理，这样可以加快我们准备数据集的速度。Datasets就是构建这个工具函数的实例参数之一。一般可以这么写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span> ，num_workers=<span class="number">4</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">False</span>，num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>下面看看dataloader代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=default_collate,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 worker_init_fn=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">dataset</span> = <span class="title">dataset</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">batch_size</span> = <span class="title">batch_size</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">num_workers</span> = <span class="title">num_workers</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">collate_fn</span> = <span class="title">collate_fn</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">pin_memory</span> = <span class="title">pin_memory</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">drop_last</span> = <span class="title">drop_last</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">timeout</span> = <span class="title">timeout</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">worker_init_fn</span> = <span class="title">worker_init_fn</span></span></span><br><span class="line"><span class="function"></span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>dataset:Dataset类，PyTorch已有的数据读取接口，决定数据从哪里读取及如何读取；</li>
<li>batch_size：批大小；默认1</li>
<li>num_works:是否多进程读取数据；默认0使用主进程来导入数据。大于0则多进程导入数据，加快数据导入速度</li>
<li>shuffle：每个epoch是否乱序；默认False。输入数据的顺序打乱，是为了使数据更有独立性，但如果数据是有序列特征的，就不要设置成True了。一般shuffle训练集即可。</li>
<li>drop_last:当样本数不能被batchsize整除时，是否舍弃最后一批数据；</li>
<li>collate_fn:将得到的数据整理成一个batch。默认设置是False。如果设置成True，系统会在返回前会将张量数据（Tensors）复制到CUDA内存中。</li>
<li>batch_sampler，批量采样，和batch_size、shuffle等参数是互斥的，一般采用默认None。batch_sampler，但每次返回的是一批数据的索引（注意：不是数据），应该是每次输入网络的数据是随机采样模式，这样能使数据更具有独立性质。所以，它和一捆一捆按顺序输入，数据洗牌，数据采样，等模式是不兼容的。</li>
<li>sampler，默认False。根据定义的策略从数据集中采样输入。如果定义采样规则，则洗牌（shuffle）设置必须为False。</li>
<li>pin_memory，内存寄存，默认为False。在数据返回前，是否将数据复制到CUDA内存中。</li>
<li>timeout，是用来设置数据读取的超时时间的，但超过这个时间还没读取到数据的话就会报错。</li>
<li>worker_init_fn（数据类型 callable），子进程导入模式，默认为Noun。在数据导入前和步长结束后，根据工作子进程的ID逐个按顺序导入数据。</li>
</ul>
<p>想用随机抽取的模式加载输入，可以设置 sampler 或 batch_sampler。如何定义抽样规则，可以看sampler.py脚本，或者这篇帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/aiwanghuan5017/article/details/102147809">《一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系》</a></p>
<h2 id="5-dataloader内部函数"><a href="#5-dataloader内部函数" class="headerlink" title="5. dataloader内部函数"></a>5. dataloader内部函数</h2><h3 id="5-1-next函数"><a href="#5-1-next函数" class="headerlink" title="5.1 next函数"></a>5.1 <strong>next</strong>函数</h3><p>DataLoader<strong>next</strong>函数用for循环来遍历数据进行读取。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span> </span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:   </span><br><span class="line">            indices = <span class="built_in">next</span>(self.sample_iter)  </span><br><span class="line">            batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># this line </span></span><br><span class="line">            <span class="keyword">if</span> self.pin_memory: </span><br><span class="line">                batch = _utils.pin_memory.pin_memory_batch(batch) </span><br><span class="line">            <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><br>仔细看可以发现，前面还有一个self.collate_fn方法，这个是干嘛用的呢?在介绍前我们需要知道每个参数的意义：</p>
<ul>
<li>indices: 表示每一个iteration，sampler返回的indices，即一个batch size大小的索引列表</li>
<li>self.dataset[i]: 前面已经介绍了，这里就是对第i个数据进行读取操作，一般来说self.dataset[i]=(img, label)</li>
</ul>
<p>看到这不难猜出collate<em>fn的作用就是将一个batch的数据进行合并操作。默认的collate<em>fn是将img和label分别合并成imgs和labels，所以如果你的__getitem</em></em>方法只是返回 img, label,那么你可以使用默认的collate_fn方法，但是如果你每次读取的数据有img, box, label等等，那么你就需要自定义collate_fn来将对应的数据合并成一个batch数据，这样方便后续的训练步骤。</p>
<h3 id="5-2-DataLoaderIter函数"><a href="#5-2-DataLoaderIter函数" class="headerlink" title="5.2 DataLoaderIter函数"></a>5.2 DataLoaderIter函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, attr, val</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.__initialized <span class="keyword">and</span> attr <span class="keyword">in</span> (<span class="string">&#x27;batch_size&#x27;</span>, <span class="string">&#x27;sampler&#x27;</span>, <span class="string">&#x27;drop_last&#x27;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;&#123;&#125; attribute should not be set after &#123;&#125; is &#x27;</span></span><br><span class="line">                             <span class="string">&#x27;initialized&#x27;</span>.<span class="built_in">format</span>(attr, self.__class__.__name__))</span><br><span class="line">        <span class="built_in">super</span>(DataLoader, self).__setattr__(attr, val)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> _DataLoaderIter(self)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.batch_sampler)</span><br></pre></td></tr></table></figure>
<p>当代码运行到要从torch.utils.data.DataLoader类生成的对象中取数据的时候，比如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data=torch.utils.data.DataLoader(...)</span><br><span class="line"><span class="keyword">for</span> i, (<span class="built_in">input</span>, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data):</span><br></pre></td></tr></table></figure><br>就会调用DataLoader类的<strong>iter</strong>方法：return DataLoaderIter(self)，此时牵扯到DataLoaderIter类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>)：</span></span><br><span class="line"><span class="function">	 <span class="title">if</span> <span class="title">self</span>.<span class="title">num_workers</span> == 0:</span></span><br><span class="line">            <span class="keyword">return</span> _SingleProcessDataLoaderIter(self)</span><br><span class="line">	 <span class="keyword">else</span>:</span><br><span class="line">            self.check_worker_number_rationality()</span><br><span class="line">            <span class="keyword">return</span> _MultiProcessingDataLoaderIter(self)</span><br></pre></td></tr></table></figure>
<ul>
<li>SingleProcessDataLoaderIter：单线程数据迭代，采用普通方式来读取数据</li>
<li>MultiProcessingDataLoaderIter：多进程数据迭代，采用队列的方式来读取。 </li>
</ul>
<p>MultiProcessingDataLoaderIter继承的是BaseDataLoaderIter,开始初始化，然后Dataloader进行初始化，然后进入 next __（）方法 随机生成索引，进而生成batch，最后调用 _get_data() 方法得到data。idx, data = self._get_data()， data = self.data_queue.get(timeout=timeout)</p>
<hr>
<p>总结一下：</p>
<ol>
<li>调用了dataloader 的<strong>iter</strong>() 方法, 产生了一个DataLoaderIter</li>
<li>反复调用DataLoaderIter 的<strong>next</strong>()来得到batch, 具体操作就是, 多次调用dataset的<strong>getitem</strong>()方法 (如果num_worker&gt;0就多线程调用), 然后用collate_fn来把它们打包成batch. 中间还会涉及到shuffle , 以及sample 的方法等,</li>
<li>当数据读完后, next()抛出一个StopIteration异常, for循环结束, dataloader 失效.</li>
</ol>
<p>DataLoaderIter的源码及详细解读参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u014380165/article/details/79058479">《PyTorch源码解读之torch.utils.data.DataLoader》</a></p>
<h2 id="6-dataloader循环"><a href="#6-dataloader循环" class="headerlink" title="6. dataloader循环"></a>6. dataloader循环</h2><p>ataloader本质上是一个可迭代对象，但是dataloader不能像列表那样用索引的形式去访问，而是使用迭代遍历的方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dataLoader:</span><br><span class="line">	<span class="built_in">print</span>(i.keys())</span><br></pre></td></tr></table></figure>
<p>也可以使用enumerate(dataloader)的形式访问。<br>在计算i的类型时，发现其为一个字典，打印这个字典的关键字可得到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dataLoader:</span><br><span class="line">	<span class="built_in">print</span>(i.keys())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict_keys([<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;audio&#x27;</span>, <span class="string">&#x27;vision&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>同理，计算 <strong>i[‘text’]</strong>发现其为一个张量，打印该张量信息<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(i[<span class="string">&#x27;text&#x27;</span>].shape)  <span class="comment">#64*39*768</span></span><br></pre></td></tr></table></figure><br>此时的64恰好就是我们设置的batchsize，并且最后一个i值的text的shape为24<em>39</em>768，即24个数据</p>
<h1 id="二、代码示例"><a href="#二、代码示例" class="headerlink" title="二、代码示例"></a>二、代码示例</h1><h2 id="1-transformer单句文本分类（HF教程）"><a href="#1-transformer单句文本分类（HF教程）" class="headerlink" title="1. transformer单句文本分类（HF教程）"></a>1. transformer单句文本分类（HF教程）</h2><h3 id="1-1使用Trainer训练"><a href="#1-1使用Trainer训练" class="headerlink" title="1.1使用Trainer训练"></a>1.1使用Trainer训练</h3><p>GLUE榜单包含了9个句子级别的分类任务，分别是：</p>
<ul>
<li>CoLA (Corpus of Linguistic Acceptability) 鉴别一个句子是否语法正确.</li>
<li>MNLI (Multi-Genre Natural Language Inference) 给定一个假设，判断另一个句子与该假设的关系：entails, contradicts 或者 unrelated。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus) 判断两个句子是否互为paraphrases.</li>
<li>QNLI (Question-answering Natural Language Inference) 判断第2句是否包含第1句问题的答案。</li>
<li>QQP (Quora Question Pairs2) 判断两个问句是否语义相同。</li>
<li>RTE (Recognizing Textual Entailment)判断一个句子是否与假设成entail关系。</li>
<li>SST-2 (Stanford Sentiment Treebank) 判断一个句子的情感正负向.</li>
<li>STS-B (Semantic Textual Similarity Benchmark) 判断两个句子的相似性（分数为1-5分）。</li>
<li>WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not.</li>
</ul>
<p>加载数据集<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>,<span class="string">&quot;sst2&quot;</span>)</span><br></pre></td></tr></table></figure><br>预处理数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;sentence&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">full_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">full_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>]</span><br></pre></td></tr></table></figure></p>
<p>定义评估函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"></span><br><span class="line">metric = load_metric(<span class="string">&quot;glue&quot;</span>,<span class="string">&quot;sst2&quot;</span>)<span class="comment">#改成&quot;accuracy&quot;效果一样吗？</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><br>加载模型<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>配置 Trainer参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments，Trainer</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;ft-sst2&quot;</span>,                          <span class="comment"># 输出路径，存放检查点和其他输出文件</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,        <span class="comment"># 定义每轮结束后进行评价</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,                 <span class="comment"># 定义初始学习率</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,     <span class="comment"># 定义训练批次大小</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">16</span>,      <span class="comment"># 定义测试批次大小</span></span><br><span class="line">    num_train_epochs=<span class="number">2</span>,                 <span class="comment"># 定义训练轮数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=small_train_dataset,</span><br><span class="line">    eval_dataset=small_eval_dataset,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>开始训练：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><br>训练完毕后，执行以下代码，得到模型在验证集上的效果：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;epoch&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;eval_loss&#x27;</span>: <span class="number">0.9351930022239685</span>，</span><br><span class="line"> <span class="string">&#x27;eval_accuracy&#x27;</span><span class="string">&#x27;: 0.7350917431192661</span></span><br><span class="line"><span class="string"> &#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-使用-PyTorch进行训练"><a href="#1-2-使用-PyTorch进行训练" class="headerlink" title="1.2 使用 PyTorch进行训练"></a>1.2 使用 PyTorch进行训练</h3><p>重新启动笔记本以释放一些内存，或执行以下代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> model</span><br><span class="line"><span class="keyword">del</span> pytorch_model</span><br><span class="line"><span class="keyword">del</span> trainer</span><br><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure><br>首先，我们需要定义数据加载器，我们将使用它来迭代批次。 在这样做之前，我们只需要对我们的 tokenized_datasets 应用一些后处理：</p>
<ol>
<li>删除与模型不期望的值相对应的列（此处为“text”列）</li>
<li>将列“label”重命名为“labels”（因为模型期望参数被命名为标签）</li>
<li>设置数据集的格式，以便它们返回 PyTorch 张量而不是列表。</li>
</ol>
<p>tokenized_datasets 对每个步骤处理如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([<span class="string">&quot;sentence&quot;</span>,<span class="string">&quot;idx&quot;</span>])<span class="comment">#删除多余的“sebtence”列和“idx”列,否则会报错forward() got an unexpected keyword argument &#x27;idx&#x27;</span></span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)<span class="comment">#列“label”重命名为“labels”，否则报错forward() got an unexpected keyword argument &#x27;label&#x27;</span></span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)<span class="comment">#返回 PyTorch 张量，否则报错&#x27;list&#x27; object has no attribute &#x27;size&#x27;</span></span><br></pre></td></tr></table></figure>
<p>二三步也可以合并：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">columns = [<span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">tokenized_datasets.set_format(<span class="built_in">type</span>=<span class="string">&#x27;torch&#x27;</span>, columns=columns)</span><br></pre></td></tr></table></figure>
<p>切出一部分数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>定义dataloaders：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>)</span><br><span class="line">eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>定义模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>定义优化器optimizer 和学习率调度器scheduler：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#默认使用的学习率调度器只是线性衰减从最大值（此处为 5e-5）到 0：</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>使用GPU进行训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>使用 tqdm 库在训练步骤数上添加了一个进度条，并定义训练循环：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()<span class="comment">#设置train状态，启用 Batch Normalization 和 Dropout。</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>编写评估循环，在循环完成时计算最终结果之前累积每个批次的预测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric= load_metric(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"></span><br><span class="line">metric.compute()</span><br></pre></td></tr></table></figure>
<h3 id="1-3-句子对文本分类（rte）："><a href="#1-3-句子对文本分类（rte）：" class="headerlink" title="1.3 句子对文本分类（rte）："></a>1.3 句子对文本分类（rte）：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = load_dataset(<span class="string">&#x27;glue&#x27;</span>, <span class="string">&#x27;rte&#x27;</span>)</span><br><span class="line">metric = load_metric(<span class="string">&#x27;glue&#x27;</span>, <span class="string">&#x27;rte&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&#x27;bert-base-cased&#x27;</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;bert-base-cased&#x27;</span>, return_dict=<span class="literal">True</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&#x27;hypothesis&#x27;</span>],examples[<span class="string">&#x27;premiere&#x27;</span>] truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>)</span><br><span class="line">dataset = dataset.<span class="built_in">map</span>(tokenize, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>其它代码一样.更多文本分类参考datawhale-transformer教程4.1：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">《文本分类》</a></p>
<h3 id="1-4-更多示例"><a href="#1-4-更多示例" class="headerlink" title="1.4 更多示例"></a>1.4 更多示例</h3><p>要查看更多微调示例，您可以参考：<br>🤗<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/examples">Transformers Examples</a>，其中包括在 PyTorch 和 TensorFlow 中训练所有常见 NLP 任务的脚本。<br>🤗  <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/notebooks.html">Transformers Notebooks</a> ，其中包含各种笔记本，尤其是每个任务一个（查找如何在 xxx 上微调模型）。</p>
<h2 id="2-科大讯飞中文相似度代码赏析"><a href="#2-科大讯飞中文相似度代码赏析" class="headerlink" title="2. 科大讯飞中文相似度代码赏析"></a>2. 科大讯飞中文相似度代码赏析</h2><p>转载自<a target="_blank" rel="noopener" href="https://gitee.com/coggle/competition-baseline/blob/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%AD%E6%96%87%E9%97%AE%E9%A2%98%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8C%91%E6%88%98%E8%B5%9B/bert-nsp-xunfei.ipynb">《10分钟 杀入科大讯飞中文相似度 Top10！》</a></p>
<h3 id="2-1赛题解析"><a href="#2-1赛题解析" class="headerlink" title="2.1赛题解析"></a>2.1赛题解析</h3><ul>
<li><p>赛题名称：中文问题相似度挑战赛<br><a target="_blank" rel="noopener" href="http://challenge.xfyun.cn/topic/info?type=chinese-question-similarity&amp;ch=dw-sq-1">http://challenge.xfyun.cn/topic/info?type=chinese-question-similarity&amp;ch=dw-sq-1</a></p>
</li>
<li><p>赛题介绍<br>重复问题检测是一个常见的文本挖掘任务，在很多实际问答社区都有相应的应用。重复问题检测可以方便进行问题的答案聚合，以及问题答案推荐，自动QA等。由于中文词语的多样性和灵活性，本赛题需要选手构建一个重复问题识别算法。</p>
</li>
<li><p>赛题任务<br>本次赛题希望参赛选手对两个问题完成相似度打分。</p>
</li>
<li><p>训练集：约5千条问题对和标签。若两个问题是相同含义，标签为1；否则为0。<br>测试集：约5千条问题对。</p>
</li>
<li><p>训练集样例：<br>句子1：有哪些女明星被潜规则啦<br>句子2：哪些女明星被潜规则了<br>标签：1<br>句子1：泰囧完整版下载<br>句子2：エウテルペ完整版下载<br>标签：0</p>
</li>
<li>解题思路<br>赛题为经典的文本匹配任务，所以可以考虑使用Bert的NSP来完成建模。</li>
</ul>
<h3 id="2-2-代码实例"><a href="#2-2-代码实例" class="headerlink" title="2.2 代码实例"></a>2.2 代码实例</h3><p>步骤1：读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, names=[<span class="string">&#x27;question1&#x27;</span>, <span class="string">&#x27;question2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<p>import re<br>并按照标签划分验证集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># stratify 按照标签进行采样，训练集和验证部分同分布</span></span><br><span class="line">q1_train, q1_val, q2_train, q2_val, train_label, test_label =  train_test_split(</span><br><span class="line">    train_df[<span class="string">&#x27;question1&#x27;</span>].iloc[:], </span><br><span class="line">    train_df[<span class="string">&#x27;question2&#x27;</span>].iloc[:],</span><br><span class="line">    train_df[<span class="string">&#x27;label&#x27;</span>].iloc[:],</span><br><span class="line">    test_size=<span class="number">0.1</span>, </span><br><span class="line">    stratify=train_df[<span class="string">&#x27;label&#x27;</span>].iloc[:])</span><br></pre></td></tr></table></figure>
<p>步骤2：文本进行tokenizer<br>使用Bert对文本进行转换，此时模型选择bert-base-chinese。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pip install transformers</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">train_encoding = tokenizer(<span class="built_in">list</span>(q1_train), <span class="built_in">list</span>(q2_train), </span><br><span class="line">                           truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br><span class="line">val_encoding = tokenizer(<span class="built_in">list</span>(q1_val), <span class="built_in">list</span>(q2_val), </span><br><span class="line">                          truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>步骤3：定义dataset</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据集读取</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XFeiDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encodings, labels</span>):</span></span><br><span class="line">        self.encodings = encodings</span><br><span class="line">        self.labels = labels</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 读取单个样本</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        item = &#123;key: torch.tensor(val[idx]) <span class="keyword">for</span> key, val <span class="keyword">in</span> self.encodings.items()&#125;</span><br><span class="line">        item[<span class="string">&#x27;labels&#x27;</span>] = torch.tensor(<span class="built_in">int</span>(self.labels[idx]))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">train_dataset = XFeiDataset(train_encoding, <span class="built_in">list</span>(train_label))</span><br><span class="line">val_dataset = XFeiDataset(val_encoding, <span class="built_in">list</span>(test_label))</span><br></pre></td></tr></table></figure>
<p>步骤4：定义匹配模型<br>使用BertForNextSentencePrediction完成文本匹配任务，并定义优化器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForNextSentencePrediction, AdamW, get_linear_schedule_with_warmup</span><br><span class="line">model = BertForNextSentencePrediction.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个读取到批量读取</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化方法</span></span><br><span class="line">optim = AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 精度计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flat_accuracy</span>(<span class="params">preds, labels</span>):</span></span><br><span class="line">    pred_flat = np.argmax(preds, axis=<span class="number">1</span>).flatten()</span><br><span class="line">    labels_flat = labels.flatten()</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(pred_flat == labels_flat) / <span class="built_in">len</span>(labels_flat)</span><br></pre></td></tr></table></figure>
<p>步骤5：模型训练与验证<br>祖传代码：模型正向传播和准确率计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    model.train()</span><br><span class="line">    total_train_loss = <span class="number">0</span></span><br><span class="line">    iter_num = <span class="number">0</span></span><br><span class="line">    total_iter = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 正向传播</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        total_train_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向梯度信息</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optim.step()</span><br><span class="line"></span><br><span class="line">        iter_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span>(iter_num % <span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoth: %d, iter_num: %d, loss: %.4f, %.2f%%&quot;</span> % (epoch, iter_num, loss.item(), iter_num/total_iter*<span class="number">100</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: %d, Average training loss: %.4f&quot;</span>%(epoch, total_train_loss/<span class="built_in">len</span>(train_loader)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_eval_accuracy = <span class="number">0</span></span><br><span class="line">    total_eval_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> val_dataloader:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># 正常传播</span></span><br><span class="line">            input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">            attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        </span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        logits = outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        total_eval_loss += loss.item()</span><br><span class="line">        logits = logits.detach().cpu().numpy()</span><br><span class="line">        label_ids = labels.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line">        total_eval_accuracy += flat_accuracy(logits, label_ids)</span><br><span class="line">        </span><br><span class="line">    avg_val_accuracy = total_eval_accuracy / <span class="built_in">len</span>(val_dataloader)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.4f&quot;</span> % (avg_val_accuracy))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Average testing loss: %.4f&quot;</span>%(total_eval_loss/<span class="built_in">len</span>(val_dataloader)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-------------------------------&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------Epoch: %d ----------------&quot;</span> % epoch)</span><br><span class="line">    train()</span><br><span class="line">    validation()</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">f&#x27;model_<span class="subst">&#123;epoch&#125;</span>.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#打印输出看看</span></span><br><span class="line">outputs = model(input_ids,attention_mask=attention_mask,labels=labels)</span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line">NextSentencePredictorOutput(loss=tensor(<span class="number">0.4528</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>), logits=tensor([[ <span class="number">2.7850</span>,  <span class="number">1.2451</span>],</span><br><span class="line">        [ <span class="number">3.9663</span>, -<span class="number">0.9795</span>],</span><br><span class="line">        [ <span class="number">0.1072</span>,  <span class="number">4.8910</span>],</span><br><span class="line">        [ <span class="number">3.2274</span>,  <span class="number">0.4685</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>), hidden_states=<span class="literal">None</span>, attentions=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>步骤6：对测试集进行预测<br>读取测试集数据，进行转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, names=[<span class="string">&#x27;question1&#x27;</span>, <span class="string">&#x27;question2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br><span class="line">test_df[<span class="string">&#x27;label&#x27;</span>] = test_df[<span class="string">&#x27;label&#x27;</span>].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_encoding = tokenizer(<span class="built_in">list</span>(test_df[<span class="string">&#x27;question1&#x27;</span>]), <span class="built_in">list</span>(test_df[<span class="string">&#x27;question2&#x27;</span>]), </span><br><span class="line">                          truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br><span class="line">test_dataset = XFeiDataset(test_encoding, <span class="built_in">list</span>(test_df[<span class="string">&#x27;label&#x27;</span>]))</span><br><span class="line">test_dataloader = DataLoader(test_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>对测试集数据进行正向传播预测，得到预测结果，并输出指定格式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_predict = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_dataloader:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># 正常传播</span></span><br><span class="line">            input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">            attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        </span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        logits = outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        logits = logits.detach().cpu().numpy()</span><br><span class="line">        label_ids = labels.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line">        test_predict += <span class="built_in">list</span>(np.argmax(logits, axis=<span class="number">1</span>).flatten())</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> test_predict</span><br><span class="line">    </span><br><span class="line">test_label = predict()</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_label&#125;).to_csv(<span class="string">&#x27;submit.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-CCF-BDCI-剧本角色情感识别"><a href="#3-CCF-BDCI-剧本角色情感识别" class="headerlink" title="3. CCF BDCI 剧本角色情感识别"></a>3. CCF BDCI 剧本角色情感识别</h2><p>本节转自<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/xl-MAlI1KroZrmpWGttEuA">《CCF BDCI 剧本角色情感识别：多目标学习开源方案》</a></p>
<h3 id="3-1-赛事解析"><a href="#3-1-赛事解析" class="headerlink" title="3.1 赛事解析"></a>3.1 赛事解析</h3><ol>
<li><p>赛题名称<br>剧本角色情感识别 比赛链接：<a target="_blank" rel="noopener" href="https://www.datafountain.cn/competitions/518">https://www.datafountain.cn/competitions/518</a><br>后台回复“爱奇艺”可以获取完整代码</p>
</li>
<li><p>赛题背景<br>剧本对影视行业的重要性不言而喻。一部好的剧本，不光是好口碑和大流量的基础，也能带来更高的商业回报。剧本分析是影视内容生产链条的第一环，其中剧本角色的情感识别是一个非常重要的任务，主要是对剧本中每句对白和动作描述中涉及到的每个角色从多个维度进行分析并识别出情感。相对于通常的新闻、评论性文本的情感分析，有其独有的业务特点和挑战。</p>
</li>
<li><p>赛题任务<br>本赛题提供一部分电影剧本作为训练集，训练集数据已由人工进行标注，参赛队伍需要对剧本场景中每句对白和动作描述中涉及到的每个角色的情感从多个维度进行分析和识别。该任务的主要难点和挑战包括：1）剧本的行文风格和通常的新闻类语料差别较大，更加口语化；2）剧本中角色情感不仅仅取决于当前的文本，对前文语义可能有深度依赖。</p>
</li>
<li><p>数据简介<br>比赛的数据来源主要是一部分电影剧本，以及爱奇艺标注团队的情感标注结果，主要用于提供给各参赛团队进行模型训练和结果验证使用。</p>
</li>
</ol>
<p>数据说明<br>训练数据：训练数据为txt格式，以英文制表符分隔，首行为表头，字段说明如下：<br>字段名称    类型    描述    说明<br>id    String    数据ID    -<br>content    String    文本内容    剧本对白或动作描写<br>character    String    角色名    文本中提到的角色<br>emotion    String    情感识别结果（按顺序）    爱情感值，乐情感值，惊情感值，怒情感值，恐情感值，哀情感值</p>
<p>备注：</p>
<ul>
<li>本赛题的情感定义共6类（按顺序）：爱、乐、惊、怒、恐、哀；  </li>
<li>情感识别结果：上述6类情感按固定顺序对应的情感值，情感值范围是[0, 1, 2, 3]，0-没有，1-弱，2-中，3-强，以英文半角逗号分隔；  </li>
<li>本赛题不需要识别剧本中的角色名；  文件编码：UTF-8 无BOM编码</li>
</ul>
<ol>
<li>评估标准<br>本赛题算法评分采用常用的均方根误差（RMSE）来计算评分，按照“文本内容+角色名”识别出的6类情感对应的情感值来统计。<br>图片score = 1/(1 + RMSE)</li>
</ol>
<p>其中是yi,j预测的情感值，xi,j是标注的情感值，n是总的测试样本数。最终按score得分来排名。</p>
<ol>
<li>基于预训练模型的对目标学习</li>
</ol>
<p>这个题目可操作的地方有很多，一开始见到这个比赛的时候见想到了multi outputs的模型构建，这里给大家分享下这个基线，希望有大佬能够针对这个思路优化上去~</p>
<h3 id="3-2-代码示例"><a href="#3-2-代码示例" class="headerlink" title="3.2  代码示例"></a>3.2  代码示例</h3><p>加载数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/train_dataset_v2.tsv&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> handler:</span><br><span class="line">    lines = handler.read().split(<span class="string">&#x27;\n&#x27;</span>)[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    data = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(lines):</span><br><span class="line">        sp = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(sp) != <span class="number">4</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;ERROR:&quot;</span>, sp)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        data.append(sp)</span><br><span class="line"></span><br><span class="line">train = pd.DataFrame(data)</span><br><span class="line">train.columns = [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;character&#x27;</span>, <span class="string">&#x27;emotions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/test_dataset.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">submit = pd.read_csv(<span class="string">&#x27;data/submit_example.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train = train[train[<span class="string">&#x27;emotions&#x27;</span>] != <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>提取情感目标<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train[<span class="string">&#x27;emotions&#x27;</span>] = train[<span class="string">&#x27;emotions&#x27;</span>].apply(<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(_i) <span class="keyword">for</span> _i <span class="keyword">in</span> x.split(<span class="string">&#x27;,&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">train[[<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;joy&#x27;</span>, <span class="string">&#x27;fright&#x27;</span>, <span class="string">&#x27;anger&#x27;</span>, <span class="string">&#x27;fear&#x27;</span>, <span class="string">&#x27;sorrow&#x27;</span>]] = train[<span class="string">&#x27;emotions&#x27;</span>].values.tolist()</span><br></pre></td></tr></table></figure></p>
<p>构建数据集<br>数据集的标签一共有六个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoleDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span></span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,item</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item 为数据索引，迭代取第item条数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        sample = &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>: text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>: encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>: encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> label_col <span class="keyword">in</span> target_cols:</span><br><span class="line">            sample[label_col] = torch.tensor(label[label_col], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>
<p>模型构建</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmotionClassifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EmotionClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line">        self.out_love = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_joy = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_fright = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_anger = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_fear = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_sorrow = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span></span><br><span class="line">        _, pooled_output = self.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            return_dict = <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        love = self.out_love(pooled_output)</span><br><span class="line">        joy = self.out_joy(pooled_output)</span><br><span class="line">        fright = self.out_fright(pooled_output)</span><br><span class="line">        anger = self.out_anger(pooled_output)</span><br><span class="line">        fear = self.out_fear(pooled_output)</span><br><span class="line">        sorrow = self.out_sorrow(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;love&#x27;</span>: love, <span class="string">&#x27;joy&#x27;</span>: joy, <span class="string">&#x27;fright&#x27;</span>: fright,</span><br><span class="line">            <span class="string">&#x27;anger&#x27;</span>: anger, <span class="string">&#x27;fear&#x27;</span>: fear, <span class="string">&#x27;sorrow&#x27;</span>: sorrow,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>6.4 模型训练<br>回归损失函数直接选取 nn.MSELoss()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EPOCHS = <span class="number">1</span> <span class="comment"># 训练轮数</span></span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">total_steps = <span class="built_in">len</span>(train_data_loader) * EPOCHS</span><br><span class="line"></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">  optimizer,</span><br><span class="line">  num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">  num_training_steps=total_steps</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss().to(device)</span><br></pre></td></tr></table></figure>
<p>模型总的loss为六个目标值的loss之和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">  model, </span></span></span><br><span class="line"><span class="params"><span class="function">  data_loader, </span></span></span><br><span class="line"><span class="params"><span class="function">  criterion, </span></span></span><br><span class="line"><span class="params"><span class="function">  optimizer, </span></span></span><br><span class="line"><span class="params"><span class="function">  device, </span></span></span><br><span class="line"><span class="params"><span class="function">  scheduler, </span></span></span><br><span class="line"><span class="params"><span class="function">  n_examples</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    model = model.train()</span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tqdm(data_loader):</span><br><span class="line">        input_ids = sample[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">        attention_mask = sample[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">        outputs = model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask</span><br><span class="line">        )</span><br><span class="line">        loss_love = criterion(outputs[<span class="string">&#x27;love&#x27;</span>], sample[<span class="string">&#x27;love&#x27;</span>].to(device))</span><br><span class="line">        loss_joy = criterion(outputs[<span class="string">&#x27;joy&#x27;</span>], sample[<span class="string">&#x27;joy&#x27;</span>].to(device))</span><br><span class="line">        loss_fright = criterion(outputs[<span class="string">&#x27;fright&#x27;</span>], sample[<span class="string">&#x27;fright&#x27;</span>].to(device))</span><br><span class="line">        loss_anger = criterion(outputs[<span class="string">&#x27;anger&#x27;</span>], sample[<span class="string">&#x27;anger&#x27;</span>].to(device))</span><br><span class="line">        loss_fear = criterion(outputs[<span class="string">&#x27;fear&#x27;</span>], sample[<span class="string">&#x27;fear&#x27;</span>].to(device))</span><br><span class="line">        loss_sorrow = criterion(outputs[<span class="string">&#x27;sorrow&#x27;</span>], sample[<span class="string">&#x27;sorrow&#x27;</span>].to(device))</span><br><span class="line">        loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">	<span class="comment">#return correct_predictions.double() / (n_examples*6), np.mean(losses)</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(losses)</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/09/25/huggingface/编写transformers的自定义pytorch训练循环（Dataset和DataLoader解析和实例代码）/">https://zhxnlp.github.io/2021/09/25/huggingface/编写transformers的自定义pytorch训练循环（Dataset和DataLoader解析和实例代码）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformers/">transformers</a><a class="post-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/27/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E3%80%81%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/"><i class="fa fa-chevron-left">  </i><span>PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分</span></a></div><div class="next-post pull-right"><a href="/2021/09/18/huggingface/transformers%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94GET%20STARTED/"><span>Hugging Face官方文档——GET STARTED</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>