<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hugging Faceå®˜æ–¹æ–‡æ¡£â€”â€”ç¼–å†™transformersçš„è‡ªå®šä¹‰pytorchè®­ç»ƒå¾ªç¯"><meta name="keywords" content="nlp,transformers,Hugging Face"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>Hugging Faceå®˜æ–¹æ–‡æ¡£â€”â€”ç¼–å†™transformersçš„è‡ªå®šä¹‰pytorchè®­ç»ƒå¾ªç¯ | zhxnlpã®Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"æœç´¢æ–‡ç« ","hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹:${query}","hits_stats":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼Œç”¨æ—¶ ${time} æ¯«ç§’"}},
  localSearch: undefined,
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpã®Blog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="åˆ‡æ¢æ–‡ç« è¯¦æƒ…">åˆ‡æ¢ç«™ç‚¹æ¦‚è§ˆ</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">ç›®å½•</div><div class="sidebar-toc__progress"><span class="progress-notice">ä½ å·²ç»è¯»äº†</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Dataset%E5%92%8CDataLoader%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">ä¸€ã€Datasetå’ŒDataLoaderåŠ è½½æ•°æ®é›†</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-torch-utils-data"><span class="toc-text">1.torch.utils.data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">2. åŠ è½½æ•°æ®æµç¨‹</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Dataset"><span class="toc-text">3. Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-dataloader%E7%B1%BB%E5%8F%8A%E5%85%B6%E5%8F%82%E6%95%B0"><span class="toc-text">4. dataloaderç±»åŠå…¶å‚æ•°</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-dataloader%E5%86%85%E9%83%A8%E5%87%BD%E6%95%B0"><span class="toc-text">5. dataloaderå†…éƒ¨å‡½æ•°</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-next%E5%87%BD%E6%95%B0"><span class="toc-text">5.1 nextå‡½æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-DataLoaderIter%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 DataLoaderIterå‡½æ•°</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-dataloader%E5%BE%AA%E7%8E%AF"><span class="toc-text">6. dataloaderå¾ªç¯</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">äºŒã€ä»£ç ç¤ºä¾‹</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-transformer%E5%8D%95%E5%8F%A5%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%88HF%E6%95%99%E7%A8%8B%EF%BC%89"><span class="toc-text">1. transformerå•å¥æ–‡æœ¬åˆ†ç±»ï¼ˆHFæ•™ç¨‹ï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E4%BD%BF%E7%94%A8Trainer%E8%AE%AD%E7%BB%83"><span class="toc-text">1.1ä½¿ç”¨Trainerè®­ç»ƒ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%BD%BF%E7%94%A8-PyTorch%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">1.2 ä½¿ç”¨ PyTorchè¿›è¡Œè®­ç»ƒ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%8F%A5%E5%AD%90%E5%AF%B9%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%88rte%EF%BC%89%EF%BC%9A"><span class="toc-text">1.3 å¥å­å¯¹æ–‡æœ¬åˆ†ç±»ï¼ˆrteï¼‰ï¼š</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E6%9B%B4%E5%A4%9A%E7%A4%BA%E4%BE%8B"><span class="toc-text">1.4 æ›´å¤šç¤ºä¾‹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E4%B8%AD%E6%96%87%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BB%A3%E7%A0%81%E8%B5%8F%E6%9E%90"><span class="toc-text">2. ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ä»£ç èµæ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E8%B5%9B%E9%A2%98%E8%A7%A3%E6%9E%90"><span class="toc-text">2.1èµ›é¢˜è§£æ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B"><span class="toc-text">2.2 ä»£ç å®ä¾‹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-CCF-BDCI-%E5%89%A7%E6%9C%AC%E8%A7%92%E8%89%B2%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB"><span class="toc-text">3. CCF BDCI å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%B5%9B%E4%BA%8B%E8%A7%A3%E6%9E%90"><span class="toc-text">3.1 èµ›äº‹è§£æ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">3.2  ä»£ç ç¤ºä¾‹</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">æ–‡ç« </span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">æ ‡ç­¾</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">åˆ†ç±»</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpã®Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> æœç´¢</span></a></span></div><div id="post-info"><div id="post-title">Hugging Faceå®˜æ–¹æ–‡æ¡£â€”â€”ç¼–å†™transformersçš„è‡ªå®šä¹‰pytorchè®­ç»ƒå¾ªç¯</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/HuggingFace/">HuggingFace</a><div class="post-meta-wordcount"><span>å­—æ•°æ€»è®¡: </span><span class="word-count">6.1k</span><span class="post-meta__separator">|</span><span>é˜…è¯»æ—¶é•¿: 25 åˆ†é’Ÿ</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>@[toc]</p>
<h1 id="ä¸€ã€Datasetå’ŒDataLoaderåŠ è½½æ•°æ®é›†"><a href="#ä¸€ã€Datasetå’ŒDataLoaderåŠ è½½æ•°æ®é›†" class="headerlink" title="ä¸€ã€Datasetå’ŒDataLoaderåŠ è½½æ•°æ®é›†"></a>ä¸€ã€Datasetå’ŒDataLoaderåŠ è½½æ•°æ®é›†</h1><h2 id="1-torch-utils-data"><a href="#1-torch-utils-data" class="headerlink" title="1.torch.utils.data"></a>1.torch.utils.data</h2><p>torch.utils.dataä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªç±»ï¼š </p>
<ol>
<li>class torch.utils.data.Dataset<br>å…¶ä»–çš„æ•°æ®é›†ç±»å¿…é¡»æ˜¯torch.utils.data.Datasetçš„å­ç±»,æ¯”å¦‚è¯´torchvision.ImageFolder. </li>
<li>class torch.utils.data.sampler.Sampler(data_source)<br>å‚æ•°: data_source (Dataset) â€“ dataset to sample from<br>ä½œç”¨: åˆ›å»ºä¸€ä¸ªé‡‡æ ·å™¨, class torch.utils.data.sampler.Sampleræ˜¯æ‰€æœ‰çš„Samplerçš„åŸºç±», å…¶ä¸­,iter(self)å‡½æ•°æ¥è·å–ä¸€ä¸ªè¿­ä»£å™¨,å¯¹æ•°æ®é›†ä¸­å…ƒç´ çš„ç´¢å¼•è¿›è¡Œè¿­ä»£,len(self)æ–¹æ³•è¿”å›è¿­ä»£å™¨ä¸­åŒ…å«å…ƒç´ çš„é•¿åº¦. </li>
<li>class torch.utils.data.DataLoader<span id="more"></span>
<h2 id="2-åŠ è½½æ•°æ®æµç¨‹"><a href="#2-åŠ è½½æ•°æ®æµç¨‹" class="headerlink" title="2. åŠ è½½æ•°æ®æµç¨‹"></a>2. åŠ è½½æ•°æ®æµç¨‹</h2>pytorchä¸­åŠ è½½æ•°æ®çš„é¡ºåºæ˜¯ï¼š</li>
<li>åŠ è½½æ•°æ®ï¼Œæå–å‡ºfeatureå’Œlabelï¼Œå¹¶è½¬æ¢æˆtensor</li>
<li>åˆ›å»ºä¸€ä¸ªdatasetå¯¹è±¡</li>
<li>åˆ›å»ºä¸€ä¸ªdataloaderå¯¹è±¡ï¼Œdataloaderç±»çš„ä½œç”¨å°±æ˜¯å®ç°æ•°æ®ä»¥ä»€ä¹ˆæ–¹å¼è¾“å…¥åˆ°ä»€ä¹ˆç½‘ç»œä¸­</li>
<li>å¾ªç¯dataloaderå¯¹è±¡ï¼Œå°†data,labelæ‹¿åˆ°æ¨¡å‹ä¸­å»è®­ç»ƒ<br>ä»£ç ä¸€èˆ¬æ˜¯è¿™ä¹ˆå†™çš„ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å®šä¹‰å­¦ä¹ é›† DataLoader</span></span><br><span class="line">train_data = torch.utils.data.DataLoader(å„ç§è®¾ç½®...) </span><br><span class="line"><span class="comment"># å°†æ•°æ®å–‚å…¥ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒ</span></span><br><span class="line"><span class="keyword">for</span> i, (<span class="built_in">input</span>, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data): </span><br><span class="line">    å¾ªç¯ä»£ç è¡Œ......</span><br></pre></td></tr></table></figure>
<h2 id="3-Dataset"><a href="#3-Dataset" class="headerlink" title="3. Dataset"></a>3. Dataset</h2><p>Datasetæ˜¯æˆ‘ä»¬ç”¨çš„æ•°æ®é›†çš„åº“ï¼Œæ˜¯Pytorchä¸­æ‰€æœ‰æ•°æ®é›†åŠ è½½ç±»ä¸­åº”è¯¥ç»§æ‰¿çš„çˆ¶ç±»ã€‚å…¶ä¸­çˆ¶ç±»ä¸­çš„ä¸¤ä¸ªç§æœ‰æˆå‘˜å‡½æ•°å¿…é¡»è¢«é‡è½½ï¼Œå¦åˆ™å°†ä¼šè§¦å‘é”™è¯¯æç¤ºã€‚å…¶ä¸­<strong>len</strong>åº”è¯¥è¿”å›æ•°æ®é›†çš„å¤§å°ï¼Œè€Œ<strong>getitem</strong>åº”è¯¥ç¼–å†™æ”¯æŒæ•°æ®é›†ç´¢å¼•çš„å‡½æ•°</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ...    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ...</span><br></pre></td></tr></table></figure>
<p>ä¸Šé¢ä¸‰ä¸ªæ–¹æ³•æ˜¯æœ€åŸºæœ¬çš„ï¼Œå…¶ä¸­<strong>getitem</strong>æ˜¯æœ€ä¸»è¦çš„æ–¹æ³•ï¼Œå®ƒè§„å®šäº†å¦‚ä½•è¯»å–æ•°æ®ã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯èƒ½è®©è¯¥ç±»å¯ä»¥åƒlistä¸€æ ·é€šè¿‡ç´¢å¼•å€¼å¯¹æ•°æ®è¿›è¡Œè®¿é—®ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstDataset</span>(<span class="params">data.Dataset</span>):</span><span class="comment">#éœ€è¦ç»§æ‰¿data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åˆå§‹åŒ–ï¼Œå®šä¹‰ä½ ç”¨äºè®­ç»ƒçš„æ•°æ®é›†(æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶ååˆ—è¡¨)ï¼Œä»¥ä»€ä¹ˆæ¯”ä¾‹è¿›è¡Œsampleï¼ˆå¤šä¸ªæ•°æ®é›†çš„æƒ…å†µï¼‰ï¼Œæ¯ä¸ªepochè®­ç»ƒæ ·æœ¬çš„æ•°ç›®ï¼Œé¢„å¤„ç†æ–¹æ³•ç­‰ç­‰</span></span><br><span class="line">        <span class="comment">#ä¹Ÿå°±æ˜¯åœ¨è¿™ä¸ªæ¨¡å—é‡Œï¼Œæˆ‘ä»¬æ‰€åšçš„å·¥ä½œå°±æ˜¯åˆå§‹åŒ–è¯¥ç±»çš„ä¸€äº›åŸºæœ¬å‚æ•°ã€‚</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">         <span class="comment">#ä»æ–‡ä»¶ä¸­è¯»å–ä¸€ä¸ªæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨numpy.fromfileï¼ŒPIL.Image.openï¼‰ã€‚</span></span><br><span class="line">         <span class="comment">#é¢„å¤„ç†æ•°æ®ï¼ˆä¾‹å¦‚torchvision.Transformï¼‰ã€‚</span></span><br><span class="line">         <span class="comment">#è¿”å›æ•°æ®å¯¹ï¼ˆä¾‹å¦‚å›¾åƒå’Œæ ‡ç­¾ï¼‰ã€‚</span></span><br><span class="line">         <span class="comment">#è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬ä¸€æ­¥ï¼šread one dataï¼Œæ˜¯ä¸€ä¸ªdata</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># å®šä¹‰ä¸ºæ•°æ®é›†çš„æ€»å¤§å°ã€‚</span></span><br></pre></td></tr></table></figure>
<p>å›¾ç‰‡åŠ è½½çš„datasetå¯ä»¥å‚è€ƒå¸–å­ï¼š<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_33431368/article/details/105463045">ã€Šå¸¦ä½ è¯¦ç»†äº†è§£å¹¶ä½¿ç”¨Datasetä»¥åŠDataLoaderã€‹</a><br>äººæ°‘å¸äºŒåˆ†ç±»å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37388085/article/details/102663166">ã€Špytorch - æ•°æ®è¯»å–æœºåˆ¶ä¸­çš„Dataloaderä¸Datasetã€‹</a></p>
<h2 id="4-dataloaderç±»åŠå…¶å‚æ•°"><a href="#4-dataloaderç±»åŠå…¶å‚æ•°" class="headerlink" title="4. dataloaderç±»åŠå…¶å‚æ•°"></a>4. dataloaderç±»åŠå…¶å‚æ•°</h2><p>dataloaderç±»è°ƒç”¨torch.utils.Data.DataLoaderï¼Œå®é™…è¿‡ç¨‹ä¸­æ•°æ®é›†å¾€å¾€å¾ˆå¤§ï¼Œé€šè¿‡DataLoaderåŠ è½½æ•°æ®é›†ä½¿ç”¨mini-batchçš„æ—¶å€™å¯ä»¥ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«æˆ‘ä»¬å‡†å¤‡æ•°æ®é›†çš„é€Ÿåº¦ã€‚Datasetså°±æ˜¯æ„å»ºè¿™ä¸ªå·¥å…·å‡½æ•°çš„å®ä¾‹å‚æ•°ä¹‹ä¸€ã€‚ä¸€èˆ¬å¯ä»¥è¿™ä¹ˆå†™ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span> ï¼Œnum_workers=<span class="number">4</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">False</span>ï¼Œnum_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢çœ‹çœ‹dataloaderä»£ç ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=default_collate,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 worker_init_fn=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">dataset</span> = <span class="title">dataset</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">batch_size</span> = <span class="title">batch_size</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">num_workers</span> = <span class="title">num_workers</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">collate_fn</span> = <span class="title">collate_fn</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">pin_memory</span> = <span class="title">pin_memory</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">drop_last</span> = <span class="title">drop_last</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">timeout</span> = <span class="title">timeout</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">worker_init_fn</span> = <span class="title">worker_init_fn</span></span></span><br><span class="line"><span class="function"></span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>dataset:Datasetç±»ï¼ŒPyTorchå·²æœ‰çš„æ•°æ®è¯»å–æ¥å£ï¼Œå†³å®šæ•°æ®ä»å“ªé‡Œè¯»å–åŠå¦‚ä½•è¯»å–ï¼›</li>
<li>batch_sizeï¼šæ‰¹å¤§å°ï¼›é»˜è®¤1</li>
<li>num_works:æ˜¯å¦å¤šè¿›ç¨‹è¯»å–æ•°æ®ï¼›é»˜è®¤0ä½¿ç”¨ä¸»è¿›ç¨‹æ¥å¯¼å…¥æ•°æ®ã€‚å¤§äº0åˆ™å¤šè¿›ç¨‹å¯¼å…¥æ•°æ®ï¼ŒåŠ å¿«æ•°æ®å¯¼å…¥é€Ÿåº¦</li>
<li>shuffleï¼šæ¯ä¸ªepochæ˜¯å¦ä¹±åºï¼›é»˜è®¤Falseã€‚è¾“å…¥æ•°æ®çš„é¡ºåºæ‰“ä¹±ï¼Œæ˜¯ä¸ºäº†ä½¿æ•°æ®æ›´æœ‰ç‹¬ç«‹æ€§ï¼Œä½†å¦‚æœæ•°æ®æ˜¯æœ‰åºåˆ—ç‰¹å¾çš„ï¼Œå°±ä¸è¦è®¾ç½®æˆTrueäº†ã€‚ä¸€èˆ¬shuffleè®­ç»ƒé›†å³å¯ã€‚</li>
<li>drop_last:å½“æ ·æœ¬æ•°ä¸èƒ½è¢«batchsizeæ•´é™¤æ—¶ï¼Œæ˜¯å¦èˆå¼ƒæœ€åä¸€æ‰¹æ•°æ®ï¼›</li>
<li>collate_fn:å°†å¾—åˆ°çš„æ•°æ®æ•´ç†æˆä¸€ä¸ªbatchã€‚é»˜è®¤è®¾ç½®æ˜¯Falseã€‚å¦‚æœè®¾ç½®æˆTrueï¼Œç³»ç»Ÿä¼šåœ¨è¿”å›å‰ä¼šå°†å¼ é‡æ•°æ®ï¼ˆTensorsï¼‰å¤åˆ¶åˆ°CUDAå†…å­˜ä¸­ã€‚</li>
<li>batch_samplerï¼Œæ‰¹é‡é‡‡æ ·ï¼Œå’Œbatch_sizeã€shuffleç­‰å‚æ•°æ˜¯äº’æ–¥çš„ï¼Œä¸€èˆ¬é‡‡ç”¨é»˜è®¤Noneã€‚batch_samplerï¼Œä½†æ¯æ¬¡è¿”å›çš„æ˜¯ä¸€æ‰¹æ•°æ®çš„ç´¢å¼•ï¼ˆæ³¨æ„ï¼šä¸æ˜¯æ•°æ®ï¼‰ï¼Œåº”è¯¥æ˜¯æ¯æ¬¡è¾“å…¥ç½‘ç»œçš„æ•°æ®æ˜¯éšæœºé‡‡æ ·æ¨¡å¼ï¼Œè¿™æ ·èƒ½ä½¿æ•°æ®æ›´å…·æœ‰ç‹¬ç«‹æ€§è´¨ã€‚æ‰€ä»¥ï¼Œå®ƒå’Œä¸€æ†ä¸€æ†æŒ‰é¡ºåºè¾“å…¥ï¼Œæ•°æ®æ´—ç‰Œï¼Œæ•°æ®é‡‡æ ·ï¼Œç­‰æ¨¡å¼æ˜¯ä¸å…¼å®¹çš„ã€‚</li>
<li>samplerï¼Œé»˜è®¤Falseã€‚æ ¹æ®å®šä¹‰çš„ç­–ç•¥ä»æ•°æ®é›†ä¸­é‡‡æ ·è¾“å…¥ã€‚å¦‚æœå®šä¹‰é‡‡æ ·è§„åˆ™ï¼Œåˆ™æ´—ç‰Œï¼ˆshuffleï¼‰è®¾ç½®å¿…é¡»ä¸ºFalseã€‚</li>
<li>pin_memoryï¼Œå†…å­˜å¯„å­˜ï¼Œé»˜è®¤ä¸ºFalseã€‚åœ¨æ•°æ®è¿”å›å‰ï¼Œæ˜¯å¦å°†æ•°æ®å¤åˆ¶åˆ°CUDAå†…å­˜ä¸­ã€‚</li>
<li>timeoutï¼Œæ˜¯ç”¨æ¥è®¾ç½®æ•°æ®è¯»å–çš„è¶…æ—¶æ—¶é—´çš„ï¼Œä½†è¶…è¿‡è¿™ä¸ªæ—¶é—´è¿˜æ²¡è¯»å–åˆ°æ•°æ®çš„è¯å°±ä¼šæŠ¥é”™ã€‚</li>
<li>worker_init_fnï¼ˆæ•°æ®ç±»å‹ callableï¼‰ï¼Œå­è¿›ç¨‹å¯¼å…¥æ¨¡å¼ï¼Œé»˜è®¤ä¸ºNounã€‚åœ¨æ•°æ®å¯¼å…¥å‰å’Œæ­¥é•¿ç»“æŸåï¼Œæ ¹æ®å·¥ä½œå­è¿›ç¨‹çš„IDé€ä¸ªæŒ‰é¡ºåºå¯¼å…¥æ•°æ®ã€‚</li>
</ul>
<p>æƒ³ç”¨éšæœºæŠ½å–çš„æ¨¡å¼åŠ è½½è¾“å…¥ï¼Œå¯ä»¥è®¾ç½® sampler æˆ– batch_samplerã€‚å¦‚ä½•å®šä¹‰æŠ½æ ·è§„åˆ™ï¼Œå¯ä»¥çœ‹sampler.pyè„šæœ¬ï¼Œæˆ–è€…è¿™ç¯‡å¸–å­ï¼š<a target="_blank" rel="noopener" href="https://blog.csdn.net/aiwanghuan5017/article/details/102147809">ã€Šä¸€æ–‡å¼„æ‡‚Pytorchçš„DataLoader, DataSet, Samplerä¹‹é—´çš„å…³ç³»ã€‹</a></p>
<h2 id="5-dataloaderå†…éƒ¨å‡½æ•°"><a href="#5-dataloaderå†…éƒ¨å‡½æ•°" class="headerlink" title="5. dataloaderå†…éƒ¨å‡½æ•°"></a>5. dataloaderå†…éƒ¨å‡½æ•°</h2><h3 id="5-1-nextå‡½æ•°"><a href="#5-1-nextå‡½æ•°" class="headerlink" title="5.1 nextå‡½æ•°"></a>5.1 <strong>next</strong>å‡½æ•°</h3><p>DataLoader<strong>next</strong>å‡½æ•°ç”¨forå¾ªç¯æ¥éå†æ•°æ®è¿›è¡Œè¯»å–ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span> </span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:   </span><br><span class="line">            indices = <span class="built_in">next</span>(self.sample_iter)  </span><br><span class="line">            batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># this line </span></span><br><span class="line">            <span class="keyword">if</span> self.pin_memory: </span><br><span class="line">                batch = _utils.pin_memory.pin_memory_batch(batch) </span><br><span class="line">            <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><br>ä»”ç»†çœ‹å¯ä»¥å‘ç°ï¼Œå‰é¢è¿˜æœ‰ä¸€ä¸ªself.collate_fnæ–¹æ³•ï¼Œè¿™ä¸ªæ˜¯å¹²å˜›ç”¨çš„å‘¢?åœ¨ä»‹ç»å‰æˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ªå‚æ•°çš„æ„ä¹‰ï¼š</p>
<ul>
<li>indices: è¡¨ç¤ºæ¯ä¸€ä¸ªiterationï¼Œsamplerè¿”å›çš„indicesï¼Œå³ä¸€ä¸ªbatch sizeå¤§å°çš„ç´¢å¼•åˆ—è¡¨</li>
<li>self.dataset[i]: å‰é¢å·²ç»ä»‹ç»äº†ï¼Œè¿™é‡Œå°±æ˜¯å¯¹ç¬¬iä¸ªæ•°æ®è¿›è¡Œè¯»å–æ“ä½œï¼Œä¸€èˆ¬æ¥è¯´self.dataset[i]=(img, label)</li>
</ul>
<p>çœ‹åˆ°è¿™ä¸éš¾çŒœå‡ºcollate<em>fnçš„ä½œç”¨å°±æ˜¯å°†ä¸€ä¸ªbatchçš„æ•°æ®è¿›è¡Œåˆå¹¶æ“ä½œã€‚é»˜è®¤çš„collate<em>fnæ˜¯å°†imgå’Œlabelåˆ†åˆ«åˆå¹¶æˆimgså’Œlabelsï¼Œæ‰€ä»¥å¦‚æœä½ çš„__getitem</em></em>æ–¹æ³•åªæ˜¯è¿”å› img, label,é‚£ä¹ˆä½ å¯ä»¥ä½¿ç”¨é»˜è®¤çš„collate_fnæ–¹æ³•ï¼Œä½†æ˜¯å¦‚æœä½ æ¯æ¬¡è¯»å–çš„æ•°æ®æœ‰img, box, labelç­‰ç­‰ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦è‡ªå®šä¹‰collate_fnæ¥å°†å¯¹åº”çš„æ•°æ®åˆå¹¶æˆä¸€ä¸ªbatchæ•°æ®ï¼Œè¿™æ ·æ–¹ä¾¿åç»­çš„è®­ç»ƒæ­¥éª¤ã€‚</p>
<h3 id="5-2-DataLoaderIterå‡½æ•°"><a href="#5-2-DataLoaderIterå‡½æ•°" class="headerlink" title="5.2 DataLoaderIterå‡½æ•°"></a>5.2 DataLoaderIterå‡½æ•°</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, attr, val</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.__initialized <span class="keyword">and</span> attr <span class="keyword">in</span> (<span class="string">&#x27;batch_size&#x27;</span>, <span class="string">&#x27;sampler&#x27;</span>, <span class="string">&#x27;drop_last&#x27;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;&#123;&#125; attribute should not be set after &#123;&#125; is &#x27;</span></span><br><span class="line">                             <span class="string">&#x27;initialized&#x27;</span>.<span class="built_in">format</span>(attr, self.__class__.__name__))</span><br><span class="line">        <span class="built_in">super</span>(DataLoader, self).__setattr__(attr, val)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> _DataLoaderIter(self)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.batch_sampler)</span><br></pre></td></tr></table></figure>
<p>å½“ä»£ç è¿è¡Œåˆ°è¦ä»torch.utils.data.DataLoaderç±»ç”Ÿæˆçš„å¯¹è±¡ä¸­å–æ•°æ®çš„æ—¶å€™ï¼Œæ¯”å¦‚ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data=torch.utils.data.DataLoader(...)</span><br><span class="line"><span class="keyword">for</span> i, (<span class="built_in">input</span>, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data):</span><br></pre></td></tr></table></figure><br>å°±ä¼šè°ƒç”¨DataLoaderç±»çš„<strong>iter</strong>æ–¹æ³•ï¼šreturn DataLoaderIter(self)ï¼Œæ­¤æ—¶ç‰µæ‰¯åˆ°DataLoaderIterç±»ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>)ï¼š</span></span><br><span class="line"><span class="function">	 <span class="title">if</span> <span class="title">self</span>.<span class="title">num_workers</span> == 0:</span></span><br><span class="line">            <span class="keyword">return</span> _SingleProcessDataLoaderIter(self)</span><br><span class="line">	 <span class="keyword">else</span>:</span><br><span class="line">            self.check_worker_number_rationality()</span><br><span class="line">            <span class="keyword">return</span> _MultiProcessingDataLoaderIter(self)</span><br></pre></td></tr></table></figure>
<ul>
<li>SingleProcessDataLoaderIterï¼šå•çº¿ç¨‹æ•°æ®è¿­ä»£ï¼Œé‡‡ç”¨æ™®é€šæ–¹å¼æ¥è¯»å–æ•°æ®</li>
<li>MultiProcessingDataLoaderIterï¼šå¤šè¿›ç¨‹æ•°æ®è¿­ä»£ï¼Œé‡‡ç”¨é˜Ÿåˆ—çš„æ–¹å¼æ¥è¯»å–ã€‚ </li>
</ul>
<p>MultiProcessingDataLoaderIterç»§æ‰¿çš„æ˜¯BaseDataLoaderIter,å¼€å§‹åˆå§‹åŒ–ï¼Œç„¶åDataloaderè¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶åè¿›å…¥ next __ï¼ˆï¼‰æ–¹æ³• éšæœºç”Ÿæˆç´¢å¼•ï¼Œè¿›è€Œç”Ÿæˆbatchï¼Œæœ€åè°ƒç”¨ _get_data() æ–¹æ³•å¾—åˆ°dataã€‚idx, data = self._get_data()ï¼Œ data = self.data_queue.get(timeout=timeout)</p>
<hr>
<p>æ€»ç»“ä¸€ä¸‹ï¼š</p>
<ol>
<li>è°ƒç”¨äº†dataloader çš„<strong>iter</strong>() æ–¹æ³•, äº§ç”Ÿäº†ä¸€ä¸ªDataLoaderIter</li>
<li>åå¤è°ƒç”¨DataLoaderIter çš„<strong>next</strong>()æ¥å¾—åˆ°batch, å…·ä½“æ“ä½œå°±æ˜¯, å¤šæ¬¡è°ƒç”¨datasetçš„<strong>getitem</strong>()æ–¹æ³• (å¦‚æœnum_worker&gt;0å°±å¤šçº¿ç¨‹è°ƒç”¨), ç„¶åç”¨collate_fnæ¥æŠŠå®ƒä»¬æ‰“åŒ…æˆbatch. ä¸­é—´è¿˜ä¼šæ¶‰åŠåˆ°shuffle , ä»¥åŠsample çš„æ–¹æ³•ç­‰,</li>
<li>å½“æ•°æ®è¯»å®Œå, next()æŠ›å‡ºä¸€ä¸ªStopIterationå¼‚å¸¸, forå¾ªç¯ç»“æŸ, dataloader å¤±æ•ˆ.</li>
</ol>
<p>DataLoaderIterçš„æºç åŠè¯¦ç»†è§£è¯»å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://blog.csdn.net/u014380165/article/details/79058479">ã€ŠPyTorchæºç è§£è¯»ä¹‹torch.utils.data.DataLoaderã€‹</a></p>
<h2 id="6-dataloaderå¾ªç¯"><a href="#6-dataloaderå¾ªç¯" class="headerlink" title="6. dataloaderå¾ªç¯"></a>6. dataloaderå¾ªç¯</h2><p>ataloaderæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼Œä½†æ˜¯dataloaderä¸èƒ½åƒåˆ—è¡¨é‚£æ ·ç”¨ç´¢å¼•çš„å½¢å¼å»è®¿é—®ï¼Œè€Œæ˜¯ä½¿ç”¨è¿­ä»£éå†çš„æ–¹å¼ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dataLoader:</span><br><span class="line">	<span class="built_in">print</span>(i.keys())</span><br></pre></td></tr></table></figure>
<p>ä¹Ÿå¯ä»¥ä½¿ç”¨enumerate(dataloader)çš„å½¢å¼è®¿é—®ã€‚<br>åœ¨è®¡ç®—içš„ç±»å‹æ—¶ï¼Œå‘ç°å…¶ä¸ºä¸€ä¸ªå­—å…¸ï¼Œæ‰“å°è¿™ä¸ªå­—å…¸çš„å…³é”®å­—å¯å¾—åˆ°</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dataLoader:</span><br><span class="line">	<span class="built_in">print</span>(i.keys())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict_keys([<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;audio&#x27;</span>, <span class="string">&#x27;vision&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>åŒç†ï¼Œè®¡ç®— <strong>i[â€˜textâ€™]</strong>å‘ç°å…¶ä¸ºä¸€ä¸ªå¼ é‡ï¼Œæ‰“å°è¯¥å¼ é‡ä¿¡æ¯<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(i[<span class="string">&#x27;text&#x27;</span>].shape)  <span class="comment">#64*39*768</span></span><br></pre></td></tr></table></figure><br>æ­¤æ—¶çš„64æ°å¥½å°±æ˜¯æˆ‘ä»¬è®¾ç½®çš„batchsizeï¼Œå¹¶ä¸”æœ€åä¸€ä¸ªiå€¼çš„textçš„shapeä¸º24<em>39</em>768ï¼Œå³24ä¸ªæ•°æ®</p>
<h1 id="äºŒã€ä»£ç ç¤ºä¾‹"><a href="#äºŒã€ä»£ç ç¤ºä¾‹" class="headerlink" title="äºŒã€ä»£ç ç¤ºä¾‹"></a>äºŒã€ä»£ç ç¤ºä¾‹</h1><h2 id="1-transformerå•å¥æ–‡æœ¬åˆ†ç±»ï¼ˆHFæ•™ç¨‹ï¼‰"><a href="#1-transformerå•å¥æ–‡æœ¬åˆ†ç±»ï¼ˆHFæ•™ç¨‹ï¼‰" class="headerlink" title="1. transformerå•å¥æ–‡æœ¬åˆ†ç±»ï¼ˆHFæ•™ç¨‹ï¼‰"></a>1. transformerå•å¥æ–‡æœ¬åˆ†ç±»ï¼ˆHFæ•™ç¨‹ï¼‰</h2><h3 id="1-1ä½¿ç”¨Trainerè®­ç»ƒ"><a href="#1-1ä½¿ç”¨Trainerè®­ç»ƒ" class="headerlink" title="1.1ä½¿ç”¨Trainerè®­ç»ƒ"></a>1.1ä½¿ç”¨Trainerè®­ç»ƒ</h3><p>GLUEæ¦œå•åŒ…å«äº†9ä¸ªå¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯ï¼š</p>
<ul>
<li>CoLA (Corpus of Linguistic Acceptability) é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®.</li>
<li>MNLI (Multi-Genre Natural Language Inference) ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails, contradicts æˆ–è€… unrelatedã€‚</li>
<li>MRPC (Microsoft Research Paraphrase Corpus) åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrases.</li>
<li>QNLI (Question-answering Natural Language Inference) åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚</li>
<li>QQP (Quora Question Pairs2) åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚</li>
<li>RTE (Recognizing Textual Entailment)åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚</li>
<li>SST-2 (Stanford Sentiment Treebank) åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘.</li>
<li>STS-B (Semantic Textual Similarity Benchmark) åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚</li>
<li>WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not.</li>
</ul>
<p>åŠ è½½æ•°æ®é›†<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>,<span class="string">&quot;sst2&quot;</span>)</span><br></pre></td></tr></table></figure><br>é¢„å¤„ç†æ•°æ®<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;sentence&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">full_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">full_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>]</span><br></pre></td></tr></table></figure></p>
<p>å®šä¹‰è¯„ä¼°å‡½æ•°<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"></span><br><span class="line">metric = load_metric(<span class="string">&quot;glue&quot;</span>,<span class="string">&quot;sst2&quot;</span>)<span class="comment">#æ”¹æˆ&quot;accuracy&quot;æ•ˆæœä¸€æ ·å—ï¼Ÿ</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><br>åŠ è½½æ¨¡å‹<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>é…ç½® Trainerå‚æ•°ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArgumentsï¼ŒTrainer</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;ft-sst2&quot;</span>,                          <span class="comment"># è¾“å‡ºè·¯å¾„ï¼Œå­˜æ”¾æ£€æŸ¥ç‚¹å’Œå…¶ä»–è¾“å‡ºæ–‡ä»¶</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,        <span class="comment"># å®šä¹‰æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,                 <span class="comment"># å®šä¹‰åˆå§‹å­¦ä¹ ç‡</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,     <span class="comment"># å®šä¹‰è®­ç»ƒæ‰¹æ¬¡å¤§å°</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">16</span>,      <span class="comment"># å®šä¹‰æµ‹è¯•æ‰¹æ¬¡å¤§å°</span></span><br><span class="line">    num_train_epochs=<span class="number">2</span>,                 <span class="comment"># å®šä¹‰è®­ç»ƒè½®æ•°</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=small_train_dataset,</span><br><span class="line">    eval_dataset=small_eval_dataset,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>å¼€å§‹è®­ç»ƒï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><br>è®­ç»ƒå®Œæ¯•åï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç ï¼Œå¾—åˆ°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ•ˆæœï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;epoch&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;eval_loss&#x27;</span>: <span class="number">0.9351930022239685</span>ï¼Œ</span><br><span class="line"> <span class="string">&#x27;eval_accuracy&#x27;</span><span class="string">&#x27;: 0.7350917431192661</span></span><br><span class="line"><span class="string"> &#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-ä½¿ç”¨-PyTorchè¿›è¡Œè®­ç»ƒ"><a href="#1-2-ä½¿ç”¨-PyTorchè¿›è¡Œè®­ç»ƒ" class="headerlink" title="1.2 ä½¿ç”¨ PyTorchè¿›è¡Œè®­ç»ƒ"></a>1.2 ä½¿ç”¨ PyTorchè¿›è¡Œè®­ç»ƒ</h3><p>é‡æ–°å¯åŠ¨ç¬”è®°æœ¬ä»¥é‡Šæ”¾ä¸€äº›å†…å­˜ï¼Œæˆ–æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> model</span><br><span class="line"><span class="keyword">del</span> pytorch_model</span><br><span class="line"><span class="keyword">del</span> trainer</span><br><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure><br>é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥è¿­ä»£æ‰¹æ¬¡ã€‚ åœ¨è¿™æ ·åšä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹æˆ‘ä»¬çš„ tokenized_datasets åº”ç”¨ä¸€äº›åå¤„ç†ï¼š</p>
<ol>
<li>åˆ é™¤ä¸æ¨¡å‹ä¸æœŸæœ›çš„å€¼ç›¸å¯¹åº”çš„åˆ—ï¼ˆæ­¤å¤„ä¸ºâ€œtextâ€åˆ—ï¼‰</li>
<li>å°†åˆ—â€œlabelâ€é‡å‘½åä¸ºâ€œlabelsâ€ï¼ˆå› ä¸ºæ¨¡å‹æœŸæœ›å‚æ•°è¢«å‘½åä¸ºæ ‡ç­¾ï¼‰</li>
<li>è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä»¥ä¾¿å®ƒä»¬è¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚</li>
</ol>
<p>tokenized_datasets å¯¹æ¯ä¸ªæ­¥éª¤å¤„ç†å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([<span class="string">&quot;sentence&quot;</span>,<span class="string">&quot;idx&quot;</span>])<span class="comment">#åˆ é™¤å¤šä½™çš„â€œsebtenceâ€åˆ—å’Œâ€œidxâ€åˆ—,å¦åˆ™ä¼šæŠ¥é”™forward() got an unexpected keyword argument &#x27;idx&#x27;</span></span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)<span class="comment">#åˆ—â€œlabelâ€é‡å‘½åä¸ºâ€œlabelsâ€ï¼Œå¦åˆ™æŠ¥é”™forward() got an unexpected keyword argument &#x27;label&#x27;</span></span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)<span class="comment">#è¿”å› PyTorch å¼ é‡ï¼Œå¦åˆ™æŠ¥é”™&#x27;list&#x27; object has no attribute &#x27;size&#x27;</span></span><br></pre></td></tr></table></figure>
<p>äºŒä¸‰æ­¥ä¹Ÿå¯ä»¥åˆå¹¶ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">columns = [<span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">tokenized_datasets.set_format(<span class="built_in">type</span>=<span class="string">&#x27;torch&#x27;</span>, columns=columns)</span><br></pre></td></tr></table></figure>
<p>åˆ‡å‡ºä¸€éƒ¨åˆ†æ•°æ®é›†</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>å®šä¹‰dataloadersï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>)</span><br><span class="line">eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>å®šä¹‰æ¨¡å‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>å®šä¹‰ä¼˜åŒ–å™¨optimizer å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨schedulerï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#é»˜è®¤ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åªæ˜¯çº¿æ€§è¡°å‡ä»æœ€å¤§å€¼ï¼ˆæ­¤å¤„ä¸º 5e-5ï¼‰åˆ° 0ï¼š</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>ä½¿ç”¨ tqdm åº“åœ¨è®­ç»ƒæ­¥éª¤æ•°ä¸Šæ·»åŠ äº†ä¸€ä¸ªè¿›åº¦æ¡ï¼Œå¹¶å®šä¹‰è®­ç»ƒå¾ªç¯ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()<span class="comment">#è®¾ç½®trainçŠ¶æ€ï¼Œå¯ç”¨ Batch Normalization å’Œ Dropoutã€‚</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>ç¼–å†™è¯„ä¼°å¾ªç¯ï¼Œåœ¨å¾ªç¯å®Œæˆæ—¶è®¡ç®—æœ€ç»ˆç»“æœä¹‹å‰ç´¯ç§¯æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric= load_metric(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"></span><br><span class="line">metric.compute()</span><br></pre></td></tr></table></figure>
<h3 id="1-3-å¥å­å¯¹æ–‡æœ¬åˆ†ç±»ï¼ˆrteï¼‰ï¼š"><a href="#1-3-å¥å­å¯¹æ–‡æœ¬åˆ†ç±»ï¼ˆrteï¼‰ï¼š" class="headerlink" title="1.3 å¥å­å¯¹æ–‡æœ¬åˆ†ç±»ï¼ˆrteï¼‰ï¼š"></a>1.3 å¥å­å¯¹æ–‡æœ¬åˆ†ç±»ï¼ˆrteï¼‰ï¼š</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = load_dataset(<span class="string">&#x27;glue&#x27;</span>, <span class="string">&#x27;rte&#x27;</span>)</span><br><span class="line">metric = load_metric(<span class="string">&#x27;glue&#x27;</span>, <span class="string">&#x27;rte&#x27;</span>)</span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&#x27;bert-base-cased&#x27;</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;bert-base-cased&#x27;</span>, return_dict=<span class="literal">True</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&#x27;hypothesis&#x27;</span>],examples[<span class="string">&#x27;premiere&#x27;</span>] truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>)</span><br><span class="line">dataset = dataset.<span class="built_in">map</span>(tokenize, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>å…¶å®ƒä»£ç ä¸€æ ·.æ›´å¤šæ–‡æœ¬åˆ†ç±»å‚è€ƒdatawhale-transformeræ•™ç¨‹4.1ï¼š<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">ã€Šæ–‡æœ¬åˆ†ç±»ã€‹</a></p>
<h3 id="1-4-æ›´å¤šç¤ºä¾‹"><a href="#1-4-æ›´å¤šç¤ºä¾‹" class="headerlink" title="1.4 æ›´å¤šç¤ºä¾‹"></a>1.4 æ›´å¤šç¤ºä¾‹</h3><p>è¦æŸ¥çœ‹æ›´å¤šå¾®è°ƒç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å‚è€ƒï¼š<br>ğŸ¤—<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/examples">Transformers Examples</a>ï¼Œå…¶ä¸­åŒ…æ‹¬åœ¨ PyTorch å’Œ TensorFlow ä¸­è®­ç»ƒæ‰€æœ‰å¸¸è§ NLP ä»»åŠ¡çš„è„šæœ¬ã€‚<br>ğŸ¤—  <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/notebooks.html">Transformers Notebooks</a> ï¼Œå…¶ä¸­åŒ…å«å„ç§ç¬”è®°æœ¬ï¼Œå°¤å…¶æ˜¯æ¯ä¸ªä»»åŠ¡ä¸€ä¸ªï¼ˆæŸ¥æ‰¾å¦‚ä½•åœ¨ xxx ä¸Šå¾®è°ƒæ¨¡å‹ï¼‰ã€‚</p>
<h2 id="2-ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ä»£ç èµæ"><a href="#2-ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ä»£ç èµæ" class="headerlink" title="2. ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ä»£ç èµæ"></a>2. ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ä»£ç èµæ</h2><p>è½¬è½½è‡ª<a target="_blank" rel="noopener" href="https://gitee.com/coggle/competition-baseline/blob/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2021/%E4%B8%AD%E6%96%87%E9%97%AE%E9%A2%98%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8C%91%E6%88%98%E8%B5%9B/bert-nsp-xunfei.ipynb">ã€Š10åˆ†é’Ÿ æ€å…¥ç§‘å¤§è®¯é£ä¸­æ–‡ç›¸ä¼¼åº¦ Top10ï¼ã€‹</a></p>
<h3 id="2-1èµ›é¢˜è§£æ"><a href="#2-1èµ›é¢˜è§£æ" class="headerlink" title="2.1èµ›é¢˜è§£æ"></a>2.1èµ›é¢˜è§£æ</h3><ul>
<li><p>èµ›é¢˜åç§°ï¼šä¸­æ–‡é—®é¢˜ç›¸ä¼¼åº¦æŒ‘æˆ˜èµ›<br><a target="_blank" rel="noopener" href="http://challenge.xfyun.cn/topic/info?type=chinese-question-similarity&amp;ch=dw-sq-1">http://challenge.xfyun.cn/topic/info?type=chinese-question-similarity&amp;ch=dw-sq-1</a></p>
</li>
<li><p>èµ›é¢˜ä»‹ç»<br>é‡å¤é—®é¢˜æ£€æµ‹æ˜¯ä¸€ä¸ªå¸¸è§çš„æ–‡æœ¬æŒ–æ˜ä»»åŠ¡ï¼Œåœ¨å¾ˆå¤šå®é™…é—®ç­”ç¤¾åŒºéƒ½æœ‰ç›¸åº”çš„åº”ç”¨ã€‚é‡å¤é—®é¢˜æ£€æµ‹å¯ä»¥æ–¹ä¾¿è¿›è¡Œé—®é¢˜çš„ç­”æ¡ˆèšåˆï¼Œä»¥åŠé—®é¢˜ç­”æ¡ˆæ¨èï¼Œè‡ªåŠ¨QAç­‰ã€‚ç”±äºä¸­æ–‡è¯è¯­çš„å¤šæ ·æ€§å’Œçµæ´»æ€§ï¼Œæœ¬èµ›é¢˜éœ€è¦é€‰æ‰‹æ„å»ºä¸€ä¸ªé‡å¤é—®é¢˜è¯†åˆ«ç®—æ³•ã€‚</p>
</li>
<li><p>èµ›é¢˜ä»»åŠ¡<br>æœ¬æ¬¡èµ›é¢˜å¸Œæœ›å‚èµ›é€‰æ‰‹å¯¹ä¸¤ä¸ªé—®é¢˜å®Œæˆç›¸ä¼¼åº¦æ‰“åˆ†ã€‚</p>
</li>
<li><p>è®­ç»ƒé›†ï¼šçº¦5åƒæ¡é—®é¢˜å¯¹å’Œæ ‡ç­¾ã€‚è‹¥ä¸¤ä¸ªé—®é¢˜æ˜¯ç›¸åŒå«ä¹‰ï¼Œæ ‡ç­¾ä¸º1ï¼›å¦åˆ™ä¸º0ã€‚<br>æµ‹è¯•é›†ï¼šçº¦5åƒæ¡é—®é¢˜å¯¹ã€‚</p>
</li>
<li><p>è®­ç»ƒé›†æ ·ä¾‹ï¼š<br>å¥å­1ï¼šæœ‰å“ªäº›å¥³æ˜æ˜Ÿè¢«æ½œè§„åˆ™å•¦<br>å¥å­2ï¼šå“ªäº›å¥³æ˜æ˜Ÿè¢«æ½œè§„åˆ™äº†<br>æ ‡ç­¾ï¼š1<br>å¥å­1ï¼šæ³°å›§å®Œæ•´ç‰ˆä¸‹è½½<br>å¥å­2ï¼šã‚¨ã‚¦ãƒ†ãƒ«ãƒšå®Œæ•´ç‰ˆä¸‹è½½<br>æ ‡ç­¾ï¼š0</p>
</li>
<li>è§£é¢˜æ€è·¯<br>èµ›é¢˜ä¸ºç»å…¸çš„æ–‡æœ¬åŒ¹é…ä»»åŠ¡ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘ä½¿ç”¨Bertçš„NSPæ¥å®Œæˆå»ºæ¨¡ã€‚</li>
</ul>
<h3 id="2-2-ä»£ç å®ä¾‹"><a href="#2-2-ä»£ç å®ä¾‹" class="headerlink" title="2.2 ä»£ç å®ä¾‹"></a>2.2 ä»£ç å®ä¾‹</h3><p>æ­¥éª¤1ï¼šè¯»å–æ•°æ®é›†</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, names=[<span class="string">&#x27;question1&#x27;</span>, <span class="string">&#x27;question2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<p>import re<br>å¹¶æŒ‰ç…§æ ‡ç­¾åˆ’åˆ†éªŒè¯é›†ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># stratify æŒ‰ç…§æ ‡ç­¾è¿›è¡Œé‡‡æ ·ï¼Œè®­ç»ƒé›†å’ŒéªŒè¯éƒ¨åˆ†åŒåˆ†å¸ƒ</span></span><br><span class="line">q1_train, q1_val, q2_train, q2_val, train_label, test_label =  train_test_split(</span><br><span class="line">    train_df[<span class="string">&#x27;question1&#x27;</span>].iloc[:], </span><br><span class="line">    train_df[<span class="string">&#x27;question2&#x27;</span>].iloc[:],</span><br><span class="line">    train_df[<span class="string">&#x27;label&#x27;</span>].iloc[:],</span><br><span class="line">    test_size=<span class="number">0.1</span>, </span><br><span class="line">    stratify=train_df[<span class="string">&#x27;label&#x27;</span>].iloc[:])</span><br></pre></td></tr></table></figure>
<p>æ­¥éª¤2ï¼šæ–‡æœ¬è¿›è¡Œtokenizer<br>ä½¿ç”¨Bertå¯¹æ–‡æœ¬è¿›è¡Œè½¬æ¢ï¼Œæ­¤æ—¶æ¨¡å‹é€‰æ‹©bert-base-chineseã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pip install transformers</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">train_encoding = tokenizer(<span class="built_in">list</span>(q1_train), <span class="built_in">list</span>(q2_train), </span><br><span class="line">                           truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br><span class="line">val_encoding = tokenizer(<span class="built_in">list</span>(q1_val), <span class="built_in">list</span>(q2_val), </span><br><span class="line">                          truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>æ­¥éª¤3ï¼šå®šä¹‰dataset</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ•°æ®é›†è¯»å–</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XFeiDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encodings, labels</span>):</span></span><br><span class="line">        self.encodings = encodings</span><br><span class="line">        self.labels = labels</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è¯»å–å•ä¸ªæ ·æœ¬</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        item = &#123;key: torch.tensor(val[idx]) <span class="keyword">for</span> key, val <span class="keyword">in</span> self.encodings.items()&#125;</span><br><span class="line">        item[<span class="string">&#x27;labels&#x27;</span>] = torch.tensor(<span class="built_in">int</span>(self.labels[idx]))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">train_dataset = XFeiDataset(train_encoding, <span class="built_in">list</span>(train_label))</span><br><span class="line">val_dataset = XFeiDataset(val_encoding, <span class="built_in">list</span>(test_label))</span><br></pre></td></tr></table></figure>
<p>æ­¥éª¤4ï¼šå®šä¹‰åŒ¹é…æ¨¡å‹<br>ä½¿ç”¨BertForNextSentencePredictionå®Œæˆæ–‡æœ¬åŒ¹é…ä»»åŠ¡ï¼Œå¹¶å®šä¹‰ä¼˜åŒ–å™¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForNextSentencePrediction, AdamW, get_linear_schedule_with_warmup</span><br><span class="line">model = BertForNextSentencePrediction.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å•ä¸ªè¯»å–åˆ°æ‰¹é‡è¯»å–</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¼˜åŒ–æ–¹æ³•</span></span><br><span class="line">optim = AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç²¾åº¦è®¡ç®—</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flat_accuracy</span>(<span class="params">preds, labels</span>):</span></span><br><span class="line">    pred_flat = np.argmax(preds, axis=<span class="number">1</span>).flatten()</span><br><span class="line">    labels_flat = labels.flatten()</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(pred_flat == labels_flat) / <span class="built_in">len</span>(labels_flat)</span><br></pre></td></tr></table></figure>
<p>æ­¥éª¤5ï¼šæ¨¡å‹è®­ç»ƒä¸éªŒè¯<br>ç¥–ä¼ ä»£ç ï¼šæ¨¡å‹æ­£å‘ä¼ æ’­å’Œå‡†ç¡®ç‡è®¡ç®—ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®­ç»ƒå‡½æ•°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    model.train()</span><br><span class="line">    total_train_loss = <span class="number">0</span></span><br><span class="line">    iter_num = <span class="number">0</span></span><br><span class="line">    total_iter = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># æ­£å‘ä¼ æ’­</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        total_train_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åå‘æ¢¯åº¦ä¿¡æ¯</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å‚æ•°æ›´æ–°</span></span><br><span class="line">        optim.step()</span><br><span class="line"></span><br><span class="line">        iter_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span>(iter_num % <span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoth: %d, iter_num: %d, loss: %.4f, %.2f%%&quot;</span> % (epoch, iter_num, loss.item(), iter_num/total_iter*<span class="number">100</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: %d, Average training loss: %.4f&quot;</span>%(epoch, total_train_loss/<span class="built_in">len</span>(train_loader)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_eval_accuracy = <span class="number">0</span></span><br><span class="line">    total_eval_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> val_dataloader:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># æ­£å¸¸ä¼ æ’­</span></span><br><span class="line">            input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">            attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        </span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        logits = outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        total_eval_loss += loss.item()</span><br><span class="line">        logits = logits.detach().cpu().numpy()</span><br><span class="line">        label_ids = labels.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line">        total_eval_accuracy += flat_accuracy(logits, label_ids)</span><br><span class="line">        </span><br><span class="line">    avg_val_accuracy = total_eval_accuracy / <span class="built_in">len</span>(val_dataloader)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.4f&quot;</span> % (avg_val_accuracy))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Average testing loss: %.4f&quot;</span>%(total_eval_loss/<span class="built_in">len</span>(val_dataloader)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-------------------------------&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------Epoch: %d ----------------&quot;</span> % epoch)</span><br><span class="line">    train()</span><br><span class="line">    validation()</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">f&#x27;model_<span class="subst">&#123;epoch&#125;</span>.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#æ‰“å°è¾“å‡ºçœ‹çœ‹</span></span><br><span class="line">outputs = model(input_ids,attention_mask=attention_mask,labels=labels)</span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line">NextSentencePredictorOutput(loss=tensor(<span class="number">0.4528</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>), logits=tensor([[ <span class="number">2.7850</span>,  <span class="number">1.2451</span>],</span><br><span class="line">        [ <span class="number">3.9663</span>, -<span class="number">0.9795</span>],</span><br><span class="line">        [ <span class="number">0.1072</span>,  <span class="number">4.8910</span>],</span><br><span class="line">        [ <span class="number">3.2274</span>,  <span class="number">0.4685</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>), hidden_states=<span class="literal">None</span>, attentions=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>æ­¥éª¤6ï¼šå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹<br>è¯»å–æµ‹è¯•é›†æ•°æ®ï¼Œè¿›è¡Œè½¬æ¢ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, names=[<span class="string">&#x27;question1&#x27;</span>, <span class="string">&#x27;question2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br><span class="line">test_df[<span class="string">&#x27;label&#x27;</span>] = test_df[<span class="string">&#x27;label&#x27;</span>].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_encoding = tokenizer(<span class="built_in">list</span>(test_df[<span class="string">&#x27;question1&#x27;</span>]), <span class="built_in">list</span>(test_df[<span class="string">&#x27;question2&#x27;</span>]), </span><br><span class="line">                          truncation=<span class="literal">True</span>, padding=<span class="literal">True</span>, max_length=<span class="number">100</span>)</span><br><span class="line">test_dataset = XFeiDataset(test_encoding, <span class="built_in">list</span>(test_df[<span class="string">&#x27;label&#x27;</span>]))</span><br><span class="line">test_dataloader = DataLoader(test_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œæ­£å‘ä¼ æ’­é¢„æµ‹ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœï¼Œå¹¶è¾“å‡ºæŒ‡å®šæ ¼å¼ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_predict = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_dataloader:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># æ­£å¸¸ä¼ æ’­</span></span><br><span class="line">            input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">            attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            labels = batch[<span class="string">&#x27;labels&#x27;</span>].to(device)</span><br><span class="line">            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)</span><br><span class="line">        </span><br><span class="line">        loss = outputs[<span class="number">0</span>]</span><br><span class="line">        logits = outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        logits = logits.detach().cpu().numpy()</span><br><span class="line">        label_ids = labels.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line">        test_predict += <span class="built_in">list</span>(np.argmax(logits, axis=<span class="number">1</span>).flatten())</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> test_predict</span><br><span class="line">    </span><br><span class="line">test_label = predict()</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_label&#125;).to_csv(<span class="string">&#x27;submit.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-CCF-BDCI-å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«"><a href="#3-CCF-BDCI-å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«" class="headerlink" title="3. CCF BDCI å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«"></a>3. CCF BDCI å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«</h2><p>æœ¬èŠ‚è½¬è‡ª<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/xl-MAlI1KroZrmpWGttEuA">ã€ŠCCF BDCI å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ«ï¼šå¤šç›®æ ‡å­¦ä¹ å¼€æºæ–¹æ¡ˆã€‹</a></p>
<h3 id="3-1-èµ›äº‹è§£æ"><a href="#3-1-èµ›äº‹è§£æ" class="headerlink" title="3.1 èµ›äº‹è§£æ"></a>3.1 èµ›äº‹è§£æ</h3><ol>
<li><p>èµ›é¢˜åç§°<br>å‰§æœ¬è§’è‰²æƒ…æ„Ÿè¯†åˆ« æ¯”èµ›é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://www.datafountain.cn/competitions/518">https://www.datafountain.cn/competitions/518</a><br>åå°å›å¤â€œçˆ±å¥‡è‰ºâ€å¯ä»¥è·å–å®Œæ•´ä»£ç </p>
</li>
<li><p>èµ›é¢˜èƒŒæ™¯<br>å‰§æœ¬å¯¹å½±è§†è¡Œä¸šçš„é‡è¦æ€§ä¸è¨€è€Œå–»ã€‚ä¸€éƒ¨å¥½çš„å‰§æœ¬ï¼Œä¸å…‰æ˜¯å¥½å£ç¢‘å’Œå¤§æµé‡çš„åŸºç¡€ï¼Œä¹Ÿèƒ½å¸¦æ¥æ›´é«˜çš„å•†ä¸šå›æŠ¥ã€‚å‰§æœ¬åˆ†ææ˜¯å½±è§†å†…å®¹ç”Ÿäº§é“¾æ¡çš„ç¬¬ä¸€ç¯ï¼Œå…¶ä¸­å‰§æœ¬è§’è‰²çš„æƒ…æ„Ÿè¯†åˆ«æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä»»åŠ¡ï¼Œä¸»è¦æ˜¯å¯¹å‰§æœ¬ä¸­æ¯å¥å¯¹ç™½å’ŒåŠ¨ä½œæè¿°ä¸­æ¶‰åŠåˆ°çš„æ¯ä¸ªè§’è‰²ä»å¤šä¸ªç»´åº¦è¿›è¡Œåˆ†æå¹¶è¯†åˆ«å‡ºæƒ…æ„Ÿã€‚ç›¸å¯¹äºé€šå¸¸çš„æ–°é—»ã€è¯„è®ºæ€§æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æï¼Œæœ‰å…¶ç‹¬æœ‰çš„ä¸šåŠ¡ç‰¹ç‚¹å’ŒæŒ‘æˆ˜ã€‚</p>
</li>
<li><p>èµ›é¢˜ä»»åŠ¡<br>æœ¬èµ›é¢˜æä¾›ä¸€éƒ¨åˆ†ç”µå½±å‰§æœ¬ä½œä¸ºè®­ç»ƒé›†ï¼Œè®­ç»ƒé›†æ•°æ®å·²ç”±äººå·¥è¿›è¡Œæ ‡æ³¨ï¼Œå‚èµ›é˜Ÿä¼éœ€è¦å¯¹å‰§æœ¬åœºæ™¯ä¸­æ¯å¥å¯¹ç™½å’ŒåŠ¨ä½œæè¿°ä¸­æ¶‰åŠåˆ°çš„æ¯ä¸ªè§’è‰²çš„æƒ…æ„Ÿä»å¤šä¸ªç»´åº¦è¿›è¡Œåˆ†æå’Œè¯†åˆ«ã€‚è¯¥ä»»åŠ¡çš„ä¸»è¦éš¾ç‚¹å’ŒæŒ‘æˆ˜åŒ…æ‹¬ï¼š1ï¼‰å‰§æœ¬çš„è¡Œæ–‡é£æ ¼å’Œé€šå¸¸çš„æ–°é—»ç±»è¯­æ–™å·®åˆ«è¾ƒå¤§ï¼Œæ›´åŠ å£è¯­åŒ–ï¼›2ï¼‰å‰§æœ¬ä¸­è§’è‰²æƒ…æ„Ÿä¸ä»…ä»…å–å†³äºå½“å‰çš„æ–‡æœ¬ï¼Œå¯¹å‰æ–‡è¯­ä¹‰å¯èƒ½æœ‰æ·±åº¦ä¾èµ–ã€‚</p>
</li>
<li><p>æ•°æ®ç®€ä»‹<br>æ¯”èµ›çš„æ•°æ®æ¥æºä¸»è¦æ˜¯ä¸€éƒ¨åˆ†ç”µå½±å‰§æœ¬ï¼Œä»¥åŠçˆ±å¥‡è‰ºæ ‡æ³¨å›¢é˜Ÿçš„æƒ…æ„Ÿæ ‡æ³¨ç»“æœï¼Œä¸»è¦ç”¨äºæä¾›ç»™å„å‚èµ›å›¢é˜Ÿè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œç»“æœéªŒè¯ä½¿ç”¨ã€‚</p>
</li>
</ol>
<p>æ•°æ®è¯´æ˜<br>è®­ç»ƒæ•°æ®ï¼šè®­ç»ƒæ•°æ®ä¸ºtxtæ ¼å¼ï¼Œä»¥è‹±æ–‡åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œé¦–è¡Œä¸ºè¡¨å¤´ï¼Œå­—æ®µè¯´æ˜å¦‚ä¸‹ï¼š<br>å­—æ®µåç§°    ç±»å‹    æè¿°    è¯´æ˜<br>id    String    æ•°æ®ID    -<br>content    String    æ–‡æœ¬å†…å®¹    å‰§æœ¬å¯¹ç™½æˆ–åŠ¨ä½œæå†™<br>character    String    è§’è‰²å    æ–‡æœ¬ä¸­æåˆ°çš„è§’è‰²<br>emotion    String    æƒ…æ„Ÿè¯†åˆ«ç»“æœï¼ˆæŒ‰é¡ºåºï¼‰    çˆ±æƒ…æ„Ÿå€¼ï¼Œä¹æƒ…æ„Ÿå€¼ï¼ŒæƒŠæƒ…æ„Ÿå€¼ï¼Œæ€’æƒ…æ„Ÿå€¼ï¼Œææƒ…æ„Ÿå€¼ï¼Œå“€æƒ…æ„Ÿå€¼</p>
<p>å¤‡æ³¨ï¼š</p>
<ul>
<li>æœ¬èµ›é¢˜çš„æƒ…æ„Ÿå®šä¹‰å…±6ç±»ï¼ˆæŒ‰é¡ºåºï¼‰ï¼šçˆ±ã€ä¹ã€æƒŠã€æ€’ã€æã€å“€ï¼›â€ƒâ€ƒ</li>
<li>æƒ…æ„Ÿè¯†åˆ«ç»“æœï¼šä¸Šè¿°6ç±»æƒ…æ„ŸæŒ‰å›ºå®šé¡ºåºå¯¹åº”çš„æƒ…æ„Ÿå€¼ï¼Œæƒ…æ„Ÿå€¼èŒƒå›´æ˜¯[0, 1, 2, 3]ï¼Œ0-æ²¡æœ‰ï¼Œ1-å¼±ï¼Œ2-ä¸­ï¼Œ3-å¼ºï¼Œä»¥è‹±æ–‡åŠè§’é€—å·åˆ†éš”ï¼›â€ƒâ€ƒ</li>
<li>æœ¬èµ›é¢˜ä¸éœ€è¦è¯†åˆ«å‰§æœ¬ä¸­çš„è§’è‰²åï¼›â€ƒâ€ƒæ–‡ä»¶ç¼–ç ï¼šUTF-8 æ— BOMç¼–ç </li>
</ul>
<ol>
<li>è¯„ä¼°æ ‡å‡†<br>æœ¬èµ›é¢˜ç®—æ³•è¯„åˆ†é‡‡ç”¨å¸¸ç”¨çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æ¥è®¡ç®—è¯„åˆ†ï¼ŒæŒ‰ç…§â€œæ–‡æœ¬å†…å®¹+è§’è‰²åâ€è¯†åˆ«å‡ºçš„6ç±»æƒ…æ„Ÿå¯¹åº”çš„æƒ…æ„Ÿå€¼æ¥ç»Ÿè®¡ã€‚<br>å›¾ç‰‡score = 1/(1 + RMSE)</li>
</ol>
<p>å…¶ä¸­æ˜¯yi,jé¢„æµ‹çš„æƒ…æ„Ÿå€¼ï¼Œxi,jæ˜¯æ ‡æ³¨çš„æƒ…æ„Ÿå€¼ï¼Œnæ˜¯æ€»çš„æµ‹è¯•æ ·æœ¬æ•°ã€‚æœ€ç»ˆæŒ‰scoreå¾—åˆ†æ¥æ’åã€‚</p>
<ol>
<li>åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„å¯¹ç›®æ ‡å­¦ä¹ </li>
</ol>
<p>è¿™ä¸ªé¢˜ç›®å¯æ“ä½œçš„åœ°æ–¹æœ‰å¾ˆå¤šï¼Œä¸€å¼€å§‹è§åˆ°è¿™ä¸ªæ¯”èµ›çš„æ—¶å€™è§æƒ³åˆ°äº†multi outputsçš„æ¨¡å‹æ„å»ºï¼Œè¿™é‡Œç»™å¤§å®¶åˆ†äº«ä¸‹è¿™ä¸ªåŸºçº¿ï¼Œå¸Œæœ›æœ‰å¤§ä½¬èƒ½å¤Ÿé’ˆå¯¹è¿™ä¸ªæ€è·¯ä¼˜åŒ–ä¸Šå»~</p>
<h3 id="3-2-ä»£ç ç¤ºä¾‹"><a href="#3-2-ä»£ç ç¤ºä¾‹" class="headerlink" title="3.2  ä»£ç ç¤ºä¾‹"></a>3.2  ä»£ç ç¤ºä¾‹</h3><p>åŠ è½½æ•°æ®</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/train_dataset_v2.tsv&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> handler:</span><br><span class="line">    lines = handler.read().split(<span class="string">&#x27;\n&#x27;</span>)[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    data = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(lines):</span><br><span class="line">        sp = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(sp) != <span class="number">4</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;ERROR:&quot;</span>, sp)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        data.append(sp)</span><br><span class="line"></span><br><span class="line">train = pd.DataFrame(data)</span><br><span class="line">train.columns = [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;character&#x27;</span>, <span class="string">&#x27;emotions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/test_dataset.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">submit = pd.read_csv(<span class="string">&#x27;data/submit_example.tsv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train = train[train[<span class="string">&#x27;emotions&#x27;</span>] != <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>æå–æƒ…æ„Ÿç›®æ ‡<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train[<span class="string">&#x27;emotions&#x27;</span>] = train[<span class="string">&#x27;emotions&#x27;</span>].apply(<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(_i) <span class="keyword">for</span> _i <span class="keyword">in</span> x.split(<span class="string">&#x27;,&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">train[[<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;joy&#x27;</span>, <span class="string">&#x27;fright&#x27;</span>, <span class="string">&#x27;anger&#x27;</span>, <span class="string">&#x27;fear&#x27;</span>, <span class="string">&#x27;sorrow&#x27;</span>]] = train[<span class="string">&#x27;emotions&#x27;</span>].values.tolist()</span><br></pre></td></tr></table></figure></p>
<p>æ„å»ºæ•°æ®é›†<br>æ•°æ®é›†çš„æ ‡ç­¾ä¸€å…±æœ‰å…­ä¸ªï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoleDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span></span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,item</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item ä¸ºæ•°æ®ç´¢å¼•ï¼Œè¿­ä»£å–ç¬¬itemæ¡æ•°æ®</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        sample = &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>: text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>: encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>: encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> label_col <span class="keyword">in</span> target_cols:</span><br><span class="line">            sample[label_col] = torch.tensor(label[label_col], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>
<p>æ¨¡å‹æ„å»º</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmotionClassifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EmotionClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line">        self.out_love = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_joy = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_fright = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_anger = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_fear = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">        self.out_sorrow = nn.Linear(self.bert.config.hidden_size, n_classes)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span></span><br><span class="line">        _, pooled_output = self.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            return_dict = <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        love = self.out_love(pooled_output)</span><br><span class="line">        joy = self.out_joy(pooled_output)</span><br><span class="line">        fright = self.out_fright(pooled_output)</span><br><span class="line">        anger = self.out_anger(pooled_output)</span><br><span class="line">        fear = self.out_fear(pooled_output)</span><br><span class="line">        sorrow = self.out_sorrow(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;love&#x27;</span>: love, <span class="string">&#x27;joy&#x27;</span>: joy, <span class="string">&#x27;fright&#x27;</span>: fright,</span><br><span class="line">            <span class="string">&#x27;anger&#x27;</span>: anger, <span class="string">&#x27;fear&#x27;</span>: fear, <span class="string">&#x27;sorrow&#x27;</span>: sorrow,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>6.4 æ¨¡å‹è®­ç»ƒ<br>å›å½’æŸå¤±å‡½æ•°ç›´æ¥é€‰å– nn.MSELoss()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EPOCHS = <span class="number">1</span> <span class="comment"># è®­ç»ƒè½®æ•°</span></span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">total_steps = <span class="built_in">len</span>(train_data_loader) * EPOCHS</span><br><span class="line"></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">  optimizer,</span><br><span class="line">  num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">  num_training_steps=total_steps</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = nn.MSELoss().to(device)</span><br></pre></td></tr></table></figure>
<p>æ¨¡å‹æ€»çš„lossä¸ºå…­ä¸ªç›®æ ‡å€¼çš„lossä¹‹å’Œ</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">  model, </span></span></span><br><span class="line"><span class="params"><span class="function">  data_loader, </span></span></span><br><span class="line"><span class="params"><span class="function">  criterion, </span></span></span><br><span class="line"><span class="params"><span class="function">  optimizer, </span></span></span><br><span class="line"><span class="params"><span class="function">  device, </span></span></span><br><span class="line"><span class="params"><span class="function">  scheduler, </span></span></span><br><span class="line"><span class="params"><span class="function">  n_examples</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    model = model.train()</span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tqdm(data_loader):</span><br><span class="line">        input_ids = sample[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">        attention_mask = sample[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">        outputs = model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask</span><br><span class="line">        )</span><br><span class="line">        loss_love = criterion(outputs[<span class="string">&#x27;love&#x27;</span>], sample[<span class="string">&#x27;love&#x27;</span>].to(device))</span><br><span class="line">        loss_joy = criterion(outputs[<span class="string">&#x27;joy&#x27;</span>], sample[<span class="string">&#x27;joy&#x27;</span>].to(device))</span><br><span class="line">        loss_fright = criterion(outputs[<span class="string">&#x27;fright&#x27;</span>], sample[<span class="string">&#x27;fright&#x27;</span>].to(device))</span><br><span class="line">        loss_anger = criterion(outputs[<span class="string">&#x27;anger&#x27;</span>], sample[<span class="string">&#x27;anger&#x27;</span>].to(device))</span><br><span class="line">        loss_fear = criterion(outputs[<span class="string">&#x27;fear&#x27;</span>], sample[<span class="string">&#x27;fear&#x27;</span>].to(device))</span><br><span class="line">        loss_sorrow = criterion(outputs[<span class="string">&#x27;sorrow&#x27;</span>], sample[<span class="string">&#x27;sorrow&#x27;</span>].to(device))</span><br><span class="line">        loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">	<span class="comment">#return correct_predictions.double() / (n_examples*6), np.mean(losses)</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(losses)</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/09/25/huggingface/ç¼–å†™transformersçš„è‡ªå®šä¹‰pytorchè®­ç»ƒå¾ªç¯ï¼ˆDatasetå’ŒDataLoaderè§£æå’Œå®ä¾‹ä»£ç ï¼‰/">https://zhxnlp.github.io/2021/09/25/huggingface/ç¼–å†™transformersçš„è‡ªå®šä¹‰pytorchè®­ç»ƒå¾ªç¯ï¼ˆDatasetå’ŒDataLoaderè§£æå’Œå®ä¾‹ä»£ç ï¼‰/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://zhxnlp.github.io">zhxnlpã®Blog</a>ï¼</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformers/">transformers</a><a class="post-meta__tags" href="/tags/Hugging-Face/">Hugging Face</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/27/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E3%80%81%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/"><i class="fa fa-chevron-left">  </i><span>PyTorchå­¦ä¹ ç¬”è®°1â€”â€”åŸºæœ¬æ¦‚å¿µã€æ¨¡å—ç®€ä»‹ã€å¼ é‡æ“ä½œã€è‡ªåŠ¨å¾®åˆ†</span></a></div><div class="next-post pull-right"><a href="/2021/09/18/huggingface/transformers%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94GET%20STARTED/"><span>Hugging Faceå®˜æ–¹æ–‡æ¡£â€”â€”GET STARTED</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>é©±åŠ¨ - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>ä¸»é¢˜ - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>