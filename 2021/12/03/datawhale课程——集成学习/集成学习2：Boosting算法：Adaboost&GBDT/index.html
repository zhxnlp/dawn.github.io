<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="集成学习2：Boosting算法：Adaboost&amp;GBDT"><meta name="keywords" content="集成学习,Boosting,Adaboost,GBDT"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>集成学习2：Boosting算法：Adaboost&amp;GBDT | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-Boosting%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">一、 Boosting算法原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-Adaboost%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">二、 Adaboost算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Adaboost%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Adaboost算法原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Adaboost%E7%AE%97%E6%B3%95%E4%B8%BE%E4%BE%8B"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Adaboost算法举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Adaboos%E4%BB%A3%E7%A0%81%E4%B8%BE%E4%BE%8B"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Adaboos代码举例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">三、 前向分步算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E5%8A%A0%E6%B3%95%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1加法模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 前向分步算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95%E4%B8%8EAdaboost%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 前向分步算法与Adaboost的关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91-GBDT"><span class="toc-number">4.</span> <span class="toc-text">四、梯度提升决策树(GBDT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Decision-Tree%EF%BC%9ACART%E5%9B%9E%E5%BD%92%E6%A0%91"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 Decision Tree：CART回归树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%9B%9E%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 回归提升树算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-GBDT"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 梯度提升决策树算法(GBDT)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-GBDT%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-number">4.4.</span> <span class="toc-text">4.3 GBDT代码示例</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">46</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">37</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">集成学习2：Boosting算法：Adaboost&amp;GBDT</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-03</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.5k</span><span class="post-meta__separator">|</span><span>阅读时长: 9 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一、-Boosting算法原理"><a href="#一、-Boosting算法原理" class="headerlink" title="一、 Boosting算法原理"></a>一、 Boosting算法原理</h2><ul>
<li>Bagging：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。</li>
<li>Bagging主要通过降低方差的方式减少预测误差&lt;/font&gt;</li>
<li>Boosting：使用同一组数据集进行反复学习，得到一系列简单模型，然后组合这些模型构成一个预测性能十分强大的机器学习模型。</li>
<li>Boosting通过不断减少偏差的形式提高最终的预测效果，与Bagging有着本质的不同。</li>
</ul>
<p>在概率近似正确（PAC）学习的框架下：</p>
<ol>
<li>弱学习：识别准确率略高于1/2（即准确率仅比随机猜测略高的学习算法）</li>
<li>强学习：识别准确率很高并能在多项式时间内完成的学习算法</li>
<li>强可学习和弱可学习是等价的，弱可学习算法，能提升至强可学习算法<span id="more"></span></li>
</ol>
<ul>
<li>提升方法：从弱学习算法出发，反复学习，得到一系列弱分类器(又称为基本分类器)，再通过一定的形式去组合这些弱分类器构成一个强分类器。而弱可学习算法比强可学习算法容易得多。</li>
<li>大多数的Boosting方法都是通过改变训练数据集的概率分布(训练数据不同样本的权值)，针对不同概率分布的数据调用弱分类算法学习一系列的弱分类器。</li>
</ul>
<p>Boosting方法关键点：</p>
<ol>
<li>每一轮学习应该如何改变数据的概率分布</li>
<li>如何将各个弱分类器组合起来</li>
</ol>
<h2 id="二、-Adaboost算法"><a href="#二、-Adaboost算法" class="headerlink" title="二、 Adaboost算法"></a>二、 Adaboost算法</h2><h3 id="2-1-Adaboost算法原理"><a href="#2-1-Adaboost算法原理" class="headerlink" title="2.1 Adaboost算法原理"></a>2.1 Adaboost算法原理</h3><p>Adaboost解决上述的两个问题的方式是：</p>
<ol>
<li>提高那些被前一轮分类器错误分类的样本的权重&lt;/font&gt;，而降低那些被正确分类的样本的权重。错误分类样本权重的增大而在后一轮的训练中“备受关注”。</li>
<li>各个弱分类器通过采取加权多数表决的方式组合&lt;/font&gt;。分类错误率低的弱分类器权重高，分类错误率较大的弱分类器权重低。</li>
<li><p>Adaboost等Boosting模型增加了计算的复杂度和计算成本，用降低偏差的方式去减少总误差，但是过程中引入了方差，可能出现过拟合</p>
</li>
<li><p>Boosting方式无法做到现在流行的并行计算的方式进行训练，因为每一步迭代都要基于上一部的基本分类器。</p>
</li>
<li>Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</li>
</ol>
<p>Adaboost算法：</p>
<ul>
<li>输入：二分类的训练数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$，特征$x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，类别$y</em>{i} \in \mathcal{Y}={-1,+1}$，$\mathcal{X}$是特征空间，$\mathcal{Y}$是类别集合，其中每个样本点由特征与类别组成。</li>
<li>输出：最终分类器$G(x)$。</li>
</ul>
<p>(1) 初始化训练集样本的权值分布：<script type="math/tex">D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N</script>其中权值是均匀分布，使得第一次没有先验信息的条件下每个样本在基本分类器的学习中作用一样。</p>
<p>(2) 对于学习轮次m=1,2,…,M</p>
<ol>
<li>使用具有权值分布$D<em>m$的训练数据集进行学习，得到基本分类器：$G</em>{m}(x): \mathcal{X} \rightarrow{-1,+1}$</li>
<li>计算$G<em>m(x)$在训练集上的分类误差率$$e</em>{m}=\sum<em>{i=1}^{N} P\left(G</em>{m}\left(x<em>{i}\right) \neq y</em>{i}\right)=\sum<em>{i=1}^{N} w</em>{m i} I\left(G<em>{m}\left(x</em>{i}\right) \neq y<em>{i}\right)$$<br>$w</em>{m i}$代表了在$G_m(x)$中分类错误的样本权重和，这点直接说明了权重分布$D_m$与$G_m(x)$的分类错误率$e_m$有直接关系。</li>
<li>计算$G<em>m(x)$的系数$\alpha</em>{m}=\frac{1}{2} \log \frac{1-e<em>{m}}{e</em>{m}}$，这里的log是自然对数ln<ul>
<li>$\alpha_{m}$表示了$G_m(x)$在最终分类器的重要性程度。</li>
<li>当$e<em>{m} \leqslant \frac{1}{2}$时，$\alpha</em>{m} \geqslant 0$，并且$\alpha_m$随着$e_m$的减少而增大，因此分类错误率越小的基本分类器在最终分类器的作用越大！                       </li>
</ul>
</li>
<li><p>更新训练数据集的权重分布</p>
<script type="math/tex; mode=display">D_{m+1}=(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N})</script><script type="math/tex; mode=display">w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.</script><script type="math/tex; mode=display">Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)</script><p>这里的$Z<em>m$是规范化因子，使得$D</em>{m+1}$成为概率分布。</p>
<p>从上式可以看到：被基本分类器$G<em>m(x)$错误分类的样本的权重扩大，被正确分类的样本权重减少，二者相比相差$\mathrm{e}^{2 \alpha</em>{m}}=\frac{1-e<em>{m}}{e</em>{m}}$倍。     </p>
</li>
</ol>
<p>(3) 构建基本分类器的线性组合$f(x)=\sum<em>{m=1}^{M} \alpha</em>{m} G_{m}(x)$，得到最终的分类器                       </p>
<script type="math/tex; mode=display">
\begin{aligned}
G(x) &=\operatorname{sign}(f(x)) \\
&=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
\end{aligned}</script><script type="math/tex; mode=display">sign(x)=\begin{cases}
1 & \text{ if } x\geqslant 0 \\ 
-1 & \text{ if } x< 0 
\end{cases}</script><p>线性组合$f(x)$实现了将M个基本分类器的加权表决，系数$\alpha_m$标志了基本分类器$G_m(x)$的重要性，值得注意的是：所有的$\alpha_m$之和不为1。$f(x)$的符号决定了样本x属于哪一类,其绝对值表示分类的确信度。</p>
<p>简单来说：计算M个基本分类器，每个分类器的错误率、模型权重及样本权重</p>
<ol>
<li>均匀初始化样本权重$D_{1}$</li>
<li>对于轮次m，针对当前权重$D<em>{m}$ 学习分类器 $G</em>{m}(x)$，并计算其分类错误率$e_{m}$。</li>
<li>计算分类器$G<em>m(x)$的权重系数$\alpha</em>{m}=\frac{1}{2} \log \frac{1-e<em>{m}}{e</em>{m}}$&lt;/font&gt;。$e<em>{m} \leqslant \frac{1}{2}$时，$\alpha</em>{m} \geqslant 0$，并且$\alpha_m$随着$e_m$的减少而增大，因此分类错误率越小的基本分类器在最终分类器的作用越大！</li>
<li>更新权重分布 <script type="math/tex">w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.</script><br>一般来说$\alpha<em>{m} \geqslant 0，e^0=1$。被基本分类器$G_m(x)$错误分类的样本的权重扩大，被正确分类的样本权重减少。$e</em>{m}$减小，$\alpha<em>{m}$增大，${w</em>{m+1, i}}$增大。即错误率越低的分类器，分类器权重和错误样本权重都越大，感觉上越准确准确的分类器学习力度越大。</li>
<li>基本分类器加权组合表决</li>
<li>总结：Adaboost不改变训练数据，而是改变其权值分布，使每一轮的基学习器学习不同权重分布的样本集，最后加权组合表决。</li>
</ol>
<h3 id="2-2-Adaboost算法举例"><a href="#2-2-Adaboost算法举例" class="headerlink" title="2.2 Adaboost算法举例"></a>2.2 Adaboost算法举例</h3><p>下面，我们使用一组简单的数据来手动计算Adaboost算法的过程：(例子来源<a target="_blank" rel="noopener" href="http://www.csie.edu.tw">http://www.csie.edu.tw</a>)                                                               </p>
<p>训练数据如下表，假设基本分类器的形式是一个分割$x<v$或$x>v$表示，阈值v由该基本分类器在训练数据集上分类错误率$e_m$最低确定。&lt;/font&gt;                                                </p>
<script type="math/tex; mode=display">
\begin{array}{ccccccccccc}
\hline \text { 序号 } & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline x & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
y & 1 & 1 & 1 & -1 & -1 & -1 & 1 & 1 & 1 & -1 \\
\hline
\end{array}</script><p>解：<br>初始化样本权值分布</p>
<script type="math/tex; mode=display">
\begin{aligned}
D_{1} &=\left(w_{11}, w_{12}, \cdots, w_{110}\right) \\
w_{1 i} &=0.1, \quad i=1,2, \cdots, 10
\end{aligned}</script><ol>
<li><p>对m=1:                      </p>
<ul>
<li>在权值分布$D_1$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=2.5时分类误差率最低，那么基本分类器为：<script type="math/tex; mode=display">
G_{1}(x)=\left\{\begin{array}{ll}
1, & x<2.5 \\
-1, & x>2.5
\end{array}\right.</script></li>
<li>样本7.8.9分错，$G<em>1(x)$在训练数据集上的误差率为$e</em>{1}=P\left(G<em>{1}\left(x</em>{i}\right) \neq y_{i}\right)=0.1*3=0.3$。                                        </li>
<li>计算$G<em>1(x)$的系数：$\alpha</em>{1}=\frac{1}{2} \log \frac{1-e<em>{1}}{e</em>{1}}=0.4236$               </li>
<li>更新训练数据的权值分布：                  <script type="math/tex; mode=display">
\begin{aligned}
D_{2}=&\left(w_{21}, \cdots, w_{2 i}, \cdots, w_{210}\right) \\
w_{2 i}=& \frac{w_{1 i}}{Z_{1}} \exp \left(-\alpha_{1} y_{i} G_{1}\left(x_{i}\right)\right), \quad i=1,2, \cdots, 10 \\
D_{2}=&(0.07143,0.07143,0.07143,0.07143,0.07143,0.07143,\\
&0.16667,0.16667,0.16667,0.07143) \\
f_{1}(x) &=0.4236 G_{1}(x)=array([ 0.4236,  0.4236,  0.4236, -0.4236, -0.4236, -0.4236, -0.4236,
  -0.4236, -0.4236, -0.4236])
\end{aligned}</script>权重为[0.07143×7,0.16667×3]</li>
</ul>
</li>
<li><p>对于m=2：                   </p>
<ul>
<li>在权值分布$D_2$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=8.5时分类误差率最低，那么基本分类器为：                  <script type="math/tex; mode=display">
G_{2}(x)=\left\{\begin{array}{ll}
1, & x<8.5 \\
-1, & x>8.5
\end{array}\right.</script></li>
<li>样本4.5.6分错，$G_2(x)$在训练数据集上的误差率为$e_2 = 0.07143*3=0.2143$                    </li>
<li>计算$G_2(x)$的系数：$\alpha_2 = 0.6496$                        </li>
<li>更新训练数据的权值分布：                  <script type="math/tex; mode=display">
\begin{aligned}
D_{3}=&(0.0455,0.0455,0.0455,0.1667,0.1667,0.1667\\
&0.1060,0.1060,0.1060,0.0455) \\
f_{2}(x) &=0.4236 G_{1}(x)+0.6496 G_{2}(x)=array([ 1.0732,  1.0732,  1.0732,  0.226 ,  0.226 ,  0.226 ,  0.226 ,
   0.226 ,  0.226 , -1.0732])
\end{aligned}</script>权重为[0.00455×4,0.1060×3,0.16667×3]。</li>
</ul>
</li>
<li>对m=3：                          <ul>
<li>在权值分布$D_3$的训练数据集上，遍历每个结点并计算分类误差率$e_m$，阈值取v=5.5时分类误差率最低，那么基本分类器为：                     <script type="math/tex; mode=display">
G_{3}(x)=\left\{\begin{array}{ll}
1, & x>5.5 \\
-1, & x<5.5
\end{array}\right.</script></li>
<li>样本1.2.3.10分错，$G_3(x)$在训练数据集上的误差率为$e_3 =0.0455*4= 0.1820$                       </li>
<li>计算$G_3(x)$的系数：$\alpha_3 = 0.7514$                                 </li>
<li>更新训练数据的权值分布：<br>$D_{4}=(0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)$                       <script type="math/tex; mode=display">f_{3}(x)=0.4236 G_{1}(x)+0.6496 G_{2}(x)+0.7514 G_{3}(x)=array([ 0.3218,  0.3218,  0.3218, -0.5254, -0.5254, -0.5254,  0.9774,
   0.9774,  0.9774, -0.3218])</script>   分类器$\operatorname{sign}\left[f<em>{3}(x)\right]$在训练数据集上的误分类点的个数为0。<br>最终分类器为：$G(x)=\operatorname{sign}\left[f</em>{3}(x)\right]=\operatorname{sign}\left[0.4236 G<em>{1}(x)+0.6496 G</em>{2}(x)+0.7514 G_{3}(x)\right]$</li>
</ul>
</li>
</ol>
<p>==可以看到每次样本权重和都为1，分类器错误率越来越低，分类器权重越来越高。最终分类器结果就是基分类器结果乘以其权重的加和==</p>
<h3 id="2-3-Adaboos代码举例"><a href="#2-3-Adaboos代码举例" class="headerlink" title="2.3 Adaboos代码举例"></a>2.3 Adaboos代码举例</h3><p>数据集：CI的机器学习库里的<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data">葡萄酒数据集</a>，该数据集包含了178个样本和13个特征，从不同的角度对不同的化学特性进行描述，最终预测红酒属于哪一个类别。(案例来源《python机器学习(第二版》)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 引入数据科学相关工具包：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据：         </span></span><br><span class="line">wine = pd.read_csv(<span class="string">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;</span>,header=<span class="literal">None</span>)</span><br><span class="line">wine.columns = [<span class="string">&#x27;Class label&#x27;</span>, <span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Malic acid&#x27;</span>, <span class="string">&#x27;Ash&#x27;</span>, <span class="string">&#x27;Alcalinity of ash&#x27;</span>,<span class="string">&#x27;Magnesium&#x27;</span>, <span class="string">&#x27;Total phenols&#x27;</span>,<span class="string">&#x27;Flavanoids&#x27;</span>, <span class="string">&#x27;Nonflavanoid phenols&#x27;</span>, </span><br><span class="line">                <span class="string">&#x27;Proanthocyanins&#x27;</span>,<span class="string">&#x27;Color intensity&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>,<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>,<span class="string">&#x27;Proline&#x27;</span>]</span><br><span class="line"><span class="comment"># 数据查看：</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Class labels&quot;</span>,np.unique(wine[<span class="string">&quot;Class label&quot;</span>]))</span><br><span class="line">wine.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/8e3e3b19c1f04d1b9257419b42646479.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>下面对数据做简单解读：</p>
<p>Class label：分类标签<br>Alcohol：酒精<br>Malic acid：苹果酸<br>Ash：灰<br>Alcalinity of ash：灰的碱度<br>Magnesium：镁<br>Total phenols：总酚<br>Flavanoids：黄酮类化合物<br>Nonflavanoid phenols：非黄烷类酚类<br>Proanthocyanins：原花青素<br>Color intensity：色彩强度<br>Hue：色调<br>OD280/OD315 of diluted wines：稀释酒OD280 OD350<br>Proline：脯氨酸</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 仅仅考虑2，3类葡萄酒，去除1类</span></span><br><span class="line">wine = wine[wine[<span class="string">&#x27;Class label&#x27;</span>] != <span class="number">1</span>]</span><br><span class="line">y = wine[<span class="string">&#x27;Class label&#x27;</span>].values</span><br><span class="line">X = wine[[<span class="string">&#x27;Alcohol&#x27;</span>,<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>]].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分类标签变成二进制编码：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">y = le.fit_transform(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按8：2分割训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1</span>,stratify=y)  <span class="comment"># stratify参数代表了按照y的类别等比例抽样</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用单一决策树建模</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">tree = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>,random_state=<span class="number">1</span>,max_depth=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">tree = tree.fit(X_train,y_train)</span><br><span class="line">y_train_pred = tree.predict(X_train)</span><br><span class="line">y_test_pred = tree.predict(X_test)</span><br><span class="line">tree_train = accuracy_score(y_train,y_train_pred)</span><br><span class="line">tree_test = accuracy_score(y_test,y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Decision tree train/test accuracies %.3f/%.3f&#x27;</span> % (tree_train,tree_test))</span><br><span class="line"></span><br><span class="line">Decision tree train/test accuracies <span class="number">0.916</span>/<span class="number">0.875</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用sklearn实现Adaboost(基分类器为决策树)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">AdaBoostClassifier相关参数：</span></span><br><span class="line"><span class="string">base_estimator：基本分类器，默认为DecisionTreeClassifier(max_depth=1)</span></span><br><span class="line"><span class="string">n_estimators：终止迭代的次数</span></span><br><span class="line"><span class="string">learning_rate：学习率</span></span><br><span class="line"><span class="string">algorithm：训练的相关算法，&#123;&#x27;SAMME&#x27;，&#x27;SAMME.R&#x27;&#125;，默认=&#x27;SAMME.R&#x27;</span></span><br><span class="line"><span class="string">random_state：随机种子</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">ada = AdaBoostClassifier(base_estimator=tree,n_estimators=<span class="number">500</span>,learning_rate=<span class="number">0.1</span>,random_state=<span class="number">1</span>)</span><br><span class="line">ada = ada.fit(X_train,y_train)</span><br><span class="line">y_train_pred = ada.predict(X_train)</span><br><span class="line">y_test_pred = ada.predict(X_test)</span><br><span class="line">ada_train = accuracy_score(y_train,y_train_pred)</span><br><span class="line">ada_test = accuracy_score(y_test,y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Adaboost train/test accuracies %.3f/%.3f&#x27;</span> % (ada_train,ada_test))</span><br><span class="line"></span><br><span class="line">Adaboost train/test accuracies <span class="number">1.000</span>/<span class="number">0.917</span></span><br></pre></td></tr></table></figure>
<p>结果分析：单层决策树似乎对训练数据欠拟合，而Adaboost模型正确地预测了训练数据的所有分类标签，而且与单层决策树相比，Adaboost的测试性能也略有提高。然而，为什么模型在训练集和测试集的性能相差这么大呢？我们使用图像来简单说明下这个道理！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出单层决策树与Adaboost的决策边界：</span></span><br><span class="line">x_min = X_train[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">x_max = X_train[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min = X_train[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">y_max = X_train[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.1</span>),np.arange(y_min, y_max, <span class="number">0.1</span>))</span><br><span class="line">f, axarr = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>,sharex=<span class="string">&#x27;col&#x27;</span>,sharey=<span class="string">&#x27;row&#x27;</span>,figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="number">0</span>, <span class="number">1</span>],[tree, ada],[<span class="string">&#x27;Decision tree&#x27;</span>, <span class="string">&#x27;Adaboost&#x27;</span>]):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx].contourf(xx, yy, Z, alpha=<span class="number">0.3</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">0</span>, <span class="number">0</span>],X_train[y_train==<span class="number">0</span>, <span class="number">1</span>],c=<span class="string">&#x27;blue&#x27;</span>, marker=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">1</span>, <span class="number">0</span>],X_train[y_train==<span class="number">1</span>, <span class="number">1</span>],c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    axarr[idx].set_title(tt)</span><br><span class="line">axarr[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;Alcohol&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.text(<span class="number">0</span>, -<span class="number">0.2</span>,s=<span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>,ha=<span class="string">&#x27;center&#x27;</span>,va=<span class="string">&#x27;center&#x27;</span>,fontsize=<span class="number">12</span>,transform=axarr[<span class="number">1</span>].transAxes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e7ee7c34eacb4218b8e52011424f4055.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>从上面的决策边界图可以看到：</p>
<ul>
<li>Adaboost模型的决策边界比单层决策树的决策边界要复杂的多。也就是说，Adaboost试图用增加模型复杂度而降低偏差的方式去减少总误差，但是过程中引入了方差，可能出现过拟合</li>
<li>与单个分类器相比，Adaboost等Boosting模型增加了计算的复杂度，在实践中需要仔细思考是否愿意为预测性能的相对改善而增加计算成本</li>
<li>Boosting方式无法做到现在流行的并行计算的方式进行训练，因为每一步迭代都要基于上一部的基本分类器。</li>
</ul>
<h2 id="三、-前向分步算法"><a href="#三、-前向分步算法" class="headerlink" title="三、 前向分步算法"></a>三、 前向分步算法</h2><p>Adaboost的算法内容：通过计算M个基本分类器，每个分类器的错误率、样本权重以及模型权重。我们可以认为：Adaboost每次学习单一分类器以及单一分类器的参数(权重)。<br>抽象出Adaboost算法的整体框架逻辑，构建集成学习的一个非常重要的框架——前向分步算法，有了这个框架，我们不仅可以解决分类问题，也可以解决回归问题。</p>
<h3 id="3-1加法模型"><a href="#3-1加法模型" class="headerlink" title="3.1加法模型"></a>3.1加法模型</h3><p>在Adaboost模型中，我们把每个基本分类器合成一个复杂分类器的方法是每个基本分类器的加权和，即：$f(x)=\sum<em>{m=1}^{M} \beta</em>{m} b\left(x ; \gamma<em>{m}\right)$，其中，$b\left(x ; \gamma</em>{m}\right)$为即基本分类器，$\gamma<em>{m}$为基本分类器的参数，$\beta_m$为基本分类器的权重，显然这与第二章所学的加法模型。为什么这么说呢？大家把$b(x ; \gamma</em>{m})$看成是即函数即可。<br>在给定训练数据以及损失函数$L(y, f(x))$的条件下，学习加法模型$f(x)$就是：                        </p>
<script type="math/tex; mode=display">
\min _{\beta_{m}, \gamma_{m}} \sum_{i=1}^{N} L\left(y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i} ; \gamma_{m}\right)\right)</script><p>通常这是一个复杂的优化问题，很难通过简单的凸优化的相关知识进行解决。前向分步算法可以用来求解这种方式的问题，它的基本思路是：因为学习的是加法模型，如果从前向后，每一步只优化一个基函数及其系数，逐步逼近目标函数，那么就可以降低优化的复杂度。具体而言，每一步只需要优化：                    </p>
<script type="math/tex; mode=display">
\min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, \beta b\left(x_{i} ; \gamma\right)\right)</script><h3 id="3-2-前向分步算法"><a href="#3-2-前向分步算法" class="headerlink" title="3.2 前向分步算法"></a>3.2 前向分步算法</h3><p>给定数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$，$x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，$y</em>{i} \in \mathcal{Y}={+1,-1}$。损失函数$L(y, f(x))$，基函数集合${b(x ; \gamma)}$，我们需要输出加法模型$f(x)$。                         </p>
<ul>
<li>初始化：$f_{0}(x)=0$                           </li>
<li>对m = 1,2,…,M:                     <ul>
<li>(a) 极小化损失函数：<script type="math/tex; mode=display">
\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)</script>得到参数$\beta<em>{m}$与$\gamma</em>{m}$                                           </li>
<li>(b) 更新：                          <script type="math/tex; mode=display">
f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left(x ; \gamma_{m}\right)</script></li>
</ul>
</li>
<li>得到加法模型：                           <script type="math/tex; mode=display">
f(x)=f_{M}(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)</script></li>
</ul>
<p>这样，前向分步算法将同时求解从m=1到M的所有参数$\beta<em>{m}$，$\gamma</em>{m}$的优化问题简化为逐次求解各个$\beta<em>{m}$，$\gamma</em>{m}$的问题。                           </p>
<h3 id="3-3-前向分步算法与Adaboost的关系"><a href="#3-3-前向分步算法与Adaboost的关系" class="headerlink" title="3.3 前向分步算法与Adaboost的关系"></a>3.3 前向分步算法与Adaboost的关系</h3><p>由于这里不是我们的重点，我们主要阐述这里的结论，不做相关证明，具体的证明见李航老师的《统计学习方法》第八章的3.2节。Adaboost算法是前向分步算法的特例，Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</p>
<h2 id="四、梯度提升决策树-GBDT"><a href="#四、梯度提升决策树-GBDT" class="headerlink" title="四、梯度提升决策树(GBDT)"></a>四、梯度提升决策树(GBDT)</h2><ul>
<li>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树。GBDT使用的决策树是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树</li>
<li>CART假设决策树都是二叉树，内部节点特征取值为“是”和“否”，等价于递归二分每个特征。对回归树用平方误差最小化准则（回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适），对分类树用基尼系数最小化准则，进行特征选择生成二叉树</li>
<li><p>回归问题没有分类错误率可言，，用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题</p>
<h3 id="4-1-Decision-Tree：CART回归树"><a href="#4-1-Decision-Tree：CART回归树" class="headerlink" title="4.1 Decision Tree：CART回归树"></a>4.1 Decision Tree：CART回归树</h3><p>最小二乘回归树生成算法见《统计学习方法》P82，算法5.5：<br><img src="https://img-blog.csdnimg.cn/15618f981d2f4b29a6d81ffceca8cfdb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h3 id="4-2-回归提升树算法"><a href="#4-2-回归提升树算法" class="headerlink" title="4.2 回归提升树算法"></a>4.2 回归提升树算法</h3><p>提升法：加法模型+前向分步算法。<br>提升树：以决策树为基函数（基本分类器）的提升方法，可表示为决策树的加法模型。<br>决策树：分类问题是二叉分类树，回归问题是二叉回归树</p>
</li>
<li><p>Adaboost：加法模型+前向分步算法的分类树模型</p>
</li>
<li>GBDT：     加法模型+前向分步算法的回归树模型</li>
</ul>
<p>分类误差率</p>
<ul>
<li>Adaboost算法：使用了分类错误率修正样本权重以及计算每个基本分类器的权重</li>
<li>GBDT：回归问题没有分类错误率可言，，用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题</li>
</ul>
<p>根据以上两点得到回归问题提升树算法：</p>
<p>输入：数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}, x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y</em>{i} \in \mathcal{Y} \subseteq \mathbf{R}$<br>输出：最终的提升树$f_{M}(x)$                             </p>
<ul>
<li>初始化$f_0(x) = 0$                        </li>
<li>对m = 1,2,…,M：                  <ul>
<li>计算每个样本的残差:$r<em>{m i}=y</em>{i}-f<em>{m-1}\left(x</em>{i}\right), \quad i=1,2, \cdots, N$                                    </li>
<li>拟合残差$r<em>{mi}$学习一棵回归树，得到$T\left(x ; \Theta</em>{m}\right)$                        </li>
<li>更新$f<em>{m}(x)=f</em>{m-1}(x)+T\left(x ; \Theta_{m}\right)$</li>
</ul>
</li>
<li>得到最终的回归问题的提升树：$f<em>{M}(x)=\sum</em>{m=1}^{M} T\left(x ; \Theta_{m}\right)$                         </li>
</ul>
<p>下面我们用一个实际的案例来使用这个算法：(案例来源：李航老师《统计学习方法》P168，此处省略)                                                             </p>
<h3 id="4-3-梯度提升决策树算法-GBDT"><a href="#4-3-梯度提升决策树算法-GBDT" class="headerlink" title="4.3 梯度提升决策树算法(GBDT)"></a>4.3 梯度提升决策树算法(GBDT)</h3><p> GBDT：利用损失函数的负梯度作为回归问题提升树算法中的残差的近似值，拟合回归树。</p>
<ul>
<li>提升树利用加法模型和前向分步算法实现学习的过程，当损失函数为平方损失和指数损失时，每一步优化是相当简单的，也就是我们前面探讨的提升树算法和Adaboost算法。对于一般的损失函数而言，往往每一步的优化不是那么容易</li>
<li>针对这一问题，Freidman提出了梯度提升算法(gradient boosting)，利用损失函数的负梯度在当前模型的值$-\left[\frac{\partial L\left(y, f\left(x<em>{i}\right)\right)}{\partial f\left(x</em>{i}\right)}\right]<em>{f(x)=f</em>{m-1}(x)}$作为回归问题提升树算法中的残差的近似值，拟合回归树。<strong>与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例。</strong></li>
</ul>
<p>以下开始具体介绍梯度提升算法：<br>输入训练数据集$T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}, x<em>{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y</em>{i} \in \mathcal{Y} \subseteq \mathbf{R}$和损失函数$L(y, f(x))$，输出回归树$\hat{f}(x)$                              </p>
<ul>
<li>初始化$f<em>{0}(x)=\arg \min </em>{c} \sum<em>{i=1}^{N} L\left(y</em>{i}, c\right)$                     </li>
<li>对于m=1,2,…,M：                   <ul>
<li>对i = 1,2,…,N计算：$r<em>{m i}=-\left[\frac{\partial L\left(y</em>{i}, f\left(x<em>{i}\right)\right)}{\partial f\left(x</em>{i}\right)}\right]<em>{f(x)=f</em>{m-1}(x)}$                </li>
<li>对$r<em>{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R</em>{m j}, j=1,2, \cdots, J$                           </li>
<li>对j=1,2,…J，计算：$c<em>{m j}=\arg \min </em>{c} \sum<em>{x</em>{i} \in R<em>{m j}} L\left(y</em>{i}, f<em>{m-1}\left(x</em>{i}\right)+c\right)$                      </li>
<li>更新$f<em>{m}(x)=f</em>{m-1}(x)+\sum<em>{j=1}^{J} c</em>{m j} I\left(x \in R_{m j}\right)$                    </li>
</ul>
</li>
<li>得到回归树：$\hat{f}(x)=f<em>{M}(x)=\sum</em>{m=1}^{M} \sum<em>{j=1}^{J} c</em>{m j} I\left(x \in R_{m j}\right)$</li>
</ul>
<p>下面，我们来使用一个具体的案例来说明GBDT是如何运作的(<a target="_blank" rel="noopener" href="https://blog.csdn.net/zpalyq110/article/details/79527653">案例来源</a> )：<br>下面的表格是数据：<br><img src="https://img-blog.csdnimg.cn/14f12b8bf27d43ad90e52d67258a25b0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-hBCnWMyY-1638484890166)(./6.png)\]"></p>
<p>学习率：learning_rate=0.1，迭代次数：n_trees=5，树的深度：max_depth=3<br>平方损失的负梯度为：</p>
<script type="math/tex; mode=display">
-\left[\frac{\left.\partial L\left(y, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{t-1}(x)}=y-f\left(x_{i}\right)</script><p>$c=(1.1+1.3+1.7+1.8)/4=1.475，f_{0}(x)=c=1.475$<br><img src="https://img-blog.csdnimg.cn/3319d5746aed4188b03e57d37cb53cca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt=""></p>
<p>学习决策树，分裂结点：<br><img src="https://img-blog.csdnimg.cn/3c1ff3f51eb7480e99248dd3fb4dba73.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/b9bcefd41c2342d99935c6070e6fbaa7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/69b7a5687482439a8025ba693cb33558.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>对于右节点，只有2，3两个样本，那么根据下表我们选择年龄30进行划分：<br><img src="https://img-blog.csdnimg.cn/7d63133f988e46648ccaa30269007385.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/cce3670868d54729933a6769d75f6a29.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-DgdpHG6v-1638484890170)(./13.png)\]"></p>
<p>因此根据$\Upsilon<em>{j 1}=\underbrace{\arg \min }</em>{\Upsilon} \sum<em>{x</em>{i} \in R<em>{j 1}} L\left(y</em>{i}, f<em>{0}\left(x</em>{i}\right)+\Upsilon\right)$：                                </p>
<script type="math/tex; mode=display">
\begin{array}{l}
\left(x_{0} \in R_{11}\right), \quad \Upsilon_{11}=-0.375 \\
\left(x_{1} \in R_{21}\right), \quad \Upsilon_{21}=-0.175 \\
\left(x_{2} \in R_{31}\right), \quad \Upsilon_{31}=0.225 \\
\left(x_{3} \in R_{41}\right), \quad \Upsilon_{41}=0.325
\end{array}</script><p>这里其实和上面初始化学习器是一个道理，平方损失，求导，令导数等于零，化简之后得到每个叶子节点的参数$\Upsilon$,其实就是标签值的均值。<br>最后得到五轮迭代：<br><img src="https://img-blog.csdnimg.cn/ba161ee53e4b487fa7509dca014222c8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>最后的强学习器为：$f(x)=f<em>{5}(x)=f</em>{0}(x)+\sum<em>{m=1}^{5} \sum</em>{j=1}^{4} \Upsilon<em>{j m} I\left(x \in R</em>{j m}\right)$。<br>其中：</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
f_{0}(x)=1.475 & f_{2}(x)=0.0205 \\
f_{3}(x)=0.1823 & f_{4}(x)=0.1640 \\
f_{5}(x)=0.1476
\end{array}</script><p>预测结果为：                       </p>
<script type="math/tex; mode=display">
f(x)=1.475+0.1 *(0.2250+0.2025+0.1823+0.164+0.1476)=1.56714</script><p>为什么要用学习率呢？这是Shrinkage的思想，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。    </p>
<h3 id="4-3-GBDT代码示例"><a href="#4-3-GBDT代码示例" class="headerlink" title="4.3 GBDT代码示例"></a>4.3 GBDT代码示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">下面我们来使用sklearn来使用GBDT</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_friedman1</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">GradientBoostingRegressor参数解释：</span></span><br><span class="line"><span class="string">loss：&#123;‘ls’, ‘lad’, ‘huber’, ‘quantile’&#125;, default=’ls’：‘ls’ 指最小二乘回归. ‘lad’ (最小绝对偏差) 是仅基于输入变量的顺序信息的高度鲁棒的损失函数。. ‘huber’ 是两者的结合. ‘quantile’允许分位数回归（用于alpha指定分位数）</span></span><br><span class="line"><span class="string">learning_rate：学习率缩小了每棵树的贡献learning_rate。在learning_rate和n_estimators之间需要权衡。</span></span><br><span class="line"><span class="string">n_estimators：要执行的提升次数。</span></span><br><span class="line"><span class="string">subsample：用于拟合各个基础学习者的样本比例。如果小于1.0，则将导致随机梯度增强。subsample与参数n_estimators。选择会导致方差减少和偏差增加。subsample &lt; 1.0</span></span><br><span class="line"><span class="string">criterion：&#123;&#x27;friedman_mse&#x27;，&#x27;mse&#x27;，&#x27;mae&#x27;&#125;，默认=&#x27;friedman_mse&#x27;：“ mse”是均方误差，“ mae”是平均绝对误差。默认值“ friedman_mse”通常是最好的，因为在某些情况下它可以提供更好的近似值。</span></span><br><span class="line"><span class="string">min_samples_split：拆分内部节点所需的最少样本数</span></span><br><span class="line"><span class="string">min_samples_leaf：在叶节点处需要的最小样本数。</span></span><br><span class="line"><span class="string">min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</span></span><br><span class="line"><span class="string">max_depth：各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量的相互作用。</span></span><br><span class="line"><span class="string">min_impurity_decrease：如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂。</span></span><br><span class="line"><span class="string">min_impurity_split：提前停止树木生长的阈值。如果节点的杂质高于阈值，则该节点将分裂</span></span><br><span class="line"><span class="string">max_features&#123;‘auto’, ‘sqrt’, ‘log2’&#125;，int或float：寻找最佳分割时要考虑的功能数量：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为int，则max_features在每个分割处考虑特征。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果“auto”，则max_features=n_features。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果是“ sqrt”，则max_features=sqrt(n_features)。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果为“ log2”，则为max_features=log2(n_features)。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果没有，则max_features=n_features。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">X, y = make_friedman1(n_samples=<span class="number">1200</span>, random_state=<span class="number">0</span>, noise=<span class="number">1.0</span>)</span><br><span class="line">X_train, X_test = X[:<span class="number">200</span>], X[<span class="number">200</span>:]</span><br><span class="line">y_train, y_test = y[:<span class="number">200</span>], y[<span class="number">200</span>:]</span><br><span class="line">est = GradientBoostingRegressor(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>, loss=<span class="string">&#x27;ls&#x27;</span>).fit(X_train, y_train)</span><br><span class="line">mean_squared_error(y_test, est.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="number">5.009154859960321</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X, y = make_regression(random_state=<span class="number">0</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, random_state=<span class="number">0</span>)</span><br><span class="line">reg = GradientBoostingRegressor(random_state=<span class="number">0</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="number">0.43848663277068134</span></span><br></pre></td></tr></table></figure>
<p>GradientBoostingRegressor与GradientBoostingClassifier函数的各个参数的意思！参考文档：<br><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor</a><br><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gra#sklearn.ensemble.GradientBoostingClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gra#sklearn.ensemble.GradientBoostingClassifier</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/12/03/datawhale课程——集成学习/集成学习2：Boosting算法：Adaboost&amp;GBDT/">https://zhxnlp.github.io/2021/12/03/datawhale课程——集成学习/集成学习2：Boosting算法：Adaboost&amp;GBDT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><a class="post-meta__tags" href="/tags/Boosting/">Boosting</a><a class="post-meta__tags" href="/tags/Adaboost/">Adaboost</a><a class="post-meta__tags" href="/tags/GBDT/">GBDT</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/12/04/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A03%EF%BC%9A%20XGBoost&amp;LightGBM/"><i class="fa fa-chevron-left">  </i><span>集成学习3： XGBoost&amp;LightGBM</span></a></div><div class="next-post pull-right"><a href="/2021/12/02/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A01%E2%80%94%E2%80%94voting%E3%80%81bagging&amp;stacking/"><span>集成学习1——voting、bagging&amp;stacking</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>