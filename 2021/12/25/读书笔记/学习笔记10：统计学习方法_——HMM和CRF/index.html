<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="学习笔记10：统计学习方法_——HMM和CRF"><meta name="keywords" content="CRF,机器学习,统计学习方法"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>学习笔记10：统计学习方法_——HMM和CRF | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="toc-text">一、概率图模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%A6%82%E8%A7%88"><span class="toc-text">1.1 概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%9C%89%E5%90%91%E5%9B%BE"><span class="toc-text">1.2 有向图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%97%A0%E5%90%91%E5%9B%BE"><span class="toc-text">1.3 无向图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.4 生成式模型和判别式模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-1%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%8C%BA%E5%88%AB"><span class="toc-text">1.4.1生成式模型和判别式模型区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-%E4%B8%BA%E5%95%A5%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%95%88%E6%9E%9C%E6%9B%B4%E5%A5%BD"><span class="toc-text">1.4.2 为啥判别式模型预测效果更好</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%9A%90%E5%BC%8F%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM"><span class="toc-text">二、隐式马尔科夫模型HMM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-HMM%E5%AE%9A%E4%B9%89"><span class="toc-text">2.1 HMM定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-HMM%E4%B8%89%E8%A6%81%E7%B4%A0%E5%92%8C%E4%B8%A4%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE"><span class="toc-text">2.2 HMM三要素和两个基本假设</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-HMM%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98"><span class="toc-text">2.3 HMM三个基本问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-HMM%E5%9F%BA%E6%9C%AC%E8%A7%A3%E6%B3%95"><span class="toc-text">2.4 HMM基本解法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88%E6%A0%B9%E6%8D%AEI%E5%92%8CO%E6%B1%82%CE%BB%EF%BC%89"><span class="toc-text">2.4.1 极大似然估计（根据I和O求λ）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95%EF%BC%88%E6%B2%A1%E6%9C%89I%EF%BC%89"><span class="toc-text">2.4.2 前向后向算法（没有I）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%EF%BC%88%E8%A7%A3%E7%A0%81%EF%BC%89%E8%BF%87%E7%A8%8B"><span class="toc-text">2.4.3 序列标注（解码）过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%9C%80%E5%A4%A7%E7%86%B5%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%ABMEMM%E6%A8%A1%E5%9E%8B"><span class="toc-text">三、最大熵马尔科夫MEMM模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-MEMM%E5%8E%9F%E7%90%86%E5%92%8C%E5%8C%BA%E5%88%AB"><span class="toc-text">3.1 MEMM原理和区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%A0%87%E6%B3%A8%E5%81%8F%E7%BD%AE"><span class="toc-text">3.2 标注偏置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF"><span class="toc-text">四、条件随机场CRF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-CRF%E5%AE%9A%E4%B9%89"><span class="toc-text">4.1 CRF定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BA%BF%E6%80%A7%E9%93%BECRF%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-text">4.2 线性链CRF的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BB%8E%E5%85%AC%E5%BC%8F%E5%88%B0%E4%BB%A3%E7%A0%81%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-text">4.3 从公式到代码的理解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81-HMM-vs-MEMM-vs-CRF"><span class="toc-text">五、 HMM vs. MEMM vs. CRF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-HMM-vs-MEMM"><span class="toc-text">5.1 HMM vs MEMM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-MEMM-vs-CRF"><span class="toc-text">5.2 MEMM vs CRF</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">学习笔记10：统计学习方法_——HMM和CRF</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">4.5k</span><span class="post-meta__separator">|</span><span>阅读时长: 16 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>参考文章<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/35866596/answer/236886066?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_content=group3_Answer&amp;utm_campaign=shareopn">《如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？》</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178731739?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《条件随机场CRF之从公式到代码》</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148813079?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《CRF条件随机场的原理、例子、公式推导和应用》</a><br><span id="more"></span></p>
<h2 id="一、概率图模型"><a href="#一、概率图模型" class="headerlink" title="一、概率图模型"></a>一、概率图模型</h2><h3 id="1-1-概览"><a href="#1-1-概览" class="headerlink" title="1.1 概览"></a>1.1 概览</h3><p>在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构：</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/c8b569f3f6d04c488431dce83ed52f8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>在概率图模型中，数据(样本)由图$G=(V,E)$建模表示：</p>
<ul>
<li>V：节点v的集合。v∈V表示随机变量$Y_v$。</li>
<li>E：边e的集合。e∈E表示随机变量之间的概率依赖关系。</li>
<li>P(Y):由图表示的联合概率分布</li>
</ul>
<p>有向图和无向图区别在于如何求概率分布P(Y)。</p>
<h3 id="1-2-有向图"><a href="#1-2-有向图" class="headerlink" title="1.2 有向图"></a>1.2 有向图</h3><p>有向图模型，这么求联合概率： </p>
<script type="math/tex; mode=display">P(x_{1}...x_{n})=\prod_{i=0}P(x_{i}|\pi (x_{i}))</script><p>对于下图求概率有：<br><img src="https://img-blog.csdnimg.cn/7256a9d4eb6e4d7ea9647c6d47746801.png" alt="在这里插入图片描述"></p>
<script type="math/tex; mode=display">P(x_{1}...x_{n})=P(x_{1})\cdot P(x_{2}| x_{1})\cdot P(x_{3}| x_{2})\cdot P(x_{4})|P(x_{2})P(x_{5}|x_{3},x_{4})</script><h3 id="1-3-无向图"><a href="#1-3-无向图" class="headerlink" title="1.3 无向图"></a>1.3 无向图</h3><p>基本概念：</p>
<ol>
<li>团：节点子集，子集中任何两个节点均有边相连</li>
<li>最大团：不能再加入节点使其更大的团</li>
<li>因子分解：联合概率分布P(Y)表示为<code>所有最大团上随机变量函数的乘积的形式</code>为因子分解。</li>
</ol>
<p>所以有：联合概率分布为最大团势函数的乘积。</p>
<script type="math/tex; mode=display">P(Y)=\frac{1}{Z}\prod_{C}\psi _{C}(Y_{C})=\frac{1}{Z}\prod_{C}exp^{-E(Y_{C})}</script><ul>
<li>C：无向图的最大团</li>
<li>$Y_{C}$:最大团C上的节点(随机变量）</li>
<li>Z：规范化因子。$Z=\sum<em>{Y}\prod</em>{C}\psi <em>{C}(Y</em>{C})$,使得输出P(Y)具有概率意义。</li>
<li>$\psi <em>{C}(Y</em>{C})$:严格正的势函数，通常为指数函数$\psi <em>{C}(Y</em>{C})=exp^{-E(Y_{C})}$。</li>
</ul>
<p>马尔科夫性，保证概率图为概率无向图：</p>
<ul>
<li>成对马尔科夫性：u和v没有边相连，O为其它所有节点，$Y_u和Y_v$互相独立。</li>
<li>局部马尔科夫性：对于任意节点v、相连节点集合W和无边相连集合O，给定$Y_W$情况下，$Y_v和Y_O$互相独立：</li>
<li>全局马尔科夫性：节点A和B被节点集合C分割，$Y_A和Y_B$互相独立：</li>
</ul>
<p>总之就是没有边相连的节点概率互相独立。</p>
<p>对于一个无向图，举例如下：</p>
<p><img src="https://img-blog.csdnimg.cn/f77a15a1b61a4829868472dce880feb5.png" alt="在这里插入图片描述"></p>
<script type="math/tex; mode=display">P(Y)=\frac{1}{Z}\prod_{C}\psi( _{X_{1},X_{3},X_{4}})\psi( _{X_{2},X_{3},X_{4}})</script><h3 id="1-4-生成式模型和判别式模型"><a href="#1-4-生成式模型和判别式模型" class="headerlink" title="1.4 生成式模型和判别式模型"></a>1.4 生成式模型和判别式模型</h3><h3 id="1-4-1生成式模型和判别式模型区别"><a href="#1-4-1生成式模型和判别式模型区别" class="headerlink" title="1.4.1生成式模型和判别式模型区别"></a>1.4.1生成式模型和判别式模型区别</h3><p>有监督学习中，训练数据包括输入X和标签Y。所以模型求的是X和Y的概率分布。根据概率论的知识可以知道，对应的概率分布（以概率密度函数指代概率分布）有两种：</p>
<ul>
<li>联合概率分布：$P_{\theta }(X,Y)$，表示数据和标签同时出现的概率，对应于生成式模型。</li>
<li>条件概率分布：P_{\theta }(Y|X)，表示给定数据条件下，对应标签的概率，对应于判别式模型。</li>
</ul>
<p>进一步理解：</p>
<ul>
<li>生成式模型：除了能够根据输入数据 X 来预测对应的标签 Y ,还能根据训练得到的模型产生服从训练数据集分布的数据( X ，Y），相当于生成一组新的数据，所以称之为生成式模型。</li>
<li>判别式模型：仅仅根据X由条件概率$P_{\theta }(Y|X)$来预测标签Y。牺牲了生成数据的能力，但是比生成式模型的预测准确率高。<h4 id="1-4-2-为啥判别式模型预测效果更好"><a href="#1-4-2-为啥判别式模型预测效果更好" class="headerlink" title="1.4.2 为啥判别式模型预测效果更好"></a>1.4.2 为啥判别式模型预测效果更好</h4>原因如下：由全概率公式和信息熵公式可以得到：<script type="math/tex; mode=display">P(X,Y)=\int P(Y|X)P(X)dX</script>即计算全概率公式$P(X,Y)$时引入了输入数据的概率分布$P(X)$，而这个并不是我们关心的。我们只关心给定X情况下Y的分布，这就相对削弱了模型的预测能力。<br>另外从信息熵的角度进行定量分析。</li>
</ul>
<ol>
<li>X的信息熵定义为：<script type="math/tex; mode=display">H(X)=-\int P(X)logP(X)dX</script></li>
<li>两个离散随机变量 X  和 Y  的联合熵 (Joint Entropy) 表示两事件同时发生系统的不确定度:<script type="math/tex; mode=display">H(X,Y)=-\int P(X,Y)logP(X,Y)dXdY</script></li>
<li>条件熵 (Conditional Entropy) H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性：<script type="math/tex; mode=display">H(Y|X)=-\int P(Y|X)logP(Y|X)dX</script></li>
</ol>
<p>可以推导出来$H(Y|X)=H(X,Y)-H(X)$.一般H(X)&gt;0（所有离散分布和很多连续分布满足这个条件），可以知道条件分布的信息熵小于联合分布，即判别模型比生成式模型含有更多的信息，所以同条件下比生成式模型效果更好。&lt;/font &gt;</p>
<h2 id="二、隐式马尔科夫模型HMM"><a href="#二、隐式马尔科夫模型HMM" class="headerlink" title="二、隐式马尔科夫模型HMM"></a>二、隐式马尔科夫模型HMM</h2><h3 id="2-1-HMM定义"><a href="#2-1-HMM定义" class="headerlink" title="2.1 HMM定义"></a>2.1 HMM定义</h3><ul>
<li>隐马尔可夫模型是关于时序的概率模型</li>
<li>描述由一个<code>隐藏的马尔可夫链</code>随机生成<code>不可观测的状态随机序列</code>(state sequence)，再由各个状态生成一个观测而产生<code>观测随机序列</code>(observation sequence )的过程,序列的每一个位置又可以看作是一个时刻。</li>
</ul>
<p>设Q是所有可能状态的集合，V是所有可能观测的集合：</p>
<script type="math/tex; mode=display">Q=(q_{1},q_{2},...q_{N})和V=(v_{1},v_{2},...v_{M})</script><p>对于长度为T的状态序列I和观测序列O有：</p>
<script type="math/tex; mode=display">i=(i_{1},i_{2},...i_{T})和O=(o_{1},o_{2},...o_{T})</script><p>其中:</p>
<ul>
<li>状态转移概率矩阵$A=(a<em>{ij})</em>{N\times N}\qquad i,j\epsilon (1,N)$。$a_{ij}$表示t时刻状态$q_i$转移到t+1时刻$q_j$的概率</li>
<li>观测概率（发射概率）矩阵$B=[b<em>{j}(k)]</em>{N\times M} \quad j\epsilon (1,N)k\epsilon (1,M)$。$b_{j}(k)$表示t时刻状态$q_i$生成观测$v_k$的概率。&lt;/font&gt;</li>
<li>初始状态概率向量$\pi =(\pi <em>{i})=P(i</em>{1}=q_{i})\quad i\epsilon (1,N)$。表示初始时刻处于状态$q_i$的概率。<br><img src="https://img-blog.csdnimg.cn/82df5f741379490f83e123408e52d70d.png" alt="在这里插入图片描述"></li>
<li>隐状态节点$i<em>t$在A的指导下生成下一个隐状态节点$i</em>{t+1}$，并且$i_t$在B的指导下生成观测节点$o_t$ , 并且我只能观测到序列O。</li>
<li>根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模:<br><img src="https://img-blog.csdnimg.cn/a468a47db0c34f2db5239e2c4bbf0d91.png" alt="在这里插入图片描述"><blockquote>
<p>只是我们都去这么来表示HMM是个生成式模型,实际不这么计算。</p>
<h3 id="2-2-HMM三要素和两个基本假设"><a href="#2-2-HMM三要素和两个基本假设" class="headerlink" title="2.2 HMM三要素和两个基本假设"></a>2.2 HMM三要素和两个基本假设</h3></blockquote>
</li>
</ul>
<ol>
<li>HMM由<code>初始状态概率向量π、状态转移概率矩阵A、 观测概率矩阵B</code>三元素构成。<br>所以HMM模型$\lambda$可以写成：$\lambda =(A,B,\pi)$。<code>三者共同决定了隐藏的马尔可夫链生成不可观测的状态序列</code>。而状态序列和矩阵B综合产生观测序列。</li>
<li>HMM模型基本假设<ul>
<li>齐次马尔科夫性假设：隐马尔可夫链任意时刻t的状态只依赖前一时刻t-1的状态，即$P(i<em>{t}|i</em>{i-1})$。&lt;/font&gt;</li>
<li>观测独立性假设：任意时刻的观测只依赖当前时刻的状态，即$P(o<em>{t}|i</em>{i})$。&lt;/font&gt;</li>
</ul>
</li>
</ol>
<h3 id="2-3-HMM三个基本问题"><a href="#2-3-HMM三个基本问题" class="headerlink" title="2.3 HMM三个基本问题"></a>2.3 HMM三个基本问题</h3><ol>
<li>概率计算：给定模型$\lambda =(A,B,\pi)$和观测序列O，计算观测序列O出现的概率$P(O|\lambda)$。</li>
<li>学习问题：已知观测序列O，用最大似然估计的方法计算模型$\lambda =(A,B,\pi)$的参数。（该模型下观测序列O的概率最大）</li>
<li>预测（解码）问题：已知模型$\lambda =(A,B,\pi)$和观测序列，求最有可能的对应状态序列。</li>
</ol>
<ul>
<li>==HMM可以用于序列标记，观测序列O为tokens，状态序列I为其对应的标记。此时问题是给定序列O预测对应序列I。==</li>
<li>问题2对应模型建立过程，问题3 对应解码过程（crf.decode）<h3 id="2-4-HMM基本解法"><a href="#2-4-HMM基本解法" class="headerlink" title="2.4 HMM基本解法"></a>2.4 HMM基本解法</h3><h4 id="2-4-1-极大似然估计（根据I和O求λ）"><a href="#2-4-1-极大似然估计（根据I和O求λ）" class="headerlink" title="2.4.1 极大似然估计（根据I和O求λ）"></a>2.4.1 极大似然估计（根据I和O求λ）</h4>一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的，即根据观测序列O和状态序列I求模型$\lambda =(A,B,\pi)$的参数，是一个有监督学习。</li>
</ul>
<ol>
<li>根据状态序列求状态转移矩阵A：<script type="math/tex; mode=display">\mathbf{a_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}}</script></li>
<li>根据状态序列I和观测序列O求观测概率矩阵B：<script type="math/tex; mode=display">\mathbf{b_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}}</script></li>
<li>直接估计π<h4 id="2-4-2-前向后向算法（没有I）"><a href="#2-4-2-前向后向算法（没有I）" class="headerlink" title="2.4.2 前向后向算法（没有I）"></a>2.4.2 前向后向算法（没有I）</h4>只有观测序列O，没有状态序列I，无监督过程。计算就是一个就EM的过程。<br><img src="https://img-blog.csdnimg.cn/1d4febff1b6043bda7cc7d466111afe4.png" alt=""><h4 id="2-4-3-序列标注（解码）过程"><a href="#2-4-3-序列标注（解码）过程" class="headerlink" title="2.4.3 序列标注（解码）过程"></a>2.4.3 序列标注（解码）过程</h4></li>
</ol>
<ul>
<li>学习完了HMM的分布参数，也就确定了一个HMM模型。序列标注问题也就是“预测过程”(解码过程)。对应了序列建模问题3。</li>
<li>学习后已知了 联合概率P(I,O),现在要求出条件概率P(I|O)：<script type="math/tex; mode=display">I_{max}=\underset{all I}{argmax}\frac{P(I,O)}{P(O)}</script></li>
<li>用Viterbi算法解码，在给定的观测序列下找出一条概率最大的隐状态序列。&lt;/font&gt;</li>
<li>Viterbi计算有向无环图的一条最大路径，用DP思想减少重复的计算。如图：<br><img src="https://img-blog.csdnimg.cn/8c629c14f21748149969da86a07b677d.png" alt="在这里插入图片描述"><h2 id="三、最大熵马尔科夫MEMM模型"><a href="#三、最大熵马尔科夫MEMM模型" class="headerlink" title="三、最大熵马尔科夫MEMM模型"></a>三、最大熵马尔科夫MEMM模型</h2><h3 id="3-1-MEMM原理和区别"><a href="#3-1-MEMM原理和区别" class="headerlink" title="3.1 MEMM原理和区别"></a>3.1 MEMM原理和区别</h3>MEMM是判别式模型，直接对条件概率建模：<br><img src="https://img-blog.csdnimg.cn/9f9ee1ef4db54e94826a3f87bf85770e.png" alt="在这里插入图片描述"><br>MEMM需要注意：</li>
</ul>
<ol>
<li><p>HMM是$o<em>t$只依赖当前时刻的隐藏状态$i_t$，HEMM是当前时刻隐状态$i_t$依赖观测节点$o_t$和上一时刻状态$i</em>{t-1}$。&lt;/font&gt;</p>
</li>
<li><p><code>判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定</code>。HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等</p>
</li>
<li>需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。</li>
<li>序列标注解码时，一样用维特比算法求概率最大的隐状态序列。</li>
</ol>
<ul>
<li>HMM中，观测节点$o_t$只依赖当前时刻的隐藏状态$i_t$。</li>
<li>更多的实际场景下，观测序列是需要很多的特征来刻画的。比如说，我在做NER时，我的标注$i<em>t$不仅跟当前状态 $o_t$相关，而且还跟前后标注 $i</em>{j}(j≠i)$相关，比如字母大小写、词性等等。&lt;/font&gt;</li>
<li>MEMM模型:允许“定义特征”，直接学习条件概率&lt;/font&gt;，即：<br><img src="https://img-blog.csdnimg.cn/87be161d470f4af4ba6e3a6a69fbce8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li>
<li>$Z(o,i{}’)$：归一化系数</li>
<li>$f(o,i)$：特征函数，需要自定义，其个数可任意制定</li>
<li>λ：特征函数系数，需要训练得到<br><img src="https://img-blog.csdnimg.cn/09d86caafb094b6f8a63771e7e77e4e5.png" alt="在这里插入图片描述"><h3 id="3-2-标注偏置"><a href="#3-2-标注偏置" class="headerlink" title="3.2 标注偏置"></a>3.2 标注偏置</h3><img src="https://img-blog.csdnimg.cn/a42fb328efa94ae4b8898d3169acd9db.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 过程细节：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">P(<span class="number">1</span>-&gt; <span class="number">1</span>-&gt; <span class="number">1</span>-&gt; <span class="number">1</span>)= <span class="number">0.4</span> x <span class="number">0.45</span> x <span class="number">0.5</span> = <span class="number">0.09</span> ，</span><br><span class="line">P(<span class="number">2</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>)= <span class="number">0.2</span> X <span class="number">0.3</span> X <span class="number">0.3</span> = <span class="number">0.018</span>，</span><br><span class="line">P(<span class="number">1</span>-&gt;<span class="number">2</span>-&gt;<span class="number">1</span>-&gt;<span class="number">2</span>)= <span class="number">0.6</span> X <span class="number">0.2</span> X <span class="number">0.5</span> = <span class="number">0.06</span>，</span><br><span class="line">P(<span class="number">1</span>-&gt;<span class="number">1</span>-&gt;<span class="number">2</span>-&gt;<span class="number">2</span>)= <span class="number">0.4</span> X <span class="number">0.55</span> X <span class="number">0.3</span> = <span class="number">0.066</span> </span><br></pre></td></tr></table></figure>
<p>但是得到的最优的状态转换路径是1-&gt;1-&gt;1-&gt;1，<br>为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态&lt;/font&gt;。原因如下：</p>
<p><img src="https://img-blog.csdnimg.cn/d8f64347c3df42a58170e7768592096d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="四、条件随机场CRF"><a href="#四、条件随机场CRF" class="headerlink" title="四、条件随机场CRF"></a>四、条件随机场CRF</h2><h3 id="4-1-CRF定义"><a href="#4-1-CRF定义" class="headerlink" title="4.1 CRF定义"></a>4.1 CRF定义</h3><ol>
<li>条件随机场：给定随机变量X条件下，输出随机变量Y的条件概率模型，其中Y构成无向图G=(V,E)表示的马尔科夫随机场。&lt;/font&gt;&lt;/font &gt;<br>对任意节点v，条件随机场满足：<script type="math/tex; mode=display">P(Y_{v}|X,Y_{w},w\neq v)=P(Y_{v}|X,Y_{w},w\sim v)</script>w≠ v表示v之外的所有结点，w~v表示与v有边相连的所有结点。即$P(Y_{v}$之和与v有边连接的结点有关。</li>
<li><code>线性链条件随机场，最大团是相邻两个结点的集合</code>。满足马尔科夫性(隐状态只和前后时刻状态有关）：<script type="math/tex; mode=display">P(Y_{i}|X,Y_{1},Y_{2}...Y_{n})=P(Y_{i}|X,Y_{i+1},Y_{i-1})</script></li>
<li><p>线性链CRF是判别模型，学习方法是利用训练数据的（正则化）极大似然估计得到条件概率模型P(Y|X)。可用于序列标注问题。此时条件概率P(Y|X)中：</p>
<ul>
<li>Y为输出变量，即标记序列（状态序列）</li>
<li>X为输入变量，即需要标注的状态序列。</li>
</ul>
</li>
<li><p>预测时，对于给定输入序列x，求出条件概率最大的输出序列y。<br><img src="https://img-blog.csdnimg.cn/8720f5b7ee7040339fd7e12f6cfee126.png" alt="在这里插入图片描述"></p>
<h3 id="4-2-线性链CRF的计算"><a href="#4-2-线性链CRF的计算" class="headerlink" title="4.2 线性链CRF的计算"></a>4.2 线性链CRF的计算</h3><p>概率无向图的联合概率分布可以在因子分解下表示为：<br><img src="https://img-blog.csdnimg.cn/984eb14e02934c4da21d22985b7d3255.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
</li>
<li>下标i表示我当前所在的节点（token）位置。&lt;/font&gt;</li>
<li>下标k表示我这是第几个特征函数&lt;/font&gt;，并且每个特征函数都附属一个权重$\lambda_{k}$ 。&lt;/font&gt;即每个团里面，我将为$token_i$构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。 </li>
<li>Z(O)是用来归一化的，形成概率值。</li>
<li>$P(I|O)$表示了在给定的一条观测序列 O的条件下，我用CRF所求出来的隐状态序列$I=(i<em>{1},i</em>{2},…i_{T})$的概率。而至于观测序列 O，它可以是一整个训练语料的所有的观测序列；也可以是在推断阶段的一句sample。比如序列标注进行预测，最终选的是最大概率的那条（by viterbi）。</li>
<li>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 我们将建模总公式展开：</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/bc28595886384e76ab860cf1eb9175d9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 转移特征针对的是前后token之间的限定。&lt;/font&gt;</p>
<p>为了简单起见，将转移特征和状态特征及其权值用统一符号表示。条件随机场简化公式如下：<br><img src="https://img-blog.csdnimg.cn/a407494d8818498dbde165b83a141b1a.png" alt="在这里插入图片描述"><br>再进一步理解的话，我们需要把特征函数部分抠出来：<br><img src="https://img-blog.csdnimg.cn/69ed4fa41ce64a27b87f8ecd717cf56d.png" alt="在这里插入图片描述"><br>我们为$token_i$打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值。<br>具体应用求解参考<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/35866596/answer/236886066?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_content=group3_Answer&amp;utm_campaign=shareopn">《如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？》</a>。</p>
<h3 id="4-3-从公式到代码的理解"><a href="#4-3-从公式到代码的理解" class="headerlink" title="4.3 从公式到代码的理解"></a>4.3 从公式到代码的理解</h3><p>实际计算时，采用概率的对数形式，即logP(Y)。使用最大似然估计来计算分布的参数，即我们的目标就是最大化ogP(Y)。<br><img src="https://img-blog.csdnimg.cn/12330b71b141418f9fb7cc3ec9039943.png" alt="在这里插入图片描述"><br>即$-logP(Y)=logZ(x)-score$。<br>对应到代码中，forward_score 就是$logZ(x)$，gold_score就是特征函数部分的score。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span>(<span class="params">self, sentence, tags</span>):</span></span><br><span class="line">    feats = self._get_lstm_features(sentence)</span><br><span class="line">    forward_score = self._forward_alg(feats)</span><br><span class="line">    gold_score = self._score_sentence(feats, tags)</span><br><span class="line">    <span class="keyword">return</span> forward_score - gold_score</span><br></pre></td></tr></table></figure></p>
<ul>
<li>因为模型建立的初衷就是要考虑到$i<em>{k-1}$对$i</em>{k}$的影响和X对观测序列的影响.所以我们将图分解成若干个$(i<em>{k-1},i</em>{k},X)$。</li>
<li>其中$i<em>{k}$表示观测变量的状态值，比如在BIO标注中状态取值范围是{B,I,O,START,STOP}，则k最大取5，$i</em>{k}$有5个状态值可取。<br><img src="https://img-blog.csdnimg.cn/6081457d466243bbbbf3e97d99cd0e9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>只关注其中的某一个团$C<em>i$,特征函数部分的gold_score表示给定序列X下，表现出的$(i</em>{k-1}，i_{k})$的费归一化概率，与两个东西有关：</li>
</ul>
<ol>
<li>给定序列X下出现$i<em>{k}$的概率，以$h(i</em>{k},X)$表示。这个概率使用lstm、cnn建模X对$i_{k})$映射就可以得到，对应结点上的状态特征。</li>
<li>给定序列X下由$i<em>{k-1}$转移到$i</em>{k}$的概率，由$g(i<em>{k-1},i</em>{k};X)$表示，对应边上的转移特征。在CRF中，观测变量只受临近节点的影响。</li>
<li>考虑到深度学习模型已经能比较充分捕捉各个$i<em>{k}$与X 的联系，所以假设 $i</em>{k-1}$ 转移到$i<em>{k}$的概率与X无关，所以有：$g(i</em>{k-1},i<em>{k};X)=g(i</em>{k-1},i_{k})$</li>
</ol>
<p>考虑以上几点，可以得到：</p>
<script type="math/tex; mode=display">gold-score=\sum_{c}\sum_{k}\lambda _{k}f_{k}(c,y,x)=\sum_{c}\sum_{k}(g(i_{k-1},i_{k})+h(i_{k},X))</script><p>剩下计算过程参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178731739?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《条件随机场CRF之从公式到代码》</a><br><img src="https://img-blog.csdnimg.cn/b2612debc2c94e7fbdcaf918a69bc0a7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="五、-HMM-vs-MEMM-vs-CRF"><a href="#五、-HMM-vs-MEMM-vs-CRF" class="headerlink" title="五、 HMM vs. MEMM vs. CRF"></a>五、 HMM vs. MEMM vs. CRF</h2><h3 id="5-1-HMM-vs-MEMM"><a href="#5-1-HMM-vs-MEMM" class="headerlink" title="5.1 HMM vs MEMM"></a>5.1 HMM vs MEMM</h3><p>HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中<code>当前状态只与前一状态有关</code>。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。<code>MEMM解决了HMM输出独立性假设的问题</code>。因为HMM只限定在了观测与状态之间的依赖，而<code>MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖</code>。</p>
<h3 id="5-2-MEMM-vs-CRF"><a href="#5-2-MEMM-vs-CRF" class="headerlink" title="5.2 MEMM vs CRF"></a>5.2 MEMM vs CRF</h3><p>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，<code>MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布</code>，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。HMM、MEMM属于有向图，所以考虑了x与y的影响，但没将x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/12/25/读书笔记/学习笔记10：统计学习方法_——HMM和CRF/">https://zhxnlp.github.io/2021/12/25/读书笔记/学习笔记10：统计学习方法_——HMM和CRF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CRF/">CRF</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/12/31/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/2021%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8hexo+github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"><i class="fa fa-chevron-left">  </i><span>2021使用hexo+github搭建个人博客</span></a></div><div class="next-post pull-right"><a href="/2021/12/17/12%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/task3%EF%BC%9A%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/"><span>task3：新闻推荐系统项目调试</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>