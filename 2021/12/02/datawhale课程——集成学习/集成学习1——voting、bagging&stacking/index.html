<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="集成学习1——voting、bagging&amp;stacking"><meta name="keywords" content="集成学习,bagging,stacking,随机森林"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>集成学习1——voting、bagging&amp;stacking | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%8A%95%E7%A5%A8%E6%B3%95%E4%B8%8Ebagging"><span class="toc-number">1.</span> <span class="toc-text">一、投票法与bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%8A%95%E7%A5%A8%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 投票法的原理分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Voting%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">1.2.</span> <span class="toc-text">1.2  Voting案例分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-bagging%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 bagging的原理分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 决策树和随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-bagging%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 bagging案例分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81stacking"><span class="toc-number">2.</span> <span class="toc-text">二、stacking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Blending%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Blending算法原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Blending%E6%A1%88%E4%BE%8B"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Blending案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Stacking%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Stacking算法原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Stacking%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 Stacking算法案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-%E5%9F%BA%E5%88%86%E7%B1%BB%E5%99%A8%E9%A2%84%E6%B5%8B%E7%B1%BB%E5%88%AB%E4%B8%BA%E7%89%B9%E5%BE%81"><span class="toc-number">2.4.1.</span> <span class="toc-text">2.4.1 基分类器预测类别为特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-%E5%9F%BA%E5%88%86%E7%B1%BB%E5%99%A8%E7%B1%BB%E5%88%AB%E6%A6%82%E7%8E%87%E5%80%BC%E4%B8%BA%E7%89%B9%E5%BE%81"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.4.2  基分类器类别概率值为特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-%E5%9F%BA%E5%88%86%E7%B1%BB%E5%99%A8%E4%BD%BF%E7%94%A8%E9%83%A8%E5%88%86%E7%89%B9%E5%BE%81"><span class="toc-number">2.4.3.</span> <span class="toc-text">2.4.3 基分类器使用部分特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-4-%E7%BB%93%E5%90%88%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E4%BC%98%E5%8C%96"><span class="toc-number">2.4.4.</span> <span class="toc-text">2.4.4 结合网格搜索优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-5-%E7%BB%98%E5%88%B6ROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">2.4.5.</span> <span class="toc-text">2.4.5 绘制ROC曲线</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-6-Blending%E4%B8%8EStacking%E5%AF%B9%E6%AF%94"><span class="toc-number">2.5.</span> <span class="toc-text">2.4.6 Blending与Stacking对比</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">集成学习1——voting、bagging&amp;stacking</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">5.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 21 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><blockquote>
<p>参考datawhale课程<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/ensemble-learning">《集成学习》</a><br>参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Christina-Notebook/p/10063146.html">《Stacking方法详解》</a></p>
</blockquote>
<p>Bagging思想的实质是：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差</p>
<h2 id="一、投票法与bagging"><a href="#一、投票法与bagging" class="headerlink" title="一、投票法与bagging"></a>一、投票法与bagging</h2><h3 id="1-1-投票法的原理分析"><a href="#1-1-投票法的原理分析" class="headerlink" title="1.1 投票法的原理分析"></a>1.1 投票法的原理分析</h3><p>投票法是一种遵循少数服从多数原则的集成学习模型，通过多个模型的集成降低方差，从而提高模型的鲁棒性。在理想情况下，投票法的预测效果应当优于任何一个基模型的预测效果。</p>
<ul>
<li>回归投票法：预测结果是所有模型预测结果的平均值。</li>
<li>分类投票法：预测结果是所有模型种出现最多的预测结果。</li>
</ul>
<p>分类投票法又可以被划分为硬投票与软投票：</p>
<ul>
<li>硬投票：预测结果是所有投票结果最多出现的类。（基模型能预测出清晰的类别标签时）</li>
<li>软投票：预测结果是所有投票结果中概率加和最大的类。（基模型能预测类别的概率，或可以输出类似于概率的预测分数值，例如支持向量机、k-最近邻和决策树等）<span id="more"></span>
相对于硬投票，软投票法考虑到了预测概率这一额外的信息，因此可以得出比硬投票法更加准确的预测结果。</li>
</ul>
<p>想要投票法产生较好的结果，基模型需要满足两个条件：</p>
<ul>
<li>基模型之间的效果不能差别过大。&lt;/font &gt;当某个基模型相对于其他基模型效果过差时，该模型很可能成为噪声。</li>
<li>基模型之间同质性较小。&lt;/font &gt;例如在基模型预测效果近似的情况下，基于树模型与线性模型的投票，往往优于两个树模型或两个线性模型。</li>
</ul>
<p>  投票法的局限性：所有模型对预测的贡献是一样的。</p>
<h3 id="1-2-Voting案例分析"><a href="#1-2-Voting案例分析" class="headerlink" title="1.2  Voting案例分析"></a>1.2  Voting案例分析</h3><ul>
<li>Sklearn中提供了 VotingRegressor 与 VotingClassifier 两个投票方法。这两种模型的操作方式相同，并采用相同的参数。</li>
<li>使用模型需要提供一个模型列表，列表中每个模型采用Tuple的结构表示，第一个元素代表名称，第二个元素代表模型，需要保证每个模型必须拥有唯一的名称。</li>
</ul>
<p>  例如这里，我们定义两个模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">models = [(<span class="string">&#x27;lr&#x27;</span>,LogisticRegression()),(<span class="string">&#x27;svm&#x27;</span>,make_pipeline(StandardScaler(),SVC()))]<span class="comment">#定义Pipeline完成模型预处理工作</span></span><br><span class="line">ensemble = VotingClassifier(estimators=models,voting=<span class="string">&#x27;soft&#x27;</span>)<span class="comment">#voting参数让我们选择软投票或者硬投票</span></span><br></pre></td></tr></table></figure>
<p>创建一个1000个样本，20个特征的随机数据集：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test classification dataset</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span>():</span></span><br><span class="line">    X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, n_redundant=<span class="number">5</span>, random_state=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># summarize the dataset</span></span><br><span class="line">    <span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure>
<p>我们使用多个KNN模型作为基模型演示投票法，其中每个模型采用不同的邻居值K参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get a voting ensemble of models</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_voting</span>():</span></span><br><span class="line">    <span class="comment"># define the base models</span></span><br><span class="line">    models = <span class="built_in">list</span>()</span><br><span class="line">    models.append((<span class="string">&#x27;knn1&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">1</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn3&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">3</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn5&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">5</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn7&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">7</span>)))</span><br><span class="line">    models.append((<span class="string">&#x27;knn9&#x27;</span>, KNeighborsClassifier(n_neighbors=<span class="number">9</span>)))</span><br><span class="line">    <span class="comment"># define the voting ensemble</span></span><br><span class="line">    ensemble = VotingClassifier(estimators=models, voting=<span class="string">&#x27;hard&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> ensemble</span><br></pre></td></tr></table></figure>
<p>创建一个模型列表来评估投票带来的提升，包括KNN模型配置的每个独立版本和硬投票模型。下面的get_models()函数可以为我们创建模型列表进行评估。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get a list of models to evaluate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_models</span>():</span></span><br><span class="line">    models = <span class="built_in">dict</span>()</span><br><span class="line">    models[<span class="string">&#x27;knn1&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn3&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn5&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn7&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">7</span>)</span><br><span class="line">    models[<span class="string">&#x27;knn9&#x27;</span>] = KNeighborsClassifier(n_neighbors=<span class="number">9</span>)</span><br><span class="line">    models[<span class="string">&#x27;hard_voting&#x27;</span>] = get_voting()</span><br><span class="line">    <span class="keyword">return</span> models</span><br></pre></td></tr></table></figure>
<p>evaluate_model()函数接收一个模型实例，并以分层10倍交叉验证三次重复的分数列表的形式返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate a give model using cross-validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score   <span class="comment">#Added by ljq</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_model</span>(<span class="params">model, X, y</span>):</span></span><br><span class="line">    cv = RepeatedStratifiedKFold(n_splits=<span class="number">10</span>, n_repeats=<span class="number">3</span>, random_state=<span class="number">1</span>)</span><br><span class="line">    scores = cross_val_score(model, X, y, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=cv, n_jobs=-<span class="number">1</span>, error_score=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<p>报告每个算法的平均性能，还可以创建一个箱形图和须状图来比较每个算法的精度分数分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = get_dataset()</span><br><span class="line"><span class="comment"># get the models to evaluate</span></span><br><span class="line">models = get_models()</span><br><span class="line"><span class="comment"># evaluate the models and store results</span></span><br><span class="line">results, names = <span class="built_in">list</span>(), <span class="built_in">list</span>()</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models.items():</span><br><span class="line">    scores = evaluate_model(model, X, y)</span><br><span class="line">    results.append(scores)</span><br><span class="line">    names.append(name)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&gt;%s %.3f (%.3f)&#x27;</span> % (name, mean(scores), std(scores)))</span><br><span class="line"><span class="comment"># plot model performance for comparison</span></span><br><span class="line">pyplot.boxplot(results, labels=names, showmeans=<span class="literal">True</span>)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote>
<p>knn1 0.873 (0.030)<br>knn3 0.889 (0.038)<br>knn5 0.895 (0.031)<br>knn7 0.899 (0.035)<br>knn9 0.900 (0.033)<br>hard_voting 0.902 (0.034）</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/546a2624da1c48eba1431876e835f8b7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_8,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>显然投票的效果略大于任何一个基模型。</p>
<p>通过箱形图我们可以看到硬投票方法对交叉验证整体预测结果分布带来的提升。</p>
<h3 id="1-3-bagging的原理分析"><a href="#1-3-bagging的原理分析" class="headerlink" title="1.3 bagging的原理分析"></a>1.3 bagging的原理分析</h3><p>Voting法希望各个模型之间具有较大的差异性，而在实际操作中的模型却往往是同质的，故考虑是通过不同的采样增加模型的差异性。&lt;/font &gt;</p>
<p>自助采样(bootstrap)：有放回的从数据集中进行采样。<br>Bagging思想的实质是：通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。Bagging主要通过降低方差的方式减少预测误差</p>
<p>bagging流程：</p>
<ul>
<li>随机取出一个训练样本放入采样集合中，再把这个样本放回初始数据集，重复K次采样，最终得到大小为K的样本集合。</li>
<li>重复以上步骤采样出T个含K个样本的采样集合，然后基于每个采样集合训练出一个基学习器</li>
<li>将这些基学习器进行结合。<ul>
<li>预测回归问题：预测值取平均来进行的。</li>
<li>预测分类问题：预测值投票取多数票</li>
</ul>
</li>
</ul>
<p>Bagging同样是一种降低方差的技术，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更加明显。&lt;/font &gt;在实际的使用中，加入列采样的Bagging技术对高维小样本往往有神奇的效果。</p>
<h3 id="1-4-决策树和随机森林"><a href="#1-4-决策树和随机森林" class="headerlink" title="1.4 决策树和随机森林"></a>1.4 决策树和随机森林</h3><p>Sklearn为我们提供了 BaggingRegressor 与 BaggingClassifier 两种Bagging方法的API，默认基模型是决策树模型。</p>
<ul>
<li>决策树，它是一种树形结构，树的每个非叶子节点表示对样本在一个特征上的判断，节点下方的分支代表对样本的划分。</li>
<li>决策树的建立过程是一个对数据不断划分的过程。</li>
<li>每次划分中，首先要选择用于划分的特征，之后要确定划分的方案（类别/阈值）。我们希望通过划分，决策树的分支节点所包含的样本“纯度”尽可能地高。节点划分过程中所用的指标主要是信息增益和GINI系数。</li>
<li>选择信息增益最大或者gini指数最小的划分方式</li>
<li>划分过程直到样本的类别被完全分开，所有特征都已使用，或达到树的最大深度为止。<br><img src="https://img-blog.csdnimg.cn/c74115db96474747b47fd3c5f0f907af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>Bagging的一个典型应用是随机森林。由许多“树”bagging组成的。每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）&lt;/font &gt;。随机森林的示意图如下：<br><img src="https://img-blog.csdnimg.cn/879df8be2f9f4183b72c64ee427b1564.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="1-5-bagging案例分析"><a href="#1-5-bagging案例分析" class="headerlink" title="1.5 bagging案例分析"></a>1.5 bagging案例分析</h3></li>
</ul>
<ol>
<li>创建一个含有1000个样本20维特征的随机分类数据集：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test classification dataset</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, </span><br><span class="line">                           n_redundant=<span class="number">5</span>, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># summarize the dataset</span></span><br><span class="line"><span class="built_in">print</span>(X.shape, y.shape)</span><br><span class="line"></span><br><span class="line">(<span class="number">1000</span>, <span class="number">20</span>) (<span class="number">1000</span>,)</span><br></pre></td></tr></table></figure>
<ol>
<li>使用重复的分层k-fold交叉验证来评估该模型，一共重复3次，每次有10个fold。我们将评估该模型在所有重复交叉验证中性能的平均值和标准差。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate bagging algorithm for classification</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mean</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> std</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RepeatedStratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="comment"># define dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">15</span>, n_redundant=<span class="number">5</span>, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># define the model</span></span><br><span class="line">model = BaggingClassifier()</span><br><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line">cv = RepeatedStratifiedKFold(n_splits=<span class="number">10</span>, n_repeats=<span class="number">3</span>, random_state=<span class="number">1</span>)</span><br><span class="line">n_scores = cross_val_score(model, X, y, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=cv, n_jobs=-<span class="number">1</span>, error_score=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line"><span class="comment"># report performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.3f (%.3f)&#x27;</span> % (mean(n_scores), std(n_scores)))</span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.851</span> (<span class="number">0.041</span>)</span><br></pre></td></tr></table></figure>
<p>最终模型的效果是Accuracy: 0.856 标准差0.037</p>
<h2 id="二、stacking"><a href="#二、stacking" class="headerlink" title="二、stacking"></a>二、stacking</h2><p>stacking被称为“懒人”算法，因为它不需要花费过多时间的调参就可以得到一个效果不错的算法。<br>Stacking集成算法可以理解为一个两层的集成，第一层含有多个基础分类器，把预测的结果(元特征)提供给第二层， 而第二层的分类器通常是逻辑回归，他把一层分类器的结果当做特征做拟合输出预测结果。</p>
<h3 id="2-1-Blending算法原理"><a href="#2-1-Blending算法原理" class="headerlink" title="2.1 Blending算法原理"></a>2.1 Blending算法原理</h3><p>Blending集成学习方式：</p>
<ul>
<li>(1) 将数据划分为训练集和测试集(test_set)，其中训练集需要再次划分为训练集(train_set)和验证集(val_set)；</li>
<li>(2) 创建第一层的多个模型（同质异质都可），使用train_set进行训练。然后用训练好的模型预测val_set和test_set得到val_predict, test_predict1</li>
<li>(3) 创建第二层的模型（一般是LR），使用val_predict作为训练集，验证集val_set的标签作为标签训练第二层的模型；</li>
<li>(4) 使用第二层训练好的模型对第二层测试集test_predict1进行预测&lt;/font &gt;，该结果为整个测试集的结果。<br><img src="https://img-blog.csdnimg.cn/3f2d967828844579b3fae50a043cf085.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="stacking"><br>Blending集成方式的优劣：</li>
<li>最重要的优点就是实现简单粗暴，没有太多的理论的分析。</li>
<li>缺点：只使用了一部分数据集作为留出集进行验证，也就是只能用上数据中的一部分，进而容易产生过拟合。<h3 id="2-2-Blending案例"><a href="#2-2-Blending案例" class="headerlink" title="2.2 Blending案例"></a>2.2 Blending案例</h3></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载相关工具包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&quot;ggplot&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">data, target = make_blobs(n_samples=<span class="number">10000</span>, centers=<span class="number">2</span>, random_state=<span class="number">1</span>, cluster_std=<span class="number">1.0</span> )</span><br><span class="line"><span class="comment">## 创建训练集和测试集</span></span><br><span class="line">X_train1,X_test,y_train1,y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">## 创建训练集和验证集</span></span><br><span class="line">X_train,X_val,y_train,y_val = train_test_split(X_train1, y_train1, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  设置第一层分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">clfs = [SVC(probability = <span class="literal">True</span>),RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),KNeighborsClassifier()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置第二层分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出第一层的验证集结果与测试集结果</span></span><br><span class="line">val_features = np.zeros((X_val.shape[<span class="number">0</span>],<span class="built_in">len</span>(clfs)))  <span class="comment"># 初始化验证集结果</span></span><br><span class="line">test_features = np.zeros((X_test.shape[<span class="number">0</span>],<span class="built_in">len</span>(clfs)))  <span class="comment"># 初始化测试集结果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    clf.fit(X_train,y_train)</span><br><span class="line">    val_feature = clf.predict_proba(X_val)[:, <span class="number">1</span>]</span><br><span class="line">    test_feature = clf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">    val_features[:,i] = val_feature</span><br><span class="line">    test_features[:,i] = test_feature</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第一层的验证集的结果输入第二层训练第二层分类器</span></span><br><span class="line">lr.fit(val_features,y_val)</span><br><span class="line"><span class="comment"># 输出预测的结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(lr,test_features,y_test,cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">array([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<p>每一折的交叉验证的效果都是非常好的，这个集成学习方法在这个数据集上是十分有效的，不过这个数据集是我们虚拟的，因此大家可以把他用在实际数据上看看效果。</p>
<h3 id="2-3-Stacking算法原理"><a href="#2-3-Stacking算法原理" class="headerlink" title="2.3 Stacking算法原理"></a>2.3 Stacking算法原理</h3><p>lending在集成的过程中只会用到验证集的数据，对数据实际上是一个很大的浪费，所以考虑交叉验证进行优化。<br><img src="https://img-blog.csdnimg.cn/b6a1d8f36b914374af38219c0d3290b9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<ol>
<li>将所有数据集生成测试集和训练集（假如训练集为10000,测试集为2500行），那么上层会进行5折交叉检验（训练集8000条、验证集2000条）。<br>最后得到5×2000条验证集预测结果(橙色），5×2500条测试集的预测结果。    </li>
<li>将验证集的预测结果拼接成10000×1的矩阵，标记为$A_1$，测试集预测结果进行加权平均，得到2500×1的矩阵，标记为$B_1$。</li>
<li><p>对3个基模型进行如上训练，得到了$A_1$、$A_2$、$A_3$、$B_1$、$B_2$、$B_3$六个矩阵。</p>
</li>
<li><p>将$A_1$、$A_2$、$A_3$并列为10000×3的矩阵作为training data,$B_1$、$B_2$、$B_3$合并在一起成2500×3的矩阵作为testing  data，5次验证集标签作为标签进行训练。</p>
</li>
<li>预测testing  data的结果为最终结果<br><img src="https://img-blog.csdnimg.cn/0bb6ae78d63c4e299f6e33d2138fd64e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-Stacking算法案例"><a href="#2-4-Stacking算法案例" class="headerlink" title="2.4 Stacking算法案例"></a>2.4 Stacking算法案例</h3>sklearn并没有直接对Stacking的方法，因此我们需要下载mlxtend工具包(pip install mlxtend)</li>
</ol>
<p>StackingClassifier使用API和参数说明：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">StackingClassifier(classifiers, meta_classifier, use_probas=<span class="literal">False</span>, average_probas=<span class="literal">False</span>, </span><br><span class="line">verbose=<span class="number">0</span>, use_features_in_secondary=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>参数：</p>
<ul>
<li>classifiers : 基分类器，数组形式，[cl1, cl2, cl3]. 每个基分类器的属性被存储在类属性 self.clfs_.</li>
<li>meta_classifier : 目标分类器，即将前面分类器合起来的分类器</li>
<li>use_probas : bool (default: False) ，如果设置为True， 那么目标分类器的输入就是前面分类输出的类别概率值而不是类别标签</li>
<li>average_probas : bool (default: False)，当上一个参数use_probas = True时需设置</li>
<li>average_probas=True表示所有基分类器输出的概率值需被平均，否则拼接。</li>
<li>verbose : int, optional (default=0)。用来控制使用过程中的日志输出，当 verbose = 0时，什么也不输出， verbose = 1，输出回归器的序号和名字。verbose = 2，输出详细的参数信息。verbose &gt; 2, 自动将verbose设置为小于2的，verbose -2.</li>
<li>use_features_in_secondary : bool (default: False). 如果设置为True，那么最终的目标分类器就被基分类器产生的数据和最初的数据集同时训练。如果设置为False，最终的分类器只会使用基分类器产生的数据训练。<h4 id="2-4-1-基分类器预测类别为特征"><a href="#2-4-1-基分类器预测类别为特征" class="headerlink" title="2.4.1 基分类器预测类别为特征"></a>2.4.1 基分类器预测类别为特征</h4>使用基分类器所产生的预测类别作为meta-classifier“特征”的输入数据<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 简单堆叠3折CV分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, <span class="number">1</span>:<span class="number">3</span>], iris.target</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting from v0.16.0, StackingCVRegressor supports</span></span><br><span class="line"><span class="comment"># `random_state` to get deterministic result.</span></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],  <span class="comment"># 第一层分类器</span></span><br><span class="line">                            meta_classifier=lr,   <span class="comment"># 第二层分类器</span></span><br><span class="line">                            random_state=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3-fold cross validation:\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>,<span class="string">&#x27;StackingClassifier&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>-fold cross validation:</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.93</span> (+/- <span class="number">0.02</span>) [StackingClassifier]</span><br></pre></td></tr></table></figure>
<p>画出决策边界：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_decision_regions</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> clf, lab, grd <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], </span><br><span class="line">                         [<span class="string">&#x27;KNN&#x27;</span>, </span><br><span class="line">                          <span class="string">&#x27;Random Forest&#x27;</span>, </span><br><span class="line">                          <span class="string">&#x27;Naive Bayes&#x27;</span>,</span><br><span class="line">                          <span class="string">&#x27;StackingCVClassifier&#x27;</span>],</span><br><span class="line">                          itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)):</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(lab)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/5241dd417cbc48b894676aa3e2428e43.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h4 id="2-4-2-基分类器类别概率值为特征"><a href="#2-4-2-基分类器类别概率值为特征" class="headerlink" title="2.4.2  基分类器类别概率值为特征"></a>2.4.2  基分类器类别概率值为特征</h4><p>使用第一层所有基分类器所产生的类别概率值作为meta-classfier的输入。需要在StackingClassifier 中增加一个参数设置：use_probas = True。</p>
<p>另外，还有一个参数设置average_probas = True,那么这些基分类器所产出的概率值将按照列被平均，否则会拼接。</p>
<ul>
<li>基分类器1：predictions=[0.2,0.2,0.7]</li>
<li>基分类器2：predictions=[0.4,0.3,0.8]</li>
<li>基分类器3：predictions=[0.1,0.4,0.6]</li>
</ul>
<ol>
<li><p>若use_probas = True，average_probas = True，</p>
<p> 产生的meta-feature 为：[0.233, 0.3, 0.7]</p>
</li>
<li><p>若use_probas = True，average_probas = False，</p>
<p> 产生的meta-feature 为：[0.2,0.2,0.7,0.4,0.3,0.8,0.1,0.4,0.6]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.使用概率作为元特征</span></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=<span class="number">1</span>)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                            use_probas=<span class="literal">True</span>,  <span class="comment"># </span></span><br><span class="line">                            meta_classifier=lr,</span><br><span class="line">                            random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3-fold cross validation:\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, sclf], </span><br><span class="line">                      [<span class="string">&#x27;KNN&#x27;</span>, </span><br><span class="line">                       <span class="string">&#x27;Random Forest&#x27;</span>, </span><br><span class="line">                       <span class="string">&#x27;Naive Bayes&#x27;</span>,</span><br><span class="line">                       <span class="string">&#x27;StackingClassifier&#x27;</span>]):</span><br><span class="line"></span><br><span class="line">    scores = cross_val_score(clf, X, y, </span><br><span class="line">                                              cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> </span><br><span class="line">          % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>-fold cross validation:</span><br><span class="line"></span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.01</span>) [KNN]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.01</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.91</span> (+/- <span class="number">0.02</span>) [Naive Bayes]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.02</span>) [StackingClassifier]</span><br></pre></td></tr></table></figure>
<h4 id="2-4-3-基分类器使用部分特征"><a href="#2-4-3-基分类器使用部分特征" class="headerlink" title="2.4.3 基分类器使用部分特征"></a>2.4.3 基分类器使用部分特征</h4><p>赋予不同的基分类器不同的特征。<br>比如：基分类器1训练前半部分的特征，基分类器2训练后半部分的特征。这部分的操作是通过sklearn中的pipelines实现。最终通过StackingClassifier组合起来。而不是给每一个基分类器全部的特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.在不同特征子集上运行的分类器的堆叠</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> ColumnSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"><span class="comment">#基分类器1：xgboost</span></span><br><span class="line">pipe1 = make_pipeline(ColumnSelector(cols=(<span class="number">0</span>, <span class="number">2</span>)),</span><br><span class="line">                      XGBClassifier())</span><br><span class="line"><span class="comment">#基分类器2：RandomForest</span></span><br><span class="line">pipe2 = make_pipeline(ColumnSelector(cols=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)),</span><br><span class="line">                      RandomForestClassifier())</span><br><span class="line"></span><br><span class="line">sclf = StackingClassifier(classifiers=[pipe1, pipe2],</span><br><span class="line">                          meta_classifier=LogisticRegression())</span><br><span class="line"></span><br><span class="line">sclf.fit(X, y)</span><br></pre></td></tr></table></figure>
<h4 id="2-4-4-结合网格搜索优化"><a href="#2-4-4-结合网格搜索优化" class="headerlink" title="2.4.4 结合网格搜索优化"></a>2.4.4 结合网格搜索优化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4. 堆叠5折CV分类与网格搜索(结合网格搜索调参优化)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing models</span></span><br><span class="line"></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], </span><br><span class="line">                            meta_classifier=lr,</span><br><span class="line">                            random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">params = &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: [<span class="number">1</span>, <span class="number">5</span>],</span><br><span class="line">          <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: [<span class="number">10</span>, <span class="number">50</span>],</span><br><span class="line">          <span class="string">&#x27;meta_classifier__C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">10.0</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># 如果我们打算多次使用回归算法，我们要做的就是在参数网格中添加一个附加的数字后缀，如下所示：</span></span><br><span class="line"><span class="string">sclf = StackingCVClassifier(classifiers=[clf1, clf1, clf2, clf3], </span></span><br><span class="line"><span class="string">                            meta_classifier=lr,</span></span><br><span class="line"><span class="string">                            random_state=RANDOM_SEED)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">params = &#123;&#x27;kneighborsclassifier-1__n_neighbors&#x27;: [1, 5],</span></span><br><span class="line"><span class="string">          &#x27;kneighborsclassifier-2__n_neighbors&#x27;: [1, 5],</span></span><br><span class="line"><span class="string">          &#x27;randomforestclassifier__n_estimators&#x27;: [10, 50],</span></span><br><span class="line"><span class="string">          &#x27;meta_classifier__C&#x27;: [0.1, 10.0]&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">grid = GridSearchCV(estimator=sclf, </span><br><span class="line">                    param_grid=params, </span><br><span class="line">                    cv=<span class="number">5</span>,</span><br><span class="line">                    refit=<span class="literal">True</span>)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line"></span><br><span class="line">cv_keys = (<span class="string">&#x27;mean_test_score&#x27;</span>, <span class="string">&#x27;std_test_score&#x27;</span>, <span class="string">&#x27;params&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> r, _ <span class="keyword">in</span> <span class="built_in">enumerate</span>(grid.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%0.3f +/- %0.2f %r&quot;</span></span><br><span class="line">          % (grid.cv_results_[cv_keys[<span class="number">0</span>]][r],</span><br><span class="line">             grid.cv_results_[cv_keys[<span class="number">1</span>]][r] / <span class="number">2.0</span>,</span><br><span class="line">             grid.cv_results_[cv_keys[<span class="number">2</span>]][r]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Best parameters: %s&#x27;</span> % grid.best_params_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy: %.2f&#x27;</span> % grid.best_score_)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0.947</span> +/- <span class="number">0.03</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.933</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.940</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.940</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">0.953</span> +/- <span class="number">0.02</span> &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">10.0</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">50</span>&#125;</span><br><span class="line">Best parameters: &#123;<span class="string">&#x27;kneighborsclassifier__n_neighbors&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;meta_classifier__C&#x27;</span>: <span class="number">0.1</span>, <span class="string">&#x27;randomforestclassifier__n_estimators&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">Accuracy: <span class="number">0.95</span></span><br></pre></td></tr></table></figure>
<h4 id="2-4-5-绘制ROC曲线"><a href="#2-4-5-绘制ROC曲线" class="headerlink" title="2.4.5 绘制ROC曲线"></a>2.4.5 绘制ROC曲线</h4><ul>
<li>像其他scikit-learn分类器一样，它StackingCVClassifier具有decision_function可用于绘制ROC曲线的方法。</li>
<li>请注意，decision_function期望并要求元分类器实现decision_function。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, [<span class="number">0</span>, <span class="number">1</span>]], iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Binarize the output</span></span><br><span class="line">y = label_binarize(y, classes=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">n_classes = y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.33</span>, random_state=RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">clf1 =  LogisticRegression()</span><br><span class="line">clf2 = RandomForestClassifier(random_state=RANDOM_SEED)</span><br><span class="line">clf3 = SVC(random_state=RANDOM_SEED)</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],</span><br><span class="line">                            meta_classifier=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learn to predict each class against the other</span></span><br><span class="line">classifier = OneVsRestClassifier(sclf)</span><br><span class="line">y_score = classifier.fit(X_train, y_train).decision_function(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute micro-average ROC curve and ROC area</span></span><br><span class="line">fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>], _ = roc_curve(y_test.ravel(), y_score.ravel())</span><br><span class="line">roc_auc[<span class="string">&quot;micro&quot;</span>] = auc(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>])</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.plot(fpr[<span class="number">2</span>], tpr[<span class="number">2</span>], color=<span class="string">&#x27;darkorange&#x27;</span>,</span><br><span class="line">         lw=lw, label=<span class="string">&#x27;ROC curve (area = %0.2f)&#x27;</span> % roc_auc[<span class="number">2</span>])</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver operating characteristic example&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/ad0f2d5e0ffd4e52905b69e5dd108fca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_9,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-6-Blending与Stacking对比"><a href="#2-4-6-Blending与Stacking对比" class="headerlink" title="2.4.6 Blending与Stacking对比"></a>2.4.6 Blending与Stacking对比</h3>Blending的优点在于：</li>
<li>比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）</li>
</ul>
<p>而缺点在于：</p>
<ul>
<li>使用了很少的数据（是划分hold-out作为测试集，并非cv）</li>
<li>blender可能会过拟合（其实大概率是第一点导致的）</li>
<li>stacking使用多次的CV会比较稳健</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/12/02/datawhale课程——集成学习/集成学习1——voting、bagging&amp;stacking/">https://zhxnlp.github.io/2021/12/02/datawhale课程——集成学习/集成学习1——voting、bagging&amp;stacking/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><a class="post-meta__tags" href="/tags/bagging/">bagging</a><a class="post-meta__tags" href="/tags/stacking/">stacking</a><a class="post-meta__tags" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">随机森林</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/12/03/datawhale%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A02%EF%BC%9ABoosting%E7%AE%97%E6%B3%95%EF%BC%9AAdaboost&amp;GBDT/"><i class="fa fa-chevron-left">  </i><span>集成学习2：Boosting算法：Adaboost&amp;GBDT</span></a></div><div class="next-post pull-right"><a href="/2021/11/30/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span>学习笔记7：循环神经网络</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>