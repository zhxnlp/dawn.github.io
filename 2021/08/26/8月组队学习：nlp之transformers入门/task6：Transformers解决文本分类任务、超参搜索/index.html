<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="task6：BERT文本分类"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>task6：BERT文本分类 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2"><span class="toc-number">1.</span> <span class="toc-text">Transformers解决文本分类任务、超参搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1 文本分类任务简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.</span> <span class="toc-text">2 加载数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E8%AF%84%E6%B5%8B%E6%96%B9%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 加载数据和对应的评测方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 查看数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%9F%A5%E7%9C%8B%E8%AF%84%E6%B5%8B%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 查看评测方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%8E%E8%AF%84%E6%B5%8B%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 文本分类任务与评测方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">3 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 数据预处理流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%AF%B9%E5%BA%94%E7%9A%84tokenizer"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 构建模型对应的tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86datasets%E6%89%80%E6%9C%89%E6%A0%B7%E6%9C%AC%E8%BF%9B%E8%A1%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 对数据集datasets所有样本进行预处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">4 微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8A%A0%E8%BD%BD%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 加载分类模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 设定训练参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 模型评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2"><span class="toc-number">1.5.</span> <span class="toc-text">5 超参数搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%AE%BE%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 设置初始化模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 超参数搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E8%AE%BE%E7%BD%AE%E6%95%88%E6%9E%9C%E6%9C%80%E5%A5%BD%E7%9A%84%E5%8F%82%E6%95%B0%E5%B9%B6%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 设置效果最好的参数并训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%80%BB%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">6 总结</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">task6：BERT文本分类</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.7k</span><span class="post-meta__separator">|</span><span>Reading time: 17 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Transformers解决文本分类任务、超参搜索"><a href="#Transformers解决文本分类任务、超参搜索" class="headerlink" title="Transformers解决文本分类任务、超参搜索"></a>Transformers解决文本分类任务、超参搜索</h1><p>本文主要内容转自天国之影笔记<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task06">Task06</a>，之后具体的API进行了一些查询，写了一些说明。<br>@[toc]</p>
<h2 id="1-文本分类任务简介"><a href="#1-文本分类任务简介" class="headerlink" title="1 文本分类任务简介"></a>1 文本分类任务简介</h2><ul>
<li>使用Transformers代码库中的模型来解决文本分类任务，任务来源于<a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE Benchmark</a></li>
<li>GLUE榜单的9个级别的分类任务：<ol>
<li>CoLA (Corpus of Linguistic Acceptability)：鉴别一个句子是否语法正确.</li>
<li>MNLI (Multi-Genre Natural Language Inference)：给定一个假设，判断另一个句子与该假设的关系：entails、contradicts、unrelated。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus)：判断两个句子是否互为paraphrases</li>
<li>QNLI (Question-answering Natural Language Inference)：判断第2句是否包含第1句问题的答案</li>
<li>QQP (Quora Question Pairs2)：判断两个问句是否语义相同</li>
<li>RTE (Recognizing Textual Entailment)：判断一个句子是否与假设成entail关系</li>
<li>SST-2 (Stanford Sentiment Treebank)：判断一个句子的情感正负向</li>
<li>STS-B (Semantic Textual Similarity Benchmark)：判断两个句子的相似性（分数为1-5分）</li>
<li>WNLI (Winograd Natural Language Inference)：判断带有匿名代词的句子中，是否存在能够替换该代词的子句<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">GLUE_TASKS = [<span class="string">&quot;cola&quot;</span>, <span class="string">&quot;mnli&quot;</span>, <span class="string">&quot;mnli-mm&quot;</span>, <span class="string">&quot;mrpc&quot;</span>,</span><br><span class="line">              <span class="string">&quot;qnli&quot;</span>, <span class="string">&quot;qqp&quot;</span>, <span class="string">&quot;rte&quot;</span>, <span class="string">&quot;sst2&quot;</span>, <span class="string">&quot;stsb&quot;</span>, <span class="string">&quot;wnli&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 任务为CoLA任务</span></span><br><span class="line">task = <span class="string">&quot;cola&quot;</span></span><br><span class="line"><span class="comment"># BERT模型</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line"><span class="comment"># 根据GPU调整batch_size大小，避免显存溢出</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br></pre></td></tr></table></figure>
<h2 id="2-加载数据"><a href="#2-加载数据" class="headerlink" title="2 加载数据"></a>2 加载数据</h2><h3 id="2-1-加载数据和对应的评测方式"><a href="#2-1-加载数据和对应的评测方式" class="headerlink" title="2.1 加载数据和对应的评测方式"></a>2.1 加载数据和对应的评测方式</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#数据加载和评测方式加载只需要简单使用load_dataset和load_metric即可</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##根据任务名称加载数据和评估方法</span></span><br><span class="line"><span class="comment">#除了mnli-mm以外，其他任务都可以直接通过任务名字进行加载。数据加载之后会自动缓存。</span></span><br><span class="line">actual_task = <span class="string">&quot;mnli&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> task</span><br><span class="line"><span class="comment"># 加载GLUE数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;glue&quot;</span>, actual_task)</span><br><span class="line"><span class="comment"># 加载GLUE的评测方式</span></span><br><span class="line">metric = load_metric(<span class="string">&#x27;glue&#x27;</span>, actual_task)</span><br></pre></td></tr></table></figure>
Reusing dataset glue (C:\Users\hurui.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)</li>
</ol>
</li>
</ul>
<h3 id="2-2-查看数据"><a href="#2-2-查看数据" class="headerlink" title="2.2 查看数据"></a>2.2 查看数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据</span></span><br><span class="line">dataset</span><br></pre></td></tr></table></figure>
<pre><code>DatasetDict(&#123;
    train: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 8551
    &#125;)
    validation: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 1043
    &#125;)
    test: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 1063
    &#125;)
&#125;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看训练集第一条数据</span></span><br><span class="line">dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;sentence&#39;: &quot;Our friends won&#39;t buy this analysis, let alone the next one we propose.&quot;,
 &#39;label&#39;: 1,
 &#39;idx&#39;: 0&#125;
</code></pre><p>为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从数据集中随机选择几条数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(</span><br><span class="line">        dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line"></span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, datasets.ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(dataset[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure>
<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence</th>
      <th>label</th>
      <th>idx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>No one can forgive you that comment.</td>
      <td>acceptable</td>
      <td>2078</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bill and Kathy married.</td>
      <td>acceptable</td>
      <td>2318</td>
    </tr>
    <tr>
      <th>2</th>
      <td>$5 will buy a ticket.</td>
      <td>acceptable</td>
      <td>2410</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Which books did Robin talk to Chris and read?</td>
      <td>unacceptable</td>
      <td>7039</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Jill offered the ball towards Bob.</td>
      <td>unacceptable</td>
      <td>2053</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Has not Henri studied for his exam?</td>
      <td>unacceptable</td>
      <td>7466</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Fanny stopped talking when in came Aunt Norris.</td>
      <td>unacceptable</td>
      <td>6778</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Who do you think that would be nominated for the position?</td>
      <td>unacceptable</td>
      <td>4784</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Mickey teamed with the women up.</td>
      <td>unacceptable</td>
      <td>440</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Baseballs toss easily.</td>
      <td>unacceptable</td>
      <td>2783</td>
    </tr>
  </tbody>
</table>

<h3 id="2-3-查看评测方法"><a href="#2-3-查看评测方法" class="headerlink" title="2.3 查看评测方法"></a>2.3 查看评测方法</h3><p>评估metic是datasets.Metric的一个实例:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric</span><br><span class="line"></span><br><span class="line">    Metric(name: <span class="string">&quot;glue&quot;</span>, features: &#123;<span class="string">&#x27;predictions&#x27;</span>: Value(dtype=<span class="string">&#x27;int64&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>), <span class="string">&#x27;references&#x27;</span>: Value(dtype=<span class="string">&#x27;int64&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>)&#125;, usage: <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute GLUE evaluation metric associated to each GLUE dataset.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predictions: list of predictions to score.</span></span><br><span class="line"><span class="string">            Each translation should be tokenized into a list of tokens.</span></span><br><span class="line"><span class="string">        references: list of lists of references for each translation.</span></span><br><span class="line"><span class="string">            Each reference should be tokenized into a list of tokens.</span></span><br><span class="line"><span class="string">    Returns: depending on the GLUE subset, one or several of:</span></span><br><span class="line"><span class="string">        &quot;accuracy&quot;: Accuracy</span></span><br><span class="line"><span class="string">        &quot;f1&quot;: F1 score</span></span><br><span class="line"><span class="string">        &quot;pearson&quot;: Pearson Correlation</span></span><br><span class="line"><span class="string">        &quot;spearmanr&quot;: Spearman Correlation</span></span><br><span class="line"><span class="string">        &quot;matthews_correlation&quot;: Matthew Correlation</span></span><br><span class="line"><span class="string">    Examples:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; glue_metric = datasets.load_metric(&#x27;glue&#x27;, &#x27;sst2&#x27;)  # &#x27;sst2&#x27; or any of [&quot;mnli&quot;, &quot;mnli_mismatched&quot;, &quot;mnli_matched&quot;, &quot;qnli&quot;, &quot;rte&quot;, &quot;wnli&quot;, &quot;hans&quot;]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; references = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(results)</span></span><br><span class="line"><span class="string">        &#123;&#x27;accuracy&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; glue_metric = datasets.load_metric(&#x27;glue&#x27;, &#x27;mrpc&#x27;)  # &#x27;mrpc&#x27; or &#x27;qqp&#x27;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; references = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(results)</span></span><br><span class="line"><span class="string">        &#123;&#x27;accuracy&#x27;: 1.0, &#x27;f1&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; glue_metric = datasets.load_metric(&#x27;glue&#x27;, &#x27;stsb&#x27;)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; references = [0., 1., 2., 3., 4., 5.]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = [0., 1., 2., 3., 4., 5.]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(&#123;&quot;pearson&quot;: round(results[&quot;pearson&quot;], 2), &quot;spearmanr&quot;: round(results[&quot;spearmanr&quot;], 2)&#125;)</span></span><br><span class="line"><span class="string">        &#123;&#x27;pearson&#x27;: 1.0, &#x27;spearmanr&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; glue_metric = datasets.load_metric(&#x27;glue&#x27;, &#x27;cola&#x27;)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; references = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(results)</span></span><br><span class="line"><span class="string">        &#123;&#x27;matthews_correlation&#x27;: 1.0&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>, stored examples: <span class="number">0</span>)</span><br></pre></td></tr></table></figure><br>调用metric的compute方法，传入labels和predictions即可得到metric的值：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#这里只是一个示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">fake_preds = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">fake_labels = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">metric.compute(predictions=fake_preds, references=fake_labels)</span><br></pre></td></tr></table></figure><br>    {‘matthews_correlation’: -0.00392156862745098}</p>
<h3 id="2-4-文本分类任务与评测方法"><a href="#2-4-文本分类任务与评测方法" class="headerlink" title="2.4 文本分类任务与评测方法"></a>2.4 文本分类任务与评测方法</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">任务</th>
<th style="text-align:center">评测方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CoLA</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Matthews Correlation Coefficient</a></td>
</tr>
<tr>
<td style="text-align:center">MNLI</td>
<td style="text-align:center">Accuracy</td>
</tr>
<tr>
<td style="text-align:center">MRPC</td>
<td style="text-align:center">Accuracy and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/F1_score">F1 score</a></td>
</tr>
<tr>
<td style="text-align:center">QNLI</td>
<td style="text-align:center">Accuracy</td>
</tr>
<tr>
<td style="text-align:center">QQP</td>
<td style="text-align:center">Accuracy and F1 score</td>
</tr>
<tr>
<td style="text-align:center">RTE</td>
<td style="text-align:center">Accuracy</td>
</tr>
<tr>
<td style="text-align:center">SST-2</td>
<td style="text-align:center">Accuracy</td>
</tr>
<tr>
<td style="text-align:center">STS-B</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson Correlation Coefficient</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s_Rank_Correlation_Coefficient</a></td>
</tr>
<tr>
<td style="text-align:center">WNLI</td>
<td style="text-align:center">Accuracy</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3 数据预处理"></a>3 数据预处理</h2><h3 id="3-1-数据预处理流程"><a href="#3-1-数据预处理流程" class="headerlink" title="3.1 数据预处理流程"></a>3.1 数据预处理流程</h3><ul>
<li>使用工具：Tokenizer</li>
<li>流程：<ol>
<li>对输入数据进行tokenize，得到tokens</li>
<li>将tokens转化为预训练模型中需要对应的token ID</li>
<li>将token ID转化为模型需要的输入格式</li>
</ol>
</li>
</ul>
<p>&#8195;&#8195;为了达到数据预处理的目的，我们使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这样可以确保：</p>
<ul>
<li>我们得到一个与预训练模型一一对应的tokenizer。</li>
<li>使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。</li>
<li>这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载<h3 id="3-2-构建模型对应的tokenizer"><a href="#3-2-构建模型对应的tokenizer" class="headerlink" title="3.2 构建模型对应的tokenizer"></a>3.2 构建模型对应的tokenizer</h3></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p> AutoTokenizer是一个通用的分词器类，使用AutoTokenizer.from_pretrained类方法实例化具体的分词器之一。<br> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">AutoTokenizer.from_pretrained(</span><br><span class="line">   pretrained_model_name_or_path,</span><br><span class="line">   *inputs,</span><br><span class="line">   **kwargs,)</span><br></pre></td></tr></table></figure></p>
<p>&#8195;&#8195;注意：use_fast=True要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。如果对应的模型没有fast tokenizer，去掉这个选项即可。</p>
<p>&#8195;&#8195;tokenizer既可以对单个文本进行预处理，也可以对一对文本进行预处理，tokenizer预处理后得到的数据满足预训练模型输入格式。这取决于我们选择的预训练模型，我们将会看到tokenizer有不同的返回，==tokenizer和预训练模型是一一对应的==，更多信息可以在<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/preprocessing.html">这里</a>进行学习。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this one sentence!&quot;</span>, <span class="string">&quot;And this sentence goes with it.&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre><h3 id="3-3-对数据集datasets所有样本进行预处理"><a href="#3-3-对数据集datasets所有样本进行预处理" class="headerlink" title="3.3 对数据集datasets所有样本进行预处理"></a>3.3 对数据集datasets所有样本进行预处理</h3><p>为了预处理我们的数据，我们需要知道不同数据和对应的数据格式，因此我们定义下面这个dict<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义如下dict，用于对数据格式进行检查</span></span><br><span class="line">task_to_keys = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mnli-mm&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>: (<span class="string">&quot;question&quot;</span>, <span class="string">&quot;sentence&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>: (<span class="string">&quot;question1&quot;</span>, <span class="string">&quot;question2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对训练数据集的第1条数据进行数据格式检查</span></span><br><span class="line">sentence1_key, sentence2_key = task_to_keys[task]</span><br><span class="line"><span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 1: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 2: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence2_key]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>Sentence: Our friends won&#39;t buy this analysis, let alone the next one we propose.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造数据预处理函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> tokenizer(examples[sentence1_key], truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;预处理函数可以处理单个样本，也可以对多个样本进行处理。如果输入是多个样本，那么返回的是一个list。<br>&#8195;&#8195;使用map函数将预处理函数应用到（map)所有样本上。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对所有数据进行预处理</span></span><br><span class="line">encoded_dataset = dataset.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\cache-fd5eee62c2b8c26e.arrow
Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\cache-0ce499346cf9c20b.arrow

HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=&#39;&#39;)))
</code></pre><h2 id="4-微调预训练模型"><a href="#4-微调预训练模型" class="headerlink" title="4 微调预训练模型"></a>4 微调预训练模型</h2><p>&#8195;&#8195;既然我们是做seq2seq任务，那么我们需要使用AutoModelForSequenceClassification 这个类。和tokenizer相似，from_pretrained方法同样可以帮助我们下载并加载模型，同时也会对模型进行缓存，就不会重复下载模型啦。</p>
<h3 id="4-1-加载分类模型"><a href="#4-1-加载分类模型" class="headerlink" title="4.1 加载分类模型"></a>4.1 加载分类模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#STS-B是一个回归问题，MNLI是一个3分类问题</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">num_labels = <span class="number">3</span> <span class="keyword">if</span> task.startswith(<span class="string">&quot;mnli&quot;</span>) <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">if</span> task == <span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(</span><br><span class="line">    model_checkpoint, num_labels=num_labels)</span><br><span class="line"><span class="comment">#模型是最前面设置的model_checkpoint=distilbert-base-uncased</span></span><br></pre></td></tr></table></figure>
<pre><code>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.bias&#39;]
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;pre_classifier.bias&#39;, &#39;classifier.weight&#39;, &#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>&#8195;&#8195;由于我们微调的任务是文本分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了文本分类的神经网络head）</p>
<h3 id="4-2-设定训练参数"><a href="#4-2-设定训练参数" class="headerlink" title="4.2 设定训练参数"></a>4.2 设定训练参数</h3><p>&#8195;&#8195;Trainer训练工具需要3个要素，最重要的是训练的设定/参数 TrainingArguments。这个训练设定包含了能够定义训练过程的所有属性。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric_name = <span class="string">&quot;pearson&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="string">&quot;matthews_correlation&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;cola&quot;</span> <span class="keyword">else</span> <span class="string">&quot;accuracy&quot;</span></span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="comment">#args包含了能够定义训练过程的所有属性</span></span><br><span class="line">metric_name = <span class="string">&quot;pearson&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="string">&quot;matthews_correlation&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;cola&quot;</span> <span class="keyword">else</span> <span class="string">&quot;accuracy&quot;</span></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-glue&quot;</span>,<span class="comment">#输出路径</span></span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,<span class="comment">#每轮结束后进行评价</span></span><br><span class="line">    save_strategy = <span class="string">&quot;epoch&quot;</span>,<span class="comment">#每个epoch保存一次权重，默认是steps</span></span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,<span class="comment">#初始学习率</span></span><br><span class="line">    per_device_train_batch_size=batch_size,<span class="comment">#训练批次大小</span></span><br><span class="line">    per_device_eval_batch_size=batch_size,<span class="comment">#测试批次大小</span></span><br><span class="line">    num_train_epochs=<span class="number">5</span>,<span class="comment">#训练轮数</span></span><br><span class="line">    weight_decay=<span class="number">0.01</span>,<span class="comment">#指数衰减？</span></span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,<span class="comment">#是否在训练结束时加载训练过程中找到的最佳模型。默认否</span></span><br><span class="line">    metric_for_best_model=metric_name,<span class="comment">#通过str方式传递评测方法。结合第一句表示stsb使用皮尔逊系数，cola使用matthews_correlation，其它都是acc</span></span><br><span class="line">    <span class="comment">#这个表示训练中评测效果更好就保存权重参数，否则继续训练但是不更新。相当于early-stop</span></span><br><span class="line">    log_level=<span class="string">&#x27;error&#x27;</span>,</span><br><span class="line">    logging_strategy=<span class="string">&quot;no&quot;</span>,</span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据任务名称获取不同的评测方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    <span class="keyword">if</span> task != <span class="string">&quot;stsb&quot;</span>:</span><br><span class="line">        predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        predictions = predictions[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造训练器Trainer</span></span><br><span class="line">validation_key = <span class="string">&quot;validation_mismatched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation_matched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation&quot;</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>还有参数优化器，默认是Adamw。</p>
<h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3 训练模型"></a>4.3 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<pre><code>TrainOutput(global_step=2675, training_loss=0.2717150308484229, metrics=&#123;&#39;train_runtime&#39;: 100.5668, &#39;train_samples_per_second&#39;: 425.14, &#39;train_steps_per_second&#39;: 26.599, &#39;total_flos&#39;: 229537542078168.0, &#39;train_loss&#39;: 0.2717150308484229, &#39;epoch&#39;: 5.0&#125;)
</code></pre><h3 id="4-4-模型评估"><a href="#4-4-模型评估" class="headerlink" title="4.4 模型评估"></a>4.4 模型评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure>
<div>

  <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [66/66 00:00]
</div>

<pre><code>&#123;&#39;eval_loss&#39;: 0.8624260425567627,
 &#39;eval_matthews_correlation&#39;: 0.519563286537562,
 &#39;eval_runtime&#39;: 0.6501,
 &#39;eval_samples_per_second&#39;: 1604.31,
 &#39;eval_steps_per_second&#39;: 101.519,
 &#39;epoch&#39;: 5.0&#125;
</code></pre><h2 id="5-超参数搜索"><a href="#5-超参数搜索" class="headerlink" title="5 超参数搜索"></a>5 超参数搜索</h2><p>Trainer同样支持超参搜索，使用optuna or Ray Tune代码库。<br>反注释下面两行安装依赖：</p>
<pre><code>! pip install optuna
! pip install ray[tune]
</code></pre><h3 id="5-1-设置初始化模型"><a href="#5-1-设置初始化模型" class="headerlink" title="5.1 设置初始化模型"></a>5.1 设置初始化模型</h3><p>&#8195;&#8195;超参搜索时，Trainer将会返回多个训练好的模型，所以需要传入一个定义好的模型从而让Trainer可以不断重新初始化该传入的模型：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_init</span>():</span></span><br><span class="line">    <span class="keyword">return</span> AutoModelForSequenceClassification.from_pretrained(</span><br><span class="line">    model_checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#调用 Trainer</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model_init=model_init,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="5-2-超参数搜索"><a href="#5-2-超参数搜索" class="headerlink" title="5.2 超参数搜索"></a>5.2 超参数搜索</h3><p>&#8195;&#8195;调用方法hyperparameter_search进行超参搜索。这个过程可能很久，故可以先用部分数据集进行超参搜索，再进行全量训练。 比如使用1/10的数据进行搜索：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用1/10数据进行搜索</span></span><br><span class="line">best_run = trainer.hyperparameter_search(n_trials=<span class="number">10</span>, direction=<span class="string">&quot;maximize&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hyperparameter_search会返回到效果最好的模型参数</span></span><br><span class="line">best_run</span><br></pre></td></tr></table></figure>
<pre><code>BestRun(run_id=&#39;3&#39;, objective=0.5504031254980248, hyperparameters=&#123;&#39;learning_rate&#39;: 4.301257551502102e-05, &#39;num_train_epochs&#39;: 5, &#39;seed&#39;: 20, &#39;per_device_train_batch_size&#39;: 8&#125;)
</code></pre><h3 id="5-3-设置效果最好的参数并训练模型"><a href="#5-3-设置效果最好的参数并训练模型" class="headerlink" title="5.3 设置效果最好的参数并训练模型"></a>5.3 设置效果最好的参数并训练模型</h3><p>将Trainner设置为搜索到的最好参数，进行训练：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n, v <span class="keyword">in</span> best_run.hyperparameters.items():</span><br><span class="line">    <span class="built_in">setattr</span>(trainer.args, n, v)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TrainOutput(global_step=<span class="number">5345</span>, training_loss=<span class="number">0.26719996967083726</span>, metrics=&#123;<span class="string">&#x27;train_runtime&#x27;</span>: <span class="number">178.4912</span>, <span class="string">&#x27;train_samples_per_second&#x27;</span>: <span class="number">239.536</span>, <span class="string">&#x27;train_steps_per_second&#x27;</span>: <span class="number">29.945</span>, <span class="string">&#x27;total_flos&#x27;</span>: <span class="number">413547436355364.0</span>, <span class="string">&#x27;train_loss&#x27;</span>: <span class="number">0.26719996967083726</span>, <span class="string">&#x27;epoch&#x27;</span>: <span class="number">5.0</span>&#125;)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br><span class="line"></span><br><span class="line">    &#123;<span class="string">&#x27;eval_loss&#x27;</span>: <span class="number">0.9789257049560547</span>,</span><br><span class="line">     <span class="string">&#x27;eval_matthews_correlation&#x27;</span>: <span class="number">0.5548273578107759</span>,</span><br><span class="line">     <span class="string">&#x27;eval_runtime&#x27;</span>: <span class="number">0.6556</span>,</span><br><span class="line">     <span class="string">&#x27;eval_samples_per_second&#x27;</span>: <span class="number">1590.796</span>,</span><br><span class="line">     <span class="string">&#x27;eval_steps_per_second&#x27;</span>: <span class="number">100.664</span>,</span><br><span class="line">     <span class="string">&#x27;epoch&#x27;</span>: <span class="number">5.0</span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h2><p>&emsp;&emsp;本次任务，主要介绍了用BERT模型解决文本分类任务的方法及步骤，步骤主要分为加载数据、数据预处理、微调预训练模型和超参数搜索。在加载数据阶段中，必须使用与分类任务相应的评测方法；在数据预处理阶段中，对tokenizer分词器的建模，并完成数据集中所有样本的预处理；在微调预训练模型阶段，通过对模型参数进行设置，并构建Trainner训练器，进行模型训练和评估；最后在超参数搜索阶段，使用hyperparameter_search方法，搜索效果最好的超参数，并进行模型训练和评估。<br>&emsp;&emsp;其中在数据集下载时，需要使用外网方式建立代理。如果使用conda安装ray[tune]包时，请下载对应ray-tune依赖包。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/26/8月组队学习：nlp之transformers入门/task6：Transformers解决文本分类任务、超参搜索/">https://zhxnlp.github.io/2021/08/26/8月组队学习：nlp之transformers入门/task6：Transformers解决文本分类任务、超参搜索/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task7%EF%BC%9ATransformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1/"><i class="fa fa-chevron-left">  </i><span>task7：BERT token分类</span></a></div><div class="next-post pull-right"><a href="/2021/08/24/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/%E9%80%89%E8%AF%BB1%EF%BC%9ATransformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89/"><span>选读1：Transformer代码解读（Pytorch）</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>