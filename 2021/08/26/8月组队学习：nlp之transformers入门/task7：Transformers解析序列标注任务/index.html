<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="task7ï¼šBERT tokenåˆ†ç±»"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>task7ï¼šBERT tokenåˆ†ç±» | zhxnlpã®Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"æœç´¢æ–‡ç« ","hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹:${query}","hits_stats":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼Œç”¨æ—¶ ${time} æ¯«ç§’"}},
  localSearch: undefined,
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpã®Blog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="åˆ‡æ¢æ–‡ç« è¯¦æƒ…">åˆ‡æ¢ç«™ç‚¹æ¦‚è§ˆ</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">ç›®å½•</div><div class="sidebar-toc__progress"><span class="progress-notice">ä½ å·²ç»è¯»äº†</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.</span> <span class="toc-text">Transformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1 åºåˆ—æ ‡æ³¨ä»»åŠ¡ç®€ä»‹</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.</span> <span class="toc-text">2 åŠ è½½æ•°æ®</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">3 é¢„å¤„ç†æ•°æ®</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 æ•°æ®é¢„å¤„ç†æµç¨‹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%AF%B9%E5%BA%94%E7%9A%84tokenizer"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 æ„å»ºæ¨¡å‹å¯¹åº”çš„tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E8%A7%A3%E5%86%B3subtokens%E5%AF%B9%E9%BD%90%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 è§£å†³subtokenså¯¹é½é—®é¢˜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%95%B4%E5%90%88%E9%A2%84%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 æ•´åˆé¢„å¤„ç†å‡½æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86datasets%E6%89%80%E6%9C%89%E6%A0%B7%E6%9C%AC%E8%BF%9B%E8%A1%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 å¯¹æ•°æ®é›†datasetsæ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">4 å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8A%A0%E8%BD%BD%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 åŠ è½½åˆ†ç±»æ¨¡å‹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 è®¾å®šè®­ç»ƒå‚æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%BE%E5%AE%9A%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 è®¾å®šè¯„ä¼°æ–¹æ³•</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 è®­ç»ƒæ¨¡å‹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 æ¨¡å‹è¯„ä¼°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E8%BE%93%E5%87%BA%E5%8D%95%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84precision-recall-f1"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 è¾“å‡ºå•ä¸ªç±»åˆ«çš„precision&#x2F;recall&#x2F;f1</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">5 æ€»ç»“</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">æ–‡ç« </span><span class="pull-right">49</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">æ ‡ç­¾</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">åˆ†ç±»</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpã®Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> æœç´¢</span></a></span></div><div id="post-info"><div id="post-title">task7ï¼šBERT tokenåˆ†ç±»</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8æœˆç»„é˜Ÿå­¦ä¹ ï¼šnlpä¹‹transformers</a><div class="post-meta-wordcount"><span>å­—æ•°æ€»è®¡: </span><span class="word-count">5.2k</span><span class="post-meta__separator">|</span><span>é˜…è¯»æ—¶é•¿: 25 åˆ†é’Ÿ</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Transformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡"><a href="#Transformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡" class="headerlink" title="Transformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡"></a>Transformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡</h1><p>æœ¬æ–‡ä¸»è¦æ¥è‡ªdatawhaleçš„<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.2-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8">transformeræ•™ç¨‹4.2</a>å’Œ<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task07">å¤©å›½ä¹‹å½±å­¦ä¹ ç¬”è®°</a>ã€‚</p>
<h2 id="1-åºåˆ—æ ‡æ³¨ä»»åŠ¡ç®€ä»‹"><a href="#1-åºåˆ—æ ‡æ³¨ä»»åŠ¡ç®€ä»‹" class="headerlink" title="1 åºåˆ—æ ‡æ³¨ä»»åŠ¡ç®€ä»‹"></a>1 åºåˆ—æ ‡æ³¨ä»»åŠ¡ç®€ä»‹</h2><ul>
<li>åºåˆ—æ ‡æ³¨å¯ä»¥çœ‹ä½œæ—¶tokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ï¼Œä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸€ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ ‡ç­¾</li>
<li><p>tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼š</p>
<ol>
<li>NERï¼ˆNamed-entity recognition åè¯-å®ä½“è¯†åˆ«ï¼‰åˆ†è¾¨å‡ºæ–‡æœ¬ä¸­çš„åè¯å’Œå®ä½“ï¼ˆpersonäººå, organizationç»„ç»‡æœºæ„å, locationåœ°ç‚¹åâ€¦ï¼‰</li>
<li>POSï¼ˆPart-of-speech taggingè¯æ€§æ ‡æ³¨ï¼‰æ ¹æ®è¯­æ³•å¯¹tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ï¼ˆnounåè¯ã€verbåŠ¨è¯ã€adjectiveå½¢å®¹è¯â€¦ï¼‰</li>
<li>Chunkï¼ˆChunkingçŸ­è¯­ç»„å—ï¼‰å°†åŒä¸€ä¸ªçŸ­è¯­çš„tokensç»„å—æ”¾åœ¨ä¸€èµ·</li>
</ol>
<span id="more"></span>
<p>&#8195;&#8195;åªè¦é¢„è®­ç»ƒçš„transformeræ¨¡å‹æœ€é¡¶å±‚æœ‰ä¸€ä¸ªtokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆæ¯”å¦‚ä¸Šä¸€ç¯‡ç« æåˆ°çš„BertForTokenClassification,éœ€è¦å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰fast tokenizerè¿™ä¸ªåŠŸèƒ½ï¼Œå‚è€ƒ<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/index.html#bigtable">è¿™ä¸ªè¡¨</a>ï¼‰ï¼Œé‚£ä¹ˆæœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/models">æ¨¡å‹é¢æ¿</a>ï¼‰ï¼Œè§£å†³ä»»ä½•tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ã€‚</p>
</li>
</ul>
<p>&#8195;&#8195;å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®¾ç½®åˆ†ç±»ä»»åŠ¡</span></span><br><span class="line">task = <span class="string">&quot;ner&quot;</span> </span><br><span class="line"><span class="comment"># è®¾ç½®BERTæ¨¡å‹</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line"><span class="comment"># æ ¹æ®GPUè°ƒæ•´batch_sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡º</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br></pre></td></tr></table></figure>
<h2 id="2-åŠ è½½æ•°æ®"><a href="#2-åŠ è½½æ•°æ®" class="headerlink" title="2 åŠ è½½æ•°æ®"></a>2 åŠ è½½æ•°æ®</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#åŠ è½½æ•°æ®å’Œè¯„æµ‹æ–¹å¼</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;æœ¬æ–‡ä½¿ç”¨çš„æ˜¯<a target="_blank" rel="noopener" href="https://aclanthology.org/W03-0419.pdf">CONLL 2003 dataset</a>æ•°æ®é›†ã€‚æ¥å¤„ç†Datasetsåº“ä¸­çš„ä»»ä½•tokenåˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœè¦åŠ è½½è‡ªå®šä¹‰çš„json/csvæ–‡ä»¶æ•°æ®é›†ï¼Œå¯ä»¥å‚è€ƒ<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files">æ•°æ®é›†æ–‡æ¡£</a>æ¥å­¦ä¹ å¦‚ä½•åŠ è½½ã€‚è‡ªå®šä¹‰æ•°æ®é›†å¯èƒ½éœ€è¦åœ¨åŠ è½½å±æ€§åå­—ä¸Šåšä¸€äº›è°ƒæ•´<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½conll2003æ•°æ®é›†</span></span><br><span class="line">datasets = load_dataset(<span class="string">&quot;conll2003&quot;</span>)</span><br></pre></td></tr></table></figure><br>    Reusing dataset conll2003 (C:\Users\hurui.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets</span><br></pre></td></tr></table></figure>
<p>datasetså¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§DatasetDictæ•°æ®ç»“æ„ã€‚å¯ä»¥ä½¿ç”¨å¯¹åº”çš„keyå¾—åˆ°ç›¸åº”çš„æ•°æ®<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    DatasetDict(&#123;</span><br><span class="line">        train: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">14041</span></span><br><span class="line">        &#125;)</span><br><span class="line">        validation: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">3250</span></span><br><span class="line">        &#125;)</span><br><span class="line">        test: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">3453</span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="comment">#labelåˆ—å¯¹åº”tokensçš„æ ‡æ³¨</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æŸ¥çœ‹è®­ç»ƒé›†ç¬¬ä¸€æ¡æ•°æ®</span></span><br><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;tokens&#x27;</span>: [<span class="string">&#x27;EU&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rejects&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;German&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;call&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;to&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;boycott&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;British&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lamb&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;pos_tags&#x27;</span>: [<span class="number">22</span>, <span class="number">42</span>, <span class="number">16</span>, <span class="number">21</span>, <span class="number">35</span>, <span class="number">37</span>, <span class="number">16</span>, <span class="number">21</span>, <span class="number">7</span>],</span><br><span class="line"> <span class="string">&#x27;chunk_tags&#x27;</span>: [<span class="number">11</span>, <span class="number">21</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">0</span>],</span><br><span class="line"> <span class="string">&#x27;ner_tags&#x27;</span>: [<span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;æ‰€æœ‰çš„æ•°æ®æ ‡ç­¾labelséƒ½å·²ç»è¢«ç¼–ç æˆäº†æ•´æ•°ï¼Œå¯ä»¥ç›´æ¥è¢«é¢„è®­ç»ƒtransformeræ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›æ•´æ•°çš„ç¼–ç æ‰€å¯¹åº”çš„å®é™…ç±»åˆ«å‚¨å­˜åœ¨featuresä¸­ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æŸ¥çœ‹featureså±æ€§</span></span><br><span class="line">datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;ner_tags&quot;</span>]</span><br></pre></td></tr></table></figure><br>    Sequence(feature=ClassLabel(num_classes=9, names=[â€˜Oâ€™, â€˜B-PERâ€™, â€˜I-PERâ€™, â€˜B-ORGâ€™, â€˜I-ORGâ€™, â€˜B-LOCâ€™, â€˜I-LOCâ€™, â€˜B-MISCâ€™, â€˜I-MISCâ€™], names_file=None, id=None), length=-1, id=None)</p>
<p>&#8195;&#8195;ä»¥NERä¸ºä¾‹ï¼Œ0å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«æ˜¯â€Oâ€œï¼Œ 1å¯¹åº”çš„æ˜¯â€B-PERâ€œç­‰ç­‰ã€‚å…·ä½“æ ‡ç­¾å«ä¹‰å¯¹åº”å¦‚ä¸‹ï¼š</p>
<ul>
<li>PERï¼šperson</li>
<li>ORGï¼šorganization</li>
<li>LOCï¼šlocation</li>
<li>MISCï¼šmiscellaneous</li>
<li>Oï¼šæ²¡æœ‰ç‰¹åˆ«å®ä½“ï¼ˆno special entityï¼‰</li>
<li>B-*ï¼šå®ä½“å¼€å§‹çš„token</li>
<li>I-*ï¼šå®ä½“ä¸­é—´çš„token</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_list = datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>].feature.names</span><br><span class="line">label_list</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</code></pre><p>å®šä¹‰ä¸‹é¢çš„å‡½æ•°ï¼Œä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤º<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel, <span class="type">Sequence</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;ä»æ•°æ®é›†ä¸­éšæœºé€‰æ‹©å‡ æ¡æ•°æ®&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(typ, <span class="type">Sequence</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(typ.feature, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> x: [typ.feature.names[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(datasets[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure></p>
<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4143</td>
      <td>[The, 85-year-old, nun, said, in, the, past, that, she, was, praying, for, the, couple, ,, whose, divorce, is, expected, to, become, final, next, week, .]</td>
      <td>[DT, JJ, NN, VBD, IN, DT, NN, IN, PRP, VBD, VBG, IN, DT, NN, ,, WP\$, NN, VBZ, VBN, TO, VB, JJ, JJ, NN, .]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-PP, B-NP, I-NP, B-SBAR, B-NP, B-VP, I-VP, B-PP, B-NP, I-NP, O, B-NP, I-NP, B-VP, I-VP, I-VP, I-VP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2442</td>
      <td>[2., Marie-Jose, Perec, (, France, ), 49.72]</td>
      <td>[CD, NNP, NNP, (, NNP, ), CD]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, O, B-NP]</td>
      <td>[O, B-PER, I-PER, O, B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1090</td>
      <td>[There, were, no, significant, differences, between, the, groups, receiving, garlic, and, placebo, ,, ", they, wrote, in, the, Journal, of, the, Royal, College, of, Physicians, .]</td>
      <td>[EX, VBD, DT, JJ, NNS, IN, DT, NNS, VBG, NN, CC, NN, ,, ", PRP, VBD, IN, DT, NNP, IN, DT, NNP, NNP, IN, NNPS, .]</td>
      <td>[B-NP, B-VP, B-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-VP, B-NP, I-NP, I-NP, O, O, B-NP, B-VP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1972</td>
      <td>[Pakistan, first, innings]</td>
      <td>[NNP, RB, NN]</td>
      <td>[B-NP, B-ADVP, B-NP]</td>
      <td>[B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13714</td>
      <td>[The, Taiwan, dollar, closed, slightly, firmer, on, Thursday, amid, tight, Taiwan, dollar, liquidity, in, the, banking, system, ,, and, dealers, said, the, rate, was, likely, to, move, narrowly, in, the, near, term, .]</td>
      <td>[DT, NNP, NN, VBD, RB, JJR, IN, NNP, IN, JJ, NNP, NN, NN, IN, DT, NN, NN, ,, CC, NNS, VBD, DT, NN, VBD, JJ, TO, VB, RB, IN, DT, JJ, NN, .]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-ADVP, B-ADJP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, O, O, B-NP, B-VP, B-NP, I-NP, B-VP, B-ADJP, B-VP, I-VP, I-VP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, B-LOC, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4806</td>
      <td>[nine, of, the, superbike, world, championship, on, Sunday, :]</td>
      <td>[CD, IN, DT, JJ, NN, NN, IN, NNP, :]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7452</td>
      <td>[The, accident, happened, when, the, Sanchez, Zarraga, family, took, their, boat, out, for, a, nighttime, spin, ,, Civil, Defence, and, Coast, Guard, officials, said, .]</td>
      <td>[DT, NN, VBD, WRB, DT, NNP, NNP, NN, VBD, PRP\$, NN, RP, IN, DT, NN, NN, ,, NNP, NN, CC, NNP, NNP, NNS, VBD, .]</td>
      <td>[B-NP, I-NP, B-VP, B-ADVP, B-NP, I-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-ADVP, B-PP, B-NP, I-NP, I-NP, O, B-NP, I-NP, O, B-NP, I-NP, I-NP, B-VP, O]</td>
      <td>[O, O, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2332</td>
      <td>[7., Julie, Baumann, (, Switzerland, ), 13.36]</td>
      <td>[NNP, NNP, NNP, (, NNP, ), CD]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, O, B-NP]</td>
      <td>[O, B-PER, I-PER, O, B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9786</td>
      <td>[The, pilot, said, several, hijackers, appeared, to, be, placed, around, the, plane, .]</td>
      <td>[DT, NN, VBD, JJ, NNS, VBD, TO, VB, VBN, IN, DT, NN, .]</td>
      <td>[B-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, I-VP, I-VP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3451</td>
      <td>[(, 7-4, ), 6-2]</td>
      <td>[(, CD, ), CD]</td>
      <td>[B-LST, B-NP, O, B-NP]</td>
      <td>[O, O, O, O]</td>
    </tr>
  </tbody>
</table>

<h2 id="3-é¢„å¤„ç†æ•°æ®"><a href="#3-é¢„å¤„ç†æ•°æ®" class="headerlink" title="3 é¢„å¤„ç†æ•°æ®"></a>3 é¢„å¤„ç†æ•°æ®</h2><h3 id="3-1-æ•°æ®é¢„å¤„ç†æµç¨‹"><a href="#3-1-æ•°æ®é¢„å¤„ç†æµç¨‹" class="headerlink" title="3.1 æ•°æ®é¢„å¤„ç†æµç¨‹"></a>3.1 æ•°æ®é¢„å¤„ç†æµç¨‹</h3><ul>
<li>æ•°æ®é¢„å¤„ç†å·¥å…·ï¼šTokenizer</li>
<li>æµç¨‹ï¼š<ol>
<li>å¯¹è¾“å…¥æ•°æ®è¿›è¡Œtokenizeï¼Œå¾—åˆ°tokens</li>
<li>å°†tokensè½¬åŒ–ä¸ºé¢„è®­ç»ƒæ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token ID</li>
<li>å°†token IDè½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼<br>&#8195;&#8195;ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨AutoTokenizer.from_pretrainedæ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š</li>
</ol>
</li>
<li>æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚</li>
<li>ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚</li>
<li>è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½<h3 id="3-2-æ„å»ºæ¨¡å‹å¯¹åº”çš„tokenizer"><a href="#3-2-æ„å»ºæ¨¡å‹å¯¹åº”çš„tokenizer" class="headerlink" title="3.2 æ„å»ºæ¨¡å‹å¯¹åº”çš„tokenizer"></a>3.2 æ„å»ºæ¨¡å‹å¯¹åº”çš„tokenizer</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
&#8195;&#8195;ä»¥ä¸‹ä»£ç è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚åœ¨è¿™é‡Œ<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/index.html#bigtable">big table of models</a>æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æœ‰fast tokenizerã€‚<br>&#8195;&#8195;tokenizeræ—¢å¯ä»¥å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸€å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œtokenizeré¢„å¤„ç†åå¾—åˆ°çš„æ•°æ®æ»¡è¶³é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="comment"># æ¨¡å‹ä½¿ç”¨çš„æ—¶fast tokenizer</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(tokenizer, transformers.PreTrainedTokenizerFast)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this is one sentence!&quot;</span>)</span><br></pre></td></tr></table></figure>
  {â€˜input_idsâ€™: [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], â€˜attention_maskâ€™: [1, 1, 1, 1, 1, 1, 1, 1, 1]}</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer([<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;,&quot;</span>, <span class="string">&quot;this&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;sentence&quot;</span>, <span class="string">&quot;split&quot;</span>,</span><br><span class="line">          <span class="string">&quot;into&quot;</span>, <span class="string">&quot;words&quot;</span>, <span class="string">&quot;.&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre><p><strong>è¡¥å……ï¼š</strong></p>
<ul>
<li>transformeré¢„è®­ç»ƒæ¨¡å‹ä¼šå°†åˆ‡åˆ†åçš„wordï¼Œç»§ç»­ç”¨tokenizeråˆ†è¯å™¨åˆ‡åˆ†ä¸ºsubword<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(example[<span class="string">&quot;tokens&quot;</span>])</span><br></pre></td></tr></table></figure>
  [â€˜Germanyâ€™, â€œâ€˜sâ€, â€˜representativeâ€™, â€˜toâ€™, â€˜theâ€™, â€˜Europeanâ€™, â€˜Unionâ€™, â€œâ€˜sâ€, â€˜veterinaryâ€™, â€˜committeeâ€™, â€˜Wernerâ€™, â€˜Zwingmannâ€™, â€˜saidâ€™, â€˜onâ€™, â€˜Wednesdayâ€™, â€˜consumersâ€™, â€˜shouldâ€™, â€˜buyâ€™, â€˜sheepmeatâ€™, â€˜fromâ€™, â€˜countriesâ€™, â€˜otherâ€™, â€˜thanâ€™, â€˜Britainâ€™, â€˜untilâ€™, â€˜theâ€™, â€˜scientificâ€™, â€˜adviceâ€™, â€˜wasâ€™, â€˜clearerâ€™, â€˜.â€™]</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_input = tokenizer(example[<span class="string">&quot;tokens&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br><span class="line">tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;[CLS]&#39;, &#39;germany&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;representative&#39;, &#39;to&#39;, &#39;the&#39;, &#39;european&#39;, &#39;union&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;veterinary&#39;, &#39;committee&#39;, &#39;werner&#39;, &#39;z&#39;, &#39;##wing&#39;, &#39;##mann&#39;, &#39;said&#39;, &#39;on&#39;, &#39;wednesday&#39;, &#39;consumers&#39;, &#39;should&#39;, &#39;buy&#39;, &#39;sheep&#39;, &#39;##me&#39;, &#39;##at&#39;, &#39;from&#39;, &#39;countries&#39;, &#39;other&#39;, &#39;than&#39;, &#39;britain&#39;, &#39;until&#39;, &#39;the&#39;, &#39;scientific&#39;, &#39;advice&#39;, &#39;was&#39;, &#39;clearer&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</code></pre><p>å•è¯â€Zwingmannâ€ å’Œ â€œsheepmeatâ€ç»§ç»­è¢«åˆ‡åˆ†æˆäº†3ä¸ªsubtokens</p>
<h3 id="3-3-è§£å†³subtokenså¯¹é½é—®é¢˜"><a href="#3-3-è§£å†³subtokenså¯¹é½é—®é¢˜" class="headerlink" title="3.3 è§£å†³subtokenså¯¹é½é—®é¢˜"></a>3.3 è§£å†³subtokenså¯¹é½é—®é¢˜</h3><p>&#8195;&#8195;ç”±äºæ ‡æ³¨æ•°æ®é€šå¸¸æ˜¯åœ¨wordçº§åˆ«è¿›è¡Œæ ‡æ³¨çš„ï¼Œæ—¢ç„¶wordè¿˜ä¼šè¢«åˆ‡åˆ†æˆsubtokensï¼Œé‚£ä¹ˆæ„å‘³ç€æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œsubtokensçš„å¯¹é½ã€‚åŒæ—¶ï¼Œç”±äºé¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼çš„è¦æ±‚ï¼Œå¾€å¾€è¿˜éœ€è¦åŠ ä¸Šä¸€äº›ç‰¹æ®Šç¬¦å·æ¯”å¦‚ï¼š [CLS] å’Œ  [SEP]ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ä½¿ç”¨word_idsæ–¹æ³•è§£å†³subtokenså¯¹é½é—®é¢˜</span></span><br><span class="line"><span class="built_in">print</span>(tokenized_input.word_ids())</span><br></pre></td></tr></table></figure>
<pre><code>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
</code></pre><p>&#8195;&#8195;word_idså°†æ¯ä¸€ä¸ªsubtokensä½ç½®éƒ½å¯¹åº”äº†ä¸€ä¸ªwordçš„ä¸‹æ ‡ã€‚æ¯”å¦‚ç¬¬1ä¸ªä½ç½®å¯¹åº”ç¬¬0ä¸ªwordï¼Œç„¶åç¬¬2ã€3ä¸ªä½ç½®å¯¹åº”ç¬¬1ä¸ªwordã€‚ç‰¹æ®Šå­—ç¬¦å¯¹åº”äº†Noneã€‚æœ‰äº†è¿™ä¸ªlistï¼Œæˆ‘ä»¬å°±èƒ½å°†subtokenså’Œwordsè¿˜æœ‰æ ‡æ³¨çš„labelså¯¹é½å•¦ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è·å–subtokensä½ç½®</span></span><br><span class="line">word_ids = tokenized_input.word_ids()</span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†subtokensã€wordså’Œæ ‡æ³¨çš„labelså¯¹é½</span></span><br><span class="line">aligned_labels = [</span><br><span class="line">    -<span class="number">100</span> <span class="keyword">if</span> i <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(aligned_labels), <span class="built_in">len</span>(tokenized_input[<span class="string">&quot;input_ids&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="number">39</span> <span class="number">39</span><span class="comment">#è¾“å‡ºç»“æœ</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;æˆ‘ä»¬é€šå¸¸å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100ï¼Œåœ¨æ¨¡å‹ä¸­-100é€šå¸¸ä¼šè¢«å¿½ç•¥æ‰ä¸è®¡ç®—loss    </p>
<p>ä¸¤ç§å¯¹é½labelçš„æ–¹å¼ï¼š</p>
<ul>
<li>å¤šä¸ªsubtokenså¯¹é½ä¸€ä¸ªwordï¼Œå¯¹é½ä¸€ä¸ªlabel</li>
<li>å¤šä¸ªsubtokensçš„ç¬¬ä¸€ä¸ªsubtokenå¯¹é½wordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼Œå…¶ä»–subtokensç›´æ¥èµ‹äºˆ-100.<br>ä»¥ä¸Šä¸¤ç§æ–¹å¼é€šè¿‡label_all_tokens = Trueåˆ‡æ¢<h3 id="3-4-æ•´åˆé¢„å¤„ç†å‡½æ•°"><a href="#3-4-æ•´åˆé¢„å¤„ç†å‡½æ•°" class="headerlink" title="3.4 æ•´åˆé¢„å¤„ç†å‡½æ•°"></a>3.4 æ•´åˆé¢„å¤„ç†å‡½æ•°</h3>å°†ä»¥ä¸Šæ‰€æœ‰å†…å®¹åˆèµ·æ¥å˜æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°ï¼Œis_split_into_words=Trueåœ¨ä¸Šé¢å·²ç»ç»“æŸå•¦ï¼ˆï¼Ÿï¼‰<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_all_tokens = <span class="literal">True</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_align_labels</span>(<span class="params">examples</span>):</span></span><br><span class="line">    tokenized_inputs = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;tokens&quot;</span>], truncation=<span class="literal">True</span>, is_split_into_words=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]):</span><br><span class="line">        <span class="comment"># è·å–subtokensä½ç½®</span></span><br><span class="line">        word_ids = tokenized_inputs.word_ids(batch_index=i)</span><br><span class="line">        previous_word_idx = <span class="literal">None</span></span><br><span class="line">        label_ids = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># éå†subtokensä½ç½®ç´¢å¼•</span></span><br><span class="line">        <span class="keyword">for</span> word_idx <span class="keyword">in</span> word_ids:</span><br><span class="line">            <span class="keyword">if</span> word_idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100</span></span><br><span class="line">                label_ids.append(-<span class="number">100</span>)</span><br><span class="line">            <span class="comment"># We set the label for the first token of each word.</span></span><br><span class="line">            <span class="keyword">elif</span> word_idx != previous_word_idx:</span><br><span class="line">                label_ids.append(label[word_idx])</span><br><span class="line">            <span class="comment"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span></span><br><span class="line">            <span class="comment"># the label_all_tokens flag.</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_ids.append(label[word_idx] <span class="keyword">if</span> label_all_tokens <span class="keyword">else</span> -<span class="number">100</span>)</span><br><span class="line">            previous_word_idx = word_idx</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å¯¹é½word</span></span><br><span class="line">        labels.append(label_ids)</span><br><span class="line"></span><br><span class="line">    tokenized_inputs[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">    <span class="keyword">return</span> tokenized_inputs</span><br></pre></td></tr></table></figure>
ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesï¼ˆè¿”å›å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistï¼‰<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenize_and_align_labels(datasets[<span class="string">&#x27;train&#x27;</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
  {â€˜input_idsâ€™: [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], â€˜attention_maskâ€™: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], â€˜labelsâ€™: [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}</li>
</ul>
<h3 id="3-5-å¯¹æ•°æ®é›†datasetsæ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†"><a href="#3-5-å¯¹æ•°æ®é›†datasetsæ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†" class="headerlink" title="3.5 å¯¹æ•°æ®é›†datasetsæ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†"></a>3.5 å¯¹æ•°æ®é›†datasetsæ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†</h3><p>&#8195;&#8195;ä½¿ç”¨mapå‡½æ•°å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(tokenize_and_align_labels, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-fa2382f441f8d16d.arrow
Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-8057d57320e0ee7a.arrow
Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-ea32e2b3f93b1edb.arrow
</code></pre><p>&#8195;&#8195;è¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨load_from_cache_file=Falseå‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„batched=Trueè¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œä»¥ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚</p>
<h2 id="4-å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹"><a href="#4-å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹" class="headerlink" title="4 å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹"></a>4 å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</h2><p>&#8195;&#8195;æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä½¿ç”¨AutoModelForSequenceClassification è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œfrom_pretrainedæ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚</p>
<h3 id="4-1-åŠ è½½åˆ†ç±»æ¨¡å‹"><a href="#4-1-åŠ è½½åˆ†ç±»æ¨¡å‹" class="headerlink" title="4.1 åŠ è½½åˆ†ç±»æ¨¡å‹"></a>4.1 åŠ è½½åˆ†ç±»æ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(</span><br><span class="line">    model_checkpoint, num_labels=<span class="built_in">len</span>(label_list))</span><br></pre></td></tr></table></figure>
<pre><code>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: [&#39;vocab_layer_norm.weight&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_layer_norm.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_transform.weight&#39;]
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>&#8195;&#8195;ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†æ–‡æœ¬åˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰</p>
<h3 id="4-2-è®¾å®šè®­ç»ƒå‚æ•°"><a href="#4-2-è®¾å®šè®­ç»ƒå‚æ•°" class="headerlink" title="4.2 è®¾å®šè®­ç»ƒå‚æ•°"></a>4.2 è®¾å®šè®­ç»ƒå‚æ•°</h3><p>&#8195;&#8195;==Traineræ˜¯ä¸€ä¸ªç®€å•ä½†åŠŸèƒ½å®Œæ•´çš„ PyTorch è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼Œé’ˆå¯¹ ğŸ¤— Transformers è¿›è¡Œäº†ä¼˜åŒ–==ã€‚Trainerè®­ç»ƒå·¥å…·éœ€è¦3ä¸ªè¦ç´ æ¨¡å‹ã€æ•°æ®é›†å’Œè®­ç»ƒå‚æ•°ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Trainer(</span><br><span class="line">    model,<span class="comment">#å¦‚æœä½¿ç”¨transformeræ¨¡å‹ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªtransformers.PreTrainedModelç±»çš„å­ç±»</span></span><br><span class="line">    args,<span class="comment">#è®­ç»ƒå‚æ•°</span></span><br><span class="line">    data_collator,</span><br><span class="line">    train_dataset,<span class="comment">#è®­ç»ƒé›†</span></span><br><span class="line">    eval_dataset,<span class="comment">#æµ‹è¯•é›†</span></span><br><span class="line">    tokenizer,<span class="comment">#åˆ†è¯å™¨</span></span><br><span class="line">    compute_metrics,<span class="comment">#è¯„æµ‹æ–¹å¼ï¼Œè¯„ä¼°æ—¶è®¡ç®—æ–¹å¼çš„å‡½æ•°</span></span><br><span class="line">    model_init: <span class="type">Callable</span>[[], transformers.modeling_utils.PreTrainedModel] = <span class="literal">None</span>,</span><br><span class="line">    callbacks: <span class="type">Union</span>[<span class="type">List</span>[transformers.trainer_callback.TrainerCallback], NoneType] = <span class="literal">None</span>,<span class="comment">#å›è°ƒå‡½æ•°ï¼Œç”¨äºä¿å­˜æœ€ä¼˜æ¨¡å‹å‚æ•°</span></span><br><span class="line">    optimizers: <span class="type">Tuple</span>[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (<span class="literal">None</span>,<span class="literal">None</span>) )<span class="comment">#ä¼˜åŒ–å™¨</span></span><br></pre></td></tr></table></figure><br>Traineræœ€é‡è¦çš„æ˜¯è®­ç»ƒå‚æ•° TrainingArgumentsã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">f&quot;test-<span class="subst">&#123;task&#125;</span>&quot;</span>,</span><br><span class="line">    <span class="comment"># æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°</span></span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    log_level=<span class="string">&#x27;error&#x27;</span>,</span><br><span class="line">    logging_strategy=<span class="string">&quot;no&quot;</span>,</span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;æœ€åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†æˆ‘ä»¬å¤„ç†å¥½çš„è¾“å…¥å–‚ç»™æ¨¡å‹ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForTokenClassification</span><br><span class="line"><span class="comment"># æ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºå°†å¤„ç†å¥½çš„æ•°æ®è¾“å…¥ç»™æ¨¡å‹</span></span><br><span class="line">data_collator = DataCollatorForTokenClassification(tokenizer)</span><br></pre></td></tr></table></figure></p>
<h3 id="4-3-è®¾å®šè¯„ä¼°æ–¹æ³•"><a href="#4-3-è®¾å®šè¯„ä¼°æ–¹æ³•" class="headerlink" title="4.3 è®¾å®šè¯„ä¼°æ–¹æ³•"></a>4.3 è®¾å®šè¯„ä¼°æ–¹æ³•</h3><p>&#8195;&#8195;æˆ‘ä»¬ä½¿ç”¨seqeval metricæ¥å®Œæˆè¯„ä¼°ã€‚å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåšä¸€äº›æ•°æ®åå¤„ç†ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric = load_metric(<span class="string">&quot;seqeval&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è¯„ä¼°çš„è¾“å…¥æ˜¯é¢„æµ‹å’Œlabelçš„list</span></span><br><span class="line">labels = [label_list[i] <span class="keyword">for</span> i <span class="keyword">in</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]]</span><br><span class="line">metric.compute(predictions=[labels], references=[labels])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;LOC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">2</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;ORG&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;PER&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;overall_precision&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_recall&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_f1&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_accuracy&#x27;</span>: <span class="number">1.0</span>&#125;</span><br></pre></td></tr></table></figure>
<p>å¯¹æ¨¡å‹é¢„æµ‹ç»“æœåšä¸€äº›åå¤„ç†ï¼š</p>
<ul>
<li>é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡</li>
<li>å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabel</li>
<li>å¿½ç•¥-100æ‰€åœ¨åœ°æ–¹<br>ä¸‹é¢çš„å‡½æ•°å°†ä¸Šé¢çš„æ­¥éª¤åˆå¹¶äº†èµ·æ¥ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">p</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;æ¨¡å‹é¢„æµ‹&quot;&quot;&quot;</span></span><br><span class="line">    predictions, labels = p</span><br><span class="line">    <span class="comment"># é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡</span></span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabelï¼Œå¹¶å¿½ç•¥-100çš„ä½ç½®</span></span><br><span class="line">    true_predictions = [</span><br><span class="line">        [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line">    true_labels = [</span><br><span class="line">        [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;precision&quot;</span>: results[<span class="string">&quot;overall_precision&quot;</span>],</span><br><span class="line">        <span class="string">&quot;recall&quot;</span>: results[<span class="string">&quot;overall_recall&quot;</span>],</span><br><span class="line">        <span class="string">&quot;f1&quot;</span>: results[<span class="string">&quot;overall_f1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;accuracy&quot;</span>: results[<span class="string">&quot;overall_accuracy&quot;</span>],</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;æˆ‘ä»¬è®¡ç®—æ‰€æœ‰ç±»åˆ«æ€»çš„precision/recall/f1ï¼Œæ‰€ä»¥ä¼šæ‰”æ‰å•ä¸ªç±»åˆ«çš„precision/recall/f1</p>
<p>æ„é€ è®­ç»ƒå™¨Trainer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ„é€ è®­ç»ƒå™¨Trainerï¼Œå°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥Trainer</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="4-4-è®­ç»ƒæ¨¡å‹"><a href="#4-4-è®­ç»ƒæ¨¡å‹" class="headerlink" title="4.4 è®­ç»ƒæ¨¡å‹"></a>4.4 è®­ç»ƒæ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>  <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress><br>  [2634/2634 02:13, Epoch 3/3]<br>&lt;/div&gt;</p>
<p><table border="0" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
    <tr>
      <td>3</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
  </tbody>
</table><p></p>
<pre><code>TrainOutput(global_step=2634, training_loss=0.02493840813546264, metrics=&#123;&#39;train_runtime&#39;: 133.2372, &#39;train_samples_per_second&#39;: 316.151, &#39;train_steps_per_second&#39;: 19.769, &#39;total_flos&#39;: 511610930296956.0, &#39;train_loss&#39;: 0.02493840813546264, &#39;epoch&#39;: 3.0&#125;)
</code></pre><h3 id="4-5-æ¨¡å‹è¯„ä¼°"><a href="#4-5-æ¨¡å‹è¯„ä¼°" class="headerlink" title="4.5 æ¨¡å‹è¯„ä¼°"></a>4.5 æ¨¡å‹è¯„ä¼°</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure>
<div>

  <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [204/204 00:07]
</div>

<pre><code>&#123;&#39;eval_loss&#39;: 0.06285537779331207,
 &#39;eval_precision&#39;: 0.9257950530035336,
 &#39;eval_recall&#39;: 0.9379125181787672,
 &#39;eval_f1&#39;: 0.931814392886913,
 &#39;eval_accuracy&#39;: 0.983843550923793,
 &#39;eval_runtime&#39;: 3.8895,
 &#39;eval_samples_per_second&#39;: 835.586,
 &#39;eval_steps_per_second&#39;: 52.449,
 &#39;epoch&#39;: 3.0&#125;
</code></pre><h3 id="4-6-è¾“å‡ºå•ä¸ªç±»åˆ«çš„precision-recall-f1"><a href="#4-6-è¾“å‡ºå•ä¸ªç±»åˆ«çš„precision-recall-f1" class="headerlink" title="4.6 è¾“å‡ºå•ä¸ªç±»åˆ«çš„precision/recall/f1"></a>4.6 è¾“å‡ºå•ä¸ªç±»åˆ«çš„precision/recall/f1</h3><p>&emsp;&emsp;å¦‚æœæƒ³è¦å¾—åˆ°å•ä¸ªç±»åˆ«çš„precision/recall/f1ï¼Œæˆ‘ä»¬ç›´æ¥å°†ç»“æœè¾“å…¥ç›¸åŒçš„è¯„ä¼°å‡½æ•°å³å¯ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions, labels, _ = trainer.predict(tokenized_datasets[<span class="string">&quot;validation&quot;</span>])</span><br><span class="line">predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove ignored index (special tokens)</span></span><br><span class="line">true_predictions = [</span><br><span class="line">    [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line">true_labels = [</span><br><span class="line">    [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">results</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;LOC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.9513574660633484</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9637127578304049</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.9574952561669828</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">2618</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;MISC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.8107255520504731</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.8350934199837531</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.8227290916366548</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">1231</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;ORG&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.8882575757575758</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9124513618677043</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.9001919385796545</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">2056</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;PER&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.9778439153439153</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9746209624258405</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.976229778804886</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">3034</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;overall_precision&#x27;</span>: <span class="number">0.9257950530035336</span>,</span><br><span class="line">  <span class="string">&#x27;overall_recall&#x27;</span>: <span class="number">0.9379125181787672</span>,</span><br><span class="line">  <span class="string">&#x27;overall_f1&#x27;</span>: <span class="number">0.931814392886913</span>,</span><br><span class="line">  <span class="string">&#x27;overall_accuracy&#x27;</span>: <span class="number">0.983843550923793</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="5-æ€»ç»“"><a href="#5-æ€»ç»“" class="headerlink" title="5 æ€»ç»“"></a>5 æ€»ç»“</h2><p>&emsp;&emsp;æœ¬æ¬¡ä»»åŠ¡ï¼Œä¸»è¦ä»‹ç»äº†ç”¨BERTæ¨¡å‹è§£å†³åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„æ–¹æ³•åŠæ­¥éª¤ï¼Œæ­¥éª¤ä¸»è¦åˆ†ä¸ºåŠ è½½æ•°æ®ã€æ•°æ®é¢„å¤„ç†ã€å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨åŠ è½½æ•°æ®é˜¶æ®µä¸­ï¼Œä½¿ç”¨CONLL 2003 datasetæ•°æ®é›†ï¼Œå¹¶è§‚å¯Ÿå®ä½“ç±»åˆ«åŠè¡¨ç¤ºå½¢å¼ï¼›åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µä¸­ï¼Œå¯¹tokenizeråˆ†è¯å™¨çš„å»ºæ¨¡ï¼Œå°†subtokensã€wordså’Œæ ‡æ³¨çš„labelså¯¹é½ï¼Œå¹¶å®Œæˆæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„é¢„å¤„ç†ï¼›åœ¨å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹é˜¶æ®µï¼Œé€šè¿‡å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œè®¾ç½®seqevalè¯„ä¼°æ–¹æ³•ï¼Œå¹¶æ„å»ºTrainnerè®­ç»ƒå™¨ï¼Œè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¯¹precisionã€recallå’Œf1å€¼è¿›è¡Œè¯„ä¼°æ¯”è¾ƒã€‚<br>&emsp;&emsp;å…¶ä¸­åœ¨æ•°æ®é›†ä¸‹è½½æ—¶ï¼Œéœ€è¦ä½¿ç”¨å¤–ç½‘æ–¹å¼å»ºç«‹ä»£ç†ã€‚</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/26/8æœˆç»„é˜Ÿå­¦ä¹ ï¼šnlpä¹‹transformerså…¥é—¨/task7ï¼šTransformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡/">https://zhxnlp.github.io/2021/08/26/8æœˆç»„é˜Ÿå­¦ä¹ ï¼šnlpä¹‹transformerså…¥é—¨/task7ï¼šTransformersè§£æåºåˆ—æ ‡æ³¨ä»»åŠ¡/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://zhxnlp.github.io">zhxnlpã®Blog</a>ï¼</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/27/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/%E8%8B%8F%E7%A5%9E%E6%96%87%E7%AB%A0%E8%A7%A3%E6%9E%90%EF%BC%886%E7%AF%87%EF%BC%89/"><i class="fa fa-chevron-left">  </i><span>è‹ç¥æ–‡ç« è§£æ</span></a></div><div class="next-post pull-right"><a href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task6%EF%BC%9ATransformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/"><span>task6ï¼šBERTæ–‡æœ¬åˆ†ç±»</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>é©±åŠ¨ - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>ä¸»é¢˜ - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>