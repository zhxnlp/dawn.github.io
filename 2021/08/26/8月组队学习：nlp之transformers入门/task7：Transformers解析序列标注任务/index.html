<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="task7：BERT token分类"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>task7：BERT token分类 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.</span> <span class="toc-text">Transformers解析序列标注任务</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1 序列标注任务简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.</span> <span class="toc-text">2 加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">3 预处理数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 数据预处理流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%AF%B9%E5%BA%94%E7%9A%84tokenizer"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 构建模型对应的tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E8%A7%A3%E5%86%B3subtokens%E5%AF%B9%E9%BD%90%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 解决subtokens对齐问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%95%B4%E5%90%88%E9%A2%84%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 整合预处理函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86datasets%E6%89%80%E6%9C%89%E6%A0%B7%E6%9C%AC%E8%BF%9B%E8%A1%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 对数据集datasets所有样本进行预处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">4 微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8A%A0%E8%BD%BD%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 加载分类模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 设定训练参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%BE%E5%AE%9A%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 设定评估方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 模型评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E8%BE%93%E5%87%BA%E5%8D%95%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84precision-recall-f1"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 输出单个类别的precision&#x2F;recall&#x2F;f1</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">5 总结</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">49</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">task7：BERT token分类</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">5.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 25 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Transformers解析序列标注任务"><a href="#Transformers解析序列标注任务" class="headerlink" title="Transformers解析序列标注任务"></a>Transformers解析序列标注任务</h1><p>本文主要来自datawhale的<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.2-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8">transformer教程4.2</a>和<a target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning/#/transformers_nlp28/task07">天国之影学习笔记</a>。</p>
<h2 id="1-序列标注任务简介"><a href="#1-序列标注任务简介" class="headerlink" title="1 序列标注任务简介"></a>1 序列标注任务简介</h2><ul>
<li>序列标注可以看作时token级别的分类问题，为文本中的每一个token预测一个标签</li>
<li><p>token级别的分类任务：</p>
<ol>
<li>NER（Named-entity recognition 名词-实体识别）分辨出文本中的名词和实体（person人名, organization组织机构名, location地点名…）</li>
<li>POS（Part-of-speech tagging词性标注）根据语法对token进行词性标注（noun名词、verb动词、adjective形容词…）</li>
<li>Chunk（Chunking短语组块）将同一个短语的tokens组块放在一起</li>
</ol>
<span id="more"></span>
<p>&#8195;&#8195;只要预训练的transformer模型最顶层有一个token分类的神经网络层（比如上一篇章提到的BertForTokenClassification,需要对应的预训练模型有fast tokenizer这个功能，参考<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/index.html#bigtable">这个表</a>），那么本notebook理论上可以使用各种各样的transformer模型（<a target="_blank" rel="noopener" href="https://huggingface.co/models">模型面板</a>），解决任何token级别的分类任务。</p>
</li>
</ul>
<p>&#8195;&#8195;如果您所处理的任务有所不同，大概率只需要很小的改动便可以使用本notebook进行处理。同时，您应该根据您的GPU显存来调整微调训练所需要的btach size大小，避免显存溢出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置分类任务</span></span><br><span class="line">task = <span class="string">&quot;ner&quot;</span> </span><br><span class="line"><span class="comment"># 设置BERT模型</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line"><span class="comment"># 根据GPU调整batch_size大小，避免显存溢出</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br></pre></td></tr></table></figure>
<h2 id="2-加载数据"><a href="#2-加载数据" class="headerlink" title="2 加载数据"></a>2 加载数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载数据和评测方式</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;本文使用的是<a target="_blank" rel="noopener" href="https://aclanthology.org/W03-0419.pdf">CONLL 2003 dataset</a>数据集。来处理Datasets库中的任何token分类任务。如果要加载自定义的json/csv文件数据集，可以参考<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files">数据集文档</a>来学习如何加载。自定义数据集可能需要在加载属性名字上做一些调整<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载conll2003数据集</span></span><br><span class="line">datasets = load_dataset(<span class="string">&quot;conll2003&quot;</span>)</span><br></pre></td></tr></table></figure><br>    Reusing dataset conll2003 (C:\Users\hurui.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets</span><br></pre></td></tr></table></figure>
<p>datasets对象本身是一种DatasetDict数据结构。可以使用对应的key得到相应的数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    DatasetDict(&#123;</span><br><span class="line">        train: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">14041</span></span><br><span class="line">        &#125;)</span><br><span class="line">        validation: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">3250</span></span><br><span class="line">        &#125;)</span><br><span class="line">        test: Dataset(&#123;</span><br><span class="line">            features: [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>],</span><br><span class="line">            num_rows: <span class="number">3453</span></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="comment">#label列对应tokens的标注</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看训练集第一条数据</span></span><br><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;tokens&#x27;</span>: [<span class="string">&#x27;EU&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rejects&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;German&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;call&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;to&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;boycott&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;British&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lamb&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;pos_tags&#x27;</span>: [<span class="number">22</span>, <span class="number">42</span>, <span class="number">16</span>, <span class="number">21</span>, <span class="number">35</span>, <span class="number">37</span>, <span class="number">16</span>, <span class="number">21</span>, <span class="number">7</span>],</span><br><span class="line"> <span class="string">&#x27;chunk_tags&#x27;</span>: [<span class="number">11</span>, <span class="number">21</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">0</span>],</span><br><span class="line"> <span class="string">&#x27;ner_tags&#x27;</span>: [<span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;所有的数据标签labels都已经被编码成了整数，可以直接被预训练transformer模型使用。这些整数的编码所对应的实际类别储存在features中。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看features属性</span></span><br><span class="line">datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;ner_tags&quot;</span>]</span><br></pre></td></tr></table></figure><br>    Sequence(feature=ClassLabel(num_classes=9, names=[‘O’, ‘B-PER’, ‘I-PER’, ‘B-ORG’, ‘I-ORG’, ‘B-LOC’, ‘I-LOC’, ‘B-MISC’, ‘I-MISC’], names_file=None, id=None), length=-1, id=None)</p>
<p>&#8195;&#8195;以NER为例，0对应的标签类别是”O“， 1对应的是”B-PER“等等。具体标签含义对应如下：</p>
<ul>
<li>PER：person</li>
<li>ORG：organization</li>
<li>LOC：location</li>
<li>MISC：miscellaneous</li>
<li>O：没有特别实体（no special entity）</li>
<li>B-*：实体开始的token</li>
<li>I-*：实体中间的token</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_list = datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>].feature.names</span><br><span class="line">label_list</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</code></pre><p>定义下面的函数，从数据集里随机选择几个例子进行展示<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel, <span class="type">Sequence</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从数据集中随机选择几条数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(typ, <span class="type">Sequence</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(typ.feature, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> x: [typ.feature.names[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(datasets[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure></p>
<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4143</td>
      <td>[The, 85-year-old, nun, said, in, the, past, that, she, was, praying, for, the, couple, ,, whose, divorce, is, expected, to, become, final, next, week, .]</td>
      <td>[DT, JJ, NN, VBD, IN, DT, NN, IN, PRP, VBD, VBG, IN, DT, NN, ,, WP\$, NN, VBZ, VBN, TO, VB, JJ, JJ, NN, .]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-PP, B-NP, I-NP, B-SBAR, B-NP, B-VP, I-VP, B-PP, B-NP, I-NP, O, B-NP, I-NP, B-VP, I-VP, I-VP, I-VP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2442</td>
      <td>[2., Marie-Jose, Perec, (, France, ), 49.72]</td>
      <td>[CD, NNP, NNP, (, NNP, ), CD]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, O, B-NP]</td>
      <td>[O, B-PER, I-PER, O, B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1090</td>
      <td>[There, were, no, significant, differences, between, the, groups, receiving, garlic, and, placebo, ,, ", they, wrote, in, the, Journal, of, the, Royal, College, of, Physicians, .]</td>
      <td>[EX, VBD, DT, JJ, NNS, IN, DT, NNS, VBG, NN, CC, NN, ,, ", PRP, VBD, IN, DT, NNP, IN, DT, NNP, NNP, IN, NNPS, .]</td>
      <td>[B-NP, B-VP, B-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-VP, B-NP, I-NP, I-NP, O, O, B-NP, B-VP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1972</td>
      <td>[Pakistan, first, innings]</td>
      <td>[NNP, RB, NN]</td>
      <td>[B-NP, B-ADVP, B-NP]</td>
      <td>[B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13714</td>
      <td>[The, Taiwan, dollar, closed, slightly, firmer, on, Thursday, amid, tight, Taiwan, dollar, liquidity, in, the, banking, system, ,, and, dealers, said, the, rate, was, likely, to, move, narrowly, in, the, near, term, .]</td>
      <td>[DT, NNP, NN, VBD, RB, JJR, IN, NNP, IN, JJ, NNP, NN, NN, IN, DT, NN, NN, ,, CC, NNS, VBD, DT, NN, VBD, JJ, TO, VB, RB, IN, DT, JJ, NN, .]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-ADVP, B-ADJP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, O, O, B-NP, B-VP, B-NP, I-NP, B-VP, B-ADJP, B-VP, I-VP, I-VP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, B-LOC, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4806</td>
      <td>[nine, of, the, superbike, world, championship, on, Sunday, :]</td>
      <td>[CD, IN, DT, JJ, NN, NN, IN, NNP, :]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7452</td>
      <td>[The, accident, happened, when, the, Sanchez, Zarraga, family, took, their, boat, out, for, a, nighttime, spin, ,, Civil, Defence, and, Coast, Guard, officials, said, .]</td>
      <td>[DT, NN, VBD, WRB, DT, NNP, NNP, NN, VBD, PRP\$, NN, RP, IN, DT, NN, NN, ,, NNP, NN, CC, NNP, NNP, NNS, VBD, .]</td>
      <td>[B-NP, I-NP, B-VP, B-ADVP, B-NP, I-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-ADVP, B-PP, B-NP, I-NP, I-NP, O, B-NP, I-NP, O, B-NP, I-NP, I-NP, B-VP, O]</td>
      <td>[O, O, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2332</td>
      <td>[7., Julie, Baumann, (, Switzerland, ), 13.36]</td>
      <td>[NNP, NNP, NNP, (, NNP, ), CD]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, O, B-NP]</td>
      <td>[O, B-PER, I-PER, O, B-LOC, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9786</td>
      <td>[The, pilot, said, several, hijackers, appeared, to, be, placed, around, the, plane, .]</td>
      <td>[DT, NN, VBD, JJ, NNS, VBD, TO, VB, VBN, IN, DT, NN, .]</td>
      <td>[B-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, I-VP, I-VP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3451</td>
      <td>[(, 7-4, ), 6-2]</td>
      <td>[(, CD, ), CD]</td>
      <td>[B-LST, B-NP, O, B-NP]</td>
      <td>[O, O, O, O]</td>
    </tr>
  </tbody>
</table>

<h2 id="3-预处理数据"><a href="#3-预处理数据" class="headerlink" title="3 预处理数据"></a>3 预处理数据</h2><h3 id="3-1-数据预处理流程"><a href="#3-1-数据预处理流程" class="headerlink" title="3.1 数据预处理流程"></a>3.1 数据预处理流程</h3><ul>
<li>数据预处理工具：Tokenizer</li>
<li>流程：<ol>
<li>对输入数据进行tokenize，得到tokens</li>
<li>将tokens转化为预训练模型中需要对应的token ID</li>
<li>将token ID转化为模型需要的输入格式<br>&#8195;&#8195;为了达到数据预处理的目的，我们使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这样可以确保：</li>
</ol>
</li>
<li>我们得到一个与预训练模型一一对应的tokenizer。</li>
<li>使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。</li>
<li>这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载<h3 id="3-2-构建模型对应的tokenizer"><a href="#3-2-构建模型对应的tokenizer" class="headerlink" title="3.2 构建模型对应的tokenizer"></a>3.2 构建模型对应的tokenizer</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
&#8195;&#8195;以下代码要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。在这里<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/index.html#bigtable">big table of models</a>查看模型是否有fast tokenizer。<br>&#8195;&#8195;tokenizer既可以对单个文本进行预处理，也可以对一对文本进行预处理，tokenizer预处理后得到的数据满足预训练模型输入格式<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="comment"># 模型使用的时fast tokenizer</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(tokenizer, transformers.PreTrainedTokenizerFast)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this is one sentence!&quot;</span>)</span><br></pre></td></tr></table></figure>
  {‘input_ids’: [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], ‘attention_mask’: [1, 1, 1, 1, 1, 1, 1, 1, 1]}</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer([<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;,&quot;</span>, <span class="string">&quot;this&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;sentence&quot;</span>, <span class="string">&quot;split&quot;</span>,</span><br><span class="line">          <span class="string">&quot;into&quot;</span>, <span class="string">&quot;words&quot;</span>, <span class="string">&quot;.&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre><p><strong>补充：</strong></p>
<ul>
<li>transformer预训练模型会将切分后的word，继续用tokenizer分词器切分为subword<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(example[<span class="string">&quot;tokens&quot;</span>])</span><br></pre></td></tr></table></figure>
  [‘Germany’, “‘s”, ‘representative’, ‘to’, ‘the’, ‘European’, ‘Union’, “‘s”, ‘veterinary’, ‘committee’, ‘Werner’, ‘Zwingmann’, ‘said’, ‘on’, ‘Wednesday’, ‘consumers’, ‘should’, ‘buy’, ‘sheepmeat’, ‘from’, ‘countries’, ‘other’, ‘than’, ‘Britain’, ‘until’, ‘the’, ‘scientific’, ‘advice’, ‘was’, ‘clearer’, ‘.’]</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_input = tokenizer(example[<span class="string">&quot;tokens&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br><span class="line">tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;[CLS]&#39;, &#39;germany&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;representative&#39;, &#39;to&#39;, &#39;the&#39;, &#39;european&#39;, &#39;union&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;veterinary&#39;, &#39;committee&#39;, &#39;werner&#39;, &#39;z&#39;, &#39;##wing&#39;, &#39;##mann&#39;, &#39;said&#39;, &#39;on&#39;, &#39;wednesday&#39;, &#39;consumers&#39;, &#39;should&#39;, &#39;buy&#39;, &#39;sheep&#39;, &#39;##me&#39;, &#39;##at&#39;, &#39;from&#39;, &#39;countries&#39;, &#39;other&#39;, &#39;than&#39;, &#39;britain&#39;, &#39;until&#39;, &#39;the&#39;, &#39;scientific&#39;, &#39;advice&#39;, &#39;was&#39;, &#39;clearer&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</code></pre><p>单词”Zwingmann” 和 “sheepmeat”继续被切分成了3个subtokens</p>
<h3 id="3-3-解决subtokens对齐问题"><a href="#3-3-解决subtokens对齐问题" class="headerlink" title="3.3 解决subtokens对齐问题"></a>3.3 解决subtokens对齐问题</h3><p>&#8195;&#8195;由于标注数据通常是在word级别进行标注的，既然word还会被切分成subtokens，那么意味着我们还需要对标注数据进行subtokens的对齐。同时，由于预训练模型输入格式的要求，往往还需要加上一些特殊符号比如： [CLS] 和  [SEP]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用word_ids方法解决subtokens对齐问题</span></span><br><span class="line"><span class="built_in">print</span>(tokenized_input.word_ids())</span><br></pre></td></tr></table></figure>
<pre><code>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
</code></pre><p>&#8195;&#8195;word_ids将每一个subtokens位置都对应了一个word的下标。比如第1个位置对应第0个word，然后第2、3个位置对应第1个word。特殊字符对应了None。有了这个list，我们就能将subtokens和words还有标注的labels对齐啦。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取subtokens位置</span></span><br><span class="line">word_ids = tokenized_input.word_ids()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将subtokens、words和标注的labels对齐</span></span><br><span class="line">aligned_labels = [</span><br><span class="line">    -<span class="number">100</span> <span class="keyword">if</span> i <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(aligned_labels), <span class="built_in">len</span>(tokenized_input[<span class="string">&quot;input_ids&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="number">39</span> <span class="number">39</span><span class="comment">#输出结果</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;我们通常将特殊字符的label设置为-100，在模型中-100通常会被忽略掉不计算loss    </p>
<p>两种对齐label的方式：</p>
<ul>
<li>多个subtokens对齐一个word，对齐一个label</li>
<li>多个subtokens的第一个subtoken对齐word，对齐一个label，其他subtokens直接赋予-100.<br>以上两种方式通过label_all_tokens = True切换<h3 id="3-4-整合预处理函数"><a href="#3-4-整合预处理函数" class="headerlink" title="3.4 整合预处理函数"></a>3.4 整合预处理函数</h3>将以上所有内容合起来变成我们的预处理函数，is_split_into_words=True在上面已经结束啦（？）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_all_tokens = <span class="literal">True</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_align_labels</span>(<span class="params">examples</span>):</span></span><br><span class="line">    tokenized_inputs = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;tokens&quot;</span>], truncation=<span class="literal">True</span>, is_split_into_words=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]):</span><br><span class="line">        <span class="comment"># 获取subtokens位置</span></span><br><span class="line">        word_ids = tokenized_inputs.word_ids(batch_index=i)</span><br><span class="line">        previous_word_idx = <span class="literal">None</span></span><br><span class="line">        label_ids = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历subtokens位置索引</span></span><br><span class="line">        <span class="keyword">for</span> word_idx <span class="keyword">in</span> word_ids:</span><br><span class="line">            <span class="keyword">if</span> word_idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 将特殊字符的label设置为-100</span></span><br><span class="line">                label_ids.append(-<span class="number">100</span>)</span><br><span class="line">            <span class="comment"># We set the label for the first token of each word.</span></span><br><span class="line">            <span class="keyword">elif</span> word_idx != previous_word_idx:</span><br><span class="line">                label_ids.append(label[word_idx])</span><br><span class="line">            <span class="comment"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span></span><br><span class="line">            <span class="comment"># the label_all_tokens flag.</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_ids.append(label[word_idx] <span class="keyword">if</span> label_all_tokens <span class="keyword">else</span> -<span class="number">100</span>)</span><br><span class="line">            previous_word_idx = word_idx</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对齐word</span></span><br><span class="line">        labels.append(label_ids)</span><br><span class="line"></span><br><span class="line">    tokenized_inputs[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">    <span class="keyword">return</span> tokenized_inputs</span><br></pre></td></tr></table></figure>
以上的预处理函数可以处理一个样本，也可以处理多个样本exapmles（返回多个样本被预处理之后的结果list）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenize_and_align_labels(datasets[<span class="string">&#x27;train&#x27;</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
  {‘input_ids’: [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], ‘attention_mask’: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], ‘labels’: [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}</li>
</ul>
<h3 id="3-5-对数据集datasets所有样本进行预处理"><a href="#3-5-对数据集datasets所有样本进行预处理" class="headerlink" title="3.5 对数据集datasets所有样本进行预处理"></a>3.5 对数据集datasets所有样本进行预处理</h3><p>&#8195;&#8195;使用map函数将预处理函数应用到（map)所有样本上。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(tokenize_and_align_labels, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-fa2382f441f8d16d.arrow
Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-8057d57320e0ee7a.arrow
Loading cached processed dataset at C:\Users\hurui\.cache\huggingface\datasets\conll2003\conll2003\1.0.0\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\cache-ea32e2b3f93b1edb.arrow
</code></pre><p>&#8195;&#8195;返回的结果会自动被缓存，避免下次处理的时候重新计算（但是也要注意，如果输入有改动，可能会被缓存影响！）。datasets库函数会对输入的参数进行检测，判断是否有变化，如果没有变化就使用缓存数据，如果有变化就重新处理。但如果输入参数不变，想改变输入的时候，最好清理调这个缓存。清理的方式是使用load_from_cache_file=False参数。另外，上面使用到的batched=True这个参数是tokenizer的特点，以为这会使用多线程同时并行对输入进行处理。</p>
<h2 id="4-微调预训练模型"><a href="#4-微调预训练模型" class="headerlink" title="4 微调预训练模型"></a>4 微调预训练模型</h2><p>&#8195;&#8195;既然我们是做seq2seq任务，那么我们需要使用AutoModelForSequenceClassification 这个类。和tokenizer相似，from_pretrained方法同样可以帮助我们下载并加载模型，同时也会对模型进行缓存，就不会重复下载模型啦。</p>
<h3 id="4-1-加载分类模型"><a href="#4-1-加载分类模型" class="headerlink" title="4.1 加载分类模型"></a>4.1 加载分类模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(</span><br><span class="line">    model_checkpoint, num_labels=<span class="built_in">len</span>(label_list))</span><br></pre></td></tr></table></figure>
<pre><code>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: [&#39;vocab_layer_norm.weight&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_layer_norm.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_transform.weight&#39;]
- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre><p>&#8195;&#8195;由于我们微调的任务是文本分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了文本分类的神经网络head）</p>
<h3 id="4-2-设定训练参数"><a href="#4-2-设定训练参数" class="headerlink" title="4.2 设定训练参数"></a>4.2 设定训练参数</h3><p>&#8195;&#8195;==Trainer是一个简单但功能完整的 PyTorch 训练和评估循环，针对 🤗 Transformers 进行了优化==。Trainer训练工具需要3个要素模型、数据集和训练参数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Trainer(</span><br><span class="line">    model,<span class="comment">#如果使用transformer模型，它将是一个transformers.PreTrainedModel类的子类</span></span><br><span class="line">    args,<span class="comment">#训练参数</span></span><br><span class="line">    data_collator,</span><br><span class="line">    train_dataset,<span class="comment">#训练集</span></span><br><span class="line">    eval_dataset,<span class="comment">#测试集</span></span><br><span class="line">    tokenizer,<span class="comment">#分词器</span></span><br><span class="line">    compute_metrics,<span class="comment">#评测方式，评估时计算方式的函数</span></span><br><span class="line">    model_init: <span class="type">Callable</span>[[], transformers.modeling_utils.PreTrainedModel] = <span class="literal">None</span>,</span><br><span class="line">    callbacks: <span class="type">Union</span>[<span class="type">List</span>[transformers.trainer_callback.TrainerCallback], NoneType] = <span class="literal">None</span>,<span class="comment">#回调函数，用于保存最优模型参数</span></span><br><span class="line">    optimizers: <span class="type">Tuple</span>[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (<span class="literal">None</span>,<span class="literal">None</span>) )<span class="comment">#优化器</span></span><br></pre></td></tr></table></figure><br>Trainer最重要的是训练参数 TrainingArguments。这个训练设定包含了能够定义训练过程的所有属性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">f&quot;test-<span class="subst">&#123;task&#125;</span>&quot;</span>,</span><br><span class="line">    <span class="comment"># 每个epcoh会做一次验证评估</span></span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    log_level=<span class="string">&#x27;error&#x27;</span>,</span><br><span class="line">    logging_strategy=<span class="string">&quot;no&quot;</span>,</span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;最后我们需要一个数据收集器data collator，将我们处理好的输入喂给模型。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForTokenClassification</span><br><span class="line"><span class="comment"># 数据收集器，用于将处理好的数据输入给模型</span></span><br><span class="line">data_collator = DataCollatorForTokenClassification(tokenizer)</span><br></pre></td></tr></table></figure></p>
<h3 id="4-3-设定评估方法"><a href="#4-3-设定评估方法" class="headerlink" title="4.3 设定评估方法"></a>4.3 设定评估方法</h3><p>&#8195;&#8195;我们使用seqeval metric来完成评估。将模型预测送入评估之前，我们也会做一些数据后处理：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metric = load_metric(<span class="string">&quot;seqeval&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#评估的输入是预测和label的list</span></span><br><span class="line">labels = [label_list[i] <span class="keyword">for</span> i <span class="keyword">in</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]]</span><br><span class="line">metric.compute(predictions=[labels], references=[labels])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;LOC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">2</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;ORG&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;PER&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;overall_precision&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_recall&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_f1&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_accuracy&#x27;</span>: <span class="number">1.0</span>&#125;</span><br></pre></td></tr></table></figure>
<p>对模型预测结果做一些后处理：</p>
<ul>
<li>选择预测分类最大概率的下标</li>
<li>将下标转化为label</li>
<li>忽略-100所在地方<br>下面的函数将上面的步骤合并了起来。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">p</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;模型预测&quot;&quot;&quot;</span></span><br><span class="line">    predictions, labels = p</span><br><span class="line">    <span class="comment"># 选择预测分类最大概率的下标</span></span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将下标转化为label，并忽略-100的位置</span></span><br><span class="line">    true_predictions = [</span><br><span class="line">        [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line">    true_labels = [</span><br><span class="line">        [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;precision&quot;</span>: results[<span class="string">&quot;overall_precision&quot;</span>],</span><br><span class="line">        <span class="string">&quot;recall&quot;</span>: results[<span class="string">&quot;overall_recall&quot;</span>],</span><br><span class="line">        <span class="string">&quot;f1&quot;</span>: results[<span class="string">&quot;overall_f1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;accuracy&quot;</span>: results[<span class="string">&quot;overall_accuracy&quot;</span>],</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;我们计算所有类别总的precision/recall/f1，所以会扔掉单个类别的precision/recall/f1</p>
<p>构造训练器Trainer</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造训练器Trainer，将数据/模型/参数传入Trainer</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="4-4-训练模型"><a href="#4-4-训练模型" class="headerlink" title="4.4 训练模型"></a>4.4 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>  <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress><br>  [2634/2634 02:13, Epoch 3/3]<br>&lt;/div&gt;</p>
<p><table border="0" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
    <tr>
      <td>3</td>
      <td>No log</td>
      <td>0.062855</td>
      <td>0.925795</td>
      <td>0.937913</td>
      <td>0.931814</td>
      <td>0.983844</td>
    </tr>
  </tbody>
</table><p></p>
<pre><code>TrainOutput(global_step=2634, training_loss=0.02493840813546264, metrics=&#123;&#39;train_runtime&#39;: 133.2372, &#39;train_samples_per_second&#39;: 316.151, &#39;train_steps_per_second&#39;: 19.769, &#39;total_flos&#39;: 511610930296956.0, &#39;train_loss&#39;: 0.02493840813546264, &#39;epoch&#39;: 3.0&#125;)
</code></pre><h3 id="4-5-模型评估"><a href="#4-5-模型评估" class="headerlink" title="4.5 模型评估"></a>4.5 模型评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure>
<div>

  <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [204/204 00:07]
</div>

<pre><code>&#123;&#39;eval_loss&#39;: 0.06285537779331207,
 &#39;eval_precision&#39;: 0.9257950530035336,
 &#39;eval_recall&#39;: 0.9379125181787672,
 &#39;eval_f1&#39;: 0.931814392886913,
 &#39;eval_accuracy&#39;: 0.983843550923793,
 &#39;eval_runtime&#39;: 3.8895,
 &#39;eval_samples_per_second&#39;: 835.586,
 &#39;eval_steps_per_second&#39;: 52.449,
 &#39;epoch&#39;: 3.0&#125;
</code></pre><h3 id="4-6-输出单个类别的precision-recall-f1"><a href="#4-6-输出单个类别的precision-recall-f1" class="headerlink" title="4.6 输出单个类别的precision/recall/f1"></a>4.6 输出单个类别的precision/recall/f1</h3><p>&emsp;&emsp;如果想要得到单个类别的precision/recall/f1，我们直接将结果输入相同的评估函数即可：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions, labels, _ = trainer.predict(tokenized_datasets[<span class="string">&quot;validation&quot;</span>])</span><br><span class="line">predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove ignored index (special tokens)</span></span><br><span class="line">true_predictions = [</span><br><span class="line">    [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line">true_labels = [</span><br><span class="line">    [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">results</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;LOC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.9513574660633484</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9637127578304049</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.9574952561669828</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">2618</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;MISC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.8107255520504731</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.8350934199837531</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.8227290916366548</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">1231</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;ORG&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.8882575757575758</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9124513618677043</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.9001919385796545</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">2056</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;PER&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">0.9778439153439153</span>,</span><br><span class="line">   <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.9746209624258405</span>,</span><br><span class="line">   <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.976229778804886</span>,</span><br><span class="line">   <span class="string">&#x27;number&#x27;</span>: <span class="number">3034</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;overall_precision&#x27;</span>: <span class="number">0.9257950530035336</span>,</span><br><span class="line">  <span class="string">&#x27;overall_recall&#x27;</span>: <span class="number">0.9379125181787672</span>,</span><br><span class="line">  <span class="string">&#x27;overall_f1&#x27;</span>: <span class="number">0.931814392886913</span>,</span><br><span class="line">  <span class="string">&#x27;overall_accuracy&#x27;</span>: <span class="number">0.983843550923793</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><p>&emsp;&emsp;本次任务，主要介绍了用BERT模型解决序列标注任务的方法及步骤，步骤主要分为加载数据、数据预处理、微调预训练模型。在加载数据阶段中，使用CONLL 2003 dataset数据集，并观察实体类别及表示形式；在数据预处理阶段中，对tokenizer分词器的建模，将subtokens、words和标注的labels对齐，并完成数据集中所有样本的预处理；在微调预训练模型阶段，通过对模型参数进行设置，设置seqeval评估方法，并构建Trainner训练器，进行模型训练，对precision、recall和f1值进行评估比较。<br>&emsp;&emsp;其中在数据集下载时，需要使用外网方式建立代理。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/26/8月组队学习：nlp之transformers入门/task7：Transformers解析序列标注任务/">https://zhxnlp.github.io/2021/08/26/8月组队学习：nlp之transformers入门/task7：Transformers解析序列标注任务/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/27/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/%E8%8B%8F%E7%A5%9E%E6%96%87%E7%AB%A0%E8%A7%A3%E6%9E%90%EF%BC%886%E7%AF%87%EF%BC%89/"><i class="fa fa-chevron-left">  </i><span>苏神文章解析</span></a></div><div class="next-post pull-right"><a href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task6%EF%BC%9ATransformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/"><span>task6：BERT文本分类</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>