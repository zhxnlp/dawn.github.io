<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="苏神文章解析"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>苏神文章解析 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%8B%8F%E7%A5%9E%E6%96%87%E7%AB%A0%E8%A7%A3%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">苏神文章解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B5%85%E8%B0%88Transformer%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%81%E5%8F%82%E6%95%B0%E5%8C%96%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">1.浅谈Transformer的初始化、参数化与标准化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E9%87%87%E6%A0%B7%E5%88%86%E5%B8%83%EF%BC%9A%E6%88%AA%E5%B0%BE%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1采样分布：截尾正态分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%AD%A3%E4%BA%A4%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9AXavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 正交初始化：Xavier初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%9B%B4%E6%8E%A5%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 直接标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-NTK%E5%8F%82%E6%95%B0%E5%8C%96"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4 NTK参数化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.5 残差连接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.</span> <span class="toc-text">2.模型参数的初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%80%BB%E7%BB%93"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%B8%BA%E5%95%A5%E8%A6%81%E6%AD%A3%E4%BA%A4%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 为啥要正交初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 Xavier初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E8%80%83%E8%99%91%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 考虑激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-BN%E4%B8%BA%E4%BB%80%E4%B9%88%E8%B5%B7%E4%BD%9C%E7%94%A8"><span class="toc-number">1.3.</span> <span class="toc-text">3.BN为什么起作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%AE%80%E8%BF%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 简述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%85%B7%E4%BD%93%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 具体推导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%8A%B1%E5%BC%8FMask%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88%E6%88%91%E6%82%9F%E4%BA%86%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">4. 花式Mask预训练（我悟了）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8D%95%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 单向语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Transformer%E4%B8%93%E5%B1%9E"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 Transformer专属</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%B9%B1%E5%BA%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 乱序语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Seq2Seq"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 Seq2Seq</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AF%8D%E5%90%91%E9%87%8F%E4%B8%8EEmbedding%E7%A9%B6%E7%AB%9F%E6%98%AF%E6%80%8E%E4%B9%88%E5%9B%9E%E4%BA%8B%EF%BC%88%E6%9C%89%E7%A9%BA%E8%A1%A5%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">5. 词向量与Embedding究竟是怎么回事（有空补）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E3%80%8AAttention-is-All-You-Need%E3%80%8B%E6%B5%85%E8%AF%BB%EF%BC%88%E6%9C%89%E7%A9%BA%E8%A1%A5%EF%BC%89"><span class="toc-number">1.6.</span> <span class="toc-text">6. 《Attention is All You Need》浅读（有空补）</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">苏神文章解析</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-27</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">7k</span><span class="post-meta__separator">|</span><span>阅读时长: 22 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="苏神文章解析"><a href="#苏神文章解析" class="headerlink" title="苏神文章解析"></a>苏神文章解析</h1><h2 id="1-浅谈Transformer的初始化、参数化与标准化"><a href="#1-浅谈Transformer的初始化、参数化与标准化" class="headerlink" title="1.浅谈Transformer的初始化、参数化与标准化"></a>1.浅谈Transformer的初始化、参数化与标准化</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/400925524?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《浅谈Transformer的初始化、参数化与标准化》</a></p>
<h3 id="1-1采样分布：截尾正态分布"><a href="#1-1采样分布：截尾正态分布" class="headerlink" title="1.1采样分布：截尾正态分布"></a>1.1采样分布：截尾正态分布</h3><p>&#8195;&#8195;一般情况下，我们都是从指定均值和方差的随机分布中进行采样来初始化。其中常用的随机分布有三个：正态分布（Normal）、均匀分布（Uniform）和截尾正态分布（Truncated Normal）<br><img src="https://img-blog.csdnimg.cn/00cc661bee994d4db236d0c07c6bc431.png#pic_center" alt="采样分布"><br>&#8195;&#8195;一般来说，==正态分布的采样结果更多样化一些，但它理论上是无界的，如果采样到绝对值过大的结果可能不利于优化；相反均匀分布是有界的，但采样结果通常更单一。于是就出现了结合两者优点的“截尾正态分布”==。<br><img src="https://img-blog.csdnimg.cn/e84f1003aac0410dbe56ebd289bc8f57.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="截尾正态分布"></p>
<h3 id="1-2-正交初始化：Xavier初始化"><a href="#1-2-正交初始化：Xavier初始化" class="headerlink" title="1.2 正交初始化：Xavier初始化"></a>1.2 正交初始化：Xavier初始化</h3><p>&#8195;&#8195;从第二节可以得出结论：不考虑激活函数的情况下，正交初始化比较好界定。==考虑激活函数==得情况就比较麻烦。relu函数可以从N(0,1/2m)采样来初始化W。如果是是其它激活函数，分期起来比较麻烦甚至做不到完全正交初始化。那么可以考虑的方案是==微调激活函数（激活函数进行各种变换）==<br>详情见第二节</p>
<h3 id="1-3-直接标准化"><a href="#1-3-直接标准化" class="headerlink" title="1.3 直接标准化"></a>1.3 直接标准化</h3><p>&#8195;&#8195;相比上一节各种“微调”，更直接的处理方法是各种Normalization方法，如Batch Normalization、Instance Normalization、Layer Normalization等，这类方法直接计算当前数据的均值方差来将输出结果标准化，而不用事先估计积分，有时候我们也称其为“归一化”。<br>&#8195;&#8195;这三种标准化方法大体上都是类似的，除了Batch Normalization多了一步滑动平均预测用的均值方差外，它们只不过是标准化的维度不一样。比如NLP尤其是Transformer模型用得比较多就是Layer Normalization是：</p>
<p>&#8195;&#8195;Normalization一般都包含了减均值（center）和除以标准差（scale）两个部分，但近来的一些工作逐渐尝试去掉center这一步，甚至有些工作的结果显示去掉center这一步后性能还略有提升。</p>
<p>&#8195;&#8195;比如2019年的论文《Root Mean Square Layer Normalization》比较了去掉center后的Layer Normalization，文章称之为RMS Norm。论文总的结果显示：==RMS Norm比Layer Normalization更快，效果也基本一致==。</p>
<p>&#8195;&#8195;比如2019年的论文《Root Mean Square Layer Normalization》比较了去掉center后的Layer Normalization，文章称之为RMS Norm。</p>
<p>&#8195;&#8195;一个直观的猜测是，center操作，类似于全连接层的bias项，储存到的是关于数据的一种先验分布信息，而把这种先验分布信息直接储存在模型中，反而可能会导致模型的迁移能力下降。所以T5不仅去掉了Layer Normalization的center操作，它把每一层的bias项也都去掉了。</p>
<h3 id="1-4-NTK参数化"><a href="#1-4-NTK参数化" class="headerlink" title="1.4 NTK参数化"></a>1.4 NTK参数化</h3><ul>
<li>Xavier初始化是用“均值为0、方差为1/m的随机分布”初始化。</li>
<li>NTK参数化：用“均值为0、方差为1的随机分布”来初始化，但是将输出结果除以$\sqrt{m}$。高斯过程中被称为“NTK参数化”</li>
</ul>
<p>&#8195;&#8195;==NTK参数化跟直接用Xavier初始化相比，有什么好处吗？==<br>&#8195;&#8195;理论上，利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的量级大致都是相同的O(1)级别，于是我们可以设置较大的学习率，比如$10^{−2}$，并且如果使用自适应优化器，其更新量大致是$\frac{梯度}{\sqrt{梯度\bigotimes 梯度}}\times 学习率$，那么我们就知道$10^{−2}$的学习率每一步对参数的调整程度大致是1%。<br>&#8195;&#8195;总的来说，NTK参数化能让我们更平等地处理每一个参数，并且比较形象地了解到训练的更新幅度，以便我们更好地调整参数。</p>
<p>==为什么Attention中除以$\sqrt{d}$这么重要？==<br>&#8195;&#8195;对于两个d维向量q,k，假设它们都采样自“均值为0、方差为1”的分布，那么它们的内积的二阶矩是：$E[(q,k)]^{2}=d$。由于均值也为0，所以这也意味着方差也是d。</p>
<p>&#8195;&#8195;Attention是内积后softmax，主要设计的运算是$e^{q⋅k}$，我们可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而$e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。</p>
<p>&#8195;&#8195;相应地，解决方法就有两个:</p>
<ul>
<li>像NTK参数化那样，在内积之后除以$\sqrt{d}$，使q⋅k的方差变为1，对应$e^3$,$e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法</li>
<li>另外就是不除以$\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</li>
</ul>
<h3 id="1-5-残差连接"><a href="#1-5-残差连接" class="headerlink" title="1.5 残差连接"></a>1.5 残差连接</h3><h2 id="2-模型参数的初始化"><a href="#2-模型参数的初始化" class="headerlink" title="2.模型参数的初始化"></a>2.模型参数的初始化</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/7180">《从几何视角来理解模型参数的初始化策略》</a></p>
<h3 id="2-1-总结"><a href="#2-1-总结" class="headerlink" title="2.1 总结"></a>2.1 总结</h3><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>初始化</th>
</tr>
</thead>
<tbody>
<tr>
<td>m=n</td>
<td>Xavier初始化：从N(0,1/n)采样来初始化W</td>
</tr>
<tr>
<td>m&lt;n</td>
<td>无法做到正交初始化</td>
</tr>
<tr>
<td>m≥n</td>
<td>从N(0,1/m)采样来初始化W</td>
</tr>
<tr>
<td>tanh激活函数</td>
<td>Xavier初始化直接适用于tanh激活</td>
</tr>
<tr>
<td>relu激活函数</td>
<td>从N(0,1/2m)采样来初始化W</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-2-为啥要正交初始化"><a href="#2-2-为啥要正交初始化" class="headerlink" title="2.2 为啥要正交初始化"></a>2.2 为啥要正交初始化</h3><p>&#8195;&#8195;对于复杂模型来说，参数的初始化显得尤为重要。糟糕的初始化，很多时候已经不单是模型效果变差的问题了，还更有可能是模型根本训练不动或者不收敛。在深度学习中常见的自适应初始化策略是Xavier初始化。<br>&#8195;&#8195;深度学习模型本身上就是一个个全连接层的嵌套，所以为了使模型最后的输出不至于在初始化阶段就过于“膨胀”或者“退化”，一个想法就是让模型在初始化时能保持模长不变。<br>&#8195;&#8195;正交矩阵是指满足$W^⊤W=I$的矩阵，也就是说它的逆等于转置。正交矩阵的重要意义在于它在变换过程中保持了向量的模长不变.用数学公式来表达，就是设$W\in \mathbb{R}^{n\times n}$是一个正交矩阵，而$x\in \mathbb{R}^{n}$是任意向量，则x的模长等于$W_{x}$的模长：</p>
<script type="math/tex; mode=display">\left \| W_{x} \right \|^{2}=x^{T}W^{T}Wx=x^{T}x=x</script><p>&#8195;&#8195;这个想法形成的一个自然的初始化策略就是“以全零初始化b，以随机正交矩阵初始化W”</p>
<h3 id="2-3-Xavier初始化"><a href="#2-3-Xavier初始化" class="headerlink" title="2.3 Xavier初始化"></a>2.3 Xavier初始化</h3><p>推论1： 高维空间中的任意两个随机向量几乎都是垂直的<br>推论2： 从N(0,1/n)中随机选取$n^2$个数，组成一个n×n的矩阵，这个矩阵近似为正交矩阵，且n越大，近似程度越好。<br>推论3： 从任意的均值为0、方差为1/n的分布p(x)中独立重复采样出来的n×n矩阵，都接近正交矩阵。<br>&#8195;&#8195;==Xavier初始化：从N(0,1/n)采样来初始化W==。从推论2可以看出，从N(0,1/n)采样而来的n×n矩阵就已经接近正交矩阵了。这种初始化也叫Glorot初始化，作者叫Xavier Glorot～<br>&#8195;&#8195;此外，采样分布也不一定是N(0,1/n)，前面推论3说了你可以从任意均值为0、方差为1/n的分布中采样。</p>
<p>&#8195;&#8195;==上面说的是输入和输出维度都是n的情况==，如果输入是n维，输出是m维呢？这时候$W\in \mathbb{R}^{m\times n}$，保持Wx模长不变的条件依然是$W^⊤W=I$。</p>
<ul>
<li>m&lt;n时，这是不可能的；</li>
<li>推论四：当m≥n时，从任意的均值为0、方差为1/m的分布p(x)中独立重复采样出来的m×n矩阵，近似满足$W^⊤W=I$（只需要把采样分布的方差改为1/m就好）。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/af9b1f597694442d841b7b07cb9c45c3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>sigmoid函数：W服从$U[-\sqrt{\frac{96}{n<em>{i}+n</em>{i+1}}},\sqrt{\frac{96}{n<em>{i}+n</em>{i+1}}}]$</p>
<p>Relu函数：W服从$U[-\sqrt{\frac{12}{n<em>{i}+n</em>{i+1}}},\sqrt{\frac{12}{n<em>{i}+n</em>{i+1}}}]$</p>
<h3 id="2-4-考虑激活函数"><a href="#2-4-考虑激活函数" class="headerlink" title="2.4 考虑激活函数"></a>2.4 考虑激活函数</h3><p>==以上都是没有考虑激活函数得场景==，考虑激活函数有：</p>
<ul>
<li>tanh(x) 在x比较小的时候有tanh(x)≈x，所以可以认为 Xavier初始化直接适用于tanh激活；</li>
<li>relu时可以认为relu(y)会有 大约一半的元素被置零，所以模长大约变为原来的$\frac{1}{\sqrt{2}}$&lt;/font&gt;，而要保持模长不变，可以让W乘上$\sqrt{2}$，也就是说初始化方差从1/m变成2/m</li>
</ul>
<p>&#8195;&#8195;事实上很难针对每一个激活函数都做好方差的调整，所以一个更通用的做法就是直接在激活函数后面加上一个类似Layer Normalization的操作，直接显式地恢复模长。这时候就轮到各种Normalization技巧登场了。</p>
<h2 id="3-BN为什么起作用"><a href="#3-BN为什么起作用" class="headerlink" title="3.BN为什么起作用"></a>3.BN为什么起作用</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/6992">《BN究竟起了什么作用？一个闭门造车的分析》</a></p>
<h3 id="3-1-简述"><a href="#3-1-简述" class="headerlink" title="3.1 简述"></a>3.1 简述</h3><p>&#8195;&#8195;BN在深度学习中可以加速训练，甚至有一定的抗过拟合作用，还允许我们用更大的学习率。因为BN使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练。<br>&#8195;&#8195;BN降低了神经网络的梯度的L常数，从而使得神经网络的学习更加容易，比如可以使用更大的学习率。而降低梯度的L常数，直观来看就是让损失函数没那么“跌宕起伏”，也就是使得landscape更光滑的意思了。（归一化、L常数请参考<a target="_blank" rel="noopener" href="https://kexue.fm/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>）<br>推导：</p>
<ol>
<li>假设f(θ)是损失函数，满足L约束。可以推导出梯度下降更新公式：<script type="math/tex; mode=display">Δθ=−η∇θf(θ)(4)</script>代入不等式2得出：保证损失函数下降，要么学习率η 足够小，要不就要L足够小。</li>
<li>神经网络中激活函数是非线性的。根据柯西不等式得出：降低L的最直接方式是降低$∣Ex∼p(x)[x⊗x]∣_{1}$。</li>
<li>在不会明显降低原来神经网络拟合能力的前提下，降低这一项最好的办法就是对输入x进行变换，即平移和缩放。推出两个结论：</li>
</ol>
<ul>
<li>结论1： 将输入减去所有样本的均值，能降低梯度的L常数，是一个有利于优化又不降低神经网络拟合能力的操作</li>
<li>结论2： 缩放输入x最佳选择是除以标准差。这更像是一个自适应的学习率校正项，可以一定程度上消除了不同层级的输入对参数优化的差异性，使得整个网络的优化更为“同步”，或者说使得神经网络的每一层更为“平权”，从而更充分地利用好了整个神经网络，减少了在某一层过拟合的可能性。当然，如果输入的量级过大时，除以标准差这一项也有助于降低梯度的L常数。</li>
</ul>
<h3 id="3-2-具体推导"><a href="#3-2-具体推导" class="headerlink" title="3.2 具体推导"></a>3.2 具体推导</h3><p>&#8195;&#8195;BN，也就是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization</a>，是当前深度学习模型（尤其是视觉相关模型）的一个相当重要的技巧，它能加速训练，甚至有一定的抗过拟合作用，还允许我们用更大的学习率，总的来说颇多好处（前提是你跑得起较大的batch size）</p>
<p>&#8195;&#8195;早期的解释主要是基于概率分布的，大概意思是将每一层的输入分布都归一化到N(0,1)上，减少了所谓的Internal Covariate Shift，从而稳定乃至加速了训练。<br>&#8195;&#8195;这种解释细思之下其实有问题的：不管哪一层的输入都不可能严格满足正态分布，从而单纯地将均值方差标准化无法实现标准分布N(0,1)；其次，就算能做到N(0,1)，这种诠释也无法进一步解释其他归一化手段（如Instance Normalization、Layer Normalization）起作用的原因。</p>
<p>&#8195;&#8195;2018年的论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.11604">《How Does Batch Normalization Help Optimization?》</a>里边，作者明确地提出了上述质疑，否定了原来的一些观点，并提出了自己关于BN的新理解：==BN主要作用是使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练==。<br>核心不等式：<br><img src="https://img-blog.csdnimg.cn/8951c52b00264283b6fe12bb4c7dcdcf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="核心不等式"><br>&#8195;&#8195;假设f(θ)是损失函数，而我们的目标是最小化f(θ)。我们自然希望f(θ)每一步都在下降，即$f(θ+Δθ)&lt;f(θ)$，而$\frac{1}{2}L\left | \Delta \theta  \right |<em>{2}^{2}$必然是非负的，所以要想下降的唯一选择就是$⟨∇</em>{θ}f(θ),Δθ⟩&lt;0$，这样一个自然的选择就是</p>
<script type="math/tex; mode=display">Δθ=−η∇θf(θ)(4)</script><p>&#8195;&#8195;这里η&gt;0是一个标量，即学习率。式(4)就是梯度下降的更新公式，它是一个严格的不等式，所以它还可以告诉我们关于训练的一些结论。<br><img src="https://img-blog.csdnimg.cn/f35d306359004137849730fa367377dc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="Lipschitz约束"><br>BN是怎样炼成的 ：<br>&#8195;&#8195;BN降低了神经网络的梯度的L常数，从而使得神经网络的学习更加容易，比如可以使用更大的学习率。而降低梯度的L常数，直观来看就是让损失函数没那么“跌宕起伏”，也就是使得landscape更光滑的意思了。</p>
<p>&#8195;&#8195;我们之前就讨论过L约束，之前我们讨论的是神经网络关于“输入”满足L约束，这导致了权重的谱正则和谱归一化，本文则是要讨论神经网络（的梯度）关于“参数”满足L约束，这导致了对输入的各种归一化手段，而BN是其中最自然的一种。</p>
<p>柯西不等式<br>探讨∇θf(θ)满足L约束的程度，并且探讨降低这个L的方法<br>结论：降低L常数最直接方法是，降低∣∣Ex∼p(x)[x⊗x]∣∣这一项（与参数无关）</p>
<p>式(12)的结果告诉我们，想办法降低L常数个做法就是对输入x进行变换，即平移和缩放。<br>结论1： 将输入减去所有样本的均值，能降低梯度的L常数，是一个有利于优化又不降低神经网络拟合能力的操作。（降低梯度的L常数前提是：必须不会明显降低原来神经网络拟合能力，否则只需要简单乘个0就可以让L降低到0了，但这并没有意义。）</p>
<p>如果一味追求更小的L，那直接σ→∞就好了，但这样的神经网络已经完全没有拟合能力了；但如果σ太小导致L过大，那又不利于优化。所以我们需要一个标准。<br>从公式可以看出：==一个相对自然的选择是将σ取为输入的标准差==。除以标准差更像是一个自适应的学习率校正项，它一定程度上消除了不同层级的输入对参数优化的差异性，使得整个网络的优化更为“同步”，或者说使得神经网络的每一层更为“平权”，从而更充分地利用好了整个神经网络，减少了在某一层过拟合的可能性。当然，如果输入的量级过大时，除以标准差这一项也有助于降低梯度的L常数。</p>
<h2 id="4-花式Mask预训练（我悟了）"><a href="#4-花式Mask预训练（我悟了）" class="headerlink" title="4. 花式Mask预训练（我悟了）"></a>4. 花式Mask预训练（我悟了）</h2><p>本节选自苏剑林的文章：<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6933#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83">《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》</a></p>
<p><strong>背景</strong><br>&#8195;&#8195;从Bert、GPT到XLNet等等，各种应用transformer结构的模型不断涌现，有基于现成的模型做应用的，有试图更好地去解释和可视化这些模型的，还有改进架构、改进预训练方式等以得到更好结果的。总的来说，这些以预训练为基础的工作层出不穷，有种琳琅满目的感觉</p>
<h3 id="4-1-单向语言模型"><a href="#4-1-单向语言模型" class="headerlink" title="4.1 单向语言模型"></a>4.1 单向语言模型</h3><p>&#8195;&#8195;语言模型可以说是一个无条件的文本生成模型（文本生成模型，可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5861">《玩转Keras之seq2seq自动生成标题》</a>）。单向语言模型相当于把训练语料通过下述条件概率分布的方式“记住”了：<br>p(x1,x2,x3,…,xn)=p(x1)p(x2|x1)p(x3|x1,x2)…p(xn|x1,…,xn−1)<br>&#8195;&#8195;我们一般说的“语言模型”，就是指单向的（更狭义的只是指正向的）语言模型。==语言模型的关键点是要防止看到“未来信息”==。如上式，预测x1的时候，是没有任何外部输入的；而预测x2的时候，只能输入x1，预测x3的时候，只能输入x1,x2；依此类推。<br><img src="https://img-blog.csdnimg.cn/8f13a78a86514fd78cc77b28520d04b3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="单向语言模型"><br>&#8195;&#8195;RNN模型是天然适合做语言模型的，因为它本身就是递归的运算；<strong>如果用CNN来做的话，则需要对卷积核进行Mask，即需要将卷积核对应右边的部分置零。如果是Transformer呢？那需要一个下三角矩阵形式的Attention矩阵，并将输入输出错开一位训练：</strong><br><img src="https://img-blog.csdnimg.cn/f5c140f2cc6e48a09da84954bd7ec656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="下三角矩阵"></p>
<p>&#8195;&#8195;如图所示，Attention矩阵的每一行事实上代表着输出，而每一列代表着输入，而<strong>Attention矩阵就表示输出和输入的关联</strong>。假定白色方格都代表0，那么第1行表示“北”只能跟起始标记$<s>$相关了，而第2行就表示“京”只能跟起始标记$<s>$和“北”相关了，依此类推。（（Mask的实现方式，也可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6810#Mask">《“让Keras更酷一些！”：层中层与mask》</a>）</p>
<h3 id="4-2-Transformer专属"><a href="#4-2-Transformer专属" class="headerlink" title="4.2 Transformer专属"></a>4.2 Transformer专属</h3><p>&#8195;&#8195;事实上，除了单向语言模型及其简单变体掩码语言模型之外，UNILM的Seq2Seq预训练、XLNet的乱序语言模型预训练，基本可以说是专为Transformer架构定制的。说白了，如果是RNN架构，根本就不能用乱序语言模型的方式来预训练。至于Seq2Seq的预训练方式，则必须同时引入两个模型（encoder和decoder），而无法像Transformer架构一样，可以一个模型搞定。</p>
<p>&#8195;&#8195;<strong>这其中的奥妙主要在Attention矩阵之上</strong>。Attention实际上相当于将输入两两地算相似度，这构成了一个$n^2$大小的相似度矩阵（即Attention矩阵，n是句子长度，本节的Attention均指Self Attention），这意味着它的空间占用量是O($n^2$)量级，相比之下，RNN模型、CNN模型只不过是O(n)，所以实际上Attention通常更耗显存。<br>&#8195;&#8195;然而，有弊也有利，更大的空间占用也意味着拥有了更多的可能性，==我们可以通过往这个O($n^2$)级别的Attention矩阵加入各种先验约束==，使得它可以做更灵活的任务。说白了，也就只有纯Attention的模型，才有那么大的“容量”去承载那么多的“花样”。</p>
<p>而==加入先验约束的方式，就是对Attention矩阵进行不同形式的Mask==，这便是本文要关注的焦点。</p>
<h3 id="4-3-乱序语言模型"><a href="#4-3-乱序语言模型" class="headerlink" title="4.3 乱序语言模型"></a>4.3 乱序语言模型</h3><p>&#8195;&#8195;乱序语言模型是XLNet提出来的概念，它主要用于XLNet的预训练上。<br>&#8195;&#8195;乱序语言模型跟语言模型一样，都是做条件概率分解，但是乱序语言模型的分解顺序是随机的：</p>
<script type="math/tex; mode=display">p(x1,x2,x3,…,xn)
=p(x1)p(x2|x1)p(x3|x1,x2)…p(xn|x1,x2,…,xn−1)
=p(x3)p(x1|x3)p(x2|x3,x1)…p(xn|x3,x1,…,xn−1)
=…
=p(xn−1)p(x1|xn−1)p(xn|xn−1,x1)…p(x2|xn−1,x1,…,x3)</script><p>&#8195;&#8195;总之，x1,x2,…,xn任意一种“出场顺序”都有可能。原则上来说，每一种顺序都对应着一个模型，所以原则上就有n!个语言模型。而基于Transformer的模型，则可以将这所有顺序都做到一个模型中去！<br>&#8195;&#8195;<strong>实现某种特定顺序的语言模型，就将原来的下三角形式的Mask以某种方式打乱</strong>。正因为Attention提供了这样的一个n×n的Attention矩阵，我们才有足够多的自由度去以不同的方式去Mask这个矩阵，从而实现多样化的效果。</p>
<p>&#8195;&#8195;以“北京欢迎你”的生成为例，假设随机的一种生成顺序为“$<s>$ → 迎 → 京 → 你 → 欢 → 北 → <e>”，那么我们只需要用下图中第二个子图的方式去Mask掉Attention矩阵，就可以达到目的了：<br><img src="https://img-blog.csdnimg.cn/2292a816c044400492613f9e2a105b9b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="l乱序语言模型"><br>&#8195;&#8195;跟前面的单向语言模型类似，第4行只有一个蓝色格，表示“迎”只能跟起始标记$<s>$相关，而第2行有两个蓝色格，表示“京”只能跟起始标记$<s>$和“迎”相关，依此类推。直观来看，这就像是把单向语言模型的下三角形式的Mask“打乱”了。</p>
<p>&#8195;&#8195;有人会问，打乱后的Mask似乎没看出什么规律呀，难道每次都要随机生成一个这样的似乎没有什么明显概率的Mask矩阵？事实上有一种更简单的、数学上等效的训练方案，即==在输入层面进行打乱==。<br>&#8195;&#8195;==纯Attention的模型本质上是一个无序的模型==，它里边的词序实际上是通过Position Embedding加上去的。也就是说，我们输入的不仅只有token本身，还包括token所在的位置id；再换言之，你觉得你是输入了序列“[北, 京, 欢, 迎, 你]”，实际上你输入的是集合“{(北, 1), (京, 2), (欢, 3), (迎, 4), (你, 5)}”。<br>&#8195;&#8195;既然只是一个集合，跟顺序无关，那么我们完全可以换一种顺序输入，比如刚才的“$<s>$→ 迎 → 京 → 你 → 欢 → 北 → $<e>$”，我们可以按“(迎, 4), (京, 2), (你, 5), (欢, 3), (北, 1)”的顺序输入，也就是说将token打乱为“迎,京,你,欢,北”输入到Transformer中，但是第1个token的position就不是1了，而是4；依此类推。这样换过来之后，Mask矩阵可以恢复为下三角矩阵。（讲了一通，乱序实现从乱序mask矩阵到乱序输入序列，有点迷。。。）</p>
<h3 id="4-4-Seq2Seq"><a href="#4-4-Seq2Seq" class="headerlink" title="4.4 Seq2Seq"></a>4.4 Seq2Seq</h3><p>&#8195;&#8195;原则上来说，任何NLP问题都可以转化为Seq2Seq来做，它是一个真正意义上的万能模型。所以如果能够做到Seq2Seq，理论上就可以实现任意任务了。<br>&#8195;&#8195;微软的<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.03197">UNILM</a>能将Bert与Seq2Seq优雅的结合起来。能够让我们==直接用单个Bert模型就可以做Seq2Seq任务，而不用区分encoder和decoder。而实现这一点几乎不费吹灰之力——只需要一个特别的Mask==。<br>&#8195;&#8195;UNILM直接将Seq2Seq当成句子补全来做。假如输入是“你想吃啥”，目标句子是“白切鸡”，那UNILM将这两个句子拼成一个：[CLS] 你 想 吃 啥 [SEP] 白 切 鸡 [SEP]。经过这样转化之后，最简单的方案就是训练一个语言模型，然后输入“[CLS] 你 想 吃 啥 [SEP]”来逐字预测“白 切 鸡”，直到出现“[SEP]”为止，即如下面的左图：<br><img src="https://img-blog.csdnimg.cn/61c52c89d2484fce9a5ab71efb6350db.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="unilm"></p>
<p>&#8195;&#8195;不过左图只是最朴素的方案，它把“你想吃啥”也加入了预测范围了（导致它这部分的Attention是单向的，即对应部分的Mask矩阵是下三角），事实上这是不必要的，属于==额外的约束==。真正要预测的只是“白切鸡”这部分，所以我们可以把“你想吃啥”这部分的Mask去掉，得到上面的右图的Mask。</p>
<p>UNILM单个Bert模型完成Seq2Seq任务的思路：<br>&#8195;&#8195;添加上述形状的Mask，==输入部分的Attention是双向的，输出部分的Attention是单向==，满足Seq2Seq的要求，而且==没有额外约束==。这样做不需要修改模型架构，并且还可以直接沿用Bert的Masked Language Model预训练权重，收敛更快。这符合“一Bert在手，天下我有”的万用模型的初衷，个人认为这是非常优雅的方案。</p>
<h3 id="4-5-实验"><a href="#4-5-实验" class="headerlink" title="4.5 实验"></a>4.5 实验</h3><p>&#8195;&#8195;事实上，上述的这些Mask方案，基本上都已经被集成在原作者写的<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6915">bert4keras</a>，读者可以直接用bert4keras加载bert的预训练权重，并且调用上述Mask方案来做相应的任务。具体UNILM实现例子，可以参考<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/6933#%E8%8A%B1%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83">原文</a></p>
<h3 id="4-6-总结"><a href="#4-6-总结" class="headerlink" title="4.6 总结"></a>4.6 总结</h3><p>&#8195;&#8195;1.原始的seq2seq训练的是一个单向语言模型，语言模型的关键点是要防止看到“未来信息”。这一点可以通过循环神经网络的递归计算来实现，比如RNN。也可以通过CNN来做，只需要对卷积核进行Mask，即需要将卷积核对应右边的部分置零。如果是Transformer呢，那就需要一个下三角矩阵形式的Attention矩阵（表示输入与输出的关联）来实现。<br>&#8195;&#8195;2.不仅如此，通过Attention矩阵的不同Mask方式，还可以实现乱序语言模型和Seq2Seq。<br>&#8195;&#8195;前者只需要乱序原来的下三角形式的Masked-Attention矩阵（也等价于乱序输入序列），后者通过句子补全来做（类似输入一个词，预测接下来会输入的词，即输入法预测）。具体做的时候，只需要mask输入部分就行（感觉就是GPT2）。<br>&#8195;&#8195;3.之所以一个transformer结构能搞出后面那么多花样的玩法（Bert、GPT、XLNet等），==关键在于Attention矩阵==。Attention实际上相当于将输入两两地算相似度，这构成了一个$n^2$大小的相似度矩阵（复杂度O($n^2$)）。比起RNN、CNN模型只是O(n)，Attention通常更耗显存。<br>&#8195;&#8195;但正因如此，却也有了更多的可能性。==通过往O($n^2$)级别的Attention矩阵加入各种先验约束，使得它可以做更灵活的任务。这种先验约束就是mask玩法==。说白了，也就只有纯Attention的模型，才有那么大的“容量”去承载那么多的“花样”。（读到这里，我悟了）。</p>
<h2 id="5-词向量与Embedding究竟是怎么回事（有空补）"><a href="#5-词向量与Embedding究竟是怎么回事（有空补）" class="headerlink" title="5. 词向量与Embedding究竟是怎么回事（有空补）"></a>5. 词向量与Embedding究竟是怎么回事（有空补）</h2><p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/4122">词向量与Embedding究竟是怎么回事</a></p>
<h2 id="6-《Attention-is-All-You-Need》浅读（有空补）"><a href="#6-《Attention-is-All-You-Need》浅读（有空补）" class="headerlink" title="6. 《Attention is All You Need》浅读（有空补）"></a>6. 《Attention is All You Need》浅读（有空补）</h2><p><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/4765">《Attention is All You Need》浅读（简介+代码）</a><br>RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息：纯Attention！单靠注意力就可以。yt=f(xt,A,B)<br>Attention层的好处是能够一步到位捕捉到全局的联系，因为它直接把序列两两比较（代价是计算量变为$O(n^2)$，当然由于是纯矩阵运算，这个计算量相当也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/27/软件应用/苏神文章解析（6篇）/">https://zhxnlp.github.io/2021/08/27/软件应用/苏神文章解析（6篇）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/07/huggingface/Hugging%20Face%E4%B8%BB%E9%A1%B5%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%89%E7%AF%87%E3%80%8AFine-tuning%20a%20pretrained%20model%E3%80%8B/"><i class="fa fa-chevron-left">  </i><span>Hugging Face官方文档——Fine-tuning a pretrained model</span></a></div><div class="next-post pull-right"><a href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task7%EF%BC%9ATransformers%E8%A7%A3%E6%9E%90%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1/"><span>task7：BERT token分类</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>