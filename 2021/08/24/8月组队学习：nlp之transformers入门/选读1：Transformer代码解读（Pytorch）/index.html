<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="选读1：Transformer代码解读（Pytorch）"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>选读1：Transformer代码解读（Pytorch） | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88Pytorch%EF%BC%89"><span class="toc-text">Transformer代码解读（Pytorch）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E8%AF%BB%E5%AE%8C%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E8%BF%99%E4%B8%80%E6%AE%B5%E6%98%AF%E8%87%AA%E5%B7%B1%E7%9A%84%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%89"><span class="toc-text">0. 读完总结：（这一段是自己的总结笔记）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-1-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%BD%A2%E7%8A%B6%EF%BC%9A"><span class="toc-text">0.1 位置编码形状：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-2-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">0.2 多头注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-3-encoder%E5%B1%82%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%9A"><span class="toc-text">0.3 encoder层前向传播：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-4-TransformerDecoder%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%9A"><span class="toc-text">0.4 TransformerDecoder前向传播：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-5-Transformer%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">0.5 Transformer前向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-text">1. 词嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-text">2. 位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">3. 多头注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-text">3.1 初始化参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">3.2 前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E7%9F%A9%E9%98%B5%E5%8F%98%E6%8D%A2"><span class="toc-text">3.2.1 矩阵变换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E9%81%AE%E6%8C%A1%E6%9C%BA%E5%88%B6"><span class="toc-text">3.2.2 遮挡机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">3.2.3 点积注意力</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E5%AE%8C%E6%95%B4%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-MultiheadAttention"><span class="toc-text">3.3完整多头注意力机制-MultiheadAttention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-TransformerEncoder"><span class="toc-text">4.TransformerEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Encoder-Layer"><span class="toc-text">4.1 Encoder Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Transformer-layer%E7%BB%84%E6%88%90Encoder"><span class="toc-text">4.2 Transformer layer组成Encoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Decoder-Layer"><span class="toc-text">5. Decoder Layer:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-TransformerDecoderLayer"><span class="toc-text">5.1 TransformerDecoderLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Transformer-layer%E7%BB%84%E6%88%90Decoder"><span class="toc-text">5.2 Transformer layer组成Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Transformer"><span class="toc-text">6. Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-text">致谢</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">选读1：Transformer代码解读（Pytorch）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-24</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">7.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 33 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Transformer代码解读（Pytorch）"><a href="#Transformer代码解读（Pytorch）" class="headerlink" title="Transformer代码解读（Pytorch）"></a>Transformer代码解读（Pytorch）</h1><p>本文是对transformer源代码的一点总结。原文在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.md">《Pytorch编写完整的Transformer》</a></p>
<p>本文涉及的jupter notebook在<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2.1-Pytorch%E7%BC%96%E5%86%99%E5%AE%8C%E6%95%B4%E7%9A%84Transformer.ipynb">Pytorch编写完整的Transformer</a><br>在阅读完<a href="./篇章2-Transformer相关原理/2.2-图解transformer.md">2.2-图解transformer</a>之后，希望大家能对transformer各个模块的设计和计算有一个形象的认识，本小节我们基于pytorch来实现一个Transformer，帮助大家进一步学习这个复杂的模型。<br>@[toc]</p>
<h2 id="0-读完总结：（这一段是自己的总结笔记）"><a href="#0-读完总结：（这一段是自己的总结笔记）" class="headerlink" title="0. 读完总结：（这一段是自己的总结笔记）"></a>0. 读完总结：（这一段是自己的总结笔记）</h2><ul>
<li>位置编码是最后加入的，输入输出形状不变。</li>
<li>每个attention层都有权重初始化（即输入映射成不同的QKV。而且每层权重不一样）<br>除了encoder-decoder-attention层q是来自前一层输出，kv是来自encoder层最后的输出memory会导致qkv维度不一致，其它层qkv维度都是一样的。并且只有第一维不一致，分别是L和S。</li>
<li>点积时会讲batch放到第一维，还有遮挡机制</li>
<li>encoderlayer1：输入src加入位置编码，进入 Multi-self-attention层。self.norm1(src + self.dropout1(src2))。即dropout输出src2，然后残差连接+Norm</li>
<li>encoderlayer2：全连接第一层3072神经元扩维4倍，之后激活并dropout，送入第二个全连接层降维回768维。之后同样的Add+Norm，即src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))、self.norm2(src + self.dropout2(src2))。<span id="more"></span></li>
<li>out = transformer_encoder(src)，两个线性层中间的激活函数，默认relu或gelu</li>
<li>decoderlayer：和encoder比加入第二层，tgt=self.multihead_attn(tgt, memory, memory）。和之前一样，三层之后都是self.norm(src + self.dropout(src2))。</li>
<li>out = decoder_layer(tgt, memory)，memory = self.encoder(src）</li>
<li>encoder和decoder参数除了各自的layer和num_layers，还有个norm参数，默认最后一层输出标准化。</li>
<li>transformer：没有自定义时使用默认的encoder和decoder层</li>
<li>out = transformer_model(src, tgt)。</li>
</ul>
<h3 id="0-1-位置编码形状："><a href="#0-1-位置编码形状：" class="headerlink" title="0.1 位置编码形状："></a>0.1 位置编码形状：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- 输入： [batch_size, seq_length, num_features]</span><br><span class="line">- 输出： [batch_size, seq_length, num_features]</span><br><span class="line">- <span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">X, num_features, dropout_p=<span class="number">0.1</span>, max_len=<span class="number">512</span></span>)</span></span><br></pre></td></tr></table></figure>
<h3 id="0-2-多头注意力"><a href="#0-2-多头注意力" class="headerlink" title="0.2 多头注意力"></a>0.2 多头注意力</h3><p>0.2.1初始化：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.kdim = kdim <span class="keyword">if</span> kdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">self.vdim = vdim <span class="keyword">if</span> vdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">self._qkv_same_embed_dim = self.kdim == embed_dim <span class="keyword">and</span> self.vdim == embed_dim</span><br></pre></td></tr></table></figure></p>
<ul>
<li>如果qkv维度不一致（encoder-decoder-attention层，主要是第一维序列长度不一致），则各自初始化。也就是三个不同的proj_weight来将X映射成qkv。</li>
<li>如果qkv维度一致，则用一个3倍的权重矩阵转换之后分割成qkv</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim)))</span><br></pre></td></tr></table></figure>
<p>0.2.2 前向传播multi_head_attention_forward：</p>
<ul>
<li>query, key, value通过_in_projection_packed变换得到q,k,v</li>
<li>遮挡机制  attn_mask的dtype为ByteTensor，非0的位置会被忽略不做注意力；若为BoolTensor，True对应的位置会被忽略；若为数值，则会直接加到attn_weights。</li>
<li>key_padding_mask是用来遮挡key里面的padding部分</li>
<li><p>点积注意力 多头拼接在一起，并且 q,k,v将Batch放在第一维以适合点积注意力（reshape）</p>
<p>  输入：</p>
<pre><code>- query：`(L, N, E)` 点积时是`(N, L, E)`
- key: `(S, N, E)`     点积时是`(N, S, E)`
- value: `(S, N, E)`   点积时是`(N, S, E)`
- key_padding_mask: `(N, S)`
- attn_mask: `(L, S)` or `(N * num_heads, L, S)`N代表着batch_size，num_heads代表注意力头的数目
输出：
- attn_output:`(L, N, E)`
- attn_output_weights:`(N, L, S)`
</code></pre><p>上面N是batch_size，L和S分别代表着目标语言tgt和源语言src序列长度。E是:词嵌入的维度embed_dim</p>
</li>
</ul>
<h3 id="0-3-encoder层前向传播："><a href="#0-3-encoder层前向传播：" class="headerlink" title="0.3 encoder层前向传播："></a>0.3 encoder层前向传播：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">src = positional_encoding(src, src.shape[-<span class="number">1</span>])</span><br><span class="line">src2 = self.self_attn(src, src, src, attn_mask=src_mask, </span><br><span class="line">key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">src = src + self.dropout1(src2)<span class="comment">#dropout对最里面的元素随机替换为0，正则手段防止过拟合</span></span><br><span class="line">src = self.norm1(src)</span><br><span class="line">src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">src = src + self.dropout(src2)</span><br><span class="line">src = self.norm2(src)</span><br><span class="line"><span class="keyword">return</span> src</span><br></pre></td></tr></table></figure>
<p>torch.nn.dropout源码在<a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.7.1/_modules/torch/nn/modules/dropout.html#Dropout">这里</a>。<br>==decoderlayer前向传播中：==</p>
<ul>
<li>tgt过masked-self-attention层后第一次Add+Norm，</li>
<li>进入encoder-decoder-self.multihead_attention层（此时输入是tgt、memory和memory），第二次Add+Norm</li>
<li>进入全连接层（两层），之后第三次Add+Norm<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = decoder_layer(tgt, memory)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;参数：</span></span><br><span class="line"><span class="string">            tgt: 目标语言序列（必备）</span></span><br><span class="line"><span class="string">            memory: 从最后一个encoder_layer跑出的句子（必备）</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">        tgt = self.norm1(tgt)</span><br><span class="line">        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">        tgt = self.norm2(tgt)</span><br><span class="line">        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))</span><br><span class="line">        tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">        tgt = self.norm3(tgt)</span><br><span class="line">        <span class="keyword">return</span> tgt</span><br></pre></td></tr></table></figure>
<h3 id="0-4-TransformerDecoder前向传播："><a href="#0-4-TransformerDecoder前向传播：" class="headerlink" title="0.4 TransformerDecoder前向传播："></a>0.4 TransformerDecoder前向传播：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = tgt</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">    output = self.layer(output, memory）</span><br></pre></td></tr></table></figure>
<h3 id="0-5-Transformer前向传播"><a href="#0-5-Transformer前向传播" class="headerlink" title="0.5 Transformer前向传播"></a>0.5 Transformer前向传播</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/b08b669e217349fbbb87c112406bf83e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="图：Transformer结构图"></li>
</ul>
<h2 id="1-词嵌入"><a href="#1-词嵌入" class="headerlink" title="1. 词嵌入"></a>1. 词嵌入</h2><p>如上图所示，Transformer图里左边的是Encoder，右边是Decoder部分。Encoder输入源语言序列，Decoder里面输入需要被翻译的语言文本（在训练时）。一个文本常有许多序列组成，常见操作为将序列进行一些预处理（如词切分等）变成列表，一个序列的列表的元素通常为词表中不可切分的最小词，整个文本就是一个大列表，元素为一个一个由序列组成的列表。如一个序列经过切分后变为[“am”, “##ro”, “##zi”, “meets”, “his”, “father”]，接下来按照它们在词表中对应的索引进行转换，假设结果如[23, 94, 13, 41, 27, 96]。假如整个文本一共100个句子，那么就有100个列表为它的元素，因为每个序列的长度不一，需要设定最大长度，这里不妨设为128，那么将整个文本转换为数组之后，形状即为100 x 128，这就对应着batch_size和seq_length。</p>
<p>输入之后，紧接着进行词嵌入处理，词嵌入就是将每一个词用预先训练好的向量进行映射。</p>
<p>词嵌入在torch里基于<code>torch.nn.Embedding</code>实现，实例化时需要设置的参数为词表的大小和被映射的向量的维度比如<code>embed = nn.Embedding(10,8)</code>。向量的维度通俗来说就是向量里面有多少个数。注意，第一个参数是词表的大小，如果你目前最多有8个词，通常填写10（多一个位置留给unk和pad），你后面万一进入与这8个词不同的词就映射到unk上，序列padding的部分就映射到pad上。</p>
<p>假如我们打算映射到8维（num_features或者embed_dim），那么，整个文本的形状变为100 x 128 x 8。接下来举个小例子解释一下：假设我们词表一共有10个词(算上unk和pad)，文本里有2个句子，每个句子有4个词，我们想要把每个词映射到8维的向量。于是2，4，8对应于batch_size, seq_length, embed_dim（如果batch在第一维的话）。</p>
<p>另外，一般深度学习任务只改变num_features，所以讲维度一般是针对最后特征所在的维度。</p>
<p>开始编程：</p>
<p>所有需要的包的导入：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.nn.init <span class="keyword">import</span> xavier_uniform_</span><br><span class="line"><span class="keyword">from</span> torch.nn.init <span class="keyword">import</span> constant_</span><br><span class="line"><span class="keyword">from</span> torch.nn.init <span class="keyword">import</span> xavier_normal_</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Tuple</span>, <span class="type">Any</span></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> warnings</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.zeros((<span class="number">2</span>,<span class="number">4</span>),dtype=torch.long)</span><br><span class="line">embed = nn.Embedding(<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(embed(X).shape)</span><br></pre></td></tr></table></figure></p>
<h2 id="2-位置编码"><a href="#2-位置编码" class="headerlink" title="2. 位置编码"></a>2. 位置编码</h2><p>词嵌入之后紧接着就是位置编码，位置编码用以区分不同词以及同词不同特征之间的关系。代码中需要注意：X_只是初始化的矩阵，并不是输入进来的；完成位置编码之后会加一个dropout。另外，位置编码是最后加上去的，因此输入输出形状不变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Tensor = torch.Tensor</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">X, num_features, dropout_p=<span class="number">0.1</span>, max_len=<span class="number">512</span></span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        给输入加入位置编码</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        - num_features: 输入进来的维度</span></span><br><span class="line"><span class="string">        - dropout_p: dropout的概率，当其为非零时执行dropout</span></span><br><span class="line"><span class="string">        - max_len: 句子的最大长度，默认512</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    形状：</span></span><br><span class="line"><span class="string">        - 输入： [batch_size, seq_length, num_features]</span></span><br><span class="line"><span class="string">        - 输出： [batch_size, seq_length, num_features]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; X = torch.randn((2,4,10))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; X = positional_encoding(X, 10)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(X.shape)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; torch.Size([2, 4, 10])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    dropout = nn.Dropout(dropout_p)</span><br><span class="line">    P = torch.zeros((<span class="number">1</span>,max_len,num_features))</span><br><span class="line">    X_ = torch.arange(max_len,dtype=torch.float32).reshape(-<span class="number">1</span>,<span class="number">1</span>) / torch.<span class="built_in">pow</span>(</span><br><span class="line">        <span class="number">10000</span>,</span><br><span class="line">        torch.arange(<span class="number">0</span>,num_features,<span class="number">2</span>,dtype=torch.float32) /num_features)</span><br><span class="line">    P[:,:,<span class="number">0</span>::<span class="number">2</span>] = torch.sin(X_)</span><br><span class="line">    P[:,:,<span class="number">1</span>::<span class="number">2</span>] = torch.cos(X_)</span><br><span class="line">    X = X + P[:,:X.shape[<span class="number">1</span>],:].to(X.device)</span><br><span class="line">    <span class="keyword">return</span> dropout(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 位置编码例子</span></span><br><span class="line">X = torch.randn((<span class="number">2</span>,<span class="number">4</span>,<span class="number">10</span>))</span><br><span class="line">X = positional_encoding(X, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br></pre></td></tr></table></figure>
<h2 id="3-多头注意力机制"><a href="#3-多头注意力机制" class="headerlink" title="3. 多头注意力机制"></a>3. 多头注意力机制</h2><p>先拆开看多头注意力机制<br><strong>完整版本可运行的多头注意里机制的class在后面，<a href="#mha">完整的多头注意力机制-MultiheadAttentionion</a> 再回来依次看下面的解释。</strong></p>
<p>多头注意力类主要成分是：参数初始化、multi_head_attention_forward</p>
<h3 id="3-1-初始化参数"><a href="#3-1-初始化参数" class="headerlink" title="3.1 初始化参数"></a>3.1 初始化参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self._qkv_same_embed_dim <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">    <span class="comment"># 初始化前后形状维持不变</span></span><br><span class="line">    <span class="comment"># (seq_length x embed_dim) x (embed_dim x embed_dim) ==&gt; (seq_length x embed_dim)</span></span><br><span class="line">    self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim)))</span><br><span class="line">    self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim)))</span><br><span class="line">    self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim)))</span><br><span class="line">    self.register_parameter(<span class="string">&#x27;in_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim)))</span><br><span class="line">    self.register_parameter(<span class="string">&#x27;q_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    self.register_parameter(<span class="string">&#x27;k_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    self.register_parameter(<span class="string">&#x27;v_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> bias:</span><br><span class="line">    self.in_proj_bias = Parameter(torch.empty(<span class="number">3</span> * embed_dim))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.register_parameter(<span class="string">&#x27;in_proj_bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 后期会将所有头的注意力拼接在一起然后乘上权重矩阵输出</span></span><br><span class="line"><span class="comment"># out_proj是为了后期准备的</span></span><br><span class="line">self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">self._reset_parameters()</span><br></pre></td></tr></table></figure>
<p>torch.empty是按照所给的形状形成对应的tensor，特点是填充的值还未初始化，类比torch.randn（标准正态分布），这就是一种初始化的方式。在PyTorch中，变量类型是tensor的话是无法修改值的，而Parameter()函数可以看作为一种类型转变函数，将不可改值的tensor转换为可训练可修改的模型参数，即与model.parameters绑定在一起，register_parameter的意思是是否将这个参数放到model.parameters，None的意思是没有这个参数。</p>
<p>这里有个if判断，用以判断q,k,v的最后一维是否一致，若一致，则一个大的权重矩阵全部乘然后分割出来，若不是，则各初始化各的，其实初始化是不会改变原来的形状的（如<img src="http://latex.codecogs.com/svg.latex?q=qW_q+b_q" alt="">，见注释）。</p>
<p>可以发现最后有一个<em>reset_parameters()函数，这个是用来初始化参数数值的。xavier_uniform意思是从<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%80%A3%E7%BA%8C%E5%9E%8B%E5%9D%87%E5%8B%BB%E5%88%86%E5%B8%83">连续型均匀分布</a>里面随机取样出值来作为初始化的值，xavier_normal</em>取样的分布是正态分布。正因为初始化值在训练神经网络的时候很重要，所以才需要这两个函数。</p>
<p>constant_意思是用所给值来填充输入的向量。</p>
<p>另外，在PyTorch的源码里，似乎projection代表是一种线性变换的意思，in_proj_bias的意思就是一开始的线性变换的偏置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">if</span> self._qkv_same_embed_dim:</span><br><span class="line">        xavier_uniform_(self.in_proj_weight)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        xavier_uniform_(self.q_proj_weight)</span><br><span class="line">        xavier_uniform_(self.k_proj_weight)</span><br><span class="line">        xavier_uniform_(self.v_proj_weight)</span><br><span class="line">    <span class="keyword">if</span> self.in_proj_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        constant_(self.in_proj_bias, <span class="number">0.</span>)</span><br><span class="line">        constant_(self.out_proj.bias, <span class="number">0.</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-2-前向传播"><a href="#3-2-前向传播" class="headerlink" title="3.2 前向传播"></a>3.2 前向传播</h3><p>multi_head_attention_forward函数如下代码所示，主要分成3个部分：</p>
<ul>
<li>query, key, value通过_in_projection_packed变换得到q,k,v</li>
<li>遮挡机制</li>
<li>点积注意力</li>
</ul>
<h4 id="3-2-1-矩阵变换"><a href="#3-2-1-矩阵变换" class="headerlink" title="3.2.1 矩阵变换"></a>3.2.1 矩阵变换</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">Tensor = torch.Tensor</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_head_attention_forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    query: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    key: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    value: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    in_proj_weight: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    in_proj_bias: <span class="type">Optional</span>[Tensor],</span></span></span><br><span class="line"><span class="params"><span class="function">    dropout_p: <span class="built_in">float</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    out_proj_weight: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    out_proj_bias: <span class="type">Optional</span>[Tensor],</span></span></span><br><span class="line"><span class="params"><span class="function">    training: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    use_seperate_proj_weight = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    q_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    k_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    v_proj_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    形状：</span></span><br><span class="line"><span class="string">        输入：</span></span><br><span class="line"><span class="string">        - query：`(L, N, E)`</span></span><br><span class="line"><span class="string">        - key: `(S, N, E)`</span></span><br><span class="line"><span class="string">        - value: `(S, N, E)`</span></span><br><span class="line"><span class="string">        - key_padding_mask: `(N, S)`</span></span><br><span class="line"><span class="string">        - attn_mask: `(L, S)` or `(N * num_heads, L, S)`</span></span><br><span class="line"><span class="string">        输出：</span></span><br><span class="line"><span class="string">        - attn_output:`(L, N, E)`</span></span><br><span class="line"><span class="string">        - attn_output_weights:`(N, L, S)`</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    tgt_len, bsz, embed_dim = query.shape</span><br><span class="line">    src_len, _, _ = key.shape</span><br><span class="line">    head_dim = embed_dim // num_heads</span><br><span class="line">    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> attn_mask.dtype == torch.uint8:</span><br><span class="line">            warnings.warn(<span class="string">&quot;Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span>)</span><br><span class="line">            attn_mask = attn_mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> attn_mask.is_floating_point() <span class="keyword">or</span> attn_mask.dtype == torch.<span class="built_in">bool</span>, \</span><br><span class="line">                <span class="string">f&quot;Only float, byte, and bool types are supported for attn_mask, not <span class="subst">&#123;attn_mask.dtype&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">            correct_2d_size = (tgt_len, src_len)</span><br><span class="line">            <span class="keyword">if</span> attn_mask.shape != correct_2d_size:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;The shape of the 2D attn_mask is <span class="subst">&#123;attn_mask.shape&#125;</span>, but should be <span class="subst">&#123;correct_2d_size&#125;</span>.&quot;</span>)</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">            correct_3d_size = (bsz * num_heads, tgt_len, src_len)</span><br><span class="line">            <span class="keyword">if</span> attn_mask.shape != correct_3d_size:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;The shape of the 3D attn_mask is <span class="subst">&#123;attn_mask.shape&#125;</span>, but should be <span class="subst">&#123;correct_3d_size&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;attn_mask&#x27;s dimension <span class="subst">&#123;attn_mask.dim()&#125;</span> is not supported&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> key_padding_mask.dtype == torch.uint8:</span><br><span class="line">        warnings.warn(<span class="string">&quot;Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span>)</span><br><span class="line">        key_padding_mask = key_padding_mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reshape q,k,v将Batch放在第一维以适合点积注意力</span></span><br><span class="line">    <span class="comment"># 同时为多头机制，将不同的头拼在一起组成一层</span></span><br><span class="line">    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    k = k.contiguous().view(-<span class="number">1</span>, bsz * num_heads, head_dim).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    v = v.contiguous().view(-<span class="number">1</span>, bsz * num_heads, head_dim).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> key_padding_mask.shape == (bsz, src_len), \</span><br><span class="line">            <span class="string">f&quot;expecting key_padding_mask shape of <span class="subst">&#123;(bsz, src_len)&#125;</span>, but got <span class="subst">&#123;key_padding_mask.shape&#125;</span>&quot;</span></span><br><span class="line">        key_padding_mask = key_padding_mask.view(bsz, <span class="number">1</span>, <span class="number">1</span>, src_len).   \</span><br><span class="line">            expand(-<span class="number">1</span>, num_heads, -<span class="number">1</span>, -<span class="number">1</span>).reshape(bsz * num_heads, <span class="number">1</span>, src_len)</span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attn_mask = key_padding_mask</span><br><span class="line">        <span class="keyword">elif</span> attn_mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">            attn_mask = attn_mask.logical_or(key_padding_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_mask = attn_mask.masked_fill(key_padding_mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">    <span class="comment"># 若attn_mask值是布尔值，则将mask转换为float</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> attn_mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        new_attn_mask.masked_fill_(attn_mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">        attn_mask = new_attn_mask</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 若training为True时才应用dropout</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> training:</span><br><span class="line">        dropout_p = <span class="number">0.0</span></span><br><span class="line">    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)</span><br><span class="line">    attn_output = attn_output.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous().view(tgt_len, bsz, embed_dim)</span><br><span class="line">    attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)</span><br><span class="line">    <span class="keyword">if</span> need_weights:</span><br><span class="line">        <span class="comment"># average attention weights over heads</span></span><br><span class="line">        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)</span><br><span class="line">        <span class="keyword">return</span> attn_output, attn_output_weights.<span class="built_in">sum</span>(dim=<span class="number">1</span>) / num_heads</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> attn_output, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p> query, key, value通过_in_projection_packed变换得到q,k,v<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)</span><br></pre></td></tr></table></figure></p>
<p>对于<code>nn.functional.linear</code>函数，其实就是一个线性变换，与<code>nn.Linear</code>不同的是，前者可以提供权重矩阵和偏置，执行<img src="http://latex.codecogs.com/svg.latex?y=xW^T+b" alt="">，而后者是可以自由决定输出的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_in_projection_packed</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    q: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    k: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    v: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    w: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    b: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">List</span>[Tensor]:</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用一个大的权重参数矩阵进行线性变换</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        q, k, v: 对自注意来说，三者都是src；对于seq2seq模型，k和v是一致的tensor。</span></span><br><span class="line"><span class="string">                 但它们的最后一维(num_features或者叫做embed_dim)都必须保持一致。</span></span><br><span class="line"><span class="string">        w: 用以线性变换的大矩阵，按照q,k,v的顺序压在一个tensor里面。</span></span><br><span class="line"><span class="string">        b: 用以线性变换的偏置，按照q,k,v的顺序压在一个tensor里面。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    形状:</span></span><br><span class="line"><span class="string">        输入:</span></span><br><span class="line"><span class="string">        - q: shape:`(..., E)`，E是词嵌入的维度（下面出现的E均为此意）。</span></span><br><span class="line"><span class="string">        - k: shape:`(..., E)`</span></span><br><span class="line"><span class="string">        - v: shape:`(..., E)`</span></span><br><span class="line"><span class="string">        - w: shape:`(E * 3, E)`</span></span><br><span class="line"><span class="string">        - b: shape:`E * 3` </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        输出:</span></span><br><span class="line"><span class="string">        - 输出列表 :`[q&#x27;, k&#x27;, v&#x27;]`，q,k,v经过线性变换前后的形状都一致。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    E = q.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 若为自注意，则q = k = v = src，因此它们的引用变量都是src</span></span><br><span class="line">    <span class="comment"># 即k is v和q is k结果均为True</span></span><br><span class="line">    <span class="comment"># 若为seq2seq，k = v，因而k is v的结果是True</span></span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">is</span> v:</span><br><span class="line">        <span class="keyword">if</span> q <span class="keyword">is</span> k:</span><br><span class="line">            <span class="keyword">return</span> F.linear(q, w, b).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># seq2seq模型</span></span><br><span class="line">            w_q, w_kv = w.split([E, E * <span class="number">2</span>])</span><br><span class="line">            <span class="keyword">if</span> b <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                b_q = b_kv = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                b_q, b_kv = b.split([E, E * <span class="number">2</span>])</span><br><span class="line">            <span class="keyword">return</span> (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        w_q, w_k, w_v = w.chunk(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            b_q = b_k = b_v = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            b_q, b_k, b_v = b.chunk(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)</span></span><br></pre></td></tr></table></figure>
<hr>
<h4 id="3-2-2-遮挡机制"><a href="#3-2-2-遮挡机制" class="headerlink" title="3.2.2 遮挡机制"></a>3.2.2 遮挡机制</h4><p>对于attn_mask来说，若为2D，形状如<code>(L, S)</code>，L和S分别代表着目标语言和源语言序列长度，若为3D,形状如<code>(N * num_heads, L, S)</code>，N代表着batch_size，num_heads代表注意力头的数目。若为attn_mask的dtype为ByteTensor，非0的位置会被忽略不做注意力；若为BoolTensor，True对应的位置会被忽略；若为数值，则会直接加到attn_weights。</p>
<p>因为在decoder解码的时候，只能看该位置和它之前的，如果看后面就犯规了，所以需要attn_mask遮挡住。</p>
<p>下面函数直接复制PyTorch的，意思是确保不同维度的mask形状正确以及不同类型的转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> attn_mask.dtype == torch.uint8:</span><br><span class="line">        warnings.warn(<span class="string">&quot;Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span>)</span><br><span class="line">        attn_mask = attn_mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> attn_mask.is_floating_point() <span class="keyword">or</span> attn_mask.dtype == torch.<span class="built_in">bool</span>, \</span><br><span class="line">            <span class="string">f&quot;Only float, byte, and bool types are supported for attn_mask, not <span class="subst">&#123;attn_mask.dtype&#125;</span>&quot;</span></span><br><span class="line">    <span class="comment"># 对不同维度的形状判定</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">        correct_2d_size = (tgt_len, src_len)</span><br><span class="line">        <span class="keyword">if</span> attn_mask.shape != correct_2d_size:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;The shape of the 2D attn_mask is <span class="subst">&#123;attn_mask.shape&#125;</span>, but should be <span class="subst">&#123;correct_2d_size&#125;</span>.&quot;</span>)</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">        correct_3d_size = (bsz * num_heads, tgt_len, src_len)</span><br><span class="line">        <span class="keyword">if</span> attn_mask.shape != correct_3d_size:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;The shape of the 3D attn_mask is <span class="subst">&#123;attn_mask.shape&#125;</span>, but should be <span class="subst">&#123;correct_3d_size&#125;</span>.&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;attn_mask&#x27;s dimension <span class="subst">&#123;attn_mask.dim()&#125;</span> is not supported&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>与<code>attn_mask</code>不同的是，<code>key_padding_mask</code>是用来遮挡住key里面的值，详细来说应该是<code>&lt;PAD&gt;</code>，被忽略的情况与attn_mask一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将key_padding_mask值改为布尔值</span></span><br><span class="line"><span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> key_padding_mask.dtype == torch.uint8:</span><br><span class="line">    warnings.warn(<span class="string">&quot;Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span>)</span><br><span class="line">    key_padding_mask = key_padding_mask.to(torch.<span class="built_in">bool</span>)</span><br></pre></td></tr></table></figure>
<p>先介绍两个小函数，<code>logical_or</code>，输入两个tensor，并对这两个tensor里的值做<code>逻辑或</code>运算，只有当两个值均为0的时候才为<code>False</code>，其他时候均为<code>True</code>，另一个是<code>masked_fill</code>，输入是一个mask，和用以填充的值。mask由1，0组成，0的位置值维持不变，1的位置用新值填充。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">0</span>],dtype=torch.int8)</span><br><span class="line">b = torch.tensor([<span class="number">4</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],dtype=torch.int8)</span><br><span class="line"><span class="built_in">print</span>(torch.logical_or(a,b))</span><br><span class="line"><span class="comment"># tensor([ True,  True,  True, False])</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">mask = torch.tensor([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(r.masked_fill(mask,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0]])</span></span><br></pre></td></tr></table></figure>
<p>其实attn_mask和key_padding_mask有些时候对象是一致的，所以有时候可以合起来看。<code>-inf</code>做softmax之后值为0，即被忽略。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> key_padding_mask.shape == (bsz, src_len), \</span><br><span class="line">        <span class="string">f&quot;expecting key_padding_mask shape of <span class="subst">&#123;(bsz, src_len)&#125;</span>, but got <span class="subst">&#123;key_padding_mask.shape&#125;</span>&quot;</span></span><br><span class="line">    key_padding_mask = key_padding_mask.view(bsz, <span class="number">1</span>, <span class="number">1</span>, src_len).   \</span><br><span class="line">        expand(-<span class="number">1</span>, num_heads, -<span class="number">1</span>, -<span class="number">1</span>).reshape(bsz * num_heads, <span class="number">1</span>, src_len)</span><br><span class="line">    <span class="comment"># 若attn_mask为空，直接用key_padding_mask</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        attn_mask = key_padding_mask</span><br><span class="line">    <span class="keyword">elif</span> attn_mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">        attn_mask = attn_mask.logical_or(key_padding_mask)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_mask = attn_mask.masked_fill(key_padding_mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若attn_mask值是布尔值，则将mask转换为float</span></span><br><span class="line"><span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> attn_mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">    new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    new_attn_mask.masked_fill_(attn_mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">    attn_mask = new_attn_mask</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<hr>
<h4 id="3-2-3-点积注意力"><a href="#3-2-3-点积注意力" class="headerlink" title="3.2.3 点积注意力"></a>3.2.3 点积注意力</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Tuple</span>, <span class="type">Any</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_scaled_dot_product_attention</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    q: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    k: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    v: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    dropout_p: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    在query, key, value上计算点积注意力，若有注意力遮盖则使用，并且应用一个概率为dropout_p的dropout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        - q: shape:`(B, Nt, E)` B代表batch size， Nt是目标语言序列长度，E是嵌入后的特征维度</span></span><br><span class="line"><span class="string">        - key: shape:`(B, Ns, E)` Ns是源语言序列长度</span></span><br><span class="line"><span class="string">        - value: shape:`(B, Ns, E)`与key形状一样</span></span><br><span class="line"><span class="string">        - attn_mask: 要么是3D的tensor，形状为:`(B, Nt, Ns)`或者2D的tensor，形状如:`(Nt, Ns)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        - Output: attention values: shape:`(B, Nt, E)`，与q的形状一致;attention weights: shape:`(B, Nt, Ns)`</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; q = torch.randn((2,3,6))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; k = torch.randn((2,4,6))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; v = torch.randn((2,4,6))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = scaled_dot_product_attention(q, k, v)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out[0].shape, out[1].shape</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; torch.Size([2, 3, 6]) torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    B, Nt, E = q.shape</span><br><span class="line">    q = q / math.sqrt(E)</span><br><span class="line">    <span class="comment"># (B, Nt, E) x (B, E, Ns) -&gt; (B, Nt, Ns)</span></span><br><span class="line">    attn = torch.bmm(q, k.transpose(-<span class="number">2</span>,-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn += attn_mask </span><br><span class="line">    <span class="comment"># attn意味着目标序列的每个词对源语言序列做注意力</span></span><br><span class="line">    attn = F.softmax(attn, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout_p:</span><br><span class="line">        attn = F.dropout(attn, p=dropout_p)</span><br><span class="line">    <span class="comment"># (B, Nt, Ns) x (B, Ns, E) -&gt; (B, Nt, E)</span></span><br><span class="line">    output = torch.bmm(attn, v)</span><br><span class="line">    <span class="keyword">return</span> output, attn </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-3完整多头注意力机制-MultiheadAttention"><a href="#3-3完整多头注意力机制-MultiheadAttention" class="headerlink" title="3.3完整多头注意力机制-MultiheadAttention"></a>3.3完整多头注意力机制-MultiheadAttention</h3><p><a href="#拆开看多头注意力机制">点击快速回到：拆开看多头注意力机制</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiheadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        embed_dim: 词嵌入的维度</span></span><br><span class="line"><span class="string">        num_heads: 平行头的数量</span></span><br><span class="line"><span class="string">        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; multihead_attn = MultiheadAttention(embed_dim, num_heads)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="comment"># factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.kdim = kdim <span class="keyword">if</span> kdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.vdim = vdim <span class="keyword">if</span> vdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self._qkv_same_embed_dim = self.kdim == embed_dim <span class="keyword">and</span> self.vdim == embed_dim</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.head_dim = embed_dim // num_heads</span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == self.embed_dim, <span class="string">&quot;embed_dim must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim)))</span><br><span class="line">            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim)))</span><br><span class="line">            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim)))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim)))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;q_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;k_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;v_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.in_proj_bias = Parameter(torch.empty(<span class="number">3</span> * embed_dim))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim:</span><br><span class="line">            xavier_uniform_(self.in_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xavier_uniform_(self.q_proj_weight)</span><br><span class="line">            xavier_uniform_(self.k_proj_weight)</span><br><span class="line">            xavier_uniform_(self.v_proj_weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.in_proj_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            constant_(self.in_proj_bias, <span class="number">0.</span>)</span><br><span class="line">            constant_(self.out_proj.bias, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>, attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            query, key, value = [x.transpose(<span class="number">1</span>, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (query, key, value)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._qkv_same_embed_dim:</span><br><span class="line">            attn_output, attn_output_weights = multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask, use_separate_proj_weight=<span class="literal">True</span>,</span><br><span class="line">                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,</span><br><span class="line">                v_proj_weight=self.v_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_output_weights = multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">return</span> attn_output.transpose(<span class="number">1</span>, <span class="number">0</span>), attn_output_weights</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> attn_output, attn_output_weights</span><br></pre></td></tr></table></figure>
<p>接下来可以实践一下，并且把位置编码加起来，可以发现加入位置编码和进行多头注意力的前后形状都是不会变的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为batch_first为False,所以src的shape：`(seq, batch, embed_dim)`</span></span><br><span class="line">src = torch.randn((<span class="number">2</span>,<span class="number">4</span>,<span class="number">100</span>))</span><br><span class="line">src = positional_encoding(src,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(src.shape)</span><br><span class="line">multihead_attn = MultiheadAttention(<span class="number">100</span>, <span class="number">4</span>, <span class="number">0.1</span>)</span><br><span class="line">attn_output, attn_output_weights = multihead_attn(src,src,src)</span><br><span class="line"><span class="built_in">print</span>(attn_output.shape, attn_output_weights.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 100])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 100]) torch.Size([4, 2, 2])</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 4, 100])
torch.Size([2, 4, 100]) torch.Size([4, 2, 2])
</code></pre><hr>
<h2 id="4-TransformerEncoder"><a href="#4-TransformerEncoder" class="headerlink" title="4.TransformerEncoder"></a>4.TransformerEncoder</h2><h3 id="4-1-Encoder-Layer"><a href="#4-1-Encoder-Layer" class="headerlink" title="4.1 Encoder Layer"></a>4.1 Encoder Layer</h3><p><img src="https://img-blog.csdnimg.cn/4f3c7696e08d47948ee6b34164c213d6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="encoder"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        d_model: 词嵌入的维度（必备）</span></span><br><span class="line"><span class="string">        nhead: 多头注意力中平行头的数目（必备）</span></span><br><span class="line"><span class="string">        dim_feedforward: 全连接层的神经元的数目，又称经过此层输入的维度（Default = 2048）</span></span><br><span class="line"><span class="string">        dropout: dropout的概率（Default = 0.1）</span></span><br><span class="line"><span class="string">        activation: 两个线性层中间的激活函数，默认relu或gelu</span></span><br><span class="line"><span class="string">        lay_norm_eps: layer normalization中的微小量，防止分母为0（Default = 1e-5）</span></span><br><span class="line"><span class="string">        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)（Default：False）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.randn((32, 10, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=F.relu,</span></span></span><br><span class="line"><span class="params"><span class="function">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)</span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line"></span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        self.activation = activation        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        src = positional_encoding(src, src.shape[-<span class="number">1</span>])</span><br><span class="line">        src2 = self.self_attn(src, src, src, attn_mask=src_mask, </span><br><span class="line">        key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src = self.norm1(src)</span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        src = self.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用小例子看一下</span></span><br><span class="line">encoder_layer = TransformerEncoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">src = torch.randn((<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>))</span><br><span class="line">out = encoder_layer(src)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([32, 10, 512])</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-Transformer-layer组成Encoder"><a href="#4-2-Transformer-layer组成Encoder" class="headerlink" title="4.2 Transformer layer组成Encoder"></a>4.2 Transformer layer组成Encoder</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        encoder_layer（必备）</span></span><br><span class="line"><span class="string">        num_layers： encoder_layer的层数（必备）</span></span><br><span class="line"><span class="string">        norm: 归一化的选择（可选）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.randn((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layer = encoder_layer</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        output = positional_encoding(src, src.shape[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            output = self.layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 例子</span></span><br><span class="line">encoder_layer = TransformerEncoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_encoder = TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">src = torch.randn((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = transformer_encoder(src)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([10, 32, 512])</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="5-Decoder-Layer"><a href="#5-Decoder-Layer" class="headerlink" title="5. Decoder Layer:"></a>5. Decoder Layer:</h2><p>以两层encoder-decoder结构举例<br><img src="https://img-blog.csdnimg.cn/ed6bb16f86ec497eae9c65be754cad7c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="2层的transformer"></p>
<h3 id="5-1-TransformerDecoderLayer"><a href="#5-1-TransformerDecoderLayer" class="headerlink" title="5.1 TransformerDecoderLayer"></a>5.1 TransformerDecoderLayer</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        d_model: 词嵌入的维度（必备）</span></span><br><span class="line"><span class="string">        nhead: 多头注意力中平行头的数目（必备）</span></span><br><span class="line"><span class="string">        dim_feedforward: 全连接层的神经元的数目，又称经过此层输入的维度（Default = 2048）</span></span><br><span class="line"><span class="string">        dropout: dropout的概率（Default = 0.1）</span></span><br><span class="line"><span class="string">        activation: 两个线性层中间的激活函数，默认relu或gelu</span></span><br><span class="line"><span class="string">        lay_norm_eps: layer normalization中的微小量，防止分母为0（Default = 1e-5）</span></span><br><span class="line"><span class="string">        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)（Default：False）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.randn((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.randn((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=F.relu,</span></span></span><br><span class="line"><span class="params"><span class="function">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)</span><br><span class="line"></span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line"></span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout3 = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            tgt: 目标语言序列（必备）</span></span><br><span class="line"><span class="string">            memory: 从最后一个encoder_layer跑出的句子（必备）</span></span><br><span class="line"><span class="string">            tgt_mask: 目标语言序列的mask（可选）</span></span><br><span class="line"><span class="string">            memory_mask（可选）</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask（可选）</span></span><br><span class="line"><span class="string">            memory_key_padding_mask（可选）</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">        tgt = self.norm1(tgt)</span><br><span class="line">        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">        tgt = self.norm2(tgt)</span><br><span class="line">        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))</span><br><span class="line">        tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">        tgt = self.norm3(tgt)</span><br><span class="line">        <span class="keyword">return</span> tgt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可爱的小例子</span></span><br><span class="line">decoder_layer = nn.TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">memory = torch.randn((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.randn((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = decoder_layer(tgt, memory)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure>
<h3 id="5-2-Transformer-layer组成Decoder"><a href="#5-2-Transformer-layer组成Decoder" class="headerlink" title="5.2 Transformer layer组成Decoder"></a>5.2 Transformer layer组成Decoder</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        decoder_layer（必备）</span></span><br><span class="line"><span class="string">        num_layers: decoder_layer的层数（必备）</span></span><br><span class="line"><span class="string">        norm: 归一化选择</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer =TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layer = decoder_layer</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        output = tgt</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            output = self.layer(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可爱的小例子</span></span><br><span class="line">decoder_layer =TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_decoder = TransformerDecoder(decoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">memory = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">tgt = torch.rand(<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">out = transformer_decoder(tgt, memory)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure>
<p>总结一下，其实经过位置编码，多头注意力，Encoder Layer和Decoder Layer形状不会变的，而Encoder和Decoder分别与src和tgt形状一致</p>
<h2 id="6-Transformer"><a href="#6-Transformer" class="headerlink" title="6. Transformer"></a>6. Transformer</h2><p><img src="https://img-blog.csdnimg.cn/656f54d652af47598faa29dc100dc4ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0,size_16,color_FFFFFF,t_70#pic_center" alt="Transformer"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        d_model: 词嵌入的维度（必备）（Default=512）</span></span><br><span class="line"><span class="string">        nhead: 多头注意力中平行头的数目（必备）（Default=8）</span></span><br><span class="line"><span class="string">        num_encoder_layers:编码层层数（Default=8）</span></span><br><span class="line"><span class="string">        num_decoder_layers:解码层层数（Default=8）</span></span><br><span class="line"><span class="string">        dim_feedforward: 全连接层的神经元的数目，又称经过此层输入的维度（Default = 2048）</span></span><br><span class="line"><span class="string">        dropout: dropout的概率（Default = 0.1）</span></span><br><span class="line"><span class="string">        activation: 两个线性层中间的激活函数，默认relu或gelu</span></span><br><span class="line"><span class="string">        custom_encoder: 自定义encoder（Default=None）</span></span><br><span class="line"><span class="string">        custom_decoder: 自定义decoder（Default=None）</span></span><br><span class="line"><span class="string">        lay_norm_eps: layer normalization中的微小量，防止分母为0（Default = 1e-5）</span></span><br><span class="line"><span class="string">        batch_first: 若`True`，则为(batch, seq, feture)，若为`False`，则为(seq, batch, feature)（Default：False）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    例子：</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 activation = F.relu, custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.encoder = custom_encoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first)</span><br><span class="line">            encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.decoder = custom_decoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first)</span><br><span class="line">            decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)</span><br><span class="line">            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.nhead = nhead</span><br><span class="line"></span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, tgt: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            src: 源语言序列（送入Encoder）（必备）</span></span><br><span class="line"><span class="string">            tgt: 目标语言序列（送入Decoder）（必备）</span></span><br><span class="line"><span class="string">            src_mask: （可选)</span></span><br><span class="line"><span class="string">            tgt_mask: （可选）</span></span><br><span class="line"><span class="string">            memory_mask: （可选）</span></span><br><span class="line"><span class="string">            src_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: （可选）</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        形状：</span></span><br><span class="line"><span class="string">            - src: shape:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: shape:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: shape:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: shape:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: shape:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: shape:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: shape:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: shape:`(N, S)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [src/tgt/memory]_mask确保有些位置不被看到，如做decode的时候，只能看该位置及其以前的，而不能看后面的。</span></span><br><span class="line"><span class="string">            若为ByteTensor，非0的位置会被忽略不做注意力；若为BoolTensor，True对应的位置会被忽略；</span></span><br><span class="line"><span class="string">            若为数值，则会直接加到attn_weights</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [src/tgt/memory]_key_padding_mask 使得key里面的某些元素不参与attention计算，三种情况同上</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - output: shape:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        注意：</span></span><br><span class="line"><span class="string">            src和tgt的最后一维需要等于d_model，batch的那一维需要相等</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        例子:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_square_subsequent_mask</span>(<span class="params">self, sz: <span class="built_in">int</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;产生关于序列的mask，被遮住的区域赋值`-inf`，未被遮住的区域赋值为`0`&#x27;&#x27;&#x27;</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.<span class="built_in">float</span>().masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class="number">1</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">r&#x27;&#x27;&#x27;用正态分布初始化参数&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                xavier_uniform_(p)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 小例子</span></span><br><span class="line">transformer_model = Transformer(nhead=<span class="number">16</span>, num_encoder_layers=<span class="number">12</span>)</span><br><span class="line">src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = transformer_model(src, tgt)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># torch.Size([20, 32, 512])</span></span><br></pre></td></tr></table></figure>
<p>到此为止，PyTorch的Transformer库我们已经全部实现，相比于官方的版本，手写的这个少了较多的判定语句。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>本文由台运鹏撰写，datawhale-learn-nlp-with-transformers项目成员重新组织和整理。最后，期待您的阅读反馈和star，谢谢。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/24/8月组队学习：nlp之transformers入门/选读1：Transformer代码解读（Pytorch）/">https://zhxnlp.github.io/2021/08/24/8月组队学习：nlp之transformers入门/选读1：Transformer代码解读（Pytorch）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/26/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task6%EF%BC%9ATransformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/"><i class="fa fa-chevron-left">  </i><span>task6：BERT文本分类</span></a></div><div class="next-post pull-right"><a href="/2021/08/23/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task5%EF%BC%9ABERT%E5%85%B7%E4%BD%93%E5%BA%94%E7%94%A8%EF%BC%88%E5%BE%85%E8%A1%A5%E5%85%85%EF%BC%89/"><span>task5：BERT具体应用</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>