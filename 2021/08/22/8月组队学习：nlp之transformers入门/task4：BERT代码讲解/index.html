<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="task4：BERT代码讲解"><meta name="keywords" content="nlp,transformer"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>task4：BERT代码讲解 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Tokenization%E5%88%86%E8%AF%8D-BertTokenizer"><span class="toc-number">1.</span> <span class="toc-text">1-Tokenization分词-BertTokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Tokenization%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 Tokenization代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Tokenization%E4%BB%A3%E7%A0%81%E8%AE%B2%E8%A7%A3"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 Tokenization代码讲解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Model-BertModel"><span class="toc-number">2.</span> <span class="toc-text">2-Model-BertModel</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1BertModel-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-number">2.1.</span> <span class="toc-text">2.1BertModel 前向传播过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-BertPreTrainedModel%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 BertPreTrainedModel完整代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-BertEmbeddings"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 BertEmbeddings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-BertEncoder"><span class="toc-number">3.</span> <span class="toc-text">3 BertEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-BertAttention"><span class="toc-number">3.1.</span> <span class="toc-text">3.2 BertAttention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-BertSelfAttention"><span class="toc-number">3.2.</span> <span class="toc-text">3.3 BertSelfAttention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-BertSelfOutput"><span class="toc-number">3.3.</span> <span class="toc-text">3.4 BertSelfOutput</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">46</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">37</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">task4：BERT代码讲解</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers/">8月组队学习：nlp之transformers</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">9.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 49 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>BERT代码实现<br>@[toc]<br>前言<br>&#8195;&#8195;本文是复制datawhale关于transformer教程里的一章<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT?id=%E5%89%8D%E8%A8%80">《如何实现一个BERT》</a>。本文包含大量源码和讲解，通过段落和横线分割了各个模块，同时网站配备了侧边栏，帮助大家在各个小节中快速跳转，希望大家阅读完能对BERT有深刻的了解。同时建议通过pycharm、vscode等工具对bert源码进行单步调试，调试到对应的模块再对比看本章节的讲解。</p>
<p>&#8195;&#8195;本篇章将基于<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">HuggingFace/Transformers, 48.9k Star</a>进行学习。本章节的全部代码在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert">huggingface bert</a>，注意由于版本更新较快，可能存在差别，请以4.4.2版本为准。<br>&#8195;&#8195;HuggingFace 是一家总部位于纽约的聊天机器人初创服务商，很早就捕捉到 BERT 大潮流的信号并着手实现基于 pytorch 的 BERT 模型。这一项目最初名为 pytorch-pretrained-bert，在复现了原始效果的同时，提供了易用的方法以方便在这一强大模型的基础上进行各种玩耍和研究。</p>
<p>&#8195;&#8195;随着使用人数的增加，这一项目也发展成为一个较大的开源社区，合并了各种预训练语言模型以及增加了 Tensorflow 的实现，并且在 2019 年下半年改名为 Transformers。截止写文章时（2021 年 3 月 30 日）这一项目已经拥有 43k+ 的star，可以说 Transformers 已经成为事实上的 NLP 基本工具。</p>
<p>本文基于 Transformers 版本 4.4.2（2021 年 3 月 19 日发布）项目中，pytorch 版的 BERT 相关代码，从代码结构、具体实现与原理，以及使用的角度进行分析。<br><span id="more"></span></p>
<h2 id="1-Tokenization分词-BertTokenizer"><a href="#1-Tokenization分词-BertTokenizer" class="headerlink" title="1-Tokenization分词-BertTokenizer"></a>1-Tokenization分词-BertTokenizer</h2><h3 id="1-1-Tokenization代码"><a href="#1-1-Tokenization代码" class="headerlink" title="1.1 Tokenization代码"></a>1.1 Tokenization代码</h3><p>和BERT 有关的 Tokenizer 主要写在models/bert/tokenization_bert.py中。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers.tokenization_utils <span class="keyword">import</span> PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace</span><br><span class="line"><span class="keyword">from</span> transformers.utils <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logger = logging.get_logger(__name__)</span><br><span class="line"></span><br><span class="line">VOCAB_FILES_NAMES = &#123;<span class="string">&quot;vocab_file&quot;</span>: <span class="string">&quot;vocab.txt&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">PRETRAINED_VOCAB_FILES_MAP = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_file&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="string">&quot;https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = &#123;</span><br><span class="line">    <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="number">512</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PRETRAINED_INIT_CONFIGURATION = &#123;</span><br><span class="line">    <span class="string">&quot;bert-base-uncased&quot;</span>: &#123;<span class="string">&quot;do_lower_case&quot;</span>: <span class="literal">True</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_vocab</span>(<span class="params">vocab_file</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads a vocabulary file into a dictionary.&quot;&quot;&quot;</span></span><br><span class="line">    vocab = collections.OrderedDict()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">        tokens = reader.readlines()</span><br><span class="line">    <span class="keyword">for</span> index, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        token = token.rstrip(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        vocab[token] = index</span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">whitespace_tokenize</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Runs basic whitespace cleaning and splitting on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = text.strip()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> text:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    tokens = text.split()</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span>(<span class="params">PreTrainedTokenizer</span>):</span></span><br><span class="line"></span><br><span class="line">    vocab_files_names = VOCAB_FILES_NAMES</span><br><span class="line">    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP</span><br><span class="line">    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION</span><br><span class="line">    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        vocab_file,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_lower_case=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_basic_tokenize=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        never_split=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        unk_token=<span class="string">&quot;[UNK]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        sep_token=<span class="string">&quot;[SEP]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        pad_token=<span class="string">&quot;[PAD]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        cls_token=<span class="string">&quot;[CLS]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        mask_token=<span class="string">&quot;[MASK]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenize_chinese_chars=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        strip_accents=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(</span><br><span class="line">            do_lower_case=do_lower_case,</span><br><span class="line">            do_basic_tokenize=do_basic_tokenize,</span><br><span class="line">            never_split=never_split,</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            sep_token=sep_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            cls_token=cls_token,</span><br><span class="line">            mask_token=mask_token,</span><br><span class="line">            tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">            strip_accents=strip_accents,</span><br><span class="line">            **kwargs,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(vocab_file):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Can&#x27;t find a vocabulary file at path &#x27;<span class="subst">&#123;vocab_file&#125;</span>&#x27;. To load the vocabulary from a Google pretrained &quot;</span></span><br><span class="line">                <span class="string">&quot;model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span></span><br><span class="line">            )</span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="keyword">for</span> tok, ids <span class="keyword">in</span> self.vocab.items()])</span><br><span class="line">        self.do_basic_tokenize = do_basic_tokenize</span><br><span class="line">        <span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case,</span><br><span class="line">                never_split=never_split,</span><br><span class="line">                tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">                strip_accents=strip_accents,</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_lower_case</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.basic_tokenizer.do_lower_case</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(self.vocab, **self.added_tokens_encoder)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">if</span> self.do_basic_tokenize:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># If the token is part of the never_split set</span></span><br><span class="line">                <span class="keyword">if</span> token <span class="keyword">in</span> self.basic_tokenizer.never_split:</span><br><span class="line">                    split_tokens.append(token)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    split_tokens += self.wordpiece_tokenizer.tokenize(token)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            split_tokens = self.wordpiece_tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_token_to_id</span>(<span class="params">self, token</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts a token (str) in an id using the vocab.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.vocab.get(token, self.vocab.get(self.unk_token))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_id_to_token</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts an index (integer) in a token (str) using the vocab.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.ids_to_tokens.get(index, self.unk_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_string</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts a sequence of tokens (string) in a single string.&quot;&quot;&quot;</span></span><br><span class="line">        out_string = <span class="string">&quot; &quot;</span>.join(tokens).replace(<span class="string">&quot; ##&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line">        <span class="keyword">return</span> out_string</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_inputs_with_special_tokens</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span></span><br><span class="line"><span class="string">        adding special tokens. A BERT sequence has the following format:</span></span><br><span class="line"><span class="string">        - single sequence: ``[CLS] X [SEP]``</span></span><br><span class="line"><span class="string">        - pair of sequences: ``[CLS] A [SEP] B [SEP]``</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs to which the special tokens will be added.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: List of `input IDs &lt;../glossary.html#input-ids&gt;`__ with the appropriate special tokens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [self.cls_token_id] + token_ids_0 + [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        <span class="keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_special_tokens_mask</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span>, already_has_special_tokens: <span class="built_in">bool</span> = <span class="literal">False</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding</span></span><br><span class="line"><span class="string">        special tokens using the tokenizer ``prepare_for_model`` method.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span></span><br><span class="line"><span class="string">                Whether or not the token list is already formatted with special tokens for the model.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> already_has_special_tokens:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">super</span>().get_special_tokens_mask(</span><br><span class="line">                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_0)) + [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_1)) + [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_0)) + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_token_type_ids_from_sequences</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence</span></span><br><span class="line"><span class="string">        pair mask has the following format:</span></span><br><span class="line"><span class="string">        ::</span></span><br><span class="line"><span class="string">            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">            | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: List of `token type IDs &lt;../glossary.html#token-type-ids&gt;`_ according to the given</span></span><br><span class="line"><span class="string">            sequence(s).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(cls + token_ids_0 + sep) * [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(cls + token_ids_0 + sep) * [<span class="number">0</span>] + <span class="built_in">len</span>(token_ids_1 + sep) * [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vocabulary</span>(<span class="params">self, save_directory: <span class="built_in">str</span>, filename_prefix: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">str</span>]:</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(save_directory):</span><br><span class="line">            vocab_file = os.path.join(</span><br><span class="line">                save_directory, (filename_prefix + <span class="string">&quot;-&quot;</span> <span class="keyword">if</span> filename_prefix <span class="keyword">else</span> <span class="string">&quot;&quot;</span>) + VOCAB_FILES_NAMES[<span class="string">&quot;vocab_file&quot;</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab_file = (filename_prefix + <span class="string">&quot;-&quot;</span> <span class="keyword">if</span> filename_prefix <span class="keyword">else</span> <span class="string">&quot;&quot;</span>) + save_directory</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(vocab_file, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> token, token_index <span class="keyword">in</span> <span class="built_in">sorted</span>(self.vocab.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> index != token_index:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">f&quot;Saving vocabulary to <span class="subst">&#123;vocab_file&#125;</span>: vocabulary indices are not consecutive.&quot;</span></span><br><span class="line">                        <span class="string">&quot; Please check that the vocabulary is not corrupted!&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    index = token_index</span><br><span class="line">                writer.write(token + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (vocab_file,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, do_lower_case=<span class="literal">True</span>, never_split=<span class="literal">None</span>, tokenize_chinese_chars=<span class="literal">True</span>, strip_accents=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            never_split = []</span><br><span class="line">        self.do_lower_case = do_lower_case</span><br><span class="line">        self.never_split = <span class="built_in">set</span>(never_split)</span><br><span class="line">        self.tokenize_chinese_chars = tokenize_chinese_chars</span><br><span class="line">        self.strip_accents = strip_accents</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Basic Tokenization of a piece of text. Split on &quot;white spaces&quot; only, for sub-word tokenization, see</span></span><br><span class="line"><span class="string">        WordPieceTokenizer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`) list of str</span></span><br><span class="line"><span class="string">                Kept for backward compatibility purposes. Now implemented directly at the base class level (see</span></span><br><span class="line"><span class="string">                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># union() returns a new set by concatenating the two sets.</span></span><br><span class="line">        never_split = self.never_split.union(<span class="built_in">set</span>(never_split)) <span class="keyword">if</span> never_split <span class="keyword">else</span> self.never_split</span><br><span class="line">        text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">        <span class="comment"># models. This is also applied to the English models now, but it doesn&#x27;t</span></span><br><span class="line">        <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">        <span class="comment"># and generally don&#x27;t have any Chinese data in them (there are Chinese</span></span><br><span class="line">        <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">        <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenize_chinese_chars:</span><br><span class="line">            text = self._tokenize_chinese_chars(text)</span><br><span class="line">        orig_tokens = whitespace_tokenize(text)</span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> never_split:</span><br><span class="line">                <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">                    token = token.lower()</span><br><span class="line">                    <span class="keyword">if</span> self.strip_accents <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">False</span>:</span><br><span class="line">                        token = self._run_strip_accents(token)</span><br><span class="line">                <span class="keyword">elif</span> self.strip_accents:</span><br><span class="line">                    token = self._run_strip_accents(token)</span><br><span class="line">            split_tokens.extend(self._run_split_on_punc(token, never_split))</span><br><span class="line"></span><br><span class="line">        output_tokens = whitespace_tokenize(<span class="string">&quot; &quot;</span>.join(split_tokens))</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">&quot;NFD&quot;</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">&quot;Mn&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = <span class="built_in">list</span>(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[-<span class="number">1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = <span class="built_in">ord</span>(char)</span><br><span class="line">            <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">                output.append(char)</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span>(<span class="params">self, cp</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:</span></span><br><span class="line">        <span class="comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span></span><br><span class="line">        <span class="comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span></span><br><span class="line">        <span class="comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span></span><br><span class="line">        <span class="comment"># space-separated words, so they are not treated specially and handled</span></span><br><span class="line">        <span class="comment"># like the all of the other languages.</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            (cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)  <span class="comment">#</span></span><br><span class="line">        ):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = <span class="built_in">ord</span>(char)</span><br><span class="line">            <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xFFFD</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Runs WordPiece tokenization.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab, unk_token, max_input_chars_per_word=<span class="number">100</span></span>):</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform</span></span><br><span class="line"><span class="string">        tokenization using the given vocabulary.</span></span><br><span class="line"><span class="string">        For example, :obj:`input = &quot;unaffable&quot;` wil return as output :obj:`[&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]`.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">            already been passed through `BasicTokenizer`.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = <span class="built_in">list</span>(token)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="literal">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">                end = <span class="built_in">len</span>(chars)</span><br><span class="line">                cur_substr = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">&quot;&quot;</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">&quot;##&quot;</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    is_bad = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span>(<span class="params">PreTrainedTokenizer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Construct a BERT tokenizer. Based on WordPiece.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.</span></span><br><span class="line"><span class="string">    Users should refer to this superclass for more information regarding those methods.</span></span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-Tokenization代码讲解"><a href="#1-2-Tokenization代码讲解" class="headerlink" title="1.2 Tokenization代码讲解"></a>1.2 Tokenization代码讲解</h3><p>&#8195;&#8195;BertTokenizer 是基于BasicTokenizer和WordPieceTokenizer的分词器：</p>
<p>&#8195;&#8195;BasicTokenizer负责处理的第一步——按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。<br>&#8195;&#8195;对于中文字符，通过预处理（加空格）来按字分割；同时可以通过never_split指定对某些词不进行分割；这一步是可选的（默认执行）。<br>&#8195;&#8195;WordPieceTokenizer在词的基础上，进一步将词分解为子词（subword）。<br>&#8195;&#8195;subword 介于 char 和 word 之间，既在一定程度保留了词的含义，又能够照顾到英文中单复数、时态导致的词表爆炸和未登录词的 OOV（Out-Of-Vocabulary）问题，将词根与时态词缀等分割出来，从而减小词表，也降低了训练难度；<br>&#8195;&#8195;例如，tokenizer 这个词就可以拆解为“token”和“##izer”两部分，注意后面一个词的“##”表示接在前一个词后面。 BertTokenizer 有以下常用方法：</p>
<ul>
<li>from_pretrained：从包含词表文件（vocab.txt）的目录中初始化一个分词器；</li>
<li>tokenize：将文本（词或者句子）分解为子词列表；</li>
<li>convert_tokens_to_ids：将子词列表转化为子词对应下标的列表；</li>
<li>convert_ids_to_tokens ：与上一个相反；</li>
<li>convert_tokens_to_string：将 subword 列表按“##”拼接回词或者句子；</li>
<li>encode：对于单个句子输入，分解词并加入特殊词形成“[CLS], x, [SEP]”的结构并转换为词表对应下标的列表；对于两个句子输入（多个句子只取前两个），分解词并加入特殊词形成“[CLS], x1, [SEP], x2, [SEP]”的结构并转换为下标列表；</li>
<li>decode：可以将 encode 方法的输出变为完整句子。 以及，类自身的方法：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bt = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">bt(<span class="string">&#x27;I like natural language progressing!&#x27;</span>)</span><br><span class="line"><span class="comment"># &#123;&#x27;input_ids&#x27;: [101, 1045, 2066, 3019, 2653, 27673, 999, 102], &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 0, 0], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1]&#125;</span></span><br><span class="line">Downloading: <span class="number">100</span>%|██████████| 232k/232k [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, 698kB/s]</span><br><span class="line">Downloading: <span class="number">100</span>%|██████████| <span class="number">28.0</span>/<span class="number">28.0</span> [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">11.1</span>kB/s]</span><br><span class="line">Downloading: <span class="number">100</span>%|██████████| 466k/466k [<span class="number">00</span>:<span class="number">00</span>&lt;<span class="number">00</span>:<span class="number">00</span>, 863kB/s]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">1045</span>, <span class="number">2066</span>, <span class="number">3019</span>, <span class="number">2653</span>, <span class="number">27673</span>, <span class="number">999</span>, <span class="number">102</span>], <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="2-Model-BertModel"><a href="#2-Model-BertModel" class="headerlink" title="2-Model-BertModel"></a>2-Model-BertModel</h2><p>&#8195;&#8195;和 BERT 模型有关的代码主要写在/models/bert/modeling_bert.py中，这一份代码有一千多行，包含 BERT 模型的基本结构和基于它的微调模型等。</p>
<p>&#8195;&#8195;下面从 BERT 模型本体入手分析：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span>(<span class="params">BertPreTrainedModel</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of</span></span><br><span class="line"><span class="string">    cross-attention is added between the self-attention layers, following the architecture described in `Attention is</span></span><br><span class="line"><span class="string">    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,</span></span><br><span class="line"><span class="string">    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration</span></span><br><span class="line"><span class="string">    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`</span></span><br><span class="line"><span class="string">    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an</span></span><br><span class="line"><span class="string">    input to the forward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;BertModel 主要为 transformer encoder 结构，包含三个部分：</p>
<ul>
<li>embeddings，即BertEmbeddings类的实体，根据单词符号获取对应的向量表示；</li>
<li>encoder，即BertEncoder类的实体；</li>
<li>pooler，即BertPooler类的实体，这一部分是可选的。<br>注意 BertModel 也可以配置为 Decoder，不过下文中不包含对这一部分的讨论。</li>
</ul>
<h3 id="2-1BertModel-前向传播过程"><a href="#2-1BertModel-前向传播过程" class="headerlink" title="2.1BertModel 前向传播过程"></a>2.1BertModel 前向传播过程</h3><p>介绍BertModel 前向传播过程中各个参数的含义以及返回值：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_values=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span> ...</span><br></pre></td></tr></table></figure></p>
<ul>
<li>input_ids：经过 tokenizer 分词后的 subword 对应的下标列表；</li>
<li>attention_mask：在 self-attention 过程中，这一块 mask 用于标记 subword 所处句子和<br>padding 的区别，将 padding 部分填充为 0；</li>
<li>token_type_ids：标记 subword 当前所处句子（第一句/第二句/ padding）；</li>
<li>position_ids：标记当前词所在句子的位置下标；</li>
<li>head_mask：用于将某些层的某些注意力计算无效化；</li>
<li>inputs_embeds：如果提供了，那就不需要input_ids，跨过 embedding lookup 过程直接作为 Embedding 进入 Encoder 计算；</li>
<li>encoder_hidden_states：这一部分在 BertModel 配置为 decoder 时起作用，将执行 cross-attention 而不是 self-attention；</li>
<li>encoder_attention_mask：同上，在 cross-attention 中用于标记 encoder 端输入的 padding；</li>
<li>past_key_values：这个参数貌似是把预先计算好的 K-V 乘积传入，以降低 cross-attention 的开销（因为原本这部分是重复计算）；</li>
<li>use_cache：将保存上一个参数并传回，加速 decoding；</li>
<li>output_attentions：是否返回中间每层的 attention 输出；</li>
<li>output_hidden_states：是否返回中间每层的输出；</li>
<li>return_dict：是否按键值对的形式（ModelOutput 类，也可以当作 tuple 用）返回输出，默认为真。<br>注意，这里的 head_mask 对注意力计算的无效化，和下文提到的注意力头剪枝不同，而仅仅把某些注意力的计算结果给乘以这一系数。</li>
</ul>
<p>输出部分如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># BertModel的前向传播返回部分</span></span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">           <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions(</span><br><span class="line">           last_hidden_state=sequence_output,</span><br><span class="line">           pooler_output=pooled_output,</span><br><span class="line">           past_key_values=encoder_outputs.past_key_values,</span><br><span class="line">           hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">           attentions=encoder_outputs.attentions,</span><br><span class="line">           cross_attentions=encoder_outputs.cross_attentions,</span><br><span class="line">       ) </span><br></pre></td></tr></table></figure></p>
<p>可以看出，返回值不但包含了 encoder 和 pooler 的输出，也包含了其他指定输出的部分（hidden_states 和 attention 等，这一部分在encoder_outputs[1:]）方便取用：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># BertEncoder的前向传播返回部分，即上面的encoder_outputs</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">        v</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> [</span><br><span class="line">            hidden_states,</span><br><span class="line">            next_decoder_cache,</span><br><span class="line">            all_hidden_states,</span><br><span class="line">            all_self_attentions,</span><br><span class="line">            all_cross_attentions,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"><span class="keyword">return</span> BaseModelOutputWithPastAndCrossAttentions(</span><br><span class="line">    last_hidden_state=hidden_states,</span><br><span class="line">    past_key_values=next_decoder_cache,</span><br><span class="line">    hidden_states=all_hidden_states,</span><br><span class="line">    attentions=all_self_attentions,</span><br><span class="line">    cross_attentions=all_cross_attentions,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>此外，BertModel 还有以下的方法，方便 BERT 玩家进行各种操作：</p>
<ul>
<li>get_input_embeddings：提取 embedding 中的 word_embeddings 即词向量部分；</li>
<li>set_input_embeddings：为 embedding 中的 word_embeddings 赋值；</li>
<li>_prune_heads：提供了将注意力头剪枝的函数，输入为{layer_num: list of heads to prune in this layer}的字典，可以将指定层的某些注意力头剪枝。</li>
</ul>
<p><strong>剪枝是一个复杂的操作，需要将保留的注意力头部分的 Wq、Kq、Vq 和拼接后全连接部分的权重拷贝到一个新的较小的权重矩阵（注意先禁止 grad 再拷贝），并实时记录被剪掉的头以防下标出错。具体参考BertAttention部分的prune_heads方法.</strong></p>
<h3 id="2-2-BertPreTrainedModel完整代码"><a href="#2-2-BertPreTrainedModel完整代码" class="headerlink" title="2.2 BertPreTrainedModel完整代码"></a>2.2 BertPreTrainedModel完整代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.models.bert.modeling_bert <span class="keyword">import</span> *</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span>(<span class="params">BertPreTrainedModel</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of</span></span><br><span class="line"><span class="string">    cross-attention is added between the self-attention layers, following the architecture described in `Attention is</span></span><br><span class="line"><span class="string">    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,</span></span><br><span class="line"><span class="string">    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</span></span><br><span class="line"><span class="string">    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration</span></span><br><span class="line"><span class="string">    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`</span></span><br><span class="line"><span class="string">    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an</span></span><br><span class="line"><span class="string">    input to the forward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line"></span><br><span class="line">        self.pooler = BertPooler(config) <span class="keyword">if</span> add_pooling_layer <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prunes heads of the model. heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125; See base</span></span><br><span class="line"><span class="string">        class PreTrainedModel</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            self.encoder.layer[layer].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_model_forward(<span class="params">BERT_INPUTS_DOCSTRING.<span class="built_in">format</span>(<span class="params"><span class="string">&quot;batch_size, sequence_length&quot;</span></span>)</span>)</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">        tokenizer_class=_TOKENIZER_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">        checkpoint=_CHECKPOINT_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">        output_type=BaseModelOutputWithPoolingAndCrossAttentions,</span></span></span><br><span class="line"><span class="params"><span class="meta">        config_class=_CONFIG_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">    </span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_values=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):</span></span><br><span class="line"><span class="string">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if</span></span><br><span class="line"><span class="string">            the model is configured as a decoder.</span></span><br><span class="line"><span class="string">        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span></span><br><span class="line"><span class="string">            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in</span></span><br><span class="line"><span class="string">            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:</span></span><br><span class="line"><span class="string">            - 1 for tokens that are **not masked**,</span></span><br><span class="line"><span class="string">            - 0 for tokens that are **masked**.</span></span><br><span class="line"><span class="string">        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span></span><br><span class="line"><span class="string">            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</span></span><br><span class="line"><span class="string">            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`</span></span><br><span class="line"><span class="string">            (those that don&#x27;t have their past key value states given to this model) of shape :obj:`(batch_size, 1)`</span></span><br><span class="line"><span class="string">            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.</span></span><br><span class="line"><span class="string">        use_cache (:obj:`bool`, `optional`):</span></span><br><span class="line"><span class="string">            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up</span></span><br><span class="line"><span class="string">            decoding (see :obj:`past_key_values`).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder:</span><br><span class="line">            use_cache = use_cache <span class="keyword">if</span> use_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_cache</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_cache = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">            batch_size, seq_length = input_shape</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:-<span class="number">1</span>]</span><br><span class="line">            batch_size, seq_length = input_shape</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="comment"># past_key_values_length</span></span><br><span class="line">        past_key_values_length = past_key_values[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">2</span>] <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self.embeddings, <span class="string">&quot;token_type_ids&quot;</span>):</span><br><span class="line">                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]</span><br><span class="line">                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)</span><br><span class="line">                token_type_ids = buffered_token_type_ids_expanded</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line">        <span class="comment"># ourselves in which case we just need to make it broadcastable to all heads.</span></span><br><span class="line">        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If a 2D or 3D attention mask is provided for the cross-attention</span></span><br><span class="line">        <span class="comment"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()</span><br><span class="line">            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)</span><br><span class="line">            <span class="keyword">if</span> encoder_attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)</span><br><span class="line">            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_extended_attention_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prepare head mask if needed</span></span><br><span class="line">        <span class="comment"># 1.0 in head_mask indicate we keep the head</span></span><br><span class="line">        <span class="comment"># attention_probs has shape bsz x n_heads x N x N</span></span><br><span class="line">        <span class="comment"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span></span><br><span class="line">        <span class="comment"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">            past_key_values_length=past_key_values_length,</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">            past_key_values=past_key_values,</span><br><span class="line">            use_cache=use_cache,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            past_key_values=encoder_outputs.past_key_values,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">            cross_attentions=encoder_outputs.cross_attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="2-3-BertEmbeddings"><a href="#2-3-BertEmbeddings" class="headerlink" title="2.3 BertEmbeddings"></a>2.3 BertEmbeddings</h3><p>包含三个部分求和得到： Bert-embedding 图：Bert-embedding</p>
<p>1.word_embeddings，上文中 subword 对应的嵌入。<br>2.token_type_embeddings，用于表示当前词所在的句子，辅助区别句子与 padding、句子对间的差异。<br>3.position_embeddings，句子中每个词的位置嵌入，用于区别词的顺序。和 transformer 论文中的设计不同，这一块是训练出来的，而不是通过 Sinusoidal 函数计算得到的固定嵌入。一般认为这种实现不利于拓展性（难以直接迁移到更长的句子中）。<br>三个 embedding 不带权重相加，并通过一层 LayerNorm+dropout 后输出，其大小为(batch_size, sequence_length, hidden_size)。</p>
<p><strong>这里为什么要用 LayerNorm+Dropout 呢？为什么要用 LayerNorm 而不是 BatchNorm？可以参考一个不错的回答：transformer 为什么使用 layer normalization，而不是其他的归一化方法？</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="string">&quot;1.6.0&quot;</span>):</span><br><span class="line">            self.register_buffer(</span><br><span class="line">                <span class="string">&quot;token_type_ids&quot;</span>,</span><br><span class="line">                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),</span><br><span class="line">                persistent=<span class="literal">False</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span>, past_key_values_length=<span class="number">0</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs</span></span><br><span class="line">        <span class="comment"># when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves</span></span><br><span class="line">        <span class="comment"># issue #5664</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;token_type_ids&quot;</span>):</span><br><span class="line">                buffered_token_type_ids = self.token_type_ids[:, :seq_length]</span><br><span class="line">                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="number">0</span>], seq_length)</span><br><span class="line">                token_type_ids = buffered_token_type_ids_expanded</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + token_type_embeddings</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;absolute&quot;</span>:</span><br><span class="line">            position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">            embeddings += position_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure></p>
<h2 id="3-BertEncoder"><a href="#3-BertEncoder" class="headerlink" title="3 BertEncoder"></a>3 BertEncoder</h2><p>包含多层 BertLayer，这一块本身没有特别需要说明的地方，不过有一个细节值得参考：利用 gradient checkpointing 技术以降低训练时的显存占用。</p>
<p>gradient checkpointing 即梯度检查点，通过减少保存的计算图节点压缩模型占用空间，但是在计算梯度的时候需要重新计算没有存储的值，参考论文《Training Deep Nets with Sublinear Memory Cost》，过程如下示意图 gradient-checkpointing 图：gradient-checkpointing</p>
<p>在 BertEncoder 中，gradient checkpoint 是通过 torch.utils.checkpoint.checkpoint 实现的，使用起来比较方便，可以参考文档：torch.utils.checkpoint - PyTorch 1.8.1 documentation，这一机制的具体实现比较复杂，在此不作展开。</p>
<p>再往深一层走，就进入了 Encoder 的某一层：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_values=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_hidden_states=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_dict=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        all_hidden_states = () <span class="keyword">if</span> output_hidden_states <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        all_self_attentions = () <span class="keyword">if</span> output_attentions <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        all_cross_attentions = () <span class="keyword">if</span> output_attentions <span class="keyword">and</span> self.config.add_cross_attention <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        next_decoder_cache = () <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layer):</span><br><span class="line">            <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_head_mask = head_mask[i] <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            past_key_value = past_key_values[i] <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">getattr</span>(self.config, <span class="string">&quot;gradient_checkpointing&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> self.training:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> use_cache:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">&quot;`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting &quot;</span></span><br><span class="line">                        <span class="string">&quot;`use_cache=False`...&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    use_cache = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">create_custom_forward</span>(<span class="params">module</span>):</span></span><br><span class="line">                    <span class="function"><span class="keyword">def</span> <span class="title">custom_forward</span>(<span class="params">*inputs</span>):</span></span><br><span class="line">                        <span class="keyword">return</span> module(*inputs, past_key_value, output_attentions)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> custom_forward</span><br><span class="line"></span><br><span class="line">                layer_outputs = torch.utils.checkpoint.checkpoint(</span><br><span class="line">                    create_custom_forward(layer_module),</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    attention_mask,</span><br><span class="line">                    layer_head_mask,</span><br><span class="line">                    encoder_hidden_states,</span><br><span class="line">                    encoder_attention_mask,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layer_outputs = layer_module(</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    attention_mask,</span><br><span class="line">                    layer_head_mask,</span><br><span class="line">                    encoder_hidden_states,</span><br><span class="line">                    encoder_attention_mask,</span><br><span class="line">                    past_key_value,</span><br><span class="line">                    output_attentions,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> use_cache:</span><br><span class="line">                next_decoder_cache += (layer_outputs[-<span class="number">1</span>],)</span><br><span class="line">            <span class="keyword">if</span> output_attentions:</span><br><span class="line">                all_self_attentions = all_self_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line">                <span class="keyword">if</span> self.config.add_cross_attention:</span><br><span class="line">                    all_cross_attentions = all_cross_attentions + (layer_outputs[<span class="number">2</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">                v</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> [</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    next_decoder_cache,</span><br><span class="line">                    all_hidden_states,</span><br><span class="line">                    all_self_attentions,</span><br><span class="line">                    all_cross_attentions,</span><br><span class="line">                ]</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPastAndCrossAttentions(</span><br><span class="line">            last_hidden_state=hidden_states,</span><br><span class="line">            past_key_values=next_decoder_cache,</span><br><span class="line">            hidden_states=all_hidden_states,</span><br><span class="line">            attentions=all_self_attentions,</span><br><span class="line">            cross_attentions=all_cross_attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure></p>
<h3 id="3-2-BertAttention"><a href="#3-2-BertAttention" class="headerlink" title="3.2 BertAttention"></a>3.2 BertAttention</h3><p>本以为 attention 的实现就在这里，没想到还要再下一层……其中，self 成员就是多头注意力的实现，而 output 成员实现 attention 后的全连接 +dropout+residual+LayerNorm 一系列操作。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self = BertSelfAttention(config)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = <span class="built_in">set</span>()</span><br><span class="line">首先还是回到这一层。这里出现了上文提到的剪枝操作，即 prune_heads 方法：</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span>(<span class="params">self, heads</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        heads, index = find_pruneable_heads_and_indices(</span><br><span class="line">            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self.query = prune_linear_layer(self.self.query, index)</span><br><span class="line">        self.self.key = prune_linear_layer(self.self.key, index)</span><br><span class="line">        self.self.value = prune_linear_layer(self.self.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self.num_attention_heads = self.self.num_attention_heads - <span class="built_in">len</span>(heads)</span><br><span class="line">        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads) </span><br></pre></td></tr></table></figure><br>这里的具体实现概括如下：</p>
<ul>
<li>find_pruneable_heads_and_indices是定位需要剪掉的 head，以及需要保留的维度下标 index；</li>
<li>prune_linear_layer则负责将 Wk/Wq/Wv 权重矩阵（连同 bias）中按照 index 保留没有被剪枝的维度后转移到新的矩阵。 接下来就到重头戏——Self-Attention 的具体实现。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self = BertSelfAttention(config)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span>(<span class="params">self, heads</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        heads, index = find_pruneable_heads_and_indices(</span><br><span class="line">            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self.query = prune_linear_layer(self.self.query, index)</span><br><span class="line">        self.self.key = prune_linear_layer(self.self.key, index)</span><br><span class="line">        self.self.value = prune_linear_layer(self.self.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self.num_attention_heads = self.self.num_attention_heads - <span class="built_in">len</span>(heads)</span><br><span class="line">        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        self_outputs = self.self(</span><br><span class="line">            hidden_states,</span><br><span class="line">            attention_mask,</span><br><span class="line">            head_mask,</span><br><span class="line">            encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask,</span><br><span class="line">            past_key_value,</span><br><span class="line">            output_attentions,</span><br><span class="line">        )</span><br><span class="line">        attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">        outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h3 id="3-3-BertSelfAttention"><a href="#3-3-BertSelfAttention" class="headerlink" title="3.3 BertSelfAttention"></a>3.3 BertSelfAttention</h3>预警：这一块可以说是模型的核心区域，也是唯一涉及到公式的地方，所以将贴出大量代码。</li>
</ul>
<p>初始化部分：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            self.max_position_embeddings = config.max_position_embeddings</span><br><span class="line">            self.distance_embedding = nn.Embedding(<span class="number">2</span> * config.max_position_embeddings - <span class="number">1</span>, self.attention_head_size)</span><br><span class="line"></span><br><span class="line">        self.is_decoder = config.is_decoder</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;除掉熟悉的 query、key、value 三个权重和一个 dropout，这里还有一个谜一样的position_embedding_type，以及 decoder 标记；</p>
<p>&#8195;&#8195;注意，hidden_size 和 all_head_size 在一开始是一样的。至于为什么要看起来多此一举地设置这一个变量——显然是因为上面那个剪枝函数，剪掉几个 attention head 以后 all_head_size 自然就小了；</p>
<p>&#8195;&#8195;hidden_size 必须是 num_attention_heads 的整数倍，以 bert-base 为例，每个 attention 包含 12 个 head，hidden_size 是 768，所以每个 head 大小即 attention_head_size=768/12=64；<br>&#8195;&#8195;position_embedding_type 是什么？继续往下看就知道了.</p>
<p>&#8195;&#8195;然后是重点，也就是前向传播过程。</p>
<p>&#8195;&#8195;首先回顾一下 multi-head self-attention 的基本公式：</p>
<script type="math/tex; mode=display">MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$ $$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$ $$SDPA(Q, K, V) = softmax(\frac{QK^T}{\sqrt(d_k)})V</script><p>&#8195;&#8195;而这些注意力头，众所周知是并行计算的，所以上面的 query、key、value 三个权重是唯一的——这并不是所有 heads 共享了权重，而是“拼接”起来了。</p>
<p>&#8195;&#8195;原论文中多头的理由为 Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. 而另一个比较靠谱的分析有：为什么 Transformer 需要进行 Multi-head Attention？</p>
<p>看看 forward 方法：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 省略一部分cross-attention的计算</span></span><br><span class="line">        key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">        value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;这里的 transpose_for_scores 用来把 hidden_size 拆成多个头输出的形状，并且将中间两维转置以进行矩阵相乘；</p>
<p>&#8195;&#8195;这里 key_layer/value_layer/query_layer 的形状为：(batch_size, num_attention_heads, sequence_length, attention_head_size)； 这里 attention_scores 的形状为：(batch_size, num_attention_heads, sequence_length, sequence_length)，符合多个头单独计算获得的 attention map 形状。</p>
<p>&#8195;&#8195;到这里实现了 K 与 Q 相乘，获得 raw attention scores 的部分，按公式接下来应该是按 $d_k$ 进行 scaling 并做 softmax 的操作。然而先出现在眼前的是一个奇怪的positional_embedding，以及一堆爱因斯坦求和：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line">       <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">           seq_length = hidden_states.size()[<span class="number">1</span>]</span><br><span class="line">           position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">           position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">           distance = position_ids_l - position_ids_r</span><br><span class="line">           positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - <span class="number">1</span>)</span><br><span class="line">           positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span>:</span><br><span class="line">               relative_position_scores = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">               attention_scores = attention_scores + relative_position_scores</span><br><span class="line">           <span class="keyword">elif</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">               relative_position_scores_query = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">               relative_position_scores_key = torch.einsum(<span class="string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)</span><br><span class="line">               attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key</span><br><span class="line">       <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;关于爱因斯坦求和约定，参考以下文档：torch.einsum - PyTorch 1.8.1 documentation<br>&#8195;&#8195;对于不同的positional_embedding_type，有三种操作：</p>
<ul>
<li>absolute：默认值，这部分就不用处理；</li>
<li>relative_key：对 key_layer 作处理，将其与这里的positional_embedding和 key 矩阵相乘作为 key 相关的位置编码；</li>
<li>relative_key_query：对 key 和 value 都进行相乘以作为位置编码。<br>回到正常 attention 的流程：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask  <span class="comment"># 这里为什么是+而不是*？</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=-<span class="number">1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 省略decoder返回值部分……</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
&#8195;&#8195;重大疑问：这里的attention_scores = attention_scores + attention_mask是在做什么？难道不应该是乘 mask 吗？</li>
</ul>
<p>&#8195;&#8195;因为这里的 attention_mask 已经【被动过手脚】，将原本为 1 的部分变为 0，而原本为 0 的部分（即 padding）变为一个较大的负数，这样相加就得到了一个较大的负值：<br>至于为什么要用【一个较大的负数】？因为这样一来经过 softmax 操作以后这一项就会变成接近 0 的小数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(Pdb) attention_mask</span><br><span class="line">tensor([[[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]],</span><br><span class="line">        [[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]],</span><br><span class="line">        [[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]],</span><br><span class="line">        ...,</span><br><span class="line">        [[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]],</span><br><span class="line">        [[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]],</span><br><span class="line">        [[[    -<span class="number">0.</span>,     -<span class="number">0.</span>,     -<span class="number">0.</span>,  ..., -<span class="number">10000.</span>, -<span class="number">10000.</span>, -<span class="number">10000.</span>]]]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;那么，这一步是在哪里执行的呢？ 在modeling_bert.py中没有找到答案，但是在modeling_utils.py中找到了一个特别的类：class ModuleUtilsMixin，在它的get_extended_attention_mask方法中发现了端倪：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_extended_attention_mask</span>(<span class="params">self, attention_mask: Tensor, input_shape: <span class="type">Tuple</span>[<span class="built_in">int</span>], device: device</span>) -&gt; Tensor:</span></span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       Arguments:</span></span><br><span class="line"><span class="string">           attention_mask (:obj:`torch.Tensor`):</span></span><br><span class="line"><span class="string">               Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</span></span><br><span class="line"><span class="string">           input_shape (:obj:`Tuple[int]`):</span></span><br><span class="line"><span class="string">               The shape of the input to the model.</span></span><br><span class="line"><span class="string">           device: (:obj:`torch.device`):</span></span><br><span class="line"><span class="string">               The device of the input to the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       Returns:</span></span><br><span class="line"><span class="string">           :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       <span class="comment"># 省略一部分……</span></span><br><span class="line"></span><br><span class="line">       <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">       <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">       <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">       <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">       <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">       extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line">       extended_attention_mask = (<span class="number">1.0</span> - extended_attention_mask) * -<span class="number">10000.0</span></span><br><span class="line">       <span class="keyword">return</span> extended_attention_mask</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;那么，这个函数是在什么时候被调用的呢？和BertModel有什么关系呢？ OK，这里涉及到 BertModel 的继承细节了：BertModel继承自BertPreTrainedModel，后者继承自PreTrainedModel，而PreTrainedModel继承自[nn.Module, ModuleUtilsMixin, GenerationMixin]三个基类。——好复杂的封装！</p>
<p>&#8195;&#8195;这也就是说，BertModel必然在中间的某个步骤对原始的attention_mask调用了get_extended_attention_mask，导致attention_mask从原始的[1, 0]变为[0, -1e4]的取值。</p>
<p>&#8195;&#8195;最终在 BertModel 的前向传播过程中找到了这一调用（第 944 行）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line">      <span class="comment"># ourselves in which case we just need to make it broadcastable to all heads.</span></span><br><span class="line">      extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;问题解决了：这一方法不但实现了改变 mask 的值，还将其广播（broadcast）为可以直接与 attention map 相加的形状。 不愧是你，HuggingFace。</p>
<p>&#8195;&#8195;除此之外，值得注意的细节有：</p>
<p>&#8195;&#8195;按照每个头的维度进行缩放，对于 bert-base 就是 64 的平方根即 8；</p>
<ul>
<li>attention_probs 不但做了 softmax，还用了一次 dropout，这是担心 attention 矩阵太稠密吗…… 这里也提到很不寻常，但是原始 Transformer 论文就是这么做的；</li>
<li>head_mask 就是之前提到的对多头计算的 mask，如果不设置默认是全 1，在这里就不会起作用；</li>
<li>context_layer 即 attention 矩阵与 value 矩阵的乘积，原始的大小为：(batch_size,  - -num_attention_heads, sequence_length, attention_head_size) ；</li>
<li>context_layer 进行转置和 view 操作以后，形状就恢复了(batch_size, sequence_length, hidden_size)。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;The hidden size (<span class="subst">&#123;config.hidden_size&#125;</span>) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">f&quot;heads (<span class="subst">&#123;config.num_attention_heads&#125;</span>)&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            self.max_position_embeddings = config.max_position_embeddings</span><br><span class="line">            self.distance_embedding = nn.Embedding(<span class="number">2</span> * config.max_position_embeddings - <span class="number">1</span>, self.attention_head_size)</span><br><span class="line"></span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        is_cross_attention = encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_cross_attention <span class="keyword">and</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k,v, cross_attentions</span></span><br><span class="line">            key_layer = past_key_value[<span class="number">0</span>]</span><br><span class="line">            value_layer = past_key_value[<span class="number">1</span>]</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">elif</span> is_cross_attention:</span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">elif</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line">            key_layer = torch.cat([past_key_value[<span class="number">0</span>], key_layer], dim=<span class="number">2</span>)</span><br><span class="line">            value_layer = torch.cat([past_key_value[<span class="number">1</span>], value_layer], dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            <span class="comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span></span><br><span class="line">            <span class="comment"># Further calls to cross_attention layer can then reuse all cross-attention</span></span><br><span class="line">            <span class="comment"># key/value_states (first &quot;if&quot; case)</span></span><br><span class="line">            <span class="comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span></span><br><span class="line">            <span class="comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span></span><br><span class="line">            <span class="comment"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span></span><br><span class="line">            <span class="comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span></span><br><span class="line">            past_key_value = (key_layer, value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            seq_length = hidden_states.size()[<span class="number">1</span>]</span><br><span class="line">            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">            distance = position_ids_l - position_ids_r</span><br><span class="line">            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - <span class="number">1</span>)</span><br><span class="line">            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span>:</span><br><span class="line">                relative_position_scores = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">                attention_scores = attention_scores + relative_position_scores</span><br><span class="line">            <span class="keyword">elif</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">                relative_position_scores_query = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">                relative_position_scores_key = torch.einsum(<span class="string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)</span><br><span class="line">                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key</span><br><span class="line"></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=-<span class="number">1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = outputs + (past_key_value,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h3 id="3-4-BertSelfOutput"><a href="#3-4-BertSelfOutput" class="headerlink" title="3.4 BertSelfOutput"></a>3.4 BertSelfOutput</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
&#8195;&#8195;这里又出现了 LayerNorm 和 Dropout 的组合，只不过这里是先 Dropout，进行残差连接后再进行 LayerNorm。至于为什么要做残差连接，最直接的目的就是降低网络层数过深带来的训练难度，对原始输入更加敏感～<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
2.2.1.2 BertIntermediate<br>&#8195;&#8195;看完了 BertAttention，在 Attention 后面还有一个全连接+激活的操作：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
&#8195;&#8195;这里的全连接做了一个扩展，以 bert-base 为例，扩展维度为 3072，是原始维度 768 的 4 倍之多；<br>&#8195;&#8195;这里的激活函数默认实现为 gelu（Gaussian Error Linerar Units(GELUS）当然，它是无法直接计算的，可以用一个包含tanh的表达式进行近似（略)。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
2.2.1.3 BertOutput<br>&#8195;&#8195;在这里又是一个全连接 +dropout+LayerNorm，还有一个残差连接 residual connect：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
&#8195;&#8195;这里的操作和 BertSelfOutput 不能说没有关系，只能说一模一样…… 非常容易混淆的两个组件。 以下内容还包含基于 BERT 的应用模型，以及 BERT 相关的优化器和用法，将在下一篇文章作详细介绍。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
2.2.3 BertPooler<br>&#8195;&#8195;这一层只是简单地取出了句子的第一个token，即[CLS]对应的向量，然后过一个全连接层和一个激活函数后输出：（这一部分是可选的，因为pooling有很多不同的操作）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.models.bert.configuration_bert <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">bert_pooler = BertPooler(config=config)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input to bert pooler size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(config.hidden_size))</span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">2</span></span><br><span class="line">hidden_size = <span class="number">768</span></span><br><span class="line">x = torch.rand(batch_size, seq_len, hidden_size)</span><br><span class="line">y = bert_pooler(x)</span><br><span class="line"><span class="built_in">print</span>(y.size())</span><br><span class="line"><span class="built_in">input</span> to bert pooler size: <span class="number">768</span></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">768</span>])</span><br></pre></td></tr></table></figure>
小总结<br>&#8195;&#8195;本小节对Bert模型的实现进行分析了学习，希望读者能对Bert实现有一个更为细致的把握。</li>
</ul>
<p>&#8195;&#8195;值得注意的是，在 HuggingFace 实现的 Bert 模型中，使用了多种节约显存的技术：</p>
<p>&#8195;&#8195;gradient checkpoint，不保留前向传播节点，只在用时计算；apply_chunking_to_forward，按多个小批量和低维度计算 FFN 部<br>&#8195;&#8195;BertModel 包含复杂的封装和较多的组件。以 bert-base 为例，主要组件如下：</p>
<ul>
<li>总计Dropout出现了1+(1+1+1)x12=37次；</li>
<li>总计LayerNorm出现了1+(1+1)x12=25次；</li>
<li>总计dense全连接层出现了(1+1+1)x12+1=37次，并不是每个dense都配了激活函数…… BertModel 有极大的参数量。以 bert-base 为例，其参数量为 109M。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/22/8月组队学习：nlp之transformers入门/task4：BERT代码讲解/">https://zhxnlp.github.io/2021/08/22/8月组队学习：nlp之transformers入门/task4：BERT代码讲解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/23/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task5%EF%BC%9ABERT%E5%85%B7%E4%BD%93%E5%BA%94%E7%94%A8%EF%BC%88%E5%BE%85%E8%A1%A5%E5%85%85%EF%BC%89/"><i class="fa fa-chevron-left">  </i><span>task5：BERT具体应用</span></a></div><div class="next-post pull-right"><a href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B%EF%BC%9A%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/"><span>GitHub 进阶教程：分支管理</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>