<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="GitHub 安装使用详细教程"><meta name="keywords" content="github,软件"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>GitHub 安装使用详细教程 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BD%BF%E7%94%A8LGBMClassifier%E5%AF%B9iris%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">一、使用LGBMClassifier对iris进行训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BD%BF%E7%94%A8lgb-LGBMClassifier"><span class="toc-text">1.1 使用lgb.LGBMClassifier</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2%E4%BD%BF%E7%94%A8pickle%E8%BF%9B%E8%A1%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8A%A0%E8%BD%BD%E9%A2%84%E6%B5%8B"><span class="toc-text">1.1.2使用pickle进行保存模型，然后加载预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-%E4%BD%BF%E7%94%A8txt%E5%92%8Cjson%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%B9%B6%E5%8A%A0%E8%BD%BD"><span class="toc-text">1.1.3 使用txt和json保存模型并加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2%E4%BD%BF%E7%94%A8%E5%8E%9F%E7%94%9F%E7%9A%84API%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B"><span class="toc-text">1.2使用原生的API进行模型训练和预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E4%BD%BF%E7%94%A8txt-json%E6%A0%BC%E5%BC%8F%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.2.2 使用txt&#x2F;json格式保存模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E4%BD%BF%E7%94%A8pickle%E8%BF%9B%E8%A1%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.2.3 使用pickle进行保存模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BB%BB%E5%8A%A13-%E5%88%86%E7%B1%BB%E3%80%81%E5%9B%9E%E5%BD%92%E5%92%8C%E6%8E%92%E5%BA%8F%E4%BB%BB%E5%8A%A1"><span class="toc-text">三、任务3 分类、回归和排序任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E4%BD%BF%E7%94%A8-make-classification%E7%94%9F%E6%88%90%E4%BA%8C%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">3.1使用 make_classification生成二分类数据进行训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-sklearn%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.1.1 sklearn接口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E5%8E%9F%E7%94%9Ftrain%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.1.2 原生train接口</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E4%BD%BF%E7%94%A8-make-classification%E7%94%9F%E6%88%90%E5%A4%9A%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-text">3.2使用 make_classification生成多分类数据进行训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-sklearn%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.2.1 sklearn接口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E5%8E%9F%E7%94%9Ftrain%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.2.2 原生train接口</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E4%BD%BF%E7%94%A8-make-regression%E7%94%9F%E6%88%90%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE"><span class="toc-text">3.3使用 make_regression生成回归数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-sklearn%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.3.1 sklearn接口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-%E5%8E%9F%E7%94%9Ftrain%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.3.2 原生train接口</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81graphviz%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">四、graphviz可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82%EF%BC%88%E7%BD%91%E6%A0%BC%E3%80%81%E9%9A%8F%E6%9C%BA%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%89"><span class="toc-text">五、模型调参（网格、随机、贝叶斯）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">5.1 加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%AD%A5%E9%AA%A42-%E8%AE%BE%E7%BD%AE%E6%A0%91%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%88%86%E5%88%AB%E4%B8%BA-3-5-6-9-%EF%BC%8C%E8%AE%B0%E5%BD%95%E4%B8%8B%E9%AA%8C%E8%AF%81%E9%9B%86AUC%E7%B2%BE%E5%BA%A6%E3%80%82"><span class="toc-text">5.2:步骤2 设置树模型深度分别为[3,5,6,9]，记录下验证集AUC精度。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E6%AD%A5%E9%AA%A43%EF%BC%9Acategory%E5%8F%98%E9%87%8F%E8%AE%BE%E7%BD%AE%E4%B8%BAcategorical-feature"><span class="toc-text">5.3  步骤3：category变量设置为categorical_feature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E6%AD%A5%E9%AA%A44%EF%BC%9A%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2"><span class="toc-text">5.4  步骤4：超参搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-1-GridSearchCV"><span class="toc-text">5.4.1 GridSearchCV</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-3-%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2"><span class="toc-text">5.4.3 随机搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-4-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%90%9C%E7%B4%A2"><span class="toc-text">5.4.4 贝叶斯搜索</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%8E%E5%8F%82%E6%95%B0%E8%A1%B0%E5%87%8F"><span class="toc-text">六、模型微调与参数衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-text">6.2 学习率衰减</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E7%89%B9%E5%BE%81%E7%AD%9B%E9%80%89%E6%96%B9%E6%B3%95"><span class="toc-text">七、特征筛选方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E7%AD%9B%E9%80%89%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%843%E4%B8%AA%E7%89%B9%E5%BE%81"><span class="toc-text">7.1 筛选最重要的3个特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E5%88%A9%E7%94%A8PermutationImportance%E6%8E%92%E5%88%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">7.2 利用PermutationImportance排列特征重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Null-Importances%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">7.3 Null Importances进行特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-1-%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3%EF%BC%9A"><span class="toc-text">7.3.1 主要思想：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-2%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-text">7.3.2实现步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-3-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E8%AE%A1%E7%AE%97Real-Targe%E5%92%8Cshuffle-Target%E4%B8%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E5%BA%A6"><span class="toc-text">7.3.3 读取数据集，计算Real Targe和shuffle Target下的特征重要度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-4%E8%AE%A1%E7%AE%97Score"><span class="toc-text">7.3.4计算Score</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AF%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-text">八、自定义损失函数和评测函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%8A%A0%E9%80%9F"><span class="toc-text">九 模型部署与加速</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">GitHub 安装使用详细教程</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/">软件应用</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">13.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 73 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><ul>
<li><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-Intro.html" title="官方文档">LightGBM 官方文档</a></li>
<li>阿水知乎贴：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266865429" title="《你应该知道的LightGBM各种操作》">《你应该知道的LightGBM各种操作》</a></li>
<li><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html" title="Python API">Python API（包括Scikit-learn API）</a></li>
<li><a target="_blank" rel="noopener" href="https://coggle.club/blog/30days-of-ml-202201" title="《Coggle 30 Days of ML（22年1&amp;2月）》">《Coggle 30 Days of ML（22年1&amp;2月）》</a></li>
</ul>
<p>学习内容：</p>
<p>LightGBM（Light Gradient Boosting Machine）是微软开源的一个实现 GBDT 算法的框架，支持高效率的并行训练。LightGBM 提出的主要原因是为了解决 GBDT 在海量数据遇到的问题。本次学习内容包括使用LightGBM完成各种操作，包括竞赛和数据挖掘中的模型训练、验证和调参过程。</p>
<p>打卡汇总：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务名称</th>
<th>难度、分数</th>
<th>所需技能</th>
</tr>
</thead>
<tbody>
<tr>
<td>任务1模型训练与预测</td>
<td>低、1</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务2：模型保存与加载</td>
<td>低、1</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务3：分类、回归和排序任务</td>
<td>高、3</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务4：模型可视化</td>
<td>低、1</td>
<td>graphviz</td>
</tr>
<tr>
<td>任务5：模型调参（网格、随机、贝叶斯）</td>
<td>中、2</td>
<td>模型调参</td>
</tr>
<tr>
<td>任务6：模型微调与参数衰减</td>
<td>中、2</td>
<td>LightGBM</td>
</tr>
<tr>
<td>任务7：特征筛选方法</td>
<td>高、3</td>
<td>特征筛选方法</td>
</tr>
<tr>
<td>任务8：自定义损失函数</td>
<td>中、2</td>
<td>损失函数&amp;评价函数</td>
</tr>
<tr>
<td>任务9：模型部署与加速</td>
<td>高、3</td>
<td>Treelite</td>
</tr>
</tbody>
</table>
</div>
<h2 id="一、使用LGBMClassifier对iris进行训练"><a href="#一、使用LGBMClassifier对iris进行训练" class="headerlink" title="一、使用LGBMClassifier对iris进行训练"></a>一、使用LGBMClassifier对iris进行训练</h2><span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br></pre></td></tr></table></figure>
<h3 id="1-1-使用lgb-LGBMClassifier"><a href="#1-1-使用lgb-LGBMClassifier" class="headerlink" title="1.1 使用lgb.LGBMClassifier"></a>1.1 使用lgb.LGBMClassifier</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">10</span>,</span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">2000</span>,<span class="comment">#提升迭代次数</span></span><br><span class="line">            objective=<span class="string">&#x27;multi:softmax&#x27;</span>,<span class="comment">#默认regression，用于设置损失函数</span></span><br><span class="line">            num_class=<span class="number">3</span> ,          </span><br><span class="line">            nthread=-<span class="number">1</span>,<span class="comment">#LightGBM 的线程数</span></span><br><span class="line">            min_child_weight=<span class="number">1</span>,</span><br><span class="line">            max_delta_step=<span class="number">0</span>,</span><br><span class="line">            subsample=<span class="number">0.85</span>,</span><br><span class="line">            colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">            reg_alpha=<span class="number">0</span>,<span class="comment">#L1正则化系数</span></span><br><span class="line">            reg_lambda=<span class="number">1</span>,<span class="comment">#L2正则化系数</span></span><br><span class="line">            scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line">gbm.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=-1 will be ignored. Current value: num_threads=-1
accuarcy: 93.33%
</code></pre><h4 id="1-1-2使用pickle进行保存模型，然后加载预测"><a href="#1-1-2使用pickle进行保存模型，然后加载预测" class="headerlink" title="1.1.2使用pickle进行保存模型，然后加载预测"></a>1.1.2使用pickle进行保存模型，然后加载预测</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">    pickle.dump(gbm, fout)</span><br><span class="line"><span class="comment"># load model with pickle to predict</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    pkl_bst = pickle.load(fin)</span><br><span class="line"><span class="comment"># can predict with any iteration when loaded in pickle way</span></span><br><span class="line">y_pred = pkl_bst.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 93.33%
</code></pre><h4 id="1-1-3-使用txt和json保存模型并加载"><a href="#1-1-3-使用txt和json保存模型并加载" class="headerlink" title="1.1.3 使用txt和json保存模型并加载"></a>1.1.3 使用txt和json保存模型并加载</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># txt格式</span></span><br><span class="line">gbm.booster_.save_model(<span class="string">&quot;skmodel.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">clf_loads = lgb.Booster(model_file=<span class="string">&#x27;skmodel.txt&#x27;</span>)</span><br><span class="line">y_pred = clf_loads.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 93.33%
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#json格式</span></span><br><span class="line">gbm.booster_.save_model(<span class="string">&quot;skmodel.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">clf_loads = lgb.Booster(model_file=<span class="string">&#x27;skmodel.json&#x27;</span>)</span><br><span class="line">y_pred = clf_loads.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 93.33%
</code></pre><h3 id="1-2使用原生的API进行模型训练和预测"><a href="#1-2使用原生的API进行模型训练和预测" class="headerlink" title="1.2使用原生的API进行模型训练和预测"></a>1.2使用原生的API进行模型训练和预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line"><span class="comment">#reference:如果这是用于验证的数据集，则应使用训练数据作为参考</span></span><br><span class="line"><span class="comment">#weight : list每个实例的权重</span></span><br><span class="line"><span class="comment">#free_raw_data：default=True，构建内部 Dataset 后释放原始数据，节省内存。</span></span><br><span class="line"><span class="comment">#silent:布尔类型，default=False。是否在构建过程中打印消息。</span></span><br><span class="line"><span class="comment">#init_score：数据集初始分数</span></span><br><span class="line"><span class="comment">#feature_name：设为 &#x27;auto&#x27; 时，如果 data 是 pandas DataFrame，则使用数据列名称。</span></span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="comment">#多分类的objective为multiclass或者别名softmax</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.85</span>,</span><br><span class="line">    <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>:<span class="number">0.01</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[10]    valid_0&#39;s multi_logloss: 0.96537
accuarcy: 93.33%
</code></pre><h4 id="1-2-2-使用txt-json格式保存模型"><a href="#1-2-2-使用txt-json格式保存模型" class="headerlink" title="1.2.2 使用txt/json格式保存模型"></a>1.2.2 使用txt/json格式保存模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用txt保存模型</span></span><br><span class="line">gbm.save_model(<span class="string">&#x27;model.txt&#x27;</span>)</span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.txt&#x27;</span>)</span><br><span class="line">y_pred = bst.predict(X_test, num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用json格式保存模型</span></span><br><span class="line">    </span><br><span class="line">gbm.save_model(<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">y_pred = bst.predict(X_test, num_iteration=gbm.best_iteration)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 93.33%
accuarcy: 93.33%
</code></pre><h4 id="1-2-3-使用pickle进行保存模型"><a href="#1-2-3-使用pickle进行保存模型" class="headerlink" title="1.2.3 使用pickle进行保存模型"></a>1.2.3 使用pickle进行保存模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">    pickle.dump(gbm, fout)</span><br><span class="line"><span class="comment"># load model with pickle to predict</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;model.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    pkl_bst = pickle.load(fin)</span><br><span class="line"><span class="comment"># can predict with any iteration when loaded in pickle way</span></span><br><span class="line">y_pred = pkl_bst.predict(X_test)</span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 93.33%
</code></pre><h2 id="三、任务3-分类、回归和排序任务"><a href="#三、任务3-分类、回归和排序任务" class="headerlink" title="三、任务3 分类、回归和排序任务"></a>三、任务3 分类、回归和排序任务</h2><h3 id="3-1使用-make-classification生成二分类数据进行训练"><a href="#3-1使用-make-classification生成二分类数据进行训练" class="headerlink" title="3.1使用 make_classification生成二分类数据进行训练"></a>3.1使用 make_classification生成二分类数据进行训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#n_samples:样本数量，默认100</span></span><br><span class="line"><span class="comment">#n_features:特征数，默认20</span></span><br><span class="line"><span class="comment">#n_informative：有效特征数量，默认2</span></span><br><span class="line"><span class="comment">#n_redundant:冗余特征，默认2</span></span><br><span class="line"><span class="comment">#n_repeated :重复的特征个数，默认0</span></span><br><span class="line"><span class="comment">#n_clusters_per_class：每个类别中cluster数量，默认2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#weight：各个类的占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#n_classes* n_clusters_per_class 必须≤ 2**有效特征数</span></span><br><span class="line">data, target = make_classification(n_samples=<span class="number">1000</span>,n_features=<span class="number">3</span>,n_informative=<span class="number">3</span>,n_redundant=<span class="number">0</span>,n_classes=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df[<span class="string">&#x27;target&#x27;</span>] = target</span><br><span class="line"></span><br><span class="line">df1 = df[df[<span class="string">&#x27;target&#x27;</span>]==<span class="number">0</span>]</span><br><span class="line">df2 = df[df[<span class="string">&#x27;target&#x27;</span>]==<span class="number">1</span>]</span><br><span class="line">df1.index = <span class="built_in">range</span>(<span class="built_in">len</span>(df1))</span><br><span class="line">df2.index = <span class="built_in">range</span>(<span class="built_in">len</span>(df2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出数据集的数据分布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">plt.scatter(df1[<span class="number">0</span>],df1[<span class="number">1</span>],color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.scatter(df2[<span class="number">0</span>],df2[<span class="number">1</span>],color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">2</span>))</span><br><span class="line">df1[<span class="number">0</span>].hist()</span><br><span class="line">df1[<span class="number">0</span>].plot(kind = <span class="string">&#x27;kde&#x27;</span>, secondary_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mean_ = df1[<span class="number">0</span>].mean()</span><br><span class="line">std_ = df1[<span class="number">0</span>].std()</span><br><span class="line"></span><br><span class="line">stats.kstest(df1[<span class="number">0</span>], <span class="string">&#x27;norm&#x27;</span>, (mean_, std_))</span><br></pre></td></tr></table></figure>
<pre><code>KstestResult(statistic=0.03723785150172143, pvalue=0.4930944895472954)
</code></pre><p><img src="lightGBM_files/lightGBM_17_1.png" alt="png"></p>
<p><img src="lightGBM_files/lightGBM_17_2.png" alt="png"></p>
<h4 id="3-1-1-sklearn接口"><a href="#3-1-1-sklearn接口" class="headerlink" title="3.1.1 sklearn接口"></a>3.1.1 sklearn接口</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier" title="sklearn接口lgb分类器参考文档">sklearn接口lgb分类器参考文档</a><br>注意：每次产生的数据都不一样，所以</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier()</span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[39]    valid_0&#39;s binary_logloss: 0.255538
accuarcy: 88.50%
</code></pre><h4 id="3-1-2-原生train接口"><a href="#3-1-2-原生train接口" class="headerlink" title="3.1.2 原生train接口"></a>3.1.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">y_pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[10]    valid_0&#39;s binary_logloss: 0.371903
accuarcy: 88.00%
</code></pre><h3 id="3-2使用-make-classification生成多分类数据进行训练"><a href="#3-2使用-make-classification生成多分类数据进行训练" class="headerlink" title="3.2使用 make_classification生成多分类数据进行训练"></a>3.2使用 make_classification生成多分类数据进行训练</h3><h4 id="3-2-1-sklearn接口"><a href="#3-2-1-sklearn接口" class="headerlink" title="3.2.1 sklearn接口"></a>3.2.1 sklearn接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data, target = make_classification(n_samples=<span class="number">1000</span>,n_features=<span class="number">3</span>,n_informative=<span class="number">3</span>,n_redundant=<span class="number">0</span>,n_classes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier()</span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;logloss&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[39]    valid_0&#39;s binary_logloss: 0.255538
accuarcy: 88.50%
</code></pre><h4 id="3-2-2-原生train接口"><a href="#3-2-2-原生train接口" class="headerlink" title="3.2.2 原生train接口"></a>3.2.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;softmax&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">y_pred=np.argmax(y_pred,axis=-<span class="number">1</span>)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[10]    valid_0&#39;s multi_logloss: 0.322024
accuarcy: 87.50%
</code></pre><h3 id="3-3使用-make-regression生成回归数据"><a href="#3-3使用-make-regression生成回归数据" class="headerlink" title="3.3使用 make_regression生成回归数据"></a>3.3使用 make_regression生成回归数据</h3><h4 id="3-3-1-sklearn接口"><a href="#3-3-1-sklearn接口" class="headerlink" title="3.3.1 sklearn接口"></a>3.3.1 sklearn接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">100</span>, n_informative=<span class="number">10</span>, n_targets=<span class="number">1</span>, bias=<span class="number">0.0</span>, </span><br><span class="line">                effective_rank=<span class="literal">None</span>, tail_strength=<span class="number">0.5</span>, noise=<span class="number">0.0</span>, shuffle=<span class="literal">True</span>, coef=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">```                </span><br><span class="line">- n_samples：样本数</span><br><span class="line">- n_features：特征数(自变量个数)</span><br><span class="line">- n_informative：参与建模特征数</span><br><span class="line">- n_targets：因变量个数</span><br><span class="line">- noise：噪音</span><br><span class="line">- bias：偏差(截距)</span><br><span class="line">- coef：是否输出coef标识</span><br><span class="line">- random_state：随机状态若为固定值则每次产生的数据都一样</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data, target = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">5</span>,n_targets=<span class="number">1</span>,noise=<span class="number">1.5</span>,random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"></span><br><span class="line">X,y = data,target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMRegressor()<span class="comment">#直接使用默认参数，mse较小。</span></span><br><span class="line"></span><br><span class="line">gbm.fit(X_train, y_train,</span><br><span class="line">        eval_set=[(X_test, y_test)],</span><br><span class="line">        eval_metric=<span class="string">&#x27;l1&#x27;</span>,</span><br><span class="line">        callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line"><span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line"><span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">y_pred = gbm.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">mse= mean_squared_error(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mse: %.2f&quot;</span> % (mse))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[99]    valid_0&#39;s l1: 8.13036    valid_0&#39;s l2: 119.246
mse: 119.25
</code></pre><h4 id="3-3-2-原生train接口"><a href="#3-3-2-原生train接口" class="headerlink" title="3.3.2 原生train接口"></a>3.3.2 原生train接口</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_train = lgb.Dataset(X_train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数为lgb.LGBMRegressor的默认参数。</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;num_leaves&quot;</span>:<span class="number">31</span>,</span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&quot;n_estimators&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;min_child_samples&quot;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">5</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">mse= mean_squared_error(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mse: %.2f&quot;</span> % (mse))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[99]    valid_0&#39;s l2: 119.246
mse: 119.25
</code></pre><h2 id="四、graphviz可视化"><a href="#四、graphviz可视化" class="headerlink" title="四、graphviz可视化"></a>四、graphviz可视化</h2><blockquote>
<p>参考文档：<a target="_blank" rel="noopener" href="https://blog.csdn.net/kyle1314608/article/details/111245782">《lightgbm 决策树 可视化 graphviz》</a>、<a target="_blank" rel="noopener" href="https://graphviz.readthedocs.io/en/stable/manual.html"> graphviz参考文档</a> 、<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting">《xgboost 可视化API文档》</a>、<a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#plotting">《lightgbm可视化API》</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2022</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line">lgb_clf = lgb.LGBMClassifier()</span><br><span class="line">lgb_clf.fit(X_train, y_train)</span><br><span class="line">lgb.create_tree_digraph(lgb_clf, tree_index=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="comment">#多分类的objective为multiclass或者别名softmax</span></span><br></pre></td></tr></table></figure>
<p><img src="lightGBM_files/lightGBM_33_0.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#lgb没有to_graphviz，无法这样保存图片通</span></span><br><span class="line">digraph = lgb.to_graphviz(lgb_clf , num_trees=<span class="number">1</span>)<span class="comment">#报错module &#x27;lightgbm&#x27; has no attribute &#x27;to_graphviz&#x27;</span></span><br><span class="line">digraph.<span class="built_in">format</span> = <span class="string">&#x27;png&#x27;</span></span><br><span class="line">digraph.view(<span class="string">&#x27;./iris_lgb&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-20-c0cca38d5eee&gt; in &lt;module&gt;
      1 #lgb没有to_graphviz，无法这样保存图片通
----&gt; 2 digraph = lgb.to_graphviz(lgb_clf , num_trees=1)#报错module &#39;lightgbm&#39; has no attribute &#39;to_graphviz&#39;
      3 digraph.format = &#39;png&#39;
      4 digraph.view(&#39;./iris_lgb&#39;)


AttributeError: module &#39;lightgbm&#39; has no attribute &#39;to_graphviz&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">xgb_clf = xgb.XGBClassifier()</span><br><span class="line">xgb_clf.fit(iris.data, iris.target)</span><br><span class="line">xgb.to_graphviz(xgb_clf, num_trees=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[02:55:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
</code></pre><p><img src="lightGBM_files/lightGBM_35_1.svg" alt="svg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过Digraph对象可以将保存文件并查看</span></span><br><span class="line">digraph = xgb.to_graphviz(xgb_clf, num_trees=<span class="number">1</span>)</span><br><span class="line">digraph.<span class="built_in">format</span> = <span class="string">&#x27;png&#x27;</span><span class="comment">#将图像保存为png图片</span></span><br><span class="line">digraph.view(<span class="string">&#x27;./iris_xgb&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;iris_xgb.png&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 步骤3：读取任务2的json格式模型文件</span></span><br><span class="line">bst = lgb.Booster(model_file=<span class="string">&#x27;model.json&#x27;</span>)</span><br><span class="line">lgb.create_tree_digraph(bst, tree_index=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="lightGBM_files/lightGBM_37_0.svg" alt="svg"></p>
<h2 id="五、模型调参（网格、随机、贝叶斯）"><a href="#五、模型调参（网格、随机、贝叶斯）" class="headerlink" title="五、模型调参（网格、随机、贝叶斯）"></a>五、模型调参（网格、随机、贝叶斯）</h2><h3 id="5-1-加载数据集"><a href="#5-1-加载数据集" class="headerlink" title="5.1 加载数据集"></a>5.1 加载数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下四列数据转换为类别</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">train, test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MONTH</th>
      <th>DAY</th>
      <th>DAY_OF_WEEK</th>
      <th>AIRLINE</th>
      <th>FLIGHT_NUMBER</th>
      <th>DESTINATION_AIRPORT</th>
      <th>ORIGIN_AIRPORT</th>
      <th>AIR_TIME</th>
      <th>DEPARTURE_TIME</th>
      <th>DISTANCE</th>
      <th>ARRIVAL_DELAY</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>88</td>
      <td>253</td>
      <td>13</td>
      <td>169.0</td>
      <td>2354.0</td>
      <td>1448</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>2120</td>
      <td>213</td>
      <td>164</td>
      <td>263.0</td>
      <td>2.0</td>
      <td>2330</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>12</td>
      <td>803</td>
      <td>60</td>
      <td>262</td>
      <td>266.0</td>
      <td>18.0</td>
      <td>2296</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>238</td>
      <td>185</td>
      <td>164</td>
      <td>258.0</td>
      <td>15.0</td>
      <td>2342</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>122</td>
      <td>14</td>
      <td>261</td>
      <td>199.0</td>
      <td>24.0</td>
      <td>1448</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9994</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>2399</td>
      <td>44</td>
      <td>215</td>
      <td>62.0</td>
      <td>1710.0</td>
      <td>473</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9995</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>7</td>
      <td>149</td>
      <td>128</td>
      <td>210</td>
      <td>28.0</td>
      <td>1716.0</td>
      <td>100</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9996</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>2510</td>
      <td>208</td>
      <td>76</td>
      <td>29.0</td>
      <td>1653.0</td>
      <td>147</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9997</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>2512</td>
      <td>62</td>
      <td>215</td>
      <td>28.0</td>
      <td>1721.0</td>
      <td>135</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9998</th>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>2541</td>
      <td>208</td>
      <td>182</td>
      <td>103.0</td>
      <td>2000.0</td>
      <td>594</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>9592 rows × 11 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br></pre></td></tr></table></figure>
<h3 id="5-2-步骤2-设置树模型深度分别为-3-5-6-9-，记录下验证集AUC精度。"><a href="#5-2-步骤2-设置树模型深度分别为-3-5-6-9-，记录下验证集AUC精度。" class="headerlink" title="5.2:步骤2 设置树模型深度分别为[3,5,6,9]，记录下验证集AUC精度。"></a>5.2:步骤2 设置树模型深度分别为[3,5,6,9]，记录下验证集AUC精度。</h3><ul>
<li>predict：lgb.LGBMClassifier()等sklearn接口中是返回预测的类别</li>
<li>predict_proba：klearn接口中是返回预测的概率。重点是求auc时，我们必须用predict_proba。因为roc曲线的阀值是根据其正样本的概率求的。</li>
</ul>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43827767/article/details/120586336">《数据挖掘竞赛lightgbm通过求最大auc调参》</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#sklearn接口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_depth</span>(<span class="params">max_depth</span>):</span>    </span><br><span class="line">    gbm = lgb.LGBMClassifier(max_depth=max_depth)</span><br><span class="line">    gbm.fit(train, y_train,</span><br><span class="line">            eval_set=[(test, y_test)],</span><br><span class="line">            eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">            callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line">    <span class="comment">#eval_metric默认值：LGBMRegressor 为“l2”，LGBMClassifier 为“logloss”，LGBMRanker 为“ndcg”。</span></span><br><span class="line">    <span class="comment">#使用binary_logloss或者logloss准确率都是一样的。默认logloss</span></span><br><span class="line">    y_pred = gbm.predict(test)</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">    auc_score=metrics.roc_auc_score(y_test,gbm.predict_proba(test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;max_depth=&quot;</span>,max_depth,<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line">test_depth(<span class="number">3</span>)</span><br><span class="line">test_depth(<span class="number">5</span>)</span><br><span class="line">test_depth(<span class="number">6</span>)</span><br><span class="line">test_depth(<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[98]    valid_0&#39;s binary_logloss: 0.429334
max_depth= 3 accuarcy: 81.90% auc_score: 76.32%
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[60]    valid_0&#39;s binary_logloss: 0.430826
max_depth= 5 accuarcy: 81.98% auc_score: 75.54%
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[65]    valid_0&#39;s binary_logloss: 0.429341
max_depth= 6 accuarcy: 81.69% auc_score: 75.63%
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[52]    valid_0&#39;s binary_logloss: 0.429146
max_depth= 9 accuarcy: 81.94% auc_score: 76.07%
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#原生train接口</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">lgb_train = lgb.Dataset(train, y_train)</span><br><span class="line">lgb_eval = lgb.Dataset(test, y_test, reference=lgb_train)</span><br><span class="line"><span class="comment">#设置参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_depth</span>(<span class="params">max_depth</span>):</span> </span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;learning_rate&quot;</span>:<span class="number">0.1</span>, </span><br><span class="line">        <span class="string">&quot;min_child_samples&quot;</span>:<span class="number">20</span>,</span><br><span class="line">         <span class="string">&quot;num_leaves&quot;</span>:<span class="number">31</span>,</span><br><span class="line">         <span class="string">&quot;max_depth&quot;</span>:max_depth&#125;</span><br><span class="line"></span><br><span class="line">    gbm2 = lgb.train(params,</span><br><span class="line">                    lgb_train,</span><br><span class="line">                    num_boost_round=<span class="number">10</span>,</span><br><span class="line">                    valid_sets=lgb_eval,</span><br><span class="line">                    callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">    y_pred = gbm2.predict(test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">    pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">    accuracy = accuracy_score(y_test,pred)</span><br><span class="line">    auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;max_depth=&quot;</span>,max_depth,<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line">test_depth(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------------------------------------&#x27;</span>)</span><br><span class="line">test_depth(<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-3-步骤3：category变量设置为categorical-feature"><a href="#5-3-步骤3：category变量设置为categorical-feature" class="headerlink" title="5.3  步骤3：category变量设置为categorical_feature"></a>5.3  步骤3：category变量设置为categorical_feature</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/anshuai_aw1/article/details/83275299">《Lightgbm如何处理类别特征？》</a><br>参考kaggle教程<a target="_blank" rel="noopener" href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">《Feature Selection with Null Importances》</a>中的代码。</p>
</blockquote>
<p>lightGBM比XGBoost的1个改进之处在于对类别特征的处理, 不再需要将类别特征转为one-hot形式。这一步通过设置categorical_feature来实现。</p>
<p>唯一疑惑的是真正的object特征只有’AIRLINE’, ‘DESTINATION_AIRPORT’, ‘ORIGIN_AIRPORT’，但是’FLIGHT_NUMBER’也设置成类别特征效果更好。</p>
<ul>
<li>‘FLIGHT_NUMBER’也为类别特征：accuarcy: 81.82% auc_score: 77.52%</li>
<li>‘FLIGHT_NUMBER’不是类别特征：accuarcy: 81.69% auc_score: 76.48%</li>
</ul>
<p>估计跟数据集有关系，没有仔细研究数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line">categorical_feats  = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="comment">#categorical_feats = [f for f in data.columns if data[f].dtype == &#x27;object&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上面四列特征转为类别特征，但不是one-hot编码</span></span><br><span class="line"><span class="keyword">for</span> f_ <span class="keyword">in</span> categorical_feats:</span><br><span class="line">    data[f_], _ = pd.factorize(data[f_])</span><br><span class="line">    <span class="comment"># Set feature type as categorical</span></span><br><span class="line">    data[f_] = data[f_].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">categorical_feats</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;AIRLINE&#39;, &#39;FLIGHT_NUMBER&#39;, &#39;DESTINATION_AIRPORT&#39;, &#39;ORIGIN_AIRPORT&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征命名</span></span><br><span class="line"><span class="comment">#num_train, num_feature = X_train.shape#X_train是7194行10列的数据集，num_feature=10表示特征数量</span></span><br><span class="line"><span class="comment">#feature_name = [&#x27;feature_&#x27; + str(col) for col in range(num_feature)]#feature_0到9</span></span><br><span class="line"></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,  <span class="comment">#验证集设置</span></span><br><span class="line">                <span class="comment">#feature_name=feature_name,  #特征命名</span></span><br><span class="line">                categorical_feature=categorical_feats,</span><br><span class="line">                callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)]) <span class="comment">#设置分类变量</span></span><br><span class="line"></span><br><span class="line">y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[10]    valid_0&#39;s binary_logloss: 0.424384
accuarcy: 81.82% auc_score: 77.52%
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不设置categorical_feature结果一样啊，不知道为何？</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行one-hot编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">train, test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                 callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)]) </span><br><span class="line"></span><br><span class="line">y_pred2 = gbm2.predict(X_test,num_iteration=gbm2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred2 =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred2]</span><br><span class="line">accuracy2 = accuracy_score(y_test,pred2)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,y_pred2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Did not meet early stopping. Best iteration is:
[10]    valid_0&#39;s binary_logloss: 0.424384
accuarcy: 81.82% auc_score: 77.52%
</code></pre><h3 id="5-4-步骤4：超参搜索"><a href="#5-4-步骤4：超参搜索" class="headerlink" title="5.4  步骤4：超参搜索"></a>5.4  步骤4：超参搜索</h3><h4 id="5-4-1-GridSearchCV"><a href="#5-4-1-GridSearchCV" class="headerlink" title="5.4.1 GridSearchCV"></a>5.4.1 GridSearchCV</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection">GridSearchCV参考文档</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, refit=<span class="literal">True</span>, cv=<span class="literal">None</span>, verbose=<span class="number">0</span>, pre_dispatch=<span class="string">&#x27;2*n_jobs&#x27;</span>, error_score=nan, return_train_score=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>其中 scoring是字符串格式或者str列表、字典。具体的参数列表参考文档<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">scoring-parameter</a></p>
<p>网格搜索——尝试所有可能的组合：<br><img src="https://img-blog.csdnimg.cn/fc0dcfd497d04ab38597dc9a4b02afe7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data = data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>],</span><br><span class="line">              <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">100</span>, <span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line">              <span class="string">&quot;num_leaves&quot;</span>:[<span class="number">25</span>,<span class="number">31</span>,<span class="number">36</span>]&#125;</span><br><span class="line"></span><br><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">10</span>,<span class="comment">#构建树的深度，越大越容易过拟合</span></span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">100</span>,         </span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">gs = GridSearchCV(gbm, param_grid=parameters, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">3</span>)</span><br><span class="line">gs.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score: %0.3f&quot;</span> % gs.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set: %s&quot;</span> % gs.best_params_ )</span><br></pre></td></tr></table></figure>
<pre><code>Best score: 0.805
Best parameters set: &#123;&#39;learning_rate&#39;: 0.05, &#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 100, &#39;num_leaves&#39;: 36&#125;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用最优参数预测验证集</span></span><br><span class="line">y_pred = gs.predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gs.predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 82.07% auc_score: 75.92%
</code></pre><h3 id="5-4-3-随机搜索"><a href="#5-4-3-随机搜索" class="headerlink" title="5.4.3 随机搜索"></a>5.4.3 随机搜索</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">参考文档</a></p>
</blockquote>
<p>网格搜索尝试超参数的所有组合，因此增加了计算的时间复杂度，在数据量较大，或者模型较为复杂等等情况下，可能导致不可行的计算成本，这样网格搜索调参方法就不适用了。然而，随机搜索提供更便利的替代方案，该方法只测试你选择的超参数组成的元组，并且超参数值的选择是完全随机的，如下图所示。</p>
<p><img src="https://img-blog.csdnimg.cn/424f79626ce841ae95ee8bd3247808e9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line">param = <span class="built_in">dict</span>(n_estimators=[<span class="number">80</span>,<span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">             max_depth=[<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],</span><br><span class="line">            learning_rate= [<span class="number">0.02</span>,<span class="number">0.05</span>, <span class="number">0.1</span>],</span><br><span class="line">            num_leaves=[<span class="number">25</span>,<span class="number">31</span>,<span class="number">36</span>])</span><br><span class="line"></span><br><span class="line">grid = RandomizedSearchCV(estimator=lgb.LGBMClassifier(),</span><br><span class="line">                          param_distributions=param,scoring=<span class="string">&#x27;accuracy&#x27;</span>,cv=<span class="number">3</span>)</span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score: %0.3f&quot;</span> % grid.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters set: %s&quot;</span> % grid.best_params_ )</span><br><span class="line"><span class="comment"># 找到最好的模型</span></span><br><span class="line">grid.best_estimator_</span><br></pre></td></tr></table></figure>
<pre><code>Best score: 0.806
Best parameters set: &#123;&#39;num_leaves&#39;: 36, &#39;n_estimators&#39;: 80, &#39;max_depth&#39;: 6, &#39;learning_rate&#39;: 0.1&#125;





LGBMClassifier(max_depth=6, n_estimators=80, num_leaves=36)
</code></pre><p>最优模型直接用grid或者rid.best<em>estimator</em>都行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用最优参数预测验证集</span></span><br><span class="line">y_pred = grid .predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,grid .predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到最好的模型</span></span><br><span class="line">gd=grid.best_estimator_</span><br><span class="line">y_pred = gd .predict(X_test)</span><br><span class="line"><span class="comment"># 计算准确率和auc</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gd .predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 81.69% auc_score: 75.62%
accuarcy: 81.69% auc_score: 75.62%
</code></pre><h3 id="5-4-4-贝叶斯搜索"><a href="#5-4-4-贝叶斯搜索" class="headerlink" title="5.4.4 贝叶斯搜索"></a>5.4.4 贝叶斯搜索</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42283960/article/details/88317003">《贝叶斯全局优化（LightGBM调参）》</a></p>
</blockquote>
<p>贝叶斯搜索使用贝叶斯优化技术对搜索空间进行建模，以尽快获得优化的参数值。它使用搜索空间的结构来优化搜索时间。贝叶斯搜索方法使用过去的评估结果来采样最有可能提供更好结果的新候选参数（如下图所示）:</p>
<p><img src="https://img-blog.csdnimg.cn/02eaae7e2d104f418e5d0331ca9cd870.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设定贝叶斯优化的黑盒函数LGB_bayesian</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LGB_bayesian</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    num_leaves,  <span class="comment"># int</span></span></span></span><br><span class="line"><span class="params"><span class="function">    min_data_in_leaf,  <span class="comment"># int</span></span></span></span><br><span class="line"><span class="params"><span class="function">    learning_rate,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_sum_hessian_in_leaf,    <span class="comment"># int  </span></span></span></span><br><span class="line"><span class="params"><span class="function">    feature_fraction,</span></span></span><br><span class="line"><span class="params"><span class="function">    lambda_l1,</span></span></span><br><span class="line"><span class="params"><span class="function">    lambda_l2,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_gain_to_split,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_depth</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LightGBM expects next three parameters need to be integer. So we make them integer</span></span><br><span class="line">    num_leaves = <span class="built_in">int</span>(num_leaves)</span><br><span class="line">    min_data_in_leaf = <span class="built_in">int</span>(min_data_in_leaf)</span><br><span class="line">    max_depth = <span class="built_in">int</span>(max_depth)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(num_leaves) == <span class="built_in">int</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(min_data_in_leaf) == <span class="built_in">int</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">type</span>(max_depth) == <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: num_leaves,</span><br><span class="line">        <span class="string">&#x27;max_bin&#x27;</span>: <span class="number">63</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: min_data_in_leaf,</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: learning_rate,</span><br><span class="line">        <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: min_sum_hessian_in_leaf,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: feature_fraction,</span><br><span class="line">        <span class="string">&#x27;lambda_l1&#x27;</span>: lambda_l1,</span><br><span class="line">        <span class="string">&#x27;lambda_l2&#x27;</span>: lambda_l2,</span><br><span class="line">        <span class="string">&#x27;min_gain_to_split&#x27;</span>: min_gain_to_split,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: max_depth,</span><br><span class="line">        <span class="string">&#x27;save_binary&#x27;</span>: <span class="literal">True</span>, </span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;drop_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;data_random_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;is_unbalance&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;boost_from_average&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    &#125;    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    lgb_train = lgb.Dataset(X_train,</span><br><span class="line">                           label=y_train)</span><br><span class="line">    lgb_valid = lgb.Dataset(X_test,label=y_test,reference=lgb_train)   </span><br><span class="line"></span><br><span class="line">    num_round = <span class="number">500</span></span><br><span class="line">    gbm= lgb.train(param, lgb_train, num_round, valid_sets = [lgb_valid],callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])   </span><br><span class="line">    predictions = gbm.predict(X_test,num_iteration=gbm.best_iteration)</span><br><span class="line">    score = metrics.roc_auc_score(y_test, predictions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>LGB_bayesian函数从贝叶斯优化框架获取num_leaves，min_data_in_leaf，learning_rate，min_sum_hessian_in_leaf，feature_fraction，lambda_l1，lambda_l2，min_gain_to_split，max_depth的值。 请记住，对于LightGBM，num_leaves，min_data_in_leaf和max_depth应该是整数。 但贝叶斯优化会发送连续的函数。 所以我强制它们是整数。 我只会找到它们的最佳参数值。 读者可以增加或减少要优化的参数数量。<br>现在需要为这些参数提供边界，以便贝叶斯优化仅在边界内搜索</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bounds_LGB = &#123;</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: (<span class="number">5</span>, <span class="number">20</span>), </span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: (<span class="number">5</span>, <span class="number">20</span>),  </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: (<span class="number">0.01</span>, <span class="number">0.3</span>),</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: (<span class="number">0.00001</span>, <span class="number">0.01</span>),    </span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: (<span class="number">0.05</span>, <span class="number">0.5</span>),</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: (<span class="number">0</span>, <span class="number">5.0</span>), </span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: (<span class="number">0</span>, <span class="number">5.0</span>), </span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: (<span class="number">0</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>:(<span class="number">3</span>,<span class="number">15</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#将它们全部放在BayesianOptimization对象中</span></span><br><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line">LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=<span class="number">13</span>)</span><br><span class="line"><span class="built_in">print</span>(LGB_BO.space.keys)<span class="comment">#显示要优化的参数</span></span><br></pre></td></tr></table></figure>
<pre><code>[&#39;feature_fraction&#39;, &#39;lambda_l1&#39;, &#39;lambda_l2&#39;, &#39;learning_rate&#39;, &#39;max_depth&#39;, &#39;min_data_in_leaf&#39;, &#39;min_gain_to_split&#39;, &#39;min_sum_hessian_in_leaf&#39;, &#39;num_leaves&#39;]
</code></pre><p>调用maximize方法LGB_BO才会开始搜索。</p>
<ul>
<li>init_points：我们想要执行的随机探索的初始随机运行次数。 在我们的例子中，LGB_bayesian将被运行n_iter次。</li>
<li>n_iter：运行init_points数后，我们要执行多少次贝叶斯优化运行。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">init_points = <span class="number">5</span></span><br><span class="line">n_iter = <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">130</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> warnings.catch_warnings():</span><br><span class="line">    warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq=<span class="string">&#x27;ucb&#x27;</span>, xi=<span class="number">0.0</span>, alpha=<span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure>
<pre><code>----------------------------------------------------------------------------------------------------------------------------------
|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |
-------------------------------------------------------------------------------------------------------------------------------------
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[22]    valid_0&#39;s auc: 0.770384
| [0m 1       [0m | [0m 0.7704  [0m | [0m 0.4     [0m | [0m 1.188   [0m | [0m 4.121   [0m | [0m 0.2901  [0m | [0m 14.67   [0m | [0m 11.8    [0m | [0m 0.609   [0m | [0m 0.007758[0m | [0m 14.62   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[5]    valid_0&#39;s auc: 0.737399
| [0m 2       [0m | [0m 0.7374  [0m | [0m 0.3749  [0m | [0m 0.1752  [0m | [0m 1.492   [0m | [0m 0.02697 [0m | [0m 13.28   [0m | [0m 10.59   [0m | [0m 0.6798  [0m | [0m 0.00257 [0m | [0m 10.21   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[8]    valid_0&#39;s auc: 0.719501
| [0m 3       [0m | [0m 0.7195  [0m | [0m 0.05424 [0m | [0m 1.792   [0m | [0m 4.745   [0m | [0m 0.07319 [0m | [0m 6.833   [0m | [0m 18.77   [0m | [0m 0.0319  [0m | [0m 0.000660[0m | [0m 14.45   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[19]    valid_0&#39;s auc: 0.760323
| [0m 4       [0m | [0m 0.7603  [0m | [0m 0.4432  [0m | [0m 0.04358 [0m | [0m 3.733   [0m | [0m 0.2457  [0m | [0m 3.909   [0m | [0m 14.85   [0m | [0m 0.5093  [0m | [0m 0.004804[0m | [0m 19.33   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[8]    valid_0&#39;s auc: 0.719412
| [0m 5       [0m | [0m 0.7194  [0m | [0m 0.05001 [0m | [0m 1.235   [0m | [0m 3.561   [0m | [0m 0.1041  [0m | [0m 6.324   [0m | [0m 15.43   [0m | [0m 0.9186  [0m | [0m 0.002452[0m | [0m 11.87   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[11]    valid_0&#39;s auc: 0.761779
| [0m 6       [0m | [0m 0.7618  [0m | [0m 0.5     [0m | [0m 1.457   [0m | [0m 5.0     [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 11.16   [0m | [0m 0.5786  [0m | [0m 0.01    [0m | [0m 17.3    [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[8]    valid_0&#39;s auc: 0.723696
| [0m 7       [0m | [0m 0.7237  [0m | [0m 0.05    [0m | [0m 5.0     [0m | [0m 5.0     [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 12.07   [0m | [0m 0.0     [0m | [0m 0.01    [0m | [0m 14.22   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770585
| [95m 8       [0m | [95m 0.7706  [0m | [95m 0.5     [0m | [95m 0.0     [0m | [95m 2.9     [0m | [95m 0.3     [0m | [95m 14.75   [0m | [95m 11.78   [0m | [95m 1.0     [0m | [95m 0.003764[0m | [95m 16.12   [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[19]    valid_0&#39;s auc: 0.769728
| [0m 9       [0m | [0m 0.7697  [0m | [0m 0.5     [0m | [0m 0.0     [0m | [0m 4.272   [0m | [0m 0.3     [0m | [0m 15.0    [0m | [0m 8.6     [0m | [0m 1.0     [0m | [0m 0.009985[0m | [0m 15.0    [0m |
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[28]    valid_0&#39;s auc: 0.770488
| [0m 10      [0m | [0m 0.7705  [0m | [0m 0.5     [0m | [0m 0.0     [0m | [0m 4.457   [0m | [0m 0.3     [0m | [0m 10.79   [0m | [0m 9.347   [0m | [0m 1.0     [0m | [0m 0.01    [0m | [0m 16.83   [0m |
=====================================================================================================================================
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;target&#x27;</span>])<span class="comment">#最佳的auc值</span></span><br><span class="line">LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>]<span class="comment">#最佳模型参数</span></span><br></pre></td></tr></table></figure>
<pre><code>0.7705848546741305





&#123;&#39;feature_fraction&#39;: 0.5,
 &#39;lambda_l1&#39;: 0.0,
 &#39;lambda_l2&#39;: 2.899605369776912,
 &#39;learning_rate&#39;: 0.3,
 &#39;max_depth&#39;: 14.752822601781512,
 &#39;min_data_in_leaf&#39;: 11.782200828907708,
 &#39;min_gain_to_split&#39;: 1.0,
 &#39;min_sum_hessian_in_leaf&#39;: 0.0037639771497955552,
 &#39;num_leaves&#39;: 16.11909067874899&#125;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将这些参数用于我们的最终模型</span></span><br><span class="line">LGB_BO.probe(</span><br><span class="line">    params=&#123;<span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">            <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">            <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">            <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">            <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">            <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">            <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">            <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">            <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>&#125;,</span><br><span class="line">            lazy=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对LGB_BO对象进行最大化调用。</span></span><br><span class="line">LGB_BO.maximize(init_points=<span class="number">0</span>, n_iter=<span class="number">0</span>) </span><br></pre></td></tr></table></figure>
<pre><code>|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |
-------------------------------------------------------------------------------------------------------------------------------------
[LightGBM] [Warning] verbosity is set=-1, verbose=1 will be ignored. Current value: verbosity=-1
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
| [95m 11      [0m | [95m 0.7706  [0m | [95m 0.5     [0m | [95m 0.0     [0m | [95m 2.9     [0m | [95m 0.3     [0m | [95m 15.0    [0m | [95m 12.0    [0m | [95m 1.0     [0m | [95m 0.0038  [0m | [95m 16.0    [0m |
=====================================================================================================================================
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过属性LGB_BO.res可以获得探测的所有参数列表及其相应的目标值。</span></span><br><span class="line"><span class="keyword">for</span> i, res <span class="keyword">in</span> <span class="built_in">enumerate</span>(LGB_BO.res):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Iteration &#123;&#125;: \n\t&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, res))</span><br></pre></td></tr></table></figure>
<p>将LGB_BO的最佳参数保存到param_lgb字典中，然后进行5折交叉训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> rankdata</span><br><span class="line">param_lgb = &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;num_leaves&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;max_bin&#x27;</span>: <span class="number">63</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_data_in_leaf&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;learning_rate&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">1.0</span>, </span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">5</span>, </span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;feature_fraction&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;lambda_l1&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;lambda_l1&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;lambda_l2&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;lambda_l2&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;min_gain_to_split&#x27;</span>: LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;min_gain_to_split&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">int</span>(LGB_BO.<span class="built_in">max</span>[<span class="string">&#x27;params&#x27;</span>][<span class="string">&#x27;max_depth&#x27;</span>]), <span class="comment"># remember to int here</span></span><br><span class="line">        <span class="string">&#x27;save_binary&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;drop_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;data_random_seed&#x27;</span>: <span class="number">1337</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;is_unbalance&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;boost_from_average&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">nfold = <span class="number">5</span></span><br><span class="line">gc.collect()</span><br><span class="line">skf = StratifiedKFold(n_splits=nfold, shuffle=<span class="literal">True</span>, random_state=<span class="number">2019</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">oof = np.zeros(<span class="built_in">len</span>(y_train))</span><br><span class="line">predictions = np.zeros((<span class="built_in">len</span>(X_test),nfold))</span><br><span class="line"></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> train_index, valid_index <span class="keyword">in</span> skf.split(X_train, y_train):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nfold &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    lgb_train = lgb.Dataset(X_train,label=y_train)</span><br><span class="line">    lgb_valid = lgb.Dataset(X_test,label=y_test,reference=lgb_train)  </span><br><span class="line">   </span><br><span class="line">    clf = lgb.train(param_lgb, lgb_train,<span class="number">500</span>, valid_sets = [lgb_valid ], verbose_eval=<span class="number">250</span>, callbacks=[lgb.early_stopping(stopping_rounds=<span class="number">5</span>)])</span><br><span class="line">    <span class="built_in">print</span>(clf.predict(X_train, num_iteration=clf.best_iteration) )</span><br><span class="line">    oof[valid_index] = clf.predict(X_train.iloc[valid_index].values, num_iteration=clf.best_iteration) </span><br><span class="line">    </span><br><span class="line">    predictions[:,i-<span class="number">1</span>] += clf.predict(X_test, num_iteration=clf.best_iteration)</span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n\nCV AUC: &#123;:&lt;0.2f&#125;&quot;</span>.<span class="built_in">format</span>(metrics.roc_auc_score(y_train, oof)))</span><br></pre></td></tr></table></figure>
<pre><code>fold 1
[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000288 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 393
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]

fold 2
[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 393
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]

fold 3
[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 393
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]

fold 4
[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 393
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]

fold 5
[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 393
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[17]    valid_0&#39;s auc: 0.770586
[0.52559221 0.40000825 0.43907974 ... 0.40122056 0.46515425 0.56678622]


CV AUC: 0.81
</code></pre><p>另一种贝叶斯搜索，参考:<br>[《网格搜索、随机搜索和贝叶斯搜索实用教程》[(<a target="_blank" rel="noopener" href="https://blog.csdn.net/fengdu78/article/details/121134090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164200107216780274194329%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164200107216780274194329&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-121134090.pc_search_insert_es_download&amp;utm_term=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%90%9C%E7%B4%A2&amp;spm=1018.2226.3001.4187">https://blog.csdn.net/fengdu78/article/details/121134090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164200107216780274194329%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164200107216780274194329&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-121134090.pc_search_insert_es_download&amp;utm_term=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%90%9C%E7%B4%A2&amp;spm=1018.2226.3001.4187</a>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#还没有写完，并不能正确运行</span></span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> BayesSearchCV</span><br><span class="line"><span class="comment"># 参数范围由下面的一个指定</span></span><br><span class="line"><span class="keyword">from</span> skopt.space <span class="keyword">import</span> Real, Categorical, Integer</span><br><span class="line">search_spaces = &#123;</span><br><span class="line">  <span class="string">&#x27;C&#x27;</span>: Real(<span class="number">0.1</span>, <span class="number">1e+4</span>),</span><br><span class="line">  <span class="string">&#x27;gamma&#x27;</span>: Real(<span class="number">1e-6</span>, <span class="number">1e+1</span>, <span class="string">&#x27;log-uniform&#x27;</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#接下来创建一个RandomizedSearchCV带参数n_iter_search的对象，并将使用训练数据来训练模型。</span></span><br><span class="line"></span><br><span class="line">n_iter_search = <span class="number">20</span> </span><br><span class="line">bayes_search = BayesSearchCV( </span><br><span class="line">    lgb.LGBMClassifier(), </span><br><span class="line">    search_spaces, </span><br><span class="line">    n_iter=n_iter_search, </span><br><span class="line">    cv=<span class="number">3</span>, </span><br><span class="line">    verbose=<span class="number">3</span> </span><br><span class="line">) </span><br><span class="line">bayes_search.fit(X_train, y_train)</span><br><span class="line">bayes_search.best_params_</span><br></pre></td></tr></table></figure>
<h2 id="六、模型微调与参数衰减"><a href="#六、模型微调与参数衰减" class="headerlink" title="六、模型微调与参数衰减"></a>六、模型微调与参数衰减</h2><h3 id="6-2-学习率衰减"><a href="#6-2-学习率衰减" class="headerlink" title="6.2 学习率衰减"></a>6.2 学习率衰减</h3><p>参考<a target="_blank" rel="noopener" href="https://www.codenong.com/cs108978573/https://www.codenong.com/cs108978573/">《python实现LightGBM(进阶)python实现LightGBM(进阶)》</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data = data[[<span class="string">&quot;MONTH&quot;</span>,<span class="string">&quot;DAY&quot;</span>,<span class="string">&quot;DAY_OF_WEEK&quot;</span>,<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行编码</span></span><br><span class="line">cols = [<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,<span class="string">&quot;ORIGIN_AIRPORT&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cols:</span><br><span class="line">    data[item] = data[item].astype(<span class="string">&quot;category&quot;</span>).cat.codes +<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 学习率指数衰减,learning_rates弃用了</span></span><br><span class="line">gbm = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                learning_rates=<span class="keyword">lambda</span> <span class="built_in">iter</span>: <span class="number">0.3</span> * (<span class="number">0.99</span> ** <span class="built_in">iter</span>),<span class="comment"># 学习率衰减</span></span><br><span class="line">                valid_sets=lgb_eval)</span><br><span class="line"><span class="comment">#设置learning_rates结果是accuarcy: 82.07% auc_score: 75.40%</span></span><br><span class="line"><span class="comment">#不设置learning_rates结果是accuarcy: 81.61% auc_score: 75.74%,还是不一样</span></span><br><span class="line"><span class="comment"># 学习率指数衰减</span></span><br><span class="line">gbm2 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=gbm,</span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.reset_parameter(bagging_fraction=<span class="keyword">lambda</span> <span class="built_in">iter</span>: <span class="number">0.3</span> * (<span class="number">0.99</span> ** <span class="built_in">iter</span>))])</span><br><span class="line"><span class="comment">#不设置init_model，结果是accuarcy: 81.69% auc_score: 75.25%</span></span><br><span class="line"><span class="comment">#  设置init_model，结果是accuarcy: 81.94% auc_score: 76.32%</span></span><br><span class="line"><span class="comment">#lgb.reset_parameter参数可以是列表或者衰减函数，不知道为啥bagging_fraction设置不同值结果是一样的</span></span><br><span class="line">y_pred1,y_pred2 = gbm.predict(X_test,num_iteration=gbm.best_iteration),gbm2.predict(X_test,num_iteration=gbm2.best_iteration)</span><br><span class="line">pred1,pred2 =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred1],[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred2]</span><br><span class="line">accuracy1,accuracy2 = accuracy_score(y_test,pred1),accuracy_score(y_test,pred2)</span><br><span class="line">auc_score1,auc_score2=metrics.roc_auc_score(y_test,y_pred1),metrics.roc_auc_score(y_test,y_pred2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy1*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score1*<span class="number">100.0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[1]    valid_0&#39;s binary_logloss: 0.48425
[2]    valid_0&#39;s binary_logloss: 0.471031
[3]    valid_0&#39;s binary_logloss: 0.46278
[4]    valid_0&#39;s binary_logloss: 0.456369
[5]    valid_0&#39;s binary_logloss: 0.449357
[6]    valid_0&#39;s binary_logloss: 0.444377
[7]    valid_0&#39;s binary_logloss: 0.440908
[8]    valid_0&#39;s binary_logloss: 0.438597
[9]    valid_0&#39;s binary_logloss: 0.435632
[10]    valid_0&#39;s binary_logloss: 0.434647
accuarcy: 82.07% auc_score: 75.40%
accuarcy: 82.19% auc_score: 76.08%
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 学习率阶梯衰减,bagging_fraction&#x27;如果使用列表，列表元素数量要和  &#x27;num_boost_round&#x27;值一样</span></span><br><span class="line">gbm3 = lgb.train(params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=gbm,<span class="comment">#</span></span><br><span class="line">                valid_sets=lgb_eval,</span><br><span class="line">                callbacks=[lgb.reset_parameter(bagging_fraction=[<span class="number">0.6</span>]*<span class="number">5</span>+[<span class="number">0.2</span>]*<span class="number">3</span>+[<span class="number">0.1</span>]*<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">y_pred = gbm3.predict(X_test,num_iteration=gbm3.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>accuarcy: 81.94% auc_score: 76.30%
</code></pre><h2 id="七、特征筛选方法"><a href="#七、特征筛选方法" class="headerlink" title="七、特征筛选方法"></a>七、特征筛选方法</h2><h3 id="7-1-筛选最重要的3个特征"><a href="#7-1-筛选最重要的3个特征" class="headerlink" title="7.1 筛选最重要的3个特征"></a>7.1 筛选最重要的3个特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过feature_importances_方法得到特征重要性，值越高越重要</span></span><br><span class="line">gbm = lgb.LGBMClassifier(max_depth=<span class="number">9</span>)</span><br><span class="line">gbm.fit(train, y_train,</span><br><span class="line">            eval_set=[(test, y_test)],</span><br><span class="line">            eval_metric=<span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">            callbacks=[lgb.early_stopping(<span class="number">5</span>)])</span><br><span class="line">df=pd.DataFrame(gbm.feature_importances_,gbm.feature_name_,columns=[<span class="string">&#x27;value&#x27;</span>])</span><br><span class="line">df.sort_values(<span class="string">&#x27;value&#x27;</span>,inplace=<span class="literal">True</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[52]    valid_0&#39;s binary_logloss: 0.429146
</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>DEPARTURE_TIME</th>
      <td>276</td>
    </tr>
    <tr>
      <th>ORIGIN_AIRPORT</th>
      <td>262</td>
    </tr>
    <tr>
      <th>DESTINATION_AIRPORT</th>
      <td>250</td>
    </tr>
    <tr>
      <th>FLIGHT_NUMBER</th>
      <td>236</td>
    </tr>
    <tr>
      <th>AIR_TIME</th>
      <td>227</td>
    </tr>
    <tr>
      <th>DISTANCE</th>
      <td>184</td>
    </tr>
    <tr>
      <th>AIRLINE</th>
      <td>124</td>
    </tr>
    <tr>
      <th>MONTH</th>
      <td>0</td>
    </tr>
    <tr>
      <th>DAY</th>
      <td>0</td>
    </tr>
    <tr>
      <th>DAY_OF_WEEK</th>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>上图看出，最重要的是DEPARTURE_TIME、ORIGIN_AIRPORT、DESTINATION_AIRPORT</p>
<h3 id="7-2-利用PermutationImportance排列特征重要性"><a href="#7-2-利用PermutationImportance排列特征重要性" class="headerlink" title="7.2 利用PermutationImportance排列特征重要性"></a>7.2 利用PermutationImportance排列特征重要性</h3><ul>
<li><a target="_blank" rel="noopener" href="https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance">eli5文档</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/lz_peter/article/details/88654198?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164201788816780357244048%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164201788816780357244048&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-88654198.pc_search_insert_es_download&amp;utm_term=PermutationImportance&amp;spm=1018.2226.3001.4187">利用PermutationImportance挑选变量</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/dansbecker/permutation-importance">kaggle教程</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> eli5</span><br><span class="line"><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line">perm = PermutationImportance(gbm, random_state=<span class="number">1</span>).fit(test,y_test)</span><br><span class="line">eli5.show_weights(perm, feature_names =gbm.feature_name_)</span><br></pre></td></tr></table></figure>
<pre><code>Training until validation scores don&#39;t improve for 5 rounds
Early stopping, best iteration is:
[52]    valid_0&#39;s binary_logloss: 0.429146






&lt;style&gt;
table.eli5-weights tr:hover &#123;
    filter: brightness(85%);
&#125;
</code></pre><p>&lt;/style&gt;</p>
<pre><code>    &lt;table class=&quot;eli5-weights eli5-feature-importances&quot; style=&quot;border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;&quot;&gt;
&lt;thead&gt;
&lt;tr style=&quot;border: none;&quot;&gt;
    &lt;th style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;Weight&lt;/th&gt;
    &lt;th style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;Feature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 80.00%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0396

                &amp;plusmn; 0.0096

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            DEPARTURE_TIME
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 90.14%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0144

                &amp;plusmn; 0.0057

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            DESTINATION_AIRPORT
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 90.50%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0137

                &amp;plusmn; 0.0043

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            ORIGIN_AIRPORT
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 92.95%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0089

                &amp;plusmn; 0.0067

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            AIR_TIME
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 95.47%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0048

                &amp;plusmn; 0.0041

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            AIRLINE
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 95.86%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0042

                &amp;plusmn; 0.0045

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            DISTANCE
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(120, 100.00%, 96.10%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0.0038

                &amp;plusmn; 0.0029

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            FLIGHT_NUMBER
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0

                &amp;plusmn; 0.0000

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            DAY_OF_WEEK
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0

                &amp;plusmn; 0.0000

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            DAY
        &lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr style=&quot;background-color: hsl(0, 100.00%, 100.00%); border: none;&quot;&gt;
        &lt;td style=&quot;padding: 0 1em 0 0.5em; text-align: right; border: none;&quot;&gt;
            0

                &amp;plusmn; 0.0000

        &lt;/td&gt;
        &lt;td style=&quot;padding: 0 0.5em 0 0.5em; text-align: left; border: none;&quot;&gt;
            MONTH
        &lt;/td&gt;
    &lt;/tr&gt;


&lt;/tbody&gt;
</code></pre><p>&lt;/table&gt;</p>
<p>所以前三重要的特征是DEPARTURE_TIME、DESTINATION_AIRPORT、ORIGIN_AIRPORT</p>
<h3 id="7-3-Null-Importances进行特征选择"><a href="#7-3-Null-Importances进行特征选择" class="headerlink" title="7.3 Null Importances进行特征选择"></a>7.3 Null Importances进行特征选择</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">kaggkle教程</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39681171/article/details/109919282?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164216516716780357210720%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164216516716780357210720&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-109919282.pc_search_insert_es_download&amp;utm_term=null%20importance%E9%87%8D%E8%A6%81%E6%80%A7&amp;spm=1018.2226.3001.4187">《数据竞赛】99%情况下都有效的特征筛选策略—Null Importance》</a></p>
</blockquote>
<p>特征筛选策略 — Null Importance 特征筛选</p>
<h4 id="7-3-1-主要思想："><a href="#7-3-1-主要思想：" class="headerlink" title="7.3.1 主要思想："></a>7.3.1 主要思想：</h4><p>通过利用跑树模型得到特征的importance来判断特征的稳定性和好坏。</p>
<ol>
<li><p>将构建好的特征和正确的标签扔进树模型中，此时可以得到每个特征的重要性（split 和 gain）</p>
</li>
<li><p>将数据的标签打乱，再扔进模型中，得到打乱标签后，每个特征的重要性（split和gain）；重复n次；取n次特征重要性的平均值。</p>
</li>
<li><p>将1中正确标签跑的特征的重要性和2中打乱标签的特征中重要性进行比较；具体比较方式可以参考上面的kernel</p>
</li>
</ol>
<ul>
<li>当一个特征非常work，那它在正确标签的树模型中的importance应该很高，但它在打乱标签的树模型中的importance将很低（无法识别随机标签）；反之，一个垃圾特征，那它在正确标签的模型中importance很一般，打乱标签的树模型中importance将大于等于正确标签模型的importance。所以通过同时判断每个特征在正确标签的模型和打乱标签的模型中的importance（split和gain），可以选择特征稳定和work的特征。</li>
<li>思想大概就是这样吧，importance受到特征相关性的影响，特征的重要性会被相关特征的重要性稀释，看importance也不一定准，用这个来对暴力特征进行筛选还是可以的。</li>
</ul>
<h4 id="7-3-2实现步骤"><a href="#7-3-2实现步骤" class="headerlink" title="7.3.2实现步骤"></a>7.3.2实现步骤</h4><p>Null Importance算法的实现步骤为：</p>
<ol>
<li>在原始数据集上运行模型并且记录每个特征重要性。以此作为基准；</li>
<li>构建Null importances分布：对我们的标签进行随机Shuffle，并且计算shuffle之后的特征的重要性；</li>
<li>对2进行多循环操作，得到多个不同shuffle之后的特征重要性；</li>
<li>设计score函数，得到未shuffle的特征重要性与shuffle之后特征重要性的偏离度，并以此设计特征筛选策略；</li>
<li>计算不同筛选情况下的模型的分数，并进行记录；</li>
<li>将分数最好的几个分数对应的特征进行返回。实现步骤</li>
</ol>
<h4 id="7-3-3-读取数据集，计算Real-Targe和shuffle-Target下的特征重要度"><a href="#7-3-3-读取数据集，计算Real-Targe和shuffle-Target下的特征重要度" class="headerlink" title="7.3.3 读取数据集，计算Real Targe和shuffle Target下的特征重要度"></a>7.3.3 读取数据集，计算Real Targe和shuffle Target下的特征重要度</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.simplefilter(<span class="string">&#x27;ignore&#x27;</span>, UserWarning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">gc.enable()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd, numpy <span class="keyword">as</span> np, time</span><br><span class="line">data= pd.read_csv(<span class="string">&quot;https://cdn.coggle.club/kaggle-flight-delays/flights_10k.csv.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取有用的列</span></span><br><span class="line">data= data[[<span class="string">&quot;AIRLINE&quot;</span>,<span class="string">&quot;FLIGHT_NUMBER&quot;</span>,<span class="string">&quot;DESTINATION_AIRPORT&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;ORIGIN_AIRPORT&quot;</span>,<span class="string">&quot;AIR_TIME&quot;</span>, <span class="string">&quot;DEPARTURE_TIME&quot;</span>,<span class="string">&quot;DISTANCE&quot;</span>,<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]]</span><br><span class="line">data.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 筛选出部分数据</span></span><br><span class="line">data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>] = (data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>]&gt;<span class="number">10</span>)*<span class="number">1</span></span><br><span class="line"><span class="comment">#categorical_feats  = [&quot;AIRLINE&quot;,&quot;FLIGHT_NUMBER&quot;,&quot;DESTINATION_AIRPORT&quot;,&quot;ORIGIN_AIRPORT&quot;]</span></span><br><span class="line">categorical_feats = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> data[f].dtype == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#将上面四列特征转为类别特征，但不是one-hot编码</span></span><br><span class="line"><span class="keyword">for</span> f_ <span class="keyword">in</span> categorical_feats:</span><br><span class="line">    data[f_], _ = pd.factorize(data[f_])</span><br><span class="line">    <span class="comment"># Set feature type as categorical</span></span><br><span class="line">    data[f_] = data[f_].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.drop([<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], axis=<span class="number">1</span>), data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>], random_state=<span class="number">10</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>
<p>创建评分函数<br>feature<em>importances</em> :特征重要性的类型。default=’split’。</p>
<ul>
<li>如果是split，则结果包含该特征在模型中使用的次数。 </li>
<li>如果为“gain”，则结果包含使用该特征的分割的总增益。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span>(<span class="params">X_train, X_test, y_train, y_test,shuffle, seed=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 获取特征</span></span><br><span class="line">    train_features = <span class="built_in">list</span>(X_train.columns)   </span><br><span class="line">    <span class="comment"># 判断是否shuffle TARGET</span></span><br><span class="line">    y_train,y_test= y_train.copy(),y_test.copy()</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Here you could as well use a binomial distribution</span></span><br><span class="line">        y_train,y_test= y_train.copy().sample(frac=<span class="number">1.0</span>),y_test.copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    clf = lgb.train(params=lgb_params,train_set=lgb_train,valid_sets=lgb_eval,num_boost_round=<span class="number">10</span>, categorical_feature=categorical_feats)<span class="comment">#将object特征设置为分类特征，但是并不需要进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#得到特征重要性</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">&quot;feature&quot;</span>] = <span class="built_in">list</span>(train_features)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_gain&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;gain&#x27;</span>)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_split&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;split&#x27;</span>)</span><br><span class="line">    imp_df[<span class="string">&#x27;trn_score&#x27;</span>] = roc_auc_score(y_test, clf.predict( X_test))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="comment"># 获得市实际的特征重要性，即没有shuffletarget</span></span><br><span class="line">actual_imp_df = get_feature_importances(X_train, X_test, y_train, y_test, shuffle=<span class="literal">False</span>)</span><br><span class="line">actual_imp_df</span><br></pre></td></tr></table></figure>
<pre><code>[LightGBM] [Info] Number of positive: 1600, number of negative: 5594
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 1406
[LightGBM] [Info] Number of data points in the train set: 7194, number of used features: 7
[1]    valid_0&#39;s binary_logloss: 0.479157
[2]    valid_0&#39;s binary_logloss: 0.46882
[3]    valid_0&#39;s binary_logloss: 0.454724
[4]    valid_0&#39;s binary_logloss: 0.445913
[5]    valid_0&#39;s binary_logloss: 0.440924
[6]    valid_0&#39;s binary_logloss: 0.438309
[7]    valid_0&#39;s binary_logloss: 0.433886
[8]    valid_0&#39;s binary_logloss: 0.432747
[9]    valid_0&#39;s binary_logloss: 0.431001
[10]    valid_0&#39;s binary_logloss: 0.429621
</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature</th>
      <th>importance_gain</th>
      <th>importance_split</th>
      <th>trn_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AIRLINE</td>
      <td>153.229680</td>
      <td>15</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>1</th>
      <td>FLIGHT_NUMBER</td>
      <td>189.481180</td>
      <td>23</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DESTINATION_AIRPORT</td>
      <td>1036.401096</td>
      <td>23</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ORIGIN_AIRPORT</td>
      <td>650.938854</td>
      <td>22</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AIR_TIME</td>
      <td>119.763649</td>
      <td>17</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>5</th>
      <td>DEPARTURE_TIME</td>
      <td>994.109417</td>
      <td>37</td>
      <td>0.764829</td>
    </tr>
    <tr>
      <th>6</th>
      <td>DISTANCE</td>
      <td>93.170790</td>
      <td>13</td>
      <td>0.764829</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">10</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_runs):</span><br><span class="line">    <span class="comment"># 获取当前的特征重要性</span></span><br><span class="line">    imp_df = get_feature_importances(X_train, X_test, y_train, y_test, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">&#x27;run&#x27;</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># 将特征重要性连起来</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 删除上一条信息</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dsp)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\b&#x27;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Display current run and time used</span></span><br><span class="line">    spent = (time.time() - start) / <span class="number">60</span></span><br><span class="line">    dsp = <span class="string">&#x27;Done with %4d of %4d (Spent %5.1f min)&#x27;</span> % (i + <span class="number">1</span>, nb_runs, spent)</span><br><span class="line">    <span class="built_in">print</span>(dsp, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">null_imp_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature</th>
      <th>importance_gain</th>
      <th>importance_split</th>
      <th>trn_score</th>
      <th>run</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AIRLINE</td>
      <td>26.436000</td>
      <td>8</td>
      <td>0.525050</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>FLIGHT_NUMBER</td>
      <td>142.159161</td>
      <td>35</td>
      <td>0.525050</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DESTINATION_AIRPORT</td>
      <td>231.459383</td>
      <td>20</td>
      <td>0.525050</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ORIGIN_AIRPORT</td>
      <td>319.862975</td>
      <td>26</td>
      <td>0.525050</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AIR_TIME</td>
      <td>97.764902</td>
      <td>24</td>
      <td>0.525050</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DESTINATION_AIRPORT</td>
      <td>254.016771</td>
      <td>20</td>
      <td>0.509197</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ORIGIN_AIRPORT</td>
      <td>271.220462</td>
      <td>20</td>
      <td>0.509197</td>
      <td>10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AIR_TIME</td>
      <td>82.260759</td>
      <td>17</td>
      <td>0.509197</td>
      <td>10</td>
    </tr>
    <tr>
      <th>5</th>
      <td>DEPARTURE_TIME</td>
      <td>137.511192</td>
      <td>25</td>
      <td>0.509197</td>
      <td>10</td>
    </tr>
    <tr>
      <th>6</th>
      <td>DISTANCE</td>
      <td>73.353821</td>
      <td>19</td>
      <td>0.509197</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
<p>70 rows × 5 columns</p>
</div>



<p>可视化演示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_distributions</span>(<span class="params">actual_imp_df_, null_imp_df_, feature_</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">6</span>))</span><br><span class="line">    gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 画出 Split importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_split&#x27;</span>].values, label=<span class="string">&#x27;Null importances&#x27;</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_split&#x27;</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.<span class="built_in">max</span>(a[<span class="number">0</span>]), color=<span class="string">&#x27;r&#x27;</span>,linewidth=<span class="number">10</span>, label=<span class="string">&#x27;Real Target&#x27;</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">&#x27;Split Importance of %s&#x27;</span> % feature_.upper(), fontweight=<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Null Importance (split) Distribution for %s &#x27;</span> % feature_.upper())</span><br><span class="line">    <span class="comment"># 画出 Gain importances</span></span><br><span class="line">    ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    a = ax.hist(null_imp_df_.loc[null_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_gain&#x27;</span>].values, label=<span class="string">&#x27;Null importances&#x27;</span>)</span><br><span class="line">    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_[<span class="string">&#x27;feature&#x27;</span>] == feature_, <span class="string">&#x27;importance_gain&#x27;</span>].mean(), </span><br><span class="line">               ymin=<span class="number">0</span>, ymax=np.<span class="built_in">max</span>(a[<span class="number">0</span>]), color=<span class="string">&#x27;r&#x27;</span>,linewidth=<span class="number">10</span>, label=<span class="string">&#x27;Real Target&#x27;</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_title(<span class="string">&#x27;Gain Importance of %s&#x27;</span> % feature_.upper(), fontweight=<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Null Importance (gain) Distribution for %s &#x27;</span> % feature_.upper())</span><br><span class="line"></span><br><span class="line"><span class="comment">#画出“DESTINATION_AIRPORT”的特征重要性</span></span><br><span class="line">display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=<span class="string">&#x27;DESTINATION_AIRPORT&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="lightGBM_files/lightGBM_91_0.png" alt="png"></p>
<h4 id="7-3-4计算Score"><a href="#7-3-4计算Score" class="headerlink" title="7.3.4计算Score"></a>7.3.4计算Score</h4><ol>
<li>以未进行特征shuffle的特征重要性除以shuffle之后的0.75分位数作为我们的score<br>因为’MONTH’,’DAY’,’DAY_OF_WEEK’三个特征没有什么用，画的的图结果不好看，所以把这三个去掉了。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">&#x27;feature&#x27;</span>].unique():</span><br><span class="line">    f_null_imps_gain = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    f_act_imps_gain = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].mean()</span><br><span class="line">    gain_score = np.log(<span class="number">1e-10</span> + f_act_imps_gain / (<span class="number">1</span> + np.percentile(f_null_imps_gain, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    f_null_imps_split = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    f_act_imps_split = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].mean()</span><br><span class="line">    split_score = np.log(<span class="number">1e-10</span> + f_act_imps_split / (<span class="number">1</span> + np.percentile(f_null_imps_split, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    feature_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">scores_df = pd.DataFrame(feature_scores, columns=[<span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;split_score&#x27;</span>, <span class="string">&#x27;gain_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Plot Split importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;split_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;split_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt split importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># Plot Gain importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;gain_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;gain_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt gain importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line">null_imp_df.to_csv(<span class="string">&#x27;null_importances_distribution_rf.csv&#x27;</span>)</span><br><span class="line">actual_imp_df.to_csv(<span class="string">&#x27;actual_importances_ditribution_rf.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="lightGBM_files/lightGBM_93_0.png" alt="png"></p>
<ol>
<li>shuffle target之后特征重要性低于实际target对应特征的重要性0.25分位数的次数百分比。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp_df[<span class="string">&#x27;feature&#x27;</span>].unique():</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    gain_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).<span class="built_in">sum</span>() / f_null_imps.size</span><br><span class="line">    f_null_imps = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    f_act_imps = actual_imp_df.loc[actual_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    split_score = <span class="number">100</span> * (f_null_imps &lt; np.percentile(f_act_imps, <span class="number">25</span>)).<span class="built_in">sum</span>() / f_null_imps.size</span><br><span class="line">    correlation_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">corr_scores_df = pd.DataFrame(correlation_scores, columns=[<span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;split_score&#x27;</span>, <span class="string">&#x27;gain_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Plot Split importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;split_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=corr_scores_df.sort_values(<span class="string">&#x27;split_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt split importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># Plot Gain importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;gain_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=corr_scores_df.sort_values(<span class="string">&#x27;gain_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt gain importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.suptitle(<span class="string">&quot;Features&#x27; split and gain scores&quot;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">fig.subplots_adjust(top=<span class="number">0.93</span>)</span><br></pre></td></tr></table></figure>
<p><img src="lightGBM_files/lightGBM_95_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_scores</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;AIRLINE&#39;, 100.0, 100.0),
 (&#39;FLIGHT_NUMBER&#39;, 0.0, 60.0),
 (&#39;DESTINATION_AIRPORT&#39;, 100.0, 100.0),
 (&#39;ORIGIN_AIRPORT&#39;, 50.0, 100.0),
 (&#39;AIR_TIME&#39;, 10.0, 90.0),
 (&#39;DEPARTURE_TIME&#39;, 100.0, 100.0),
 (&#39;DISTANCE&#39;, 30.0, 100.0)]
</code></pre><ol>
<li>计算特征筛选之后的最佳分数并记录相应特征<br>通过运行下面的代码，train_features选择不同的特征来拟合模型，最终Results for threshold  20/30效果最好。此时的模型特征为<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;=<span class="number">20</span>]</span><br><span class="line">split_feats</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;AIRLINE&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DESTINATION_AIRPORT&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ORIGIN_AIRPORT&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DEPARTURE_TIME&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DISTANCE&#x27;</span>]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#此时的特征为</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_feature_selection</span>(<span class="params">data,train_features=<span class="literal">None</span>, cat_feats=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># Fit LightGBM </span></span><br><span class="line">    lgb_train = lgb.Dataset(data[train_features], data[<span class="string">&quot;ARRIVAL_DELAY&quot;</span>],free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line">    <span class="string">&quot;verbosity&quot;</span>:-<span class="number">1</span>&#125;</span><br><span class="line">    <span class="comment">#&quot;force_col_wise&quot;:true&#125;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    hist = lgb.cv(params=lgb_params,train_set=lgb_train,</span><br><span class="line">    num_boost_round=<span class="number">10</span>, categorical_feature=cat_feats,</span><br><span class="line">    nfold=<span class="number">5</span>,stratified=<span class="literal">True</span>,shuffle=<span class="literal">True</span>,early_stopping_rounds=<span class="number">5</span>,seed=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the last mean / std values </span></span><br><span class="line">    <span class="keyword">return</span> hist[<span class="string">&#x27;auc-mean&#x27;</span>][-<span class="number">1</span>], hist[<span class="string">&#x27;auc-stdv&#x27;</span>][-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># features = [f for f in data.columns if f not in [&#x27;SK_ID_CURR&#x27;, &#x27;TARGET&#x27;]]</span></span><br><span class="line"><span class="comment"># score_feature_selection(df=data[features], train_features=features, target=data[&#x27;TARGET&#x27;])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span> , <span class="number">40</span>, <span class="number">50</span> ,<span class="number">60</span> , <span class="number">70</span>, <span class="number">80</span> , <span class="number">90</span>, <span class="number">99</span>]:</span><br><span class="line">    split_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    split_cat_feats = [_f <span class="keyword">for</span> _f, _score, _ <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">    gain_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> _score &gt;= threshold]</span><br><span class="line">    gain_cat_feats = [_f <span class="keyword">for</span> _f, _, _score <span class="keyword">in</span> correlation_scores <span class="keyword">if</span> (_score &gt;= threshold) &amp; (_f <span class="keyword">in</span> categorical_feats)]</span><br><span class="line">                                                                                             </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Results for threshold %3d&#x27;</span> % threshold)</span><br><span class="line">    split_results = score_feature_selection(data,train_features=split_feats, cat_feats=split_cat_feats)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\t SPLIT : %.6f +/- %.6f&#x27;</span> % (split_results[<span class="number">0</span>], split_results[<span class="number">1</span>]))</span><br><span class="line">    gain_results = score_feature_selection(data,train_features=gain_feats, cat_feats=gain_cat_feats)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\t GAIN  : %.6f +/- %.6f&#x27;</span> % (gain_results[<span class="number">0</span>], gain_results[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Results for threshold   0
     SPLIT : 0.757882 +/- 0.012114
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  10
     SPLIT : 0.756999 +/- 0.011506
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  20
     SPLIT : 0.757959 +/- 0.012558
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  30
     SPLIT : 0.757959 +/- 0.012558
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  40
     SPLIT : 0.745729 +/- 0.013217
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  50
     SPLIT : 0.745729 +/- 0.013217
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  60
     SPLIT : 0.727063 +/- 0.006758
     GAIN  : 0.757882 +/- 0.012114
Results for threshold  70
     SPLIT : 0.727063 +/- 0.006758
     GAIN  : 0.756999 +/- 0.011506
Results for threshold  80
     SPLIT : 0.727063 +/- 0.006758
     GAIN  : 0.756999 +/- 0.011506
Results for threshold  90
     SPLIT : 0.727063 +/- 0.006758
     GAIN  : 0.756999 +/- 0.011506
Results for threshold  99
     SPLIT : 0.727063 +/- 0.006758
     GAIN  : 0.757959 +/- 0.012558
</code></pre><h2 id="八、自定义损失函数和评测函数"><a href="#八、自定义损失函数和评测函数" class="headerlink" title="八、自定义损失函数和评测函数"></a>八、自定义损失函数和评测函数</h2><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/zwqjoy/article/details/121289448?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164217860316780255221706%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164217860316780255221706&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-121289448.pc_search_insert_es_download&amp;utm_term=lgb%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187">XGB/LGB—-自定义损失函数与评价函数</a><br><a target="_blank" rel="noopener" href="https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py">参考示例</a></p>
<ul>
<li>自定义损失函数，预测概率小于0.1的正样本（标签为正样本，但模型预测概率小于0.1），梯度增加一倍。</li>
<li>自定义评价函数，阈值大于0.8视为正样本（标签为正样本，但模型预测概率大于0.8）。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#正常模型效果</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征去掉&#x27;MONTH&#x27;,&#x27;DAY&#x27;,&#x27;DAY_OF_WEEK&#x27;三个没用的之后，正常模型阈值0.5时accuarcy: 83.03% auc_score: 83.67%</span></span><br><span class="line"><span class="comment">#acc阈值 0.8时accuarcy: 80.40% auc_score: 83.67%</span></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=<span class="literal">False</span>,silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 在 RF 模式下安装 LightGBM，它比 sklearn RandomForest 更快   </span></span><br><span class="line">lgb_params = &#123;</span><br><span class="line"><span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,<span class="comment">#别名binary_error</span></span><br><span class="line"><span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">16</span>,</span><br><span class="line"><span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.3</span>,</span><br><span class="line"><span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l1&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"><span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">2.9</span>,</span><br><span class="line"><span class="string">&#x27;max_depth&#x27;</span>: <span class="number">15</span>,</span><br><span class="line"><span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">12</span>,</span><br><span class="line"><span class="string">&#x27;min_gain_to_split&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"><span class="string">&#x27;min_sum_hessian_in_leaf&#x27;</span>: <span class="number">0.0038</span>,</span><br><span class="line"><span class="string">&quot;verbosity&quot;</span>:-<span class="number">5</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">clf2 = lgb.train(params=lgb_params,train_set=lgb_train,valid_sets=lgb_eval,num_boost_round=<span class="number">10</span>, </span><br><span class="line">                categorical_feature=categorical_feats)</span><br><span class="line"></span><br><span class="line">y_pred = clf2.predict(X_test,num_iteration=clf2.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy = accuracy_score(y_test,pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[1]    valid_0&#39;s binary_logloss: 0.479157
[2]    valid_0&#39;s binary_logloss: 0.46882
[3]    valid_0&#39;s binary_logloss: 0.454724
[4]    valid_0&#39;s binary_logloss: 0.445913
[5]    valid_0&#39;s binary_logloss: 0.440924
[6]    valid_0&#39;s binary_logloss: 0.438309
[7]    valid_0&#39;s binary_logloss: 0.433886
[8]    valid_0&#39;s binary_logloss: 0.432747
[9]    valid_0&#39;s binary_logloss: 0.431001
[10]    valid_0&#39;s binary_logloss: 0.429621
accuarcy: 81.69% auc_score: 76.48%
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义目标函数，预测概率小于0.1的正样本（标签为正样本，但模型预测概率小于0.1），梯度增加一倍。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span>(<span class="params">preds, train_data</span>):</span></span><br><span class="line">    labels=train_data.get_label()</span><br><span class="line">    preds=<span class="number">1.</span>/(<span class="number">1.</span>+np.exp(-preds))</span><br><span class="line">    </span><br><span class="line">    grad=[(p-l) <span class="keyword">if</span> p&gt;=<span class="number">0.1</span> <span class="keyword">else</span> <span class="number">2</span>*(p-l) <span class="keyword">for</span> (p,l) <span class="keyword">in</span> <span class="built_in">zip</span>(preds,labels) ]</span><br><span class="line">    hess=[p*(<span class="number">1.</span>-p) <span class="keyword">if</span> p&gt;=<span class="number">0.1</span> <span class="keyword">else</span> <span class="number">2</span>*p*(<span class="number">1.</span>-p) <span class="keyword">for</span> p <span class="keyword">in</span> preds ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad, hess</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义评价指标binary_error，阈值大于0.8视为正样本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_error</span>(<span class="params">preds, train_data</span>):</span></span><br><span class="line">    labels = train_data.get_label()</span><br><span class="line">    preds = <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-preds))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;error&#x27;</span>, np.mean(labels != (preds &gt; <span class="number">0.8</span>)), <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">clf3 = lgb.train(lgb_params,</span><br><span class="line">                lgb_train,</span><br><span class="line">                num_boost_round=<span class="number">10</span>,</span><br><span class="line">                init_model=clf2,</span><br><span class="line">                fobj=loglikelihood, <span class="comment"># 目标函数</span></span><br><span class="line">                feval=binary_error, <span class="comment"># 评价指标</span></span><br><span class="line">                valid_sets=lgb_eval)</span><br><span class="line"></span><br><span class="line">y_pred = clf3.predict(X_test,num_iteration=clf3.best_iteration)<span class="comment">#结果是0-1之间的概率值，是一维数组</span></span><br><span class="line">pred =[<span class="number">1</span> <span class="keyword">if</span> x &gt;<span class="number">0.8</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> y_pred]</span><br><span class="line">accuracy2 = accuracy_score(y_test,pred)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[11]    valid_0&#39;s binary_logloss: 4.68218    valid_0&#39;s error: 0.196414
[12]    valid_0&#39;s binary_logloss: 4.51541    valid_0&#39;s error: 0.196414
[13]    valid_0&#39;s binary_logloss: 4.4647    valid_0&#39;s error: 0.19558
[14]    valid_0&#39;s binary_logloss: 4.5248    valid_0&#39;s error: 0.196414
[15]    valid_0&#39;s binary_logloss: 4.51904    valid_0&#39;s error: 0.196414
[16]    valid_0&#39;s binary_logloss: 4.52481    valid_0&#39;s error: 0.196414
[17]    valid_0&#39;s binary_logloss: 4.4928    valid_0&#39;s error: 0.196414
[18]    valid_0&#39;s binary_logloss: 4.43027    valid_0&#39;s error: 0.196414
[19]    valid_0&#39;s binary_logloss: 4.4285    valid_0&#39;s error: 0.196414
[20]    valid_0&#39;s binary_logloss: 4.42314    valid_0&#39;s error: 0.196831
accuarcy: 81.19% auc_score: 76.47%
</code></pre><h2 id="九-模型部署与加速"><a href="#九-模型部署与加速" class="headerlink" title="九 模型部署与加速"></a>九 模型部署与加速</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://treelite.readthedocs.io/en/latest/tutorials/import.html">参考文档</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_26917383/article/details/113287642?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164218367916780271548371%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164218367916780271548371&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-113287642.pc_search_insert_es_download&amp;utm_term=import+treelite.sklearn%E5%A4%B1%E8%B4%A5&amp;spm=1018.2226.3001.4187">python+Treelite：Sklearn树模型训练迁移到c、java部署</a></p>
</blockquote>
<p>由于 Treelite 的范围仅限于预测，因此必须使用其他机器学习包来训练决策树集成模型。在本文档中，我们将展示如何导入已在其他地方训练过的集成模型。</p>
<p>import treelite失败，无法导入，不知道为什么</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm9 = lgb.LGBMClassifier()</span><br><span class="line">gbm9.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = gbm9.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score=metrics.roc_auc_score(y_test,gbm9.predict_proba(X_test)[:,<span class="number">1</span>])<span class="comment">#predict_proba输出正负样本概率值，取第二列为正样本概率值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line">gbm9.booster_.save_model(<span class="string">&quot;model9.txt&quot;</span>)<span class="comment">#保存模型为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> treelite</span><br><span class="line"><span class="keyword">import</span> treelite.sklearn</span><br><span class="line">model = treelite.sklearn.import_model(gbm9)<span class="comment">#导入 scikit-learn 模型</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy2 = accuracy_score(y_test,y_pred)</span><br><span class="line">auc_score2=metrics.roc_auc_score(y_test,model.predict_proba(X_test)[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuarcy: %.2f%%&quot;</span> % (accuracy2*<span class="number">100.0</span>),<span class="string">&quot;auc_score: %.2f%%&quot;</span> % (auc_score2*<span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/08/21/软件应用/lightGBM/">https://zhxnlp.github.io/2021/08/21/软件应用/lightGBM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/github/">github</a><a class="post-meta__tags" href="/tags/%E8%BD%AF%E4%BB%B6/">软件</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/08/21/%E8%BD%AF%E4%BB%B6%E5%BA%94%E7%94%A8/GitHub%20%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/"><i class="fa fa-chevron-left">  </i><span>GitHub 安装使用详细教程</span></a></div><div class="next-post pull-right"><a href="/2021/08/21/8%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9Anlp%E4%B9%8Btransformers%E5%85%A5%E9%97%A8/task3%EF%BC%9A%E5%9B%BE%E8%A7%A3GPT-2/"><span>task3：图解GPT-2</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>