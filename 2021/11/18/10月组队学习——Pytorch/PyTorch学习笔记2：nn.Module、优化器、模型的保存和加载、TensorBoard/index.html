<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard"><meta name="keywords" content="Pytorch"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81nn-Module"><span class="toc-text">一、nn.Module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-nn-Module%E7%9A%84%E8%B0%83%E7%94%A8"><span class="toc-text">1.1 nn.Module的调用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">1.2 线性回归的实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">二、损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">三、优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-SGD%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">3.1.  SGD优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Adagrad%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">3.2 Adagrad优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%88%86%E5%B1%82%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">3.3 分层学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E5%99%A8torch-optim-lr-scheduler"><span class="toc-text">3.4 学习率调度器torch.optim.lr_scheduler</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-%E3%80%81%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BDtorch-utils-data"><span class="toc-text">四 、数据加载torch.utils.data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-DataLoader%E5%8F%82%E6%95%B0"><span class="toc-text">4.1 DataLoader参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%A4%E7%A7%8D%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E5%9E%8B"><span class="toc-text">4.2 两种数据集类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%E5%92%8C-Sampler"><span class="toc-text">4.3 数据加载顺序和 Sampler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%89%B9%E5%A4%84%E7%90%86%E5%92%8Ccollate-fn"><span class="toc-text">4.4 批处理和collate_fn</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-text">五、模型的保存和加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%A8%A1%E5%9D%97%E3%80%81%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">5.1 模块、张量的序列化和反序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-state-dict%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">5.2 state_dict保存模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BF%9D%E5%AD%98-%E5%8A%A0%E8%BD%BD%E5%AE%8C%E6%95%B4%E6%A8%A1%E5%9E%8B"><span class="toc-text">5.2 保存&#x2F;加载完整模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Checkpoint-%E7%94%A8%E4%BA%8E%E6%8E%A8%E7%90%86-%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83"><span class="toc-text">5.3  Checkpoint 用于推理&#x2F;继续训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E5%9C%A8%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E4%B8%AD%E4%BF%9D%E5%AD%98%E5%A4%9A%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-text">5.4 在一个文件中保存多个模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81TensorBoard%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="toc-text">六、TensorBoard的安装和使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-TensorBoard%E7%94%A8%E6%B3%95%E7%A4%BA%E4%BE%8B%EF%BC%9A"><span class="toc-text">6.1 TensorBoard用法示例：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%85%B7%E4%BD%93%E5%87%BD%E6%95%B0"><span class="toc-text">6.2 具体函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-1-SummaryWriter"><span class="toc-text">6.2.1 SummaryWriter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-2-add-scalar-%E5%92%8Cadd-scalars"><span class="toc-text">6.2.2  add_scalar()和add_scalars()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-3-add-histogram"><span class="toc-text">6.2.3 add_histogram()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-4-add-graph"><span class="toc-text">6.2.4 add_graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-5-add-pr-curve"><span class="toc-text">6.2.5 add_pr_curve</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-6-tensorboard-%E2%80%94logdir"><span class="toc-text">6.2.6 tensorboard —logdir&#x3D;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-7-add-image%E3%80%81add-vide%E3%80%81add-audio%E3%80%81add-text"><span class="toc-text">6.2.7  add_image、add_vide、add_audio、add_text</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-tensorboard%E7%95%8C%E9%9D%A2%E7%AE%80%E4%BB%8B"><span class="toc-text">6.3 tensorboard界面简介</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-18</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%EF%BC%9APytorch/">10月组队学习：Pytorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">6.5k</span><span class="post-meta__separator">|</span><span>阅读时长: 24 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>@[toc]<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/265394674">《PyTorch 学习笔记汇总（完结撒花）》</a></p>
<h2 id="一、nn-Module"><a href="#一、nn-Module" class="headerlink" title="一、nn.Module"></a>一、nn.Module</h2><h3 id="1-1-nn-Module的调用"><a href="#1-1-nn-Module的调用" class="headerlink" title="1.1 nn.Module的调用"></a>1.1 nn.Module的调用</h3><p>pytorch通过继承nn.Module类，定义子模块的实例化和前向传播，实现深度学习模型的搭建。其构建代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *kargs</span>):</span> <span class="comment"># 定义类的初始化函数，...是用户的传入参数</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()<span class="comment">#调用父类nn.Module的初始化方法</span></span><br><span class="line">        ... <span class="comment"># 根据传入的参数来定义子模块</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *kargs</span>):</span> <span class="comment"># 定义前向计算的输入参数，...一般是张量或者其他的参数</span></span><br><span class="line">        ret = ... <span class="comment"># 根据传入的张量和子模块计算返回张量</span></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>init\</strong>方法初始化整个模型</li>
<li>super(Model, self).<strong>init\</strong>():调用父类nn.Module的初始化方法，初始化必要的变量和参数</li>
<li>定义前向传播模块</li>
</ul>
<span id="more"></span>
<h3 id="1-2-线性回归的实现"><a href="#1-2-线性回归的实现" class="headerlink" title="1.2 线性回归的实现"></a>1.2 线性回归的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ndim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.ndim = ndim<span class="comment">#输入的特征数</span></span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.randn(ndim, <span class="number">1</span>)) <span class="comment"># 定义权重</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>)) <span class="comment"># 定义偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 定义线性模型 y = Wx + b</span></span><br><span class="line">        <span class="keyword">return</span> x.mm(self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<ul>
<li>定义权重和偏置self.weight和self.bias。采用标准正态分布torch.randn进行初始化。</li>
<li>self.weight和self.bias是模型的参数，使用nn.Parameter包装，表示将这些初始化的张量转换为模型的参数。只有参数才可以进行优化（被优化器访问到）</li>
</ul>
<p>实例化方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm = LinearModel(<span class="number">5</span>) <span class="comment"># 定义线性回归模型，特征数为5</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">5</span>) <span class="comment"># 定义随机输入，迷你批次大小为4</span></span><br><span class="line">lm(x) <span class="comment"># 得到每个迷你批次的输出</span></span><br></pre></td></tr></table></figure>
<ol>
<li>使用model.named_parameters()或者model.parameters()获取模型参数的生成器。区别是前者包含参数名和对应的张量值，后者只含有张量值。</li>
<li>优化器optimzer直接接受参数生成器作为参数，反向传播时根据梯度来优化生成器里的所有张量。</li>
<li>model.train()的作用是启用 Batch Normalization 和 Dropout。model.eval()的作用是不启用 Batch Normalization 和 Dropout。</li>
<li>named_buffers和buffers获取张量的缓存（不参与梯度传播但是会被更新的参数，例如BN的均值和方差）register_buffers可以加入这种张量</li>
<li>使用apply递归地对子模块进行函数应用（可以是匿名函数lambda）</li>
</ol>
<p>对于model.train()和model.eval()用法和区别进一步可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357075502?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《Pytorch：model.train()和model.eval()用法和区别》</a></p>
<p>对于上面定义的线性模型来举例:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm.named_parameters() <span class="comment"># 获取模型参数（带名字）的生成器</span></span><br><span class="line"><span class="comment">#&lt;generator object Module.named_parameters at 0x00000279A1809510&gt;</span></span><br><span class="line"><span class="built_in">list</span>(lm.named_parameters()) <span class="comment"># 转换生成器为列表</span></span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;weight&#x27;</span>,</span><br><span class="line">  Parameter containing:</span><br><span class="line">  tensor([[-<span class="number">1.0407</span>],</span><br><span class="line">          [ <span class="number">0.0427</span>],</span><br><span class="line">          [ <span class="number">0.4069</span>],</span><br><span class="line">          [-<span class="number">0.7064</span>],</span><br><span class="line">          [-<span class="number">1.1938</span>]], requires_grad=<span class="literal">True</span>)),</span><br><span class="line"> (<span class="string">&#x27;bias&#x27;</span>,</span><br><span class="line">  Parameter containing:</span><br><span class="line">  tensor([-<span class="number">0.7493</span>], requires_grad=<span class="literal">True</span>))]</span><br><span class="line">  </span><br><span class="line">lm.parameters() <span class="comment"># 获取模型参数（不带名字）的生成器</span></span><br><span class="line"><span class="built_in">list</span>(lm.parameters()) <span class="comment"># 转换生成器为列表</span></span><br><span class="line"></span><br><span class="line">[Parameter containing:</span><br><span class="line"> tensor([[-<span class="number">1.0407</span>],</span><br><span class="line">         [ <span class="number">0.0427</span>],</span><br><span class="line">         [ <span class="number">0.4069</span>],</span><br><span class="line">         [-<span class="number">0.7064</span>],</span><br><span class="line">         [-<span class="number">1.1938</span>]], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> Parameter containing:</span><br><span class="line"> tensor([-<span class="number">0.7493</span>], requires_grad=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line">lm.cuda()<span class="comment">#模型参数转到GPU上</span></span><br><span class="line"><span class="built_in">list</span>(lm.parameters()) <span class="comment"># 转换生成器为列表</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是在训练中随机去除神经元，用一部分网络连接来训练更新参数。如果被删除的神经元（叉号）是唯一促成正确结果的神经元。一旦我们移除了被删除的神经元，它就迫使其他神经元训练和学习如何在没有被删除神经元的情况下保持准确。这种dropout提高了最终测试的性能，但它对训练期间的性能产生了负面影响，因为网络是不全的</p>
</li>
<li><p>在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变（model.eval()时，框架会自动把BN和Dropout固定住，不会取平均，直接使用在训练阶段已经学出的mean和var值）</p>
</li>
</ul>
<h2 id="二、损失函数"><a href="#二、损失函数" class="headerlink" title="二、损失函数"></a>二、损失函数</h2><p>pytorch损失函数有两种形式：</p>
<ul>
<li>torch.nn.functional调用的函数形式&lt;/font&gt;.传入神经网络预测值和目标值来计算损失函数</li>
<li>torch.nn库里面的模块形式&lt;/font&gt;。新建模块的实例，调用模块化方法计算<br>最后输出的是标量，对一个批次的损失函数的值有两种归约方式：求和和求均值。</li>
</ul>
<ol>
<li>回归问题一般调用torch.nn.MSEloss模块。使用默认参数创建实例，输出的是损失函数对一个batch的均值。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">mse = nn.MSELoss() <span class="comment"># 初始化平方损失函数模块</span></span><br><span class="line"><span class="comment">#class torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#x27;mean&#x27;)</span></span><br><span class="line">t1 = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机生成张量t1</span></span><br><span class="line">tensor([ <span class="number">0.6582</span>,  <span class="number">0.0529</span>, -<span class="number">0.9693</span>, -<span class="number">0.9313</span>, -<span class="number">0.7288</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">t2 = torch.randn(<span class="number">5</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机生成张量t2</span></span><br><span class="line">tensor([ <span class="number">0.8095</span>, -<span class="number">0.3384</span>, -<span class="number">0.9510</span>,  <span class="number">0.1581</span>, -<span class="number">0.1863</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mse(t1, t2) <span class="comment"># 计算张量t1和t2之间的平方损失函数</span></span><br><span class="line">tensor(<span class="number">0.3315</span>, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure>
<ol>
<li>二分类问题:<ul>
<li>使用  torch.nn.BCELoss&lt;/font&gt;二分类交叉熵损失函数。输出的是损失函数的均值。接受两个张量。前一个是正分类标签的概率值（预测值必须经过 <font color='deeppink'> nn.Sigmoid()</font>输出概率），后者是二分类标签的目标数据值（1是正分类）。两个都必须是浮点类型。</li>
<li>torch.nn.BCEWithLogitsLoss&lt;/font&gt;：自动在损失函数内部实现sigmoid函数的功能，可以增加计算的稳定性。因为概率接近0或1的时候，二分类交叉熵损失函数接受的对数部分容易接近无穷大，造成数值不稳定。使用torch.nn.BCEWithLogitsLoss可以避免此种情况</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1s = torch.sigmoid(t1)</span><br><span class="line">t2 = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">5</span>, )).<span class="built_in">float</span>() <span class="comment"># 随机生成0，1的整数序列，并转换为浮点数</span></span><br><span class="line">bce=torch.nn.BCELoss()</span><br><span class="line">(t1s, t2) <span class="comment"># 计算二分类的交叉熵</span></span><br><span class="line">bce_logits = nn.BCEWithLogitsLoss() <span class="comment"># 使用交叉熵对数损失函数</span></span><br><span class="line">bce_logits(t1, t2) <span class="comment"># 计算二分类的交叉熵，可以发现和前面的结果一致</span></span><br></pre></td></tr></table></figure>
<ol>
<li>多分类问题<ul>
<li>torch.nn.NLLLoss&lt;/font&gt;:负对数损失函数，计算之前预测值必须经过softmax函数输出概率值（<font color='deeppink'> torch.nn.functional.log_softmax或torch.nn.LogSoftmax(dim=dim)函数</font>）</li>
<li>torch.nn.CrossEntropyLoss&lt;/font&gt;:交叉熵损失函数，内部已经整合softmax输出概率，不需要再另外对预测值进行softmax计算。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">N=<span class="number">10</span> <span class="comment"># 定义分类数目</span></span><br><span class="line">t1 = torch.randn(<span class="number">5</span>, N, requires_grad=<span class="literal">True</span>) <span class="comment"># 随机产生预测张量</span></span><br><span class="line">t2 = torch.randint(<span class="number">0</span>, N, (<span class="number">5</span>, )) <span class="comment"># 随机产生目标张量</span></span><br><span class="line">t1s = torch.nn.functional.log_softmax(t1, -<span class="number">1</span>) <span class="comment"># 计算预测张量的LogSoftmax</span></span><br><span class="line">nll = nn.NLLLoss() <span class="comment"># 定义NLL损失函数</span></span><br><span class="line">nll(t1s, t2) <span class="comment"># 计算损失函数</span></span><br><span class="line">ce = nn.CrossEntropyLoss() <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">ce(t1, t2) <span class="comment"># 计算损失函数，可以发现和NLL损失函数的结果一致</span></span><br></pre></td></tr></table></figure>
<h2 id="三、优化器"><a href="#三、优化器" class="headerlink" title="三、优化器"></a>三、优化器</h2><blockquote>
<p>完整文档参考：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">《torch.optim 》</a></p>
<h3 id="3-1-SGD优化器"><a href="#3-1-SGD优化器" class="headerlink" title="3.1.  SGD优化器"></a>3.1.  SGD优化器</h3><p>以波士顿房价问题举例，构建SGD优化器。第一个参数是模型的参数生成器（lm.parameters()调用），第二个参数是学习率。训练时通过 optim.step()进行优化计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">lm = LinearModel(<span class="number">13</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(lm.parameters(), lr=<span class="number">1e-6</span>) <span class="comment"># 定义优化器</span></span><br><span class="line">data = torch.tensor(boston[<span class="string">&quot;data&quot;</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">target = torch.tensor(boston[<span class="string">&quot;target&quot;</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    predict = lm(data) <span class="comment"># 输出模型预测结果</span></span><br><span class="line">    loss = criterion(predict, target) <span class="comment"># 输出损失函数</span></span><br><span class="line">    <span class="keyword">if</span> step <span class="keyword">and</span> step % <span class="number">1000</span> == <span class="number">0</span> :</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loss: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(loss.item()))</span><br><span class="line">    optim.zero_grad() <span class="comment"># 清零梯度</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optim.step()</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.SGD(params,lr=&lt;required parameter&gt;,momentum=<span class="number">0</span>,</span><br><span class="line">    dampening=<span class="number">0</span>,weight_decay=<span class="number">0</span>,nesterov=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#momentum：动量因子</span></span><br><span class="line"><span class="comment">#dampening：动量抑制因子</span></span><br><span class="line"><span class="comment">#nesterov：设为True时使用nesterov动量</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-Adagrad优化器"><a href="#3-2-Adagrad优化器" class="headerlink" title="3.2 Adagrad优化器"></a>3.2 Adagrad优化器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.Adagrad(</span><br><span class="line">    params,lr=<span class="number">0.01</span>,lr_decay=<span class="number">0</span>,weight_decay=<span class="number">0</span>,</span><br><span class="line">    initial_accumulator_value=<span class="number">0</span>,eps=<span class="number">1e-10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#lr_decay：学习率衰减速率</span></span><br><span class="line"><span class="comment">#weight_decay：权重衰减</span></span><br><span class="line"><span class="comment">#initial_accumulator_value：梯度初始累加值</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-分层学习率"><a href="#3-3-分层学习率" class="headerlink" title="3.3 分层学习率"></a>3.3 分层学习率</h3><p>对不同参数指定不同的学习率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: model.base.parameters()&#125;,</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>这意味着model.base的参数将使用 的默认学习率1e-2， model.classifier的参数将使用 的学习率1e-3，0.9所有参数将使用动量 。</p>
<h3 id="3-4-学习率调度器torch-optim-lr-scheduler"><a href="#3-4-学习率调度器torch-optim-lr-scheduler" class="headerlink" title="3.4 学习率调度器torch.optim.lr_scheduler"></a>3.4 学习率调度器torch.optim.lr_scheduler</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment">#没经过30的个迭代周期，学习率降为原来的0.1倍。每个epoch之后学习率都会衰减。</span></span><br><span class="line"><span class="keyword">or</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>大多数学习率调度器都可以称为背靠背（也称为链式调度器）。结果是每个调度器都被一个接一个地应用于前一个调度器获得的学习率。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = [Parameter(torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>))]</span><br><span class="line">optimizer = SGD(model, <span class="number">0.1</span>)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler2 = MultiStepLR(optimizer, milestones=[<span class="number">30</span>,<span class="number">80</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    scheduler1.step()</span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure>
<h2 id="四-、数据加载torch-utils-data"><a href="#四-、数据加载torch-utils-data" class="headerlink" title="四 、数据加载torch.utils.data"></a>四 、数据加载torch.utils.data</h2><blockquote>
<p>本节也可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/120467968">《编写transformers的自定义pytorch训练循环（Dataset和DataLoader解析和实例代码）》</a></p>
</blockquote>
<h3 id="4-1-DataLoader参数"><a href="#4-1-DataLoader参数" class="headerlink" title="4.1 DataLoader参数"></a>4.1 DataLoader参数</h3><p>PyTorch 数据加载实用程序的核心是torch.utils.data.DataLoader 类。它代表一个 Python 可迭代的数据集，支持：</p>
<ul>
<li>map类型和可迭代类型数据集</li>
<li>自定义数据加载顺序</li>
<li>自动batching</li>
<li>单进程和多进程数据加载</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span> ，num_workers=<span class="number">4</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">6</span>, shuffle=<span class="literal">False</span>，num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>下面看看dataloader代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 worker_init_fn=<span class="literal">None</span>,*, prefetch_factor=<span class="number">2</span>,persistent_workers=<span class="literal">False</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">dataset</span> = <span class="title">dataset</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">batch_size</span> = <span class="title">batch_size</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">num_workers</span> = <span class="title">num_workers</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">collate_fn</span> = <span class="title">collate_fn</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">pin_memory</span> = <span class="title">pin_memory</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">drop_last</span> = <span class="title">drop_last</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">timeout</span> = <span class="title">timeout</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">worker_init_fn</span> = <span class="title">worker_init_fn</span></span></span><br><span class="line"><span class="function"></span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>dataset:Dataset类，PyTorch已有的数据读取接口，决定数据从哪里读取及如何读取；</li>
<li>batch_size：批大小；默认1</li>
<li>num_works:是否多进程读取数据；默认0使用主进程来导入数据。大于0则多进程导入数据，加快数据导入速度</li>
<li>shuffle：每个epoch是否乱序；默认False。输入数据的顺序打乱，是为了使数据更有独立性，但如果数据是有序列特征的，就不要设置成True了。一般shuffle训练集即可。</li>
<li>drop_last:当样本数不能被batchsize整除时，是否舍弃最后一批数据；</li>
<li>collate_fn:将得到的数据整理成一个batch。默认设置是False。如果设置成True，系统会在返回前会将张量数据（Tensors）复制到CUDA内存中。</li>
<li>batch_sampler，批量采样，和batch_size、shuffle等参数是互斥的，一般采用默认None。batch_sampler，但每次返回的是一批数据的索引（注意：不是数据），应该是每次输入网络的数据是随机采样模式，这样能使数据更具有独立性质。所以，它和一捆一捆按顺序输入，数据洗牌，数据采样，等模式是不兼容的。</li>
<li>sampler，默认False。根据定义的策略从数据集中采样输入。如果定义采样规则，则洗牌（shuffle）设置必须为False。</li>
<li>pin_memory，内存寄存，默认为False。在数据返回前，是否将数据复制到CUDA内存中。</li>
<li>timeout，是用来设置数据读取的超时时间的，但超过这个时间还没读取到数据的话就会报错。</li>
<li>worker_init_fn（数据类型 callable），子进程导入模式，默认为Noun。在数据导入前和步长结束后，根据工作子进程的ID逐个按顺序导入数据。</li>
</ul>
<p>想用随机抽取的模式加载输入，可以设置 sampler 或 batch_sampler。如何定义抽样规则，可以看sampler.py脚本，或者这篇帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/aiwanghuan5017/article/details/102147809">《一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系》</a></p>
<h3 id="4-2-两种数据集类型"><a href="#4-2-两种数据集类型" class="headerlink" title="4.2 两种数据集类型"></a>4.2 两种数据集类型</h3><p>DataLoader 构造函数最重要的参数是dataset，它表示要从中加载数据的数据集对象。PyTorch 支持两种不同类型的数据集：</p>
<ul>
<li>map-style datasets：映射类型数据集。每个数据有一个对应的索引，通过输入具体的索引，就可以得到对应的数据</li>
</ul>
<p>其构造方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># index: 数据缩索引（整数，范围为0到数据数目-1）</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># 返回数据张量</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 返回数据的数目</span></span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>主要重写两个方法：</p>
<ul>
<li><strong>getitem\</strong>:python内置的操作符方法，对应索引操作符[]。通过输入整数索引，返回具体某一条数据。具体的内部逻辑根据数据集类型决定</li>
<li><strong>len\</strong>：返回数据总数</li>
</ul>
<p>更具体的可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">《torch.utils.data.Dataset》</a></p>
<ul>
<li>iterable-style datasets：可迭代数据集：实现<strong>iter</strong>()协议的子类的实例。不需要<strong>getitem\</strong>和<strong>len\</strong>方法，其实类似python的迭代器</li>
<li>不同于映射，索引之间相互独立。多线程载入时，多线程独立分配索引。迭代中索引右前后关系，需要考虑如何分割数据。</li>
<li>这种类型的数据集特别适用于随机读取代价高昂甚至不可能的情况，以及批量大小取决于获取的数据的情况。</li>
<li>在调用iter(dataset)时可以返回从数据库、远程服务器甚至实时生成的日志中读取的数据流</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, start, end</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyIterableDataset).__init__()</span><br><span class="line">            <span class="keyword">assert</span> end &gt; start, \</span><br><span class="line"><span class="string">&quot;this example code only works with end &gt;= start&quot;</span></span><br><span class="line">            self.start = start</span><br><span class="line">            self.end = end</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">        <span class="keyword">if</span> worker_info <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 单进程数据载入</span></span><br><span class="line">            iter_start = self.start</span><br><span class="line">            iter_end = self.end</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 多进程，分割数据</span></span><br><span class="line">        	   <span class="comment">#根据不同工作进程序号worker_id，设置不同进程数据迭代器取值范围。保证不同进程获取不同的迭代器。</span></span><br><span class="line">            per_worker = <span class="built_in">int</span>(math.ceil((self.end - self.start) \</span><br><span class="line">                            / <span class="built_in">float</span>(worker_info.num_workers)))</span><br><span class="line">            worker_id = worker_info.<span class="built_in">id</span></span><br><span class="line">            iter_start = self.start + worker_id * per_worker</span><br><span class="line">            iter_end = <span class="built_in">min</span>(iter_start + per_worker, self.end)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(iter_start, iter_end))</span><br></pre></td></tr></table></figure>
<p>更多详细信息，请参阅<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset">IterableDataset</a></p>
<h3 id="4-3-数据加载顺序和-Sampler"><a href="#4-3-数据加载顺序和-Sampler" class="headerlink" title="4.3 数据加载顺序和 Sampler"></a>4.3 数据加载顺序和 Sampler</h3><ul>
<li>对于iterable-style datasets，数据加载顺序完全由用户定义的 iterable 控制。这允许更容易地实现块读取和动态批量大小（例如，通过每次产生批量样本）。</li>
<li>map 类型数据，torch.utils.data.Sampler 类用于指定数据加载中使用的索引/键的序列。它们表示数据集索引上的可迭代对象。例如，在随机梯度下降 (SGD) 的常见情况下，Sampler可以随机排列索引列表并一次产生一个，或者为小批量 SGD 产生少量索引。</li>
</ul>
<p>将根据shufflea的参数自动构建顺序或混洗采样器DataLoader。或者，用户可以使用该sampler参数来指定一个自定义Sampler对象，该对象每次都会生成下一个要获取的索引/键。</p>
<p>一次Sampler生成批量索引列表的自定义可以作为batch_sampler参数传递。也可以通过batch_size和 drop_last参数启用自动批处理。</p>
<h3 id="4-4-批处理和collate-fn"><a href="#4-4-批处理和collate-fn" class="headerlink" title="4.4 批处理和collate_fn"></a>4.4 批处理和collate_fn</h3><p>经由参数 batch_size，drop_last和batch_sampler，DataLoader支持批处理数据<br>当启用自动批处理时，每次都会使用数据样本列表调用 collat​​e_fn。预计将输入样本整理成一个批次，以便从数据加载器迭代器中产生。</p>
<p>例如，如果每个数据样本由一个 3 通道图像和一个完整的类标签组成，即数据集的每个元素返回一个元组 (image, class_index)，则默认 collat​​e_fn 将此类元组的列表整理成单个元组一个批处理图像张量和一个批处理类标签张量。特别是，默认 collat​​e_fn 具有以下属性：</p>
<ul>
<li><p>它总是预先添加一个新维度作为批次维度。</p>
</li>
<li><p>它会自动将 NumPy 数组和 Python 数值转换为 PyTorch 张量。</p>
</li>
<li><p>它保留了数据结构，例如，如果每个样本是一个字典，它输出一个具有相同键集但批量张量作为值的字典（如果值不能转换为张量，则为列表）。列表 s、元组 s、namedtuple s 等也是如此。</p>
</li>
</ul>
<p>用户可以使用自定义 collat​​e_fn 来实现自定义批处理，例如，沿着除第一个维度之外的维度进行整理，填充各种长度的序列，或添加对自定义数据类型的支持。</p>
<h2 id="五、模型的保存和加载"><a href="#五、模型的保存和加载" class="headerlink" title="五、模型的保存和加载"></a>五、模型的保存和加载</h2><h3 id="5-1-模块、张量的序列化和反序列化"><a href="#5-1-模块、张量的序列化和反序列化" class="headerlink" title="5.1 模块、张量的序列化和反序列化"></a>5.1 模块、张量的序列化和反序列化</h3><ul>
<li>PyTorch模块和张量本质是torch.nn.Module和torch.tensor类的实例。PyTorch自带了一系列方法， 可以将这些类的实例转化成字成串&lt;/font &gt;。所以这些实例可以通过Python序列化方法进行序列化和反序列化。</li>
<li>张量的序列化： 本质上是把张量的信息，包括数据类型和存储位置、以及携带的数据，转换为字符串，然后使用Python自带的文件IO函数进行存储&lt;/font &gt;。当然也是这个过程是可逆的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(obj, f, pickle_module=pickle, pickle_protocol=<span class="number">2</span>)</span><br><span class="line">torch.load(f, map_location=<span class="literal">None</span>, pickle_module=pickle, **pickle_load_args)</span><br></pre></td></tr></table></figure>
<ul>
<li>torch.save参数<ol>
<li>pytorch中可以被序列化的对象，包括模型和张量</li>
<li>存储文件路径</li>
<li>序列化的库，默认pickle</li>
<li>pickle协议，版本0-4</li>
</ol>
</li>
<li><p>torch.load函数</p>
<ol>
<li>文件路径</li>
<li>张量存储位置的映射（默认CPU，也可以是GPU）</li>
<li><p>pickle参数，和save时一样。</p>
<p>如果模型保存在GPU中，而加载的当前计算机没有GPU，或者GPU设备号不对，可以使用map_location=’cpu’。</p>
</li>
</ol>
</li>
</ul>
<p>PyTorch默认有两种模型保存方式：</p>
<ul>
<li>保存模型的实例</li>
<li>保存模型的状态字典state_dict：state_dict包含模型所有参数名和对应的张量，通过调用load_state_dict可以获取当前模型的状态字典,载入模型参数。<h3 id="5-2-state-dict保存模型参数"><a href="#5-2-state-dict保存模型参数" class="headerlink" title="5.2 state_dict保存模型参数"></a>5.2 state_dict保存模型参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure></li>
<li>保存模型状态字典state_dict ：只保存模型学习到的参数，与模块关联较小，即不依赖版本。</li>
<li>PyTorch 中最常见的模型保存使‘.pt’或者是‘.pth’作为模型文件扩展名</li>
<li>在运行推理之前，务必调用 model.eval() 去设置 dropout 和 batch normalization 层为评<br>估模式。如果不这么做，可能导致 模型推断结果不一致</li>
</ul>
<h3 id="5-2-保存-加载完整模型"><a href="#5-2-保存-加载完整模型" class="headerlink" title="5.2 保存/加载完整模型"></a>5.2 保存/加载完整模型</h3><p>以 Python `pickle 模块的方式来保存模型。这种方法的缺点是：</p>
<ul>
<li>序列化数据受 限于某种特殊的类而且需要确切的字典结构。当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。</li>
<li>PyTorch模块的实现依赖于具体的版本。所依一个版本保存的模块序列化文件，在另一个版本可能无法载入。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br><span class="line"><span class="comment"># 模型类必须在此之前被定义</span></span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h3 id="5-3-Checkpoint-用于推理-继续训练"><a href="#5-3-Checkpoint-用于推理-继续训练" class="headerlink" title="5.3  Checkpoint 用于推理/继续训练"></a>5.3  Checkpoint 用于推理/继续训练</h3><ul>
<li>在训练时，不仅要保存模型相关的信息，还要保存优化器相关的信息。因为可能要从检查点出发，继续训练。所以可以保存优化器本身的状态字典，存储包括当前学习率、调度器等信息。</li>
<li>最新记录的训练损失，外部的 torch.nn.Embedding 层等等都可以保存。</li>
<li>PyTorch 中常见的保存checkpoint 是使用 .tar 文件扩展名。</li>
<li>要加载项目，首先需要初始化模型和优化器，然后使用 torch.load() 来加载本地字典</li>
</ul>
<p>一个模型的检查点代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line"><span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line"><span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line"><span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">...</span><br><span class="line">&#125;, PATH)</span><br></pre></td></tr></table></figure><br>加载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">model.<span class="built_in">eval</span>()<span class="comment">#或model.train()</span></span><br></pre></td></tr></table></figure>
<p>或者是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">save_info = &#123; <span class="comment"># 保存的信息</span></span><br><span class="line">    <span class="string">&quot;iter_num&quot;</span>: iter_num,  <span class="comment"># 迭代步数 </span></span><br><span class="line">    <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict(), <span class="comment"># 优化器的状态字典</span></span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model.state_dict(), <span class="comment"># 模型的状态字典</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 保存信息</span></span><br><span class="line">torch.save(save_info, save_path)</span><br><span class="line"><span class="comment"># 载入信息</span></span><br><span class="line">save_info = torch.load(save_path)</span><br><span class="line">optimizer.load_state_dict(save_info[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">model.load_state_dict(sae_info[<span class="string">&quot;model&quot;</span>])</span><br></pre></td></tr></table></figure>
<h3 id="5-4-在一个文件中保存多个模型"><a href="#5-4-在一个文件中保存多个模型" class="headerlink" title="5.4 在一个文件中保存多个模型"></a>5.4 在一个文件中保存多个模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line"><span class="string">&#x27;modelA_state_dict&#x27;</span>: modelA.state_dict(),</span><br><span class="line"><span class="string">&#x27;modelB_state_dict&#x27;</span>: modelB.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizerA_state_dict&#x27;</span>: optimizerA.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizerB_state_dict&#x27;</span>: optimizerB.state_dict(),</span><br><span class="line">...</span><br><span class="line">&#125;, PATH)</span><br></pre></td></tr></table></figure>
<p>加载<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">modelA = TheModelAClass(*args, **kwargs)</span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)</span><br><span class="line">optimizerA = TheOptimizerAClass(*args, **kwargs)</span><br><span class="line">optimizerB = TheOptimizerBClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">modelA.load_state_dict(checkpoint[<span class="string">&#x27;modelA_state_dict&#x27;</span>])</span><br><span class="line">modelB.load_state_dict(checkpoint[<span class="string">&#x27;modelB_state_dict&#x27;</span>])</span><br><span class="line">optimizerA.load_state_dict(checkpoint[<span class="string">&#x27;optimizerA_state_dict&#x27;</span>])</span><br><span class="line">optimizerB.load_state_dict(checkpoint[<span class="string">&#x27;optimizerB_state_dict&#x27;</span>])</span><br><span class="line">modelA.<span class="built_in">eval</span>()</span><br><span class="line">modelB.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><br>当保存一个模型由多个 torch.nn.Modules 组成时，例如GAN(对抗生成网络)、sequence-to-<br>sequence (序列到序列模型), 或者是多个模 型融合, 可以采用与保存常规检查点相同的方法。<br>换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，可以通 过简单地<br>将它们附加到字典的方式来保存任何其他项目，这样有助于恢复训练。</p>
<h2 id="六、TensorBoard的安装和使用"><a href="#六、TensorBoard的安装和使用" class="headerlink" title="六、TensorBoard的安装和使用"></a>六、TensorBoard的安装和使用</h2><blockquote>
<p>pip install tensorflow-tensorboard<br>pip install tensorboard<br>安装完之后import tensorboard时报错ImportError: TensorBoard logging requires TensorBoard version 1.15 or above<br>试了几种方法。最后关掉ipynb文件，新建一个ipynb文件复制代码运行就好了。</p>
</blockquote>
<h3 id="6-1-TensorBoard用法示例："><a href="#6-1-TensorBoard用法示例：" class="headerlink" title="6.1 TensorBoard用法示例："></a>6.1 TensorBoard用法示例：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义线性回归模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ndim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.ndim = ndim</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.randn(ndim, <span class="number">1</span>)) <span class="comment"># 定义权重</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>)) <span class="comment"># 定义偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 定义线性模型 y = Wx + b</span></span><br><span class="line">        <span class="keyword">return</span> x.mm(self.weight) + self.bias</span><br><span class="line"></span><br><span class="line">boston = load_boston()</span><br><span class="line">lm = LinearModel(<span class="number">13</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optim = torch.optim.SGD(lm.parameters(), lr=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">data = torch.tensor(boston[<span class="string">&quot;data&quot;</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">target = torch.tensor(boston[<span class="string">&quot;target&quot;</span>], dtype=torch.float32)</span><br><span class="line">writer = SummaryWriter() <span class="comment"># 构造摘要生成器，定义TensorBoard输出类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    predict = lm(data)</span><br><span class="line">    loss = criterion(predict, target)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;Loss/train&quot;</span>, loss, step) <span class="comment"># 输出损失函数</span></span><br><span class="line">    writer.add_histogram(<span class="string">&quot;Param/weight&quot;</span>, lm.weight, step) <span class="comment"># 输出权重直方图</span></span><br><span class="line">    writer.add_histogram(<span class="string">&quot;Param/bias&quot;</span>, lm.bias, step) <span class="comment"># 输出偏置直方图</span></span><br><span class="line">    <span class="keyword">if</span> step <span class="keyword">and</span> step % <span class="number">1000</span> == <span class="number">0</span> :</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loss: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(loss.item()))</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>训练完之后，在当前目录下面会生成一个文件夹runs。runs下面还有一个文件夹（名字和训练时间、主机名称有关）</p>
<ul>
<li>from torch.utils.tensorboard import SummaryWriter是从tensorboard构造一个摘要写入器SummaryWriter。实例化之后调用实例化方法添加要写入摘要的张量信息。</li>
<li>add_scalar：添加标量数据，比如loss、acc等</li>
<li>add_histogram：添加直方图</li>
<li>add_graph()：创建Graphs，Graphs中存放了网络结构</li>
<li>运行tensorboard-logdir./run命令，启动tensorboard服务器。默认端口6006。访问<a href="http://127.0.0.1:6006可以看到tensorboard网页界面。">http://127.0.0.1:6006可以看到tensorboard网页界面。</a></li>
</ul>
<h3 id="6-2-具体函数"><a href="#6-2-具体函数" class="headerlink" title="6.2 具体函数"></a>6.2 具体函数</h3><h4 id="6-2-1-SummaryWriter"><a href="#6-2-1-SummaryWriter" class="headerlink" title="6.2.1 SummaryWriter"></a>6.2.1 SummaryWriter</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(log_dir=<span class="literal">None</span>, comment=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">   purge_step=<span class="literal">None</span>, max_queue=<span class="number">10</span>, flush_secs=<span class="number">120</span>, filename_suffix=<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>log_dir：tensorboard文件的存放路径，默认是创建runs文件夹</li>
<li>flush_secs：表示写入tensorboard文件的时间间隔</li>
<li>purge_step:可视化数据不是实时写入，而是有个队列。积累的数据超过队列限制的时候，触发数据文件写入。如果写入的可视化数据崩溃，purge_step步数之后的数据将会被舍弃</li>
<li>max_queue:写入磁盘之前内存中最多可以保留的事件（数据）的数量</li>
<li>filaname_suffix:可视化数据文件的后缀，默认为空字符串</li>
</ul>
<h4 id="6-2-2-add-scalar-和add-scalars"><a href="#6-2-2-add-scalar-和add-scalars" class="headerlink" title="6.2.2  add_scalar()和add_scalars()"></a>6.2.2  add_scalar()和add_scalars()</h4><ul>
<li>add_scalar()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalar(tag, scalar_value, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>于在tensorboard中加入loss，其中常用参数有：</p>
<pre><code>- tag：不同图表的标签，如下图所示的Train_loss。
- scalar_value：标签的值，浮点数
- global_step：当前迭代步数，标签的x轴坐标
- walltime：迭代时间函数。如果不传入，方法内部使用time.time()返回一个浮点数代表时间
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.add_scalar(<span class="string">&#x27;Train_loss&#x27;</span>, loss, (epoch*epoch_size + iteration))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/ee5a72c90ce742c981ba80b6c5cb4877.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<ul>
<li>add_scalars()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalars(main_tag, tag_scalar_dict, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
和上一个方法类似，通过传入一个主标签（main_tag），然后传入键值对是标签和标量值的一个字典（tag_scalar_dict），对每个标量值进行显示。</li>
</ul>
<h4 id="6-2-3-add-histogram"><a href="#6-2-3-add-histogram" class="headerlink" title="6.2.3 add_histogram()"></a>6.2.3 add_histogram()</h4><p>显示张量分量的直方图和对应的分布<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_histogram(tag, values, global_step=<span class="literal">None</span>, bins=<span class="string">&#x27;tensorflow&#x27;</span>, walltime=<span class="literal">None</span>, max_bins=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>bins：产生直方图的方法，可以是tensorflow、auto、fd</li>
<li>max_bins:最大直方图分段数<h4 id="6-2-4-add-graph"><a href="#6-2-4-add-graph" class="headerlink" title="6.2.4 add_graph"></a>6.2.4 add_graph</h4>传入pytorch模块及输入，显示模块对应的计算图</li>
<li>model：pytorch模型</li>
<li>input_to_model：pytorch模型的输入</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> Cuda:</span><br><span class="line">    graph_inputs = torch.from_numpy(np.random.rand(<span class="number">1</span>,<span class="number">3</span>,input_shape[<span class="number">0</span>],input_shape[<span class="number">1</span>])).<span class="built_in">type</span>(torch.FloatTensor).cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    graph_inputs = torch.from_numpy(np.random.rand(<span class="number">1</span>,<span class="number">3</span>,input_shape[<span class="number">0</span>],input_shape[<span class="number">1</span>])).<span class="built_in">type</span>(torch.FloatTensor)</span><br><span class="line">writer.add_graph(model, (graph_inputs,))</span><br></pre></td></tr></table></figure>
<h4 id="6-2-5-add-pr-curve"><a href="#6-2-5-add-pr-curve" class="headerlink" title="6.2.5 add_pr_curve"></a>6.2.5 add_pr_curve</h4><p>显示准确率-召回率曲线（Prediction-Recall Curve）。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_pr_curve(tag, labels, predictions, global_step=<span class="literal">None</span>, num_thresholds=<span class="number">127</span>,</span><br><span class="line">    weights=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>labels：目标值</li>
<li>predictions：预测值</li>
<li>num_thresholds：曲线中间插值点数</li>
<li>weights：每个点的权重<h4 id="6-2-6-tensorboard-—logdir"><a href="#6-2-6-tensorboard-—logdir" class="headerlink" title="6.2.6 tensorboard —logdir="></a>6.2.6 tensorboard —logdir=</h4>完成tensorboard文件的生成后，可在命令行调用该文件，tensorboard网址。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#打开cmd命令</span></span><br><span class="line">tensorboard --logdir=.\Chapter2\runs --bind_all</span><br><span class="line"><span class="comment">#TensorBoard 2.2.2 at http://DESKTOP-OHLNREI:6006/ (Press CTRL+C to quit)</span></span><br></pre></td></tr></table></figure>
<h4 id="6-2-7-add-image、add-vide、add-audio、add-text"><a href="#6-2-7-add-image、add-vide、add-audio、add-text" class="headerlink" title="6.2.7  add_image、add_vide、add_audio、add_text"></a>6.2.7  add_image、add_vide、add_audio、add_text</h4><h3 id="6-3-tensorboard界面简介"><a href="#6-3-tensorboard界面简介" class="headerlink" title="6.3 tensorboard界面简介"></a>6.3 tensorboard界面简介</h3><p>右上方三个依次是：</p>
<ul>
<li>SCALARS：损失函数图像</li>
<li>DISTRIBUTIONS：权重分布（随时间）</li>
<li>HISTOGRAMS：权重直方图分布</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/39cf2f69f92948a1ab248ff51adb72f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="DISTRIBUTIONS"><br><img src="https://img-blog.csdnimg.cn/f9f9756f47954534a608cc9c6b6ba73b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="HISTOGRAMS"><br>权重分布和直方图应该是随着训练一直变化，直到分布稳定。如果一直没有变化，可能模型结构有问题或者反向传播有问题。</p>
<p>Scalars：这个面板是最常用的面板，主要用于将神经网络训练过程中的acc（训练集准确率）val_acc（验证集准确率），loss（损失值），weight（权重）等等变化情况绘制成折线图。</p>
<ul>
<li>Ignore outlines in chart scaling（忽略图表缩放中的轮廓），可以消除离散值</li>
<li>data downloadlinks：显示数据下载链接，用来下载图片</li>
<li>smoothing：图像的曲线平滑程度，值越大越平滑。每个mini-batch的loss不一定下降，smoothing越大时，代表平均的mini-batch越多。</li>
<li>Horizontal Axis：水平轴表示方式。<ul>
<li>STEP：表示迭代次数</li>
<li>RELATIVE：表示按照训练集和测试集的相对值</li>
<li>WALL：表示按照时间。</li>
</ul>
</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/11/18/10月组队学习——Pytorch/PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard/">https://zhxnlp.github.io/2021/11/18/10月组队学习——Pytorch/PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E8%81%9A%E7%B1%BB/"><i class="fa fa-chevron-left">  </i><span>学习笔记2：线性回归、决策树、聚类</span></a></div><div class="next-post pull-right"><a href="/2021/11/17/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%E3%80%81%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"><span>python类与对象</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>