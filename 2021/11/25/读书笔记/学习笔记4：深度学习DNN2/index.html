<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="学习笔记4：深度学习DNN2"><meta name="keywords" content="深度学习,神经网络,DNN"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>学习笔记4：深度学习DNN2 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.</span> <span class="toc-text">一、神经网络参数优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-SGD%EF%BC%88%E6%97%A0%E5%8A%A8%E9%87%8F%EF%BC%89%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%82"><span class="toc-number">1.1.</span> <span class="toc-text">1.2 SGD（无动量）随机梯度下降。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-SGDM%E2%80%94%E2%80%94%E5%BC%95%E5%85%A5%E5%8A%A8%E9%87%8F%E5%87%8F%E5%B0%91%E9%9C%87%E8%8D%A1"><span class="toc-number">1.2.</span> <span class="toc-text">1.3 SGDM——引入动量减少震荡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-SGD-with-Nesterov-Acceleration"><span class="toc-number">1.3.</span> <span class="toc-text">1.4 SGD with Nesterov Acceleration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-AdaGrad%E2%80%94%E2%80%94%E7%B4%AF%E7%A7%AF%E5%85%A8%E9%83%A8%E6%A2%AF%E5%BA%A6%EF%BC%8C%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.4.</span> <span class="toc-text">1.5 AdaGrad——累积全部梯度，自适应学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-RMSProp%E2%80%94%E2%80%94%E7%B4%AF%E7%A7%AF%E6%9C%80%E8%BF%91%E6%97%B6%E5%88%BB%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.5.</span> <span class="toc-text">1.6 RMSProp——累积最近时刻梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-Adam"><span class="toc-number">1.6.</span> <span class="toc-text">1.7 Adam</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-%E6%82%AC%E5%B4%96%E3%80%81%E9%9E%8D%E7%82%B9%E9%97%AE%E9%A2%98"><span class="toc-number">1.7.</span> <span class="toc-text">1.8 悬崖、鞍点问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">2.</span> <span class="toc-text">二、过拟合解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-dropout"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Batch-Normalization"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Layer-Normalization"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 Layer Normalization</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">学习笔记4：深度学习DNN2</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-25</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 12 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>@[toc]</p>
<h2 id="一、神经网络参数优化器"><a href="#一、神经网络参数优化器" class="headerlink" title="一、神经网络参数优化器"></a>一、神经网络参数优化器</h2><p>参考曹健<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45558569/article/details/110728137?spm=1001.2014.3001.5501">《人工智能实践：Tensorflow2.0 》</a><br>深度学习优化算法经历了SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam<br>这样的发展历程。<br><img src="https://img-blog.csdnimg.cn/65f1bb4bf04f4aa8b260f2126b37099a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<span id="more"></span>
<ul>
<li>上图中f一般指loss</li>
<li>一阶动量：与梯度相关的函数</li>
<li>二阶动量：与梯度平方相关的函数</li>
<li><font color='red'>不同的优化器，实质上只是定义了不同的一阶动量和二阶动量公式</li>
</ul>
<h3 id="1-2-SGD（无动量）随机梯度下降。"><a href="#1-2-SGD（无动量）随机梯度下降。" class="headerlink" title="1.2 SGD（无动量）随机梯度下降。"></a>1.2 SGD（无动量）随机梯度下降。</h3><ul>
<li>最常用的梯度下降方法是随机梯度下降，即随机采集样本来计算梯度。根据统计学知识有：采样数据的平均值为全量数据平均值的无偏估计。</li>
<li>实际计算出来的SGD在全量梯度附近以一定概率出现，batch_size越大，概率分布的方差越小，SGD梯度就越确定，相当于在全量梯度上注入了噪声。适当的噪声是有益的。</li>
<li>batch_size越小，计算越快，w更新越频繁，学习越快。但是太小的话，SGD梯度和真实梯度差异过大，而且震荡厉害，不利于学习</li>
<li><font color='red'>SGD梯度有一定随机性，所以可以逃离鞍点、平坦极小值或尖锐极小值区域</li>
<li>最初版本是vanilla SGD，没有动量。$m_{t}=g_{t}，V_{t}=1$（p32—sgd.py）</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2003ca0870694c979e9728c609832aa6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<ul>
<li><p>每个bacth_size样本的梯度都不一样，甚至前后差异很大，造成梯度震荡。（比如一个很大的正值接一个很大的负值）</p>
</li>
<li><p>神经网络中，输入归一化。但是多层非线性变化后，中间层输入各维度数值差别可能很大。不同方向的敏感度不一样。有的方向更陡，容易震荡。</p>
</li>
<li><p>vanilla SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优<br>点</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sgd</span></span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>])</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="1-3-SGDM——引入动量减少震荡"><a href="#1-3-SGDM——引入动量减少震荡" class="headerlink" title="1.3 SGDM——引入动量减少震荡"></a>1.3 SGDM——引入动量减少震荡</h3><ul>
<li>动量法是一种使梯度向量向相关方向加速变化，抑制震荡，最终实现加速收敛的方法。</li>
<li>为了抑制SGD的震荡，SGDM认为 <font color='red'>梯度下降过程可以加入惯性。如果前后梯度方向相反，动量对冲，减少震荡。如果前后方向相同，步伐加大，加快学习速度。</li>
<li>SGDM全称是SGD with Momentum，在SGD基础上引入了一阶动量：<br><img src="https://img-blog.csdnimg.cn/441deed8edc541bea8cbe99d30e23bf2.png" alt="在这里插入图片描述"></li>
<li>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近$1/(1-\beta <em>{1})$个时刻的梯度向量和的平均值。(指数移动平均值大约是过去一段时间的平均值，反映“局部的”参数信息$)。\beta <em>{1}$的经验值为0.9。所以t 时刻的下降方向主要偏向此前累积的下降方向，并略微偏向当前时刻的下降方向。<br><img src="https://img-blog.csdnimg.cn/7d18ec7ad9fa4604b213b4b6d862d374.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>老师书上写$$W</em>{t+1}=W</em>{t}-v_{t}=W_{t}-(\alpha v_{t-1}+\varepsilon g_{t})$$<br>$\alpha、\varepsilon$是超参数。</li>
<li>SGDM问题1：时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算。</li>
<li>SGD/SGDM 问题2:会被困在一个局部最优点里</li>
<li>SGDM 问题3：如果梯度连续多次迭代都是一个方向，剃度一直增大，最后造成梯度爆炸</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sgd-momentun</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(learning_rate * m_w)</span><br><span class="line">b1.assign_sub(learning_rate * m_b)</span><br></pre></td></tr></table></figure>

<h3 id="1-4-SGD-with-Nesterov-Acceleration"><a href="#1-4-SGD-with-Nesterov-Acceleration" class="headerlink" title="1.4 SGD with Nesterov Acceleration"></a>1.4 SGD with Nesterov Acceleration</h3><ul>
<li>SGDM：主要看当前梯度方向。计算当前loss对w梯度，再根据历史梯度计算一阶动量$m_{t}$</li>
<li>NAG：主要看动量累积之后的梯度方向。 <font color='red'>计算参数在累积动量之后的更新值，并计算此时梯度</font>，再将这个梯度带入计算一阶动量$m_{t}$</li>
<li> <font color='red'>主要差别在于是否先对w减去累积动量</font><br><img src="https://img-blog.csdnimg.cn/c3d6a6cb8c7243a0b6512790c58135c4.png" alt="在这里插入图片描述"><br>思路如下图：</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/9b5c723108434a3181da8d87ebfe9d09.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>首先，按照原来的更新方向更新一步（棕色线），然后计算该新位置的梯度方向（红色线），然后<br>用这个梯度方向修正最终的更新方向（绿色线）。上图中描述了两步的更新示意图，其中蓝色线是标准momentum更新路径。</p>
<h3 id="1-5-AdaGrad——累积全部梯度，自适应学习率"><a href="#1-5-AdaGrad——累积全部梯度，自适应学习率" class="headerlink" title="1.5 AdaGrad——累积全部梯度，自适应学习率"></a>1.5 AdaGrad——累积全部梯度，自适应学习率</h3><ul>
<li>SGD:对所有的参数使用统一的、固定的学习率</li>
<li>AdaGrad:<font color='red'>自适应学习率。对于频繁更新的参数，不希望被单个样本影响太大，我们给它们很小的学习率；对于偶尔出现的参数，希望能多得到一些信息，我们给它较大的学习率。</li>
<li>另一个解释是：初始时刻W离最优点远，学习率需要设置的大一些。随着学习你的进行，离最优点越近，学习率需要不断减小。</li>
<li>引入二阶动量——该维度上，所有梯度值的平方和(梯度按位相乘后求和），来度量参数更新频率，用以对学习率进行缩放。（<font color='red'>频繁更新的参数、越到学习后期参数也被更新的越多，二阶动量都越大，学习率越小）</font><br>$$V_{t}=\sum_{\tau =1}^{t}g_{\tau }^{2}$$<br>$$\eta <em>{t}=lr\cdot m</em>{t}/\sqrt{V_{t}}=lr\cdot g_{t}/\sqrt{\sum_{\tau =1}^{t}g_{\tau }^{2}}$$<br>$$W_{t+1}=W_{t}-\eta _{t}$$<br><img src="https://img-blog.csdnimg.cn/97b9398d2e804171be3368ff83b30b9c.png" alt="在这里插入图片描述"></li>
<li>优点：AdaGrad 在稀疏数据场景下表现最好。因为对于频繁出现的参数，学习率衰减得快；对于稀疏的参数，学习率衰减得更慢。</li>
<li>缺点：<font color='red'>在实际很多情况下，频繁更新的参数，学习率会很快减至 0 </font>，导致参数不再更新，训练过程提前结束。（二阶动量呈单调递增，累计从训练开始的梯度)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adagrad</span></span><br><span class="line">v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>
<h3 id="1-6-RMSProp——累积最近时刻梯度"><a href="#1-6-RMSProp——累积最近时刻梯度" class="headerlink" title="1.6 RMSProp——累积最近时刻梯度"></a>1.6 RMSProp——累积最近时刻梯度</h3><ul>
<li>RMSProp算法的全称叫 Root Mean Square Prop。AdaGrad 的学习率衰减太过激进，考虑改变二阶动量的计算策略：<font color='red'>不累计全部梯度，只关注过去某一窗口内的梯度。</li>
<li>指数移动平均值大约是过去一段时间的平均值，反映“局部的”参数信息，因此我们用这个方法来计算二阶累积动量：（分母会再加一个平滑项，防止为0）<br><img src="https://img-blog.csdnimg.cn/63397961c363425ea85e2f4b700d85e3.png" alt="在这里插入图片描述"><br>对照SGDM的一阶动量公式：<br><img src="https://img-blog.csdnimg.cn/247f69bbc3b8420f8c66818346b35a58.png" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RMSProp</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(learning_rate * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(learning_rate * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>
<h3 id="1-7-Adam"><a href="#1-7-Adam" class="headerlink" title="1.7 Adam"></a>1.7 Adam</h3><p>将SGDM一阶动量和RMSProp二阶动量结合起来，再修正偏差，就是Adam。<br><img src="https://img-blog.csdnimg.cn/ef129fd5ab3f47b4ac9a9667d17969fe.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>一阶动量和二阶动量都是按照指数移动平均值进行计算的。初始化 $m_{0}=0,V_{0}=0$，在初期，迭<br>代得到的$m_{t}、V_{t}$会接近于0。我们可以通过偏差修正来解决这一问题：<br>$$\widehat{m_{t}}=\frac{m_{t}}{1-\beta <em>{1}^{t}}$$<br>$$\widehat{V</em>{t}}=\frac{V_{t}}{1-\beta <em>{2}^{t}}$$<br>$$\eta <em>{t}=lr\cdot \widehat{m</em>{t}}/(\sqrt{\widehat{V</em>{t}}}+\varepsilon )$$<br>$$W_{t+1}=W_{t}-\eta _{t}$$</p>
<ul>
<li>${1-\beta <em>{1}^{t}}、1-\beta <em>{2}^{t}$的取值范围为（0,1）,可以将开始阶段$m</em>{t}、V</em>{t}$放大至$\widehat{m_{t}}、\widehat{V_{t}}$。</li>
<li>随着迭代次数t的增加，$\beta <em>{1}^{t}、\beta <em>{2}^{t}$趋近于0，放大倍数趋近于1，即不再放大$m</em>{t}、V</em>{t}$。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># adam</span></span><br><span class="line">m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">w1.assign_sub(learning_rate * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">b1.assign_sub(learning_rate * m_b_correction / tf.sqrt(v_b_correction))</span><br></pre></td></tr></table></figure>
<h3 id="1-8-悬崖、鞍点问题"><a href="#1-8-悬崖、鞍点问题" class="headerlink" title="1.8 悬崖、鞍点问题"></a>1.8 悬崖、鞍点问题</h3></li>
<li>高维空间中，一个点各个方向导数为0，只要有一个方向对应的极值和其它方向不一样，该点就是鞍点。鞍点是一个不稳定的点，梯度轻微扰动就可以逃离。</li>
<li>SGD梯度因为具有一定的随机性，反而可以逃离鞍点</li>
<li>例如n维空间中，某点各方向导数都为0。假设其中极大或者极小的概率为0.5，则该点为极大或极小值概率均为$0.5^{n}$，反之为鞍点的概率为$1-2*0.5^{n}=1-0.5^{n-1}$。高维空间中鞍点概率几乎为1。因此不必非要找到极小值，只需要loss降到比较低就行了。</li>
<li>梯度裁剪：悬崖部位loss会突然下降，梯度太大W容易走过头，所以可以梯度裁剪，限制梯度最大值。<h2 id="二、过拟合解决方案"><a href="#二、过拟合解决方案" class="headerlink" title="二、过拟合解决方案"></a>二、过拟合解决方案</h2><h3 id="2-1-正则化"><a href="#2-1-正则化" class="headerlink" title="2.1 正则化"></a>2.1 正则化</h3></li>
<li>逻辑回归中，损失函数为$Loss+\lambda \left | W \right |or Loss+\lambda \left | W \right |$。在神经网络中则更加灵活。可以为不同层分别设置L1或者L2正则，且系数$\lambda$也可以不同。即各层正则化项可以完全独立。</li>
<li><font color='red'>L1正则可以对神经网络剪枝</font >（很多权重趋近于0），网络运行速度可以大大加快。所以对实时要求高的场景可以用L1正则。</li>
<li>神经网络中可以设置前层的正则化项系数$\lambda$更大，后层小一些。因为前层面对真实物理信号，噪声较大。为了过滤噪声，W应当较小，$\lambda$更大。且后层面对输出，正则化太厉害，影响分类效果。<h3 id="2-2-dropout"><a href="#2-2-dropout" class="headerlink" title="2.2 dropout"></a>2.2 dropout</h3></li>
</ul>
<ol>
<li><font color='red'>Dropout：每次训练时直接随机去除部分神经元。可以达到剪枝的目的，生成新的子网络。所以本质也是一种正则化，类比于L1正则。各层可以独立设置Dropout</li>
<li>dropout相当于训练大量子网络，预测时使用全部神经元，等同于集成学习，增加泛化能力。</li>
<li> 降低模型复杂度和各神经元之间的依赖程度，降低模型对某一特征的过度依赖，从而降低过拟合</li>
</ol>
<ul>
<li>子网络可以是海量的，大量子网络可能没有训练或者就训练一次，但是由于大部分神经元是一样的，少部分才被去除，所以大部分自网络高度相关。而且训练一个子网络，等于主网络大部分参数也得到了更新</li>
<li>dropout是随机去除神经元，所以发生在$d^{k}=W^{k}\cdot a^{k-1}$加权求和的时候，而不是$a^{k-1}=f(d^{l-1}+w_{0}^{k-1})$激活之后。</li>
<li>训练时dropout概率为p，即只有（1-p)个神经元进行计算。则预测时结果要乘以（1-p）</li>
<li><font color='red'>跟正则化系数一样，dropout应该是前层设置的高。前层噪声多，需要过滤。后层主要用来精确拟合，dropout过高，影响预测结果。</li>
<li>dropout可以取代正则，例如训练100个神经元，dropout=0.2；效果好于训练80个神经元。<h3 id="2-3-Batch-Normalization"><a href="#2-3-Batch-Normalization" class="headerlink" title="2.3 Batch Normalization"></a>2.3 Batch Normalization</h3>神经网络学习隐含条件：<font color='red'>输入数据有一定的规律，符合一定的概率分布，模型就是学习输入分布和类别之间的映射关系。但是神经网络中，各层输入分布并不稳定：</li>
</ul>
<ol>
<li>SGD训练时，随机选取样本，造成分布的差异，即$\mu /\sigma$都不一样</li>
<li>前层权重W的变化造成后层输入的变化，进而导致后层最优的参数W变化。即使后层参数已接近最优，也要重新学习，造成网络不稳定。后层分布累积了前层所有的W，所以不同轮次输入分布差异很大，学习速度会很慢</li>
</ol>
<p>即训练样本数据本身是符合一定的概率分布的，但是因为以上两个原因，造成每个batch训练时概率分布差异很大。</p>
<p>对于一个batch的数据，第l层第m个神经元来说：</p>
<ul>
<li> 计算神经元在一个batch样本的输入$d_{i,m}^{l}$的均值和方差，将数据分布转为标准正态分布<br>$$\mu <em>{m}^{l}=\frac{1}{batch-size}\sum</em>{i\epsilon batch }d_{i,m}^{l}$$</li>
<li>通过两个待学习参数$\beta_{1} ,\gamma <em>{1}$，将标准正态分布调整至$N(\beta</em>{1} ,\gamma _{1})$。</li>
</ul>
<p>BN的作用：</p>
<ul>
<li>使输入数据对应的概率分布保持不变（不是数值不变）。<font color='red'>无论训练集数据如何选择，前层网络参数如何变化，各层接受的输入分布在不同的训练阶段都是一致的，各层最优参数稳定，提高了收敛速度</li>
<li>不容易受极端数据影响，所以可以提高学习率，使收敛速度进一步提高。</li>
<li>降低对参数初始化的依赖程度</li>
<li>对参数进行正则化，提高了模型泛化能力</li>
</ul>
<p>其它注意点：</p>
<ul>
<li>batch_size不宜过低，否则均值方差估计不准</li>
<li><font color='red'>预测时没有批量的概念，都是对单个样本进行预测，无法进行BN操作计算$\mu /\sigma$ 。所以要保存训练中计算的$\mu /\sigma$ ，预测时使用它们进行无偏估计（公式中t表示当前迭代次数，T为总迭代次数)<br>$$\mu =\frac{1}{T}\sum_{t=1}^{T}\mu_{t} $$<br>$$\sigma  =\sqrt{\frac{1}{T-1}\sum_{t=1}^{T}\sigma _{t}^{2} }$$</li>
<li>BN的位置在激活函数之前。因为将输入转换为正态分布$N(\beta_{1} ,\gamma _{1})$的前提是输入数据本身符合正态分布。而神经网络中激活函数的值域往往有限，造成激活后的输出不会符合正态分布。这样BN之后也不会符合正态分布。<h3 id="2-4-Layer-Normalization"><a href="#2-4-Layer-Normalization" class="headerlink" title="2.4 Layer Normalization"></a>2.4 Layer Normalization</h3></li>
<li>Batch Normalization：在某一个神经元，所有batch个样本上进行归一化，即<font color='red'>归一化不同样本的同一特征</li>
<li>Layer Normalization：对某一个样本，同一层全部神经元上进行归一化，即<font color='red'>归一化一个样本所有特征</li>
<li>BN和LN都可以用的时候BN一般更好，因为不同数据，同一特征得到的归一化结果更不容易造成信息损失。LN会造成神经元耦合。</li>
<li>batch_size过小的场合，或者RNN、LSTM、Attention等变长神经网络一般使用LN。（变长神经网络虽然填充到同一长度，但是pad部分是没有意义的，进行特征维度归一化也没用实际意义。NLP任务中一个序列的所有token都是同一语义空间，进行LN归一化有实际意义）</li>
</ul>
<p>对于l层第i个样本有：（l层共有m=1、2、……$M^{l}$个神经元）<br>$$\mu <em>{i}^{l}=\frac{1}{M^{l}}\sum</em>{m=1 }^{M^{l}}d_{i,m}^{l}$$<br>计算出$\mu /\sigma$后，和BN一样将数据归一化为$N(\beta_{1} ,\gamma _{1})$分布。</p>
<p>综合之前的讲解：<br><font color='red'>model.train()的作用是启用 Batch Normalization 和 Dropout。model.eval()的作用是不启用 Batch Normalization 和 Dropout。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/11/25/读书笔记/学习笔记4：深度学习DNN2/">https://zhxnlp.github.io/2021/11/25/读书笔记/学习笔记4：深度学习DNN2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/DNN/">DNN</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05%EF%BC%9Aword2vec%E5%92%8Cfasttext/"><i class="fa fa-chevron-left">  </i><span>学习笔记5：word2vec、FastText原理</span></a></div><div class="next-post pull-right"><a href="/2021/11/23/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN/"><span>学习笔记3：深度学习DNN</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>