<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="学习笔记5：word2vec、FastText原理"><meta name="keywords" content="深度学习,神经网络,DNN"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>学习笔记5：word2vec、FastText原理 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81word2vec"><span class="toc-number">1.</span> <span class="toc-text">一、word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-word2vec%E4%B8%BA%E4%BB%80%E4%B9%88-%E4%B8%8D%E7%94%A8%E7%8E%B0%E6%88%90%E7%9A%84DNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 word2vec为什么 不用现成的DNN模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-word2vec%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%9E%8B%EF%BC%9ACBOW%E5%92%8CSkip-gram"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 word2vec两种模型：CBOW和Skip-gram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-word2vec%E4%B8%A4%E7%A7%8D%E4%BC%98%E5%8C%96%E8%A7%A3%E6%B3%95%EF%BC%9A%E9%9C%8D%E5%A4%AB%E6%9B%BC%E6%A0%91%E5%92%8C%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">1.3.</span> <span class="toc-text">1.2 word2vec两种优化解法：霍夫曼树和负采样</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84CBOW%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.2.2 基于Hierarchical Softmax的CBOW模型算法流程：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E8%B4%9F%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">1.2.3 负采样方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.4.</span> <span class="toc-text">1.3 总结：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81fasttext"><span class="toc-number">2.</span> <span class="toc-text">二、fasttext</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E3%80%81%E7%AE%80%E4%BB%8B"><span class="toc-number">2.1.</span> <span class="toc-text">2.1、简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-FastText%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 FastText原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 模型架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E5%B1%82%E6%AC%A1SoftMax"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 层次SoftMax</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-N-gram%E7%89%B9%E5%BE%81"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 N-gram特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-4-subword"><span class="toc-number">2.2.4.</span> <span class="toc-text">2.2.4 subword</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-5-fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.5.</span> <span class="toc-text">2.2.5 fasttext文本分类总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81fastText%E5%92%8Cword2vec%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">三、fastText和word2vec对比总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-fastText%E5%92%8Cword2vec%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 fastText和word2vec的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%B0%8F%E7%BB%93"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 小结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-fasttext%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 fasttext适用范围</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-fasttext%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 fasttext应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-fastText%E4%BC%98%E7%82%B9"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 fastText优点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%94%A8gensim%E5%AD%A6%E4%B9%A0word2vec"><span class="toc-number">4.</span> <span class="toc-text">四、用gensim学习word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 使用技巧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84Word2vec"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 推荐系统中的Word2vec</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81-%E5%9F%BA%E4%BA%8EfastText%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">5.</span> <span class="toc-text">五、 基于fastText实现文本分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-fasttext%E5%8F%82%E6%95%B0%EF%BC%9A"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 fasttext参数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-bin%E6%A0%BC%E5%BC%8F%E8%AF%8D%E5%90%91%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BAvec%E6%A0%BC%E5%BC%8F"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 bin格式词向量转换为vec格式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94fasttext"><span class="toc-number">6.</span> <span class="toc-text">六、新闻文本分类——fasttext</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E6%AD%A3%E5%B8%B8fasttext%E5%88%86%E7%B1%BB"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 正常fasttext分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%B0%8F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9Aword2vec-fasttext-%E9%A6%96%E5%B0%BE%E6%88%AA%E6%96%AD"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 小数据集：word2vec+fasttext+首尾截断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%85%A8%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9Aword2vec-fasttext-%E9%A6%96%E5%B0%BE%E6%88%AA%E6%96%AD"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 全数据集：word2vec+fasttext+首尾截断</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">学习笔记5：word2vec、FastText原理</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-27</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">7.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 29 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一、word2vec"><a href="#一、word2vec" class="headerlink" title="一、word2vec"></a>一、word2vec</h2><p>参考文档<a target="_blank" rel="noopener" href="https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80">《word2vec原理和gensim实现》</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">《深入浅出Word2Vec原理解析》</a></p>
<h3 id="1-1-word2vec为什么-不用现成的DNN模型"><a href="#1-1-word2vec为什么-不用现成的DNN模型" class="headerlink" title="1.1 word2vec为什么 不用现成的DNN模型"></a>1.1 word2vec为什么 不用现成的DNN模型</h3><ol>
<li>最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，==从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值==。解决办法有两个：霍夫曼树和负采样。</li>
<li>对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。输入从多个词向量变成了一个词向量</li>
<li>在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量<span id="more"></span>
<h3 id="1-2-word2vec两种模型：CBOW和Skip-gram"><a href="#1-2-word2vec两种模型：CBOW和Skip-gram" class="headerlink" title="1.2 word2vec两种模型：CBOW和Skip-gram"></a>1.2 word2vec两种模型：CBOW和Skip-gram</h3>&#8195;&#8195;Word2Vec是轻量级的神经网络，其模型仅仅包括输入层、隐藏层和输出层，模型框架根据输入输出的不同，主要包括CBOW和Skip-gram模型。 </li>
</ol>
<ul>
<li>CBOW的方式是在知道词$w<em>{t}$的上下文$w</em>{t-2}$、$w<em>{t-1}$和$w</em>{t+1}$、$w<em>{t+2}$的情况下预测当前词$w</em>{t}$。</li>
<li>Skip-gram是在知道了词$w_{t}$的情况下,对词的上下文进行预测，如下图所示：<br><img src="https://img-blog.csdnimg.cn/8486814a2075418a807be2402c23abf1.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></li>
</ul>
<h3 id="1-2-word2vec两种优化解法：霍夫曼树和负采样"><a href="#1-2-word2vec两种优化解法：霍夫曼树和负采样" class="headerlink" title="1.2 word2vec两种优化解法：霍夫曼树和负采样"></a>1.2 word2vec两种优化解法：霍夫曼树和负采样</h3><ol>
<li><p>霍夫曼树解法：</p>
<ul>
<li>采用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</li>
<li>把之前计算所有词的softmax概率变成了查找二叉霍夫曼树。那么我们的softmax概率计算只需要沿着树形结构进行，从根节点一直走到我们的叶子节点的词。将每个节点向左或向右走的概率连乘就是最终预测的概率。训练时只更新对应通路的w，与全连接W相比大大减少。</li>
<li>因为涉及连乘，每次乘的概率都是小于1，所以越到深层概率越低。所以其实存在一个词与词之间概率不对等的问题。</li>
<li>霍夫曼编码：由于权重高的叶子节点越靠近根节点，编码值较短。而权重低的叶子节点会远离根节点，编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望==越常用的词拥有更短的编码，查找就更快==。如何编码呢？参见上面提的文档</li>
</ul>
</li>
<li><p>负采样：</p>
<ul>
<li>使用霍夫曼树可以提高模型训练的效率。但是如果我们的训练样本里的中心词是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。</li>
<li>Negative Sampling：word2vec用神经网络解法时，输出是计算V类的概率，其中1类是中心词，概率往大的方向走，剩下一类是V-1个其它词，概率往小的方向走。==真正计算复杂的就是负类别。负采样法就是从V-1个负样本中随机挑几个词做负样本==。每个词被选为负样本的概率和其词频正相关00</li>
<li>Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，利用这一个正例和neg个负例，我们进行二元逻辑回归，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。二元逻辑回归算法见文档。</li>
<li>负采样中每个词有两套向量，分别作为输入和预测时使用。</li>
</ul>
</li>
<li>两种解法进行一定优化，牺牲了一定的分类的准确度。比如负采样的负样本是随机选取的，所以相对已经没那么准了。<h4 id="1-2-2-基于Hierarchical-Softmax的CBOW模型算法流程："><a href="#1-2-2-基于Hierarchical-Softmax的CBOW模型算法流程：" class="headerlink" title="1.2.2 基于Hierarchical Softmax的CBOW模型算法流程："></a>1.2.2 基于Hierarchical Softmax的CBOW模型算法流程：</h4></li>
</ol>
<ul>
<li>输入：根据词向量的维度大小M，以及CBOW的上下文大小2c，步长$\eta$，得到训练样本。</li>
<li>建立霍夫曼树，整体语料的各个词频 决定 huffman树。</li>
<li>随机初始化所有的模型参数$\theta$，所有的词向量w。这些训练样本所用的huffman树是一棵</li>
<li>随机梯度上升法，对于训练集中的每一个样本$(context(w), w)$中的每一个词向量$x_i$(共2c个)进行迭代更新。</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到上一步继续迭代<script type="math/tex; mode=display">h=\sum_{i=1}^{2c} embedding_{i}</script><script type="math/tex; mode=display">y=softmax(d)=softmax(Wh)=\frac{1}{\sum_{i=1}^{V}e^{d_{i}}}\begin{bmatrix}
e^{d_{1}}\\ 
e^{d_{2}}\\ 
...\\
e^{d_{V}}\end{bmatrix}</script>W为全连接层参数，将词向量维度映射为V维（词表大小），表示预测词的概率。<h4 id="1-2-3-负采样方法"><a href="#1-2-3-负采样方法" class="headerlink" title="1.2.3 负采样方法"></a>1.2.3 负采样方法</h4>如果词汇表的大小为$V$,那么我们就将一段长度为1的线段分成$V$份，每份对应词汇表中的一个词。&lt;/font&gt;高频词对应的线段长，低频词对应的线段短(高频词数量多，分子count就大)。每个词$w$的线段长度&lt;/font&gt;由下式决定：<script type="math/tex">len(w) = \frac{count(w)}{\sum\limits_{u \in vocab} count(u)}</script></li>
</ul>
<p>　　　　在word2vec中，分子和分母都取了3/4次幂（经验参数，提高低频词被选取的概率）如下：<script type="math/tex">len(w) = \frac{count(w)^{3/4}}{\sum\limits_{u \in vocab} count(u)^{3/4}}</script></p>
<p>　　　　在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M &gt;&gt; V$，这样可以保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要==从$M$个位置中采样出$neg$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词==<br><img src="https://img-blog.csdnimg.cn/3753e2f61df74e54878f6726356c0b50.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><strong>在word2vec中，$M$取值默认为$10^8$。</strong><br><img src="https://img-blog.csdnimg.cn/68d66db9b92141d9addd2a4831098b61.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h3 id="1-3-总结："><a href="#1-3-总结：" class="headerlink" title="1.3 总结："></a>1.3 总结：</h3><ol>
<li>one-hot：词表大大时内存不够。且所有词相似度都是一样的没有区别</li>
<li>word embedding：考虑使用使用神经网络语言模型，通过训练，将每个词都映射到一个较短的词向量上来</li>
<li>神经网络语言模型的输入输出，有连续词袋模型CBOW(Continuous Bag-of-Words） 和Skip-Gram两种模型。<ul>
<li>CBOW模型的训练输入是某个中心词的上下文词向量，输出是词表所有词的softmax概率，训练的目标是期望中心词对应的softmax概率最大。</li>
<li>Skip-Gram模型和CBOW的思路是反着来的，即输入中心词词向量，而输出是中心词对应的上下文词向量。比如窗口大小为4，就是输出softmax概率排前8的8个词。</li>
</ul>
</li>
<li>word2vec有两种解法，霍夫曼树和负采样。负采样用得较多，因为构建霍夫曼树比较麻烦。</li>
<li>一般来说， Skip-Gram模型比CBOW模型更好，因为：<ul>
<li>Skip-Gram模型有更多的训练样本。Skip-Gram是一个词预测n个词，而CBOW是n个词预测一个词。</li>
<li>误差反向更新中，CBOW是中心词误差更新n个周边词，这n个周边词被更新的力度是一样的。而Skip-Gram中，每个周边词都可以根据误差更新中心词，所以Skip-Gram是更细粒度的学习方法。</li>
<li>Skip-Gram效果更好（默认Skip-Gram模型）但是缺点就是训练次数更多，时间更长。 </li>
</ul>
</li>
</ol>
<h2 id="二、fasttext"><a href="#二、fasttext" class="headerlink" title="二、fasttext"></a>二、fasttext</h2><h3 id="2-1、简介"><a href="#2-1、简介" class="headerlink" title="2.1、简介"></a>2.1、简介</h3><p>&#8195;&#8195;fasttext是facebook开源的一个词向量与文本分类工具，在2016年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。</p>
<p>&#8195;&#8195;fastText的核心思想：==将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类==。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。</p>
<p>这些不同概念被用于两个不同任务：<br>•    有效文本分类 ：有监督学习（短文本）<br>•    学习词向量表征：无监督学习</p>
<h3 id="2-2-FastText原理"><a href="#2-2-FastText原理" class="headerlink" title="2.2 FastText原理"></a>2.2 FastText原理</h3><p>fastText方法包含三部分，模型架构，层次SoftMax和N-gram特征。用词向量的叠加代表文档向量，全连接之后softmax分类。</p>
<h4 id="2-2-1-模型架构"><a href="#2-2-1-模型架构" class="headerlink" title="2.2.1 模型架构"></a>2.2.1 模型架构</h4><p>fastText的架构和word2vec中的CBOW的架构类似，因为它们的作者都是Facebook的科学家Tomas Mikolov，而且确实fastText（2016）也算是words2vec（2014）所衍生出来的。<br>Continuous Bog-Of-Words： </p>
<p><img src="https://img-blog.csdnimg.cn/4e880a15d1b14c82a7f1797676a87bf8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f9891d5be1134c50aded04345f5e8b72.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>==隐藏层就是叠加后的句子（文档）向量==<br>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/375614469">《理解文本分类利器fastText》</a></p>
<ul>
<li>序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 </li>
<li>fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。</li>
<li>所以fastText只有CBOW模型，对应fastText.train_supervised 没有model参数。 Word2Vec有两种模型，所以fastText.train_unsupervised可以选择model={cbow, skipgram} ，默认skipgram。</li>
</ul>
<h4 id="2-2-2-层次SoftMax"><a href="#2-2-2-层次SoftMax" class="headerlink" title="2.2.2 层次SoftMax"></a>2.2.2 层次SoftMax</h4><p>层次softmax的基本思想是根据类别的频率构造霍夫曼树来代替扁平化的标准softmax。通过层次softmax，获得概率分布的时间复杂度可以从O(N)降至O(logN)。(多分类转成一系列二分类）</p>
<p>下图为层次softmax的一个具体示例：<br><img src="https://img-blog.csdnimg.cn/267e028268f346a184e211105c3fd76d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/eb9f5beaf0f14ec4b076c663037ad927.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>（见速通一书162页）</p>
<h4 id="2-2-3-N-gram特征"><a href="#2-2-3-N-gram特征" class="headerlink" title="2.2.3 N-gram特征"></a>2.2.3 N-gram特征</h4><p>&#8195;&#8195; n-gram解决词袋模型没有词序的问题，Hash解决n-gram膨胀问题。最大问题是有Hash冲突，但是实际中问题不大。</p>
<p>&#8195;&#8195; fastText 本身是词袋模型，为了分类的准确性，所以加入了 N-gram 特征提取词序信息。&lt;/font&gt;“我 爱 她”如果加入 2-Ngram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然啦，为了提高效率，我们需要过滤掉低频的 N-gram。<br>&#8195;&#8195; n-gram的问题是词表会急剧扩大，变为$|V|^n$，没有机器扛得住。所以使用散列法（Hash）对n-gram特征进行压缩。&lt;/font&gt;<br>&#8195;&#8195; Hash：使用Hash函数将字符串映射到某个整数。这样不管n-gram词表有多大，最后整数范围都是函数输出范围（比如4000亿词表。hash函数是对10526取余，最后输出就10526个数值，数值再转成向量）</p>
<h4 id="2-2-4-subword"><a href="#2-2-4-subword" class="headerlink" title="2.2.4 subword"></a>2.2.4 subword</h4><ul>
<li>word2vec中每个词都是一个基本信息单元，不可再切分。忽略了词内部特征。fasttext采样子词模型表示词，可以从词的构造上学习词义，解决未登录词的问题。</li>
<li>fasttext中子词的n-gram长度在minn和maxn之间。如果模型输入是ID之类的特征，子词没有任何意义，应取消子词。即minn=maxn=0。</li>
<li>中文中子词是两个相邻的字，英文中是词根和词缀。</li>
</ul>
<h4 id="2-2-5-fasttext文本分类总结"><a href="#2-2-5-fasttext文本分类总结" class="headerlink" title="2.2.5 fasttext文本分类总结"></a>2.2.5 fasttext文本分类总结</h4><ul>
<li>一个句子进行分词，每个词进行embedding转换成一个词向量，默认100维。</li>
<li>每个词按位相加成一个新的100维向量。再过一个全连接矩阵，100行(词向量维度)22列（分类数）</li>
<li>经过softmax得到每一类的类别概率。</li>
</ul>
<h2 id="三、fastText和word2vec对比总结"><a href="#三、fastText和word2vec对比总结" class="headerlink" title="三、fastText和word2vec对比总结"></a>三、fastText和word2vec对比总结</h2><h3 id="3-1-fastText和word2vec的区别"><a href="#3-1-fastText和word2vec的区别" class="headerlink" title="3.1 fastText和word2vec的区别"></a>3.1 fastText和word2vec的区别</h3><p>相似处：<br>1.图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。<br>2.都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。</p>
<p>不同处：<br>==word2vec用词预测词，而且是词袋模型，没有n-gram。fasttext用文章/句子词向量预测类别，加入了n-gram信息==。所以有：</p>
<ol>
<li>模型的输入层：word2vec的输入层，是 context window 内的词；而fasttext 对应的整个sentence的内容，包括word、n-gram、subword。</li>
<li>模型的输出层：word2vec的输出层，计算某个词的softmax概率最大；而fasttext的输出层对应的是 分类的label；</li>
<li>两者本质的不同，体现在 h-softmax的使用：<ul>
<li>word2vec用的负采样或者霍夫曼树解法（计算所有词概率，类别过大）。</li>
<li>fasttext用的softmsx全连接分类（类别少）</li>
</ul>
</li>
<li>word2vec主要目的的得到词向量，该词向量 最终是在输入层得到（不关注预测的结果准不准，因为霍夫曼树和负采样解法虽然优化了训练速度，但是分类结果没那么准了）。fasttext主要是做分类 ，虽然也会生成一系列的向量，但最终都被抛弃，不会使用。</li>
<li>word2vec有两种模型cbow和 skipgram，fasttext只有cbow模型。</li>
<li>word2vec属于监督模型，但是不需要标注样本。fasttext也属于监督模型，但是需要标注样本。</li>
</ol>
<h3 id="3-2-小结"><a href="#3-2-小结" class="headerlink" title="3.2 小结"></a>3.2 小结</h3><h4 id="3-2-1-fasttext适用范围"><a href="#3-2-1-fasttext适用范围" class="headerlink" title="3.2.1 fasttext适用范围"></a>3.2.1 fasttext适用范围</h4><p>总的来说，fastText的学习速度比较快，效果还不错。</p>
<ol>
<li>==fastText适用与分类类别比较大而且数据集足够多的情况，当分类类别比较小或者数据集比较少的话，很容易过拟合==。</li>
<li>适用于短文本。因为第一步是多个向量相加，文本越长，高频词越多，最后相加结果越趋于相同。（比如关键词只有那么几个，如果长文本词向量相加，关键词就被淹没了）如果非要用于长文本分类，就先去停用词或者干脆提取关键词（这个软件没有分开计算词的权重）<h4 id="3-2-2-fasttext应用场景"><a href="#3-2-2-fasttext应用场景" class="headerlink" title="3.2.2 fasttext应用场景"></a>3.2.2 fasttext应用场景</h4></li>
<li>可以完成无监督的词向量的学习，可以学习出来词向量，来保持住词和词之间，相关词之间是一个距离比较近的情况；</li>
<li>也可以用于有监督学习的文本分类任务，（新闻文本分类，垃圾邮件分类、情感分析中文本情感分析，电商中用户评论的褒贬分析）</li>
<li>封装的特别好，用了很多加速模块包括多线程实现。非常简单。Keras可以做模型，定制化，很灵活，但是需要自己搭。Fasttext任务单一，用起来方便。<h4 id="3-2-3-fastText优点"><a href="#3-2-3-fastText优点" class="headerlink" title="3.2.3 fastText优点"></a>3.2.3 fastText优点</h4>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</li>
<li>fastText在保持高精度的情况下加快了训练速度和测试速度</li>
<li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li>
<li>fastText两个重要的优化：Hierarchical Softmax、N-gram<br>训练代码中，如果电脑一开始训练就卡了，可以设置线程thread=2。（卡住只能kill进程ps - aux│grep python,kill – 9 1531(进程数）</li>
</ol>
<p>fasttext已经嵌入word2vec，可以用它做有监督和无监督（就是word2vec）。涉及到离散特征都可以用fasttext。比如招聘网站预测求职者和职位的匹配度。（求职者和职位分别提取关键词特征，然后用fasttext训练，输出录用和不录用的概率。但是求职者简历写本科就是本科学位，职位要求的本科是指本科及以上。二者还是有些不一样。需要把求职者关键字/标签加P，职位标签加J予以区分。即当数据来源不同纬度时，语义可能不同，前面加一个field予以区分）</p>
<h2 id="四、用gensim学习word2vec"><a href="#四、用gensim学习word2vec" class="headerlink" title="四、用gensim学习word2vec"></a>四、用gensim学习word2vec</h2><p>参考文档<a target="_blank" rel="noopener" href="https://maxiang.io/note/#%E4%B8%80-cbow%E4%B8%8Eskip-gram%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80">《word2vec原理和gensim实现》</a></p>
<h3 id="4-1-使用技巧"><a href="#4-1-使用技巧" class="headerlink" title="4.1 使用技巧"></a>4.1 使用技巧</h3><ol>
<li><p>用哪种方法看需求：<br>      1.使用时需要将多个向量相加（文本向量化） 用cbow<br>      2.使用时都是单个词向量使用（找近义词） 用skip-gram<br>     大原则：使用的过程和训练的过程越一致 ，效果一般越好<br>如果实在不知道怎么选，一般来说skip-gram+ns负采样效果好一点点。&lt;/font&gt;</p>
</li>
<li><p>同一批词分别进行两次训练，embedding也不在同一语义空间，不同语义空间的向量没有可比性。word2vec不能进行增量更新，有新词只能全量训练，因为语料库变了one-hot也变了，V也变了。&lt;/font&gt;</p>
</li>
<li>孤岛效应：有一堆词，明明不相关，训练出来确是显示相似的。<ul>
<li>某部分词总是一起出现，另一堆词也是一起出现，但是这两堆词互相没有任何交集，虽然在一起训练是一个向量空间，但实际上是两个向量空间。这两堆词互相比较是没有意义的。</li>
<li>孤岛效应本质是由一些不相关语料或者弱相关语料组成。Word2vec本身不能解决这个问题，这个只能在样本选取上下功夫，让训练样本尽可能相关。==所以各领域自己训练自己的，不要把一堆不相关的东西放到一起训练。几个行业几套词向量==。<h3 id="4-2-推荐系统中的Word2vec"><a href="#4-2-推荐系统中的Word2vec" class="headerlink" title="4.2 推荐系统中的Word2vec"></a>4.2 推荐系统中的Word2vec</h3></li>
</ul>
</li>
</ol>
<ul>
<li>word2vec可以计算向量之间的相似度，所以可以在其它领域广泛使用。比如视频分类</li>
<li>nlp和推荐系统中最大区别是nlp的词向量比较固定，而推荐系统中用户不断推陈出新，用户向量变化很快。</li>
<li>可以使用Hash技术，将用户ID(如手机设备号）进行hash作为类别。</li>
<li>将视频ID作为词，用户的点击序列作为句子（一连串视频），用word2vec对点击序列进行训练。最后每个视频ID对应一个embedding，用来计算不同视频的相似度，或者作为视频向量输入后续模型。<h2 id="五、-基于fastText实现文本分类"><a href="#五、-基于fastText实现文本分类" class="headerlink" title="五、 基于fastText实现文本分类"></a>五、 基于fastText实现文本分类</h2></li>
</ul>
<p>直接pip安装报错：“Microsoft Visual C++ 14.0 or greater is required”。在<a target="_blank" rel="noopener" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#fasttext">此页面</a>下载fasttext文件，然后安装：pip install C:\Users\LS\Downloads\fasttext-0.9.2-cp38-cp38-win_amd64.whl</p>
<p>FastText可以快速的在CPU上进行训练，最好的实践方法就是<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fastText/tree/master/python">github教程</a>，以及<a target="_blank" rel="noopener" href="https://fasttext.cc/docs/en/cheatsheet.html">官网教程</a>。</p>
<h3 id="5-1-fasttext参数："><a href="#5-1-fasttext参数：" class="headerlink" title="5.1 fasttext参数："></a>5.1 fasttext参数：</h3><p>参考官方文档<a target="_blank" rel="noopener" href="https://fasttext.cc/docs/en/python-module.html">《Python模块》</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52154254?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1400823417357139968&amp;utm_campaign=shareopn">《FastText代码详解》</a></p>
<ul>
<li>FUNCTIONS<br>  load_model(path)：加载给定文件路径的模型并返回模型对象。<br>  read_args(arg_list, arg_dict, arg_names, default_values)<br>  tokenize(text)：给定一串文本，对其进行标记并返回一个标记列表<br>  ==train_supervised(<em>kargs, **kwargs)：监督训练，样本包含标签，即fasttext==。<br>   train_unsupervised(</em>kargs, **kwargs)：无监督训练，样本没有标签，即word2vec。</li>
</ul>
<ul>
<li>fasttext.train_unsupervised函数：调用此函数学习词向量，即word2vec模型。<ul>
<li>维度 ( dim ) ：向量维度的大小，defult=100 ，也可以选100-300 。</li>
<li>子词是包含在最小大小 ( minn ) 和最大大小 ( maxn )之间的单词中的所有子字符串。默认minn=3， maxn=6。</li>
<li>minn和maxn分别代表subwords的最小长度和最大长度</li>
<li>bucket表示可容纳的subwords和wordNgrams的数量，可以理解成是它们存放的表，与word存放的表是分开的。</li>
<li>t表示过滤高频词的阈值，像”the”，”a”这种高频但语义很少的词应该过滤掉。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span>             <span class="comment"># training file path (required)</span></span><br><span class="line">model             <span class="comment"># unsupervised fasttext model &#123;cbow, skipgram&#125; [skipgram]</span></span><br><span class="line">lr                <span class="comment"># 学习率 [0.05]</span></span><br><span class="line">dim               <span class="comment"># 词向量维度 [100]</span></span><br><span class="line">ws                <span class="comment"># 上下文窗口大小 [5]</span></span><br><span class="line">epoch             <span class="comment"># 训练轮数 [5]</span></span><br><span class="line">minCount          <span class="comment"># 最少单词词频，过滤过少的单词 [5]</span></span><br><span class="line">minn              <span class="comment"># min length of char ngram [3]</span></span><br><span class="line">maxn              <span class="comment"># max length of char ngram [6]</span></span><br><span class="line">neg               <span class="comment"># 负采样个数 [5]</span></span><br><span class="line">wordNgrams        <span class="comment"># 词ngram最大长度 [1]</span></span><br><span class="line">loss              <span class="comment"># loss function &#123;ns, hs, softmax, ova&#125;[ns]</span></span><br><span class="line">                  <span class="comment">#（负采样、霍夫曼树、softmax和多分类采用多个二分类计算，即loss one-vs-all） </span></span><br><span class="line">bucket            <span class="comment"># number of buckets，放的是subwords [2000000]</span></span><br><span class="line">thread            <span class="comment"># cpu线程 [number of cpus]</span></span><br><span class="line">lrUpdateRate      <span class="comment"># change the rate of updates for the learning rate，实现阶梯动态学习率 [100]</span></span><br><span class="line">t                 <span class="comment"># sampling threshold，过滤高频词，越大被保留的概率越大 [0.0001]</span></span><br><span class="line">verbose           <span class="comment"># verbose [2]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>train_supervised 参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span>             <span class="comment"># training file path (required)</span></span><br><span class="line">lr                <span class="comment"># 学习率 [0.05]</span></span><br><span class="line">dim               <span class="comment"># 词向量维度 [100]</span></span><br><span class="line">ws                <span class="comment"># 上下文窗口大小 [5]</span></span><br><span class="line">epoch             <span class="comment"># 训练轮数 [5]</span></span><br><span class="line">minCount          <span class="comment"># 最小词频 [1]</span></span><br><span class="line">minCountLabel     <span class="comment"># minimal number of label occurences [1]</span></span><br><span class="line">minn              <span class="comment"># min length of char ngram [0]</span></span><br><span class="line">maxn              <span class="comment"># max length of char ngram [0]</span></span><br><span class="line">neg               <span class="comment"># 负采样个数 [5]</span></span><br><span class="line">wordNgrams        <span class="comment"># n-gram [1]</span></span><br><span class="line">loss              <span class="comment"># loss function &#123;ns, hs, softmax, ova&#125; [softmax]</span></span><br><span class="line">bucket            <span class="comment"># number of buckets [2000000]</span></span><br><span class="line">thread            <span class="comment"># cpu线程数 [number of cpus]</span></span><br><span class="line">lrUpdateRate      <span class="comment"># change the rate of updates for the learning rate [100]</span></span><br><span class="line">t                 <span class="comment"># sampling threshold [0.0001]</span></span><br><span class="line">label             <span class="comment"># 标签前缀 [&#x27;__label__&#x27;]</span></span><br><span class="line">verbose           <span class="comment"># verbose [2]</span></span><br><span class="line">pretrainedVectors <span class="comment"># 从 (.vec file)加载预训练的词向量，用于监督训练 []</span></span><br></pre></td></tr></table></figure>
<p>model属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_dimension           <span class="comment"># 获取向量（隐藏层）的维度（大小）.这等价于 `dim` 属性           </span></span><br><span class="line">get_input_vector        <span class="comment"># 给定一个索引，得到输入矩阵对应的向量 </span></span><br><span class="line">get_input_matrix        <span class="comment"># 获取模型的完整输入矩阵的副本</span></span><br><span class="line">get_labels              <span class="comment"># 获取字典的整个标签列表，这相当于 `labels` 属性。</span></span><br><span class="line">get_line                <span class="comment"># 将一行文本拆分为单词和标签</span></span><br><span class="line">get_output_matrix       <span class="comment"># 获取模型的完整输出矩阵的副本。</span></span><br><span class="line">get_sentence_vector     <span class="comment"># 给定一个字符串，获得向量表示。这个函数</span></span><br><span class="line">                        <span class="comment"># assumes to be given a single line of text. We split words on</span></span><br><span class="line">                        <span class="comment"># whitespace (space, newline, tab, vertical tab) and the control</span></span><br><span class="line">                        <span class="comment"># characters carriage return, formfeed and the null character.</span></span><br><span class="line">get_subword_id          <span class="comment"># 给定一个subword，获取字典中的词 id hashes to.</span></span><br><span class="line">get_subwords            <span class="comment"># 给定一个词，获取子词及其索引。</span></span><br><span class="line">get_word_id             <span class="comment"># 给定一个词，获取字典中的词 id</span></span><br><span class="line">get_word_vector         <span class="comment"># 获取训练好的词向量。</span></span><br><span class="line">get_words               <span class="comment"># 获取字典的整个单词列表，这相当于 `words` 属性。</span></span><br><span class="line">is_quantized            <span class="comment"># 模型是否已经量化过</span></span><br><span class="line">predict                 <span class="comment"># 给定一个字符串，得到一个标签列表和一个对应概率列表</span></span><br><span class="line">quantize                <span class="comment"># 量化模型，减少模型的大小和内存占用</span></span><br><span class="line">save_model              <span class="comment"># 保存模型</span></span><br><span class="line">test                    <span class="comment"># Evaluate supervised model using file given by path</span></span><br><span class="line">test_label              <span class="comment"># 返回每个标签的准确率和召回率。</span></span><br></pre></td></tr></table></figure>
<h3 id="5-2-基本使用"><a href="#5-2-基本使用" class="headerlink" title="5.2 基本使用"></a>5.2 基本使用</h3><p>当 fastText 运行时，进度和预计完成时间会显示在您的屏幕上。训练完成后，model变量包含有关训练模型的信息，可用于查询：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_unsupervised(<span class="string">&#x27;data/fil9&#x27;</span>)<span class="comment">#维基百科文件</span></span><br><span class="line">model.words</span><br><span class="line"></span><br><span class="line">[<span class="string">u&#x27;the&#x27;</span>, <span class="string">u&#x27;of&#x27;</span>, <span class="string">u&#x27;one&#x27;</span>, <span class="string">u&#x27;zero&#x27;</span>, <span class="string">u&#x27;and&#x27;</span>, <span class="string">u&#x27;in&#x27;</span>, <span class="string">u&#x27;two&#x27;</span>, <span class="string">u&#x27;a&#x27;</span>, <span class="string">u&#x27;nine&#x27;</span>, <span class="string">u&#x27;to&#x27;</span>, <span class="string">u&#x27;is&#x27;</span>, ...</span><br></pre></td></tr></table></figure>
<p>==获得词向量==：（它返回词汇表中的所有单词，按频率递减排序。）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.get_word_vector(<span class="string">&quot;the&quot;</span>)</span><br><span class="line">array([-<span class="number">0.03087516</span>,  <span class="number">0.09221972</span>,  <span class="number">0.17660329</span>,  <span class="number">0.17308897</span>,  <span class="number">0.12863874</span>,</span><br><span class="line">        <span class="number">0.13912526</span>, -<span class="number">0.09851588</span>,  <span class="number">0.00739991</span>,  <span class="number">0.37038437</span>, -<span class="number">0.00845221</span>,</span><br><span class="line">        ...</span><br><span class="line">       -<span class="number">0.21184735</span>, -<span class="number">0.05048715</span>, -<span class="number">0.34571868</span>,  <span class="number">0.23765688</span>,  <span class="number">0.23726143</span>],</span><br><span class="line">      dtype=float32)</span><br></pre></td></tr></table></figure>
<p>==保存模型（二进制），后续加载==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_model(<span class="string">&quot;result/fil9.bin&quot;</span>)</span><br><span class="line">model = fasttext.load_model(<span class="string">&quot;result/fil9.bin&quot;</span>)</span><br></pre></td></tr></table></figure><br>cobw和skipgram：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_unsupervised(<span class="string">&#x27;data/fil9&#x27;</span>, <span class="string">&quot;cbow&quot;</span>)</span><br></pre></td></tr></table></figure><br>==预测结果==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取测试集，预测模型输出</span></span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">10000</span>)</span><br><span class="line">results=[model.predict(x)  <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">results</span><br><span class="line"></span><br><span class="line">[((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.99827653</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__11&#x27;</span>,), array([<span class="number">0.84706676</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__3&#x27;</span>,), array([<span class="number">0.99988556</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.99980879</span>])),</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">((<span class="string">&#x27;__label__2&#x27;</span>,), array([<span class="number">0.9998678</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__1&#x27;</span>,), array([<span class="number">0.87650901</span>])),</span><br><span class="line"> ((<span class="string">&#x27;__label__3&#x27;</span>,), array([<span class="number">1.00001013</span>])),</span><br><span class="line"> ...]</span><br></pre></td></tr></table></figure><br>所以输出结果是带前缀的标签和分类概率。想只得到类别，可以这样写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result=[model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">result</span><br><span class="line">[<span class="string">&#x27;2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;11&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;9&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;12&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">...]</span><br></pre></td></tr></table></figure>
<h3 id="5-3-bin格式词向量转换为vec格式"><a href="#5-3-bin格式词向量转换为vec格式" class="headerlink" title="5.3 bin格式词向量转换为vec格式"></a>5.3 bin格式词向量转换为vec格式</h3><p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/huyidu/article/details/112712526?utm_source=app&amp;app_version=4.16.0&amp;code=app_1562916241&amp;uLinkId=usr1mkqgl919blen">《fasttext训练的bin格式词向量转换为vec格式词向量》</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载的fasttext预训练词向量都是vec格式的，但fasttext无监督训练后却是bin格式，因此需要进行转换</span></span><br><span class="line"><span class="comment"># 以下代码为fasttext官方推荐：</span></span><br><span class="line"><span class="comment"># 请将以下代码保存在bin_to_vec.py文件中</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, absolute_import, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fasttext <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 整个代码逻辑非常简单</span></span><br><span class="line">    <span class="comment"># 以bin格式的模型为输入参数</span></span><br><span class="line">    <span class="comment"># 按照vec格式进行文本写入</span></span><br><span class="line">    <span class="comment"># 可通过head -5 xxx.vec进行文件查看</span></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=(<span class="string">&quot;Print fasttext .vec file to stdout from .bin file&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;model&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Model to use&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    f = load_model(args.model)</span><br><span class="line">    words = f.get_words()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(<span class="built_in">len</span>(words)) + <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(f.get_dimension()))</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        v = f.get_word_vector(w)</span><br><span class="line">        vstr = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> vi <span class="keyword">in</span> v:</span><br><span class="line">            vstr += <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(vi)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="built_in">print</span>(w + vstr)</span><br><span class="line">        <span class="keyword">except</span> IOError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">if</span> e.errno == errno.EPIPE:</span><br><span class="line">                <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开cmd，在bin_to_vec.py路径下执行该命令，生成unsupervised_data.vec</span></span><br><span class="line">python bin_to_vec.py word15000.<span class="built_in">bin</span> &gt; word15000.vec</span><br></pre></td></tr></table></figure>
<p>==在实践中，我们观察到 skipgram 模型在处理子词信息方面比 cbow 更好==</p>
<h2 id="六、新闻文本分类——fasttext"><a href="#六、新闻文本分类——fasttext" class="headerlink" title="六、新闻文本分类——fasttext"></a>六、新闻文本分类——fasttext</h2><h3 id="6-1-正常fasttext分类"><a href="#6-1-正常fasttext分类" class="headerlink" title="6.1 正常fasttext分类"></a>6.1 正常fasttext分类</h3><p>单纯的fasttext分类，参数用讨论区默认参数，没有调整。分数0.9151。<br>fasttext训练很快，大概十来分钟吧。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>]=<span class="string">&#x27;__label__&#x27;</span>+train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;./train.csv&#x27;</span>,index=<span class="literal">None</span>,header=<span class="literal">None</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model=fasttext.train_supervised(<span class="string">&#x27;./train.csv&#x27;</span>,lr=<span class="number">1.0</span>,wordNgrams=<span class="number">2</span>, </span><br><span class="line">verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">25</span>,loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">result=[model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line">result[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:result&#125;).to_csv(<span class="string">&#x27;fasttext.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><br>最终上传，得分0.9151。<br>调整部分参数后，最终得分0.9358。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=fasttext.train_supervised(<span class="string">&#x27;./train.csv&#x27;</span>,lr=<span class="number">0.8</span>,wordNgrams=<span class="number">3</span>, </span><br><span class="line">verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">25</span>,loss=<span class="string">&quot;softmax&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="6-2-小数据集：word2vec-fasttext-首尾截断"><a href="#6-2-小数据集：word2vec-fasttext-首尾截断" class="headerlink" title="6.2 小数据集：word2vec+fasttext+首尾截断"></a>6.2 小数据集：word2vec+fasttext+首尾截断</h3><p>首先拿15000条数据进行试验，前10000条fasttext训练，后5000条测试，代码见讨论区：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.64063dadaDY31g&amp;postId=118255">《Task4 基于深度学习的文本分类1-fastText》</a>（其实就是上面代码改了点数据集）：</p>
<ol>
<li>试验正常fasttext效果，f1 score=0.8272<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;../data/train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, nrows=<span class="number">15000</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].iloc[:-<span class="number">5000</span>].to_csv(<span class="string">&#x27;train.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_supervised(<span class="string">&#x27;train.csv&#x27;</span>, lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">25</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">val_pred = [model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li>试验word2vec+fasttext效果，f1 score=0.8426<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先进行word2vec训练，含全部15000条数据</span></span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train15000.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">model1 = fasttext.train_unsupervised(<span class="string">&#x27;train15000.csv&#x27;</span>, lr=<span class="number">0.1</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">8</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#保存模型转为词向量</span></span><br><span class="line">model1.save_model(<span class="string">&quot;word15000.bin&quot;</span>)</span><br><span class="line"><span class="comment">#cmd命令行执行python bin_to_vec.py result1000.bin &lt; result1000.vec，转换为vec词向量</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#fasttext进行训练，词向量为前一步训练好的词向量，训练数据为10000条</span></span><br><span class="line">model2 = fasttext.train_supervised(<span class="string">&#x27;train.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word15000.vec&#x27;</span>,lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line"><span class="comment">#                                  verbose=2, minCount=1, epoch=16, loss=&quot;hs&quot;)</span></span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">val_pred = [model2.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure>
<ol>
<li>试验首尾截断效果，f1 score=0.8222(首尾各50词），0.8304（首尾各100词）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断实验效果</span></span><br><span class="line"><span class="comment">#准备将text文本首尾截断，各取100tokens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slipt2</span>(<span class="params">x</span>):</span></span><br><span class="line">  ls=x.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">  le=<span class="built_in">len</span>(ls)</span><br><span class="line">  <span class="keyword">if</span> le&lt;<span class="number">201</span>:</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(ls[:<span class="number">100</span>]+ls[-<span class="number">100</span>:])</span><br><span class="line">    </span><br><span class="line">trains_df[<span class="string">&#x27;summary&#x27;</span>]=trains_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">train_df[[<span class="string">&#x27;summary&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].iloc[:-<span class="number">5000</span>].to_csv(<span class="string">&#x27;trains_summary10000.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model3 = fasttext.train_supervised(<span class="string">&#x27;trains_summary10000.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word15000.vec&#x27;</span>,lr=<span class="number">1.0</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">16</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">val_pred = [model3.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[-<span class="number">5000</span>:][<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[-<span class="number">5000</span>:].astype(<span class="built_in">str</span>), val_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="6-3-全数据集：word2vec-fasttext-首尾截断"><a href="#6-3-全数据集：word2vec-fasttext-首尾截断" class="headerlink" title="6.3 全数据集：word2vec+fasttext+首尾截断"></a>6.3 全数据集：word2vec+fasttext+首尾截断</h3><ol>
<li>数据处理<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取训练测试集数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为FastText需要的格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[[<span class="string">&#x27;text&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train_20w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_df = pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">df=pd.concat([train_df,test_df])</span><br><span class="line">df[[<span class="string">&#x27;text&#x27;</span>]].to_csv(<span class="string">&#x27;train_25w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>用word2vec进行train+test数据的词向量训练，这一步花了2个小时。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line"></span><br><span class="line">model1 = fasttext.train_unsupervised(<span class="string">&#x27;train_25w.csv&#x27;</span>, lr=<span class="number">0.1</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">8</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line">model1.save_model(<span class="string">&quot;word_25w.bin&quot;</span>)</span><br><span class="line"><span class="comment">#cmd下运行python bin_to_vec.py word_25w.bin &gt; word_25w.vec</span></span><br></pre></td></tr></table></figure></li>
<li>fasttext进行有监督训练，相当于分类微调。最终上传，得分0.9162，吐血。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2=fasttext.train_supervised(<span class="string">&#x27;train_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,lr=<span class="number">0.8</span>, wordNgrams=<span class="number">2</span>, verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">18</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_df = pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test_pred = [model2.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;text&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_pred&#125;).to_csv(<span class="string">&#x27;word_fast.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></li>
<li>接下来进行首尾截断测试：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断进行训练</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_df[<span class="string">&#x27;label_ft&#x27;</span>] = <span class="string">&#x27;__label__&#x27;</span> + train_df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">train_df[<span class="string">&#x27;summary&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">train_df[[<span class="string">&#x27;summary&#x27;</span>,<span class="string">&#x27;label_ft&#x27;</span>]].to_csv(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>, index=<span class="literal">None</span>, header=<span class="literal">None</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model3 = fasttext.train_supervised(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,lr=<span class="number">0.8</span>, wordNgrams=<span class="number">2</span>, </span><br><span class="line">                                  verbose=<span class="number">2</span>, minCount=<span class="number">1</span>, epoch=<span class="number">18</span>, loss=<span class="string">&quot;hs&quot;</span>)</span><br><span class="line"><span class="comment">#预测结果</span></span><br><span class="line">test_df[<span class="string">&#x27;summary&#x27;</span>]=test_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">test_pred = [model3.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">&#x27;__&#x27;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> test_df[<span class="string">&#x27;summary&#x27;</span>]]</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:test_pred&#125;).to_csv(<span class="string">&#x27;word_fast_cut.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>最终得分0.9203，至少证明了长文本分类，数据集够多的时候，进行部分截断比较好。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据量</th>
<th>fasttext</th>
<th>word2vec+fasttext</th>
<th>word2vec+fasttext+首尾截断</th>
</tr>
</thead>
<tbody>
<tr>
<td>10000+5000</td>
<td>0.8272</td>
<td>0.8426</td>
<td>0.8304</td>
</tr>
<tr>
<td>20w+5w</td>
<td>0.9151（没调参）</td>
<td>0.9162（没调参）</td>
<td>0.9203（没调参）</td>
</tr>
<tr>
<td>20w+5w</td>
<td>0.9358（已调参）</td>
<td></td>
<td>0.9421（已调参）</td>
</tr>
</tbody>
</table>
</div>
<p>截断比不截断高0.4-0.6个点。</p>
<ol>
<li>下面是部分调参记录<br>继续首尾截断试验，训练集前19w为悬链数据，最后1w为测试数据。</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>首尾截断</th>
<th>f1</th>
<th>loss</th>
<th>n-gram</th>
</tr>
</thead>
<tbody>
<tr>
<td>各30，同时epoch=18，lr=0.8，下同</td>
<td>0.9190</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>各30</td>
<td>0.9352</td>
<td>softmax</td>
<td>2</td>
</tr>
<tr>
<td>各30</td>
<td>0.9388</td>
<td>softmax</td>
<td>3</td>
</tr>
<tr>
<td>各30</td>
<td>0.9382</td>
<td>softmax</td>
<td>4</td>
</tr>
<tr>
<td>各30</td>
<td></td>
<td>softmax</td>
<td>5</td>
</tr>
<tr>
<td>各30，同时epoch=18，lr=0.5</td>
<td></td>
<td>softmax</td>
<td>4</td>
</tr>
<tr>
<td>各30，同时epoch=27，lr=0.5</td>
<td></td>
<td>softmax</td>
<td>4</td>
</tr>
<tr>
<td>——-</td>
<td>——-</td>
<td>——-</td>
<td>——-</td>
</tr>
<tr>
<td>各50</td>
<td>0.9192</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>各80</td>
<td>0.9170</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>各100</td>
<td>0.9200/0.9184</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>各150</td>
<td>0.9226</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>各150</td>
<td>0.9371</td>
<td>softmax</td>
<td>2</td>
</tr>
<tr>
<td>各150</td>
<td>0.9436</td>
<td>softmax</td>
<td>3</td>
</tr>
<tr>
<td>各150</td>
<td>0.9417</td>
<td>softmax</td>
<td>4</td>
</tr>
<tr>
<td>各200</td>
<td>0.9212</td>
<td>hs</td>
<td>2</td>
</tr>
<tr>
<td>不截断</td>
<td>0.9158</td>
<td>hs</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>不截断，加和平均的词向量太多，无用信息冲淡了关键信息。<br>fasttext分类的loss必须选择softmex，不需要hs和ng，因为类别少。<br>n-gram中，n增大可以表示一部分词序，有利于文本表征。但是太大的话，词向量和n-gram向量太多，分类效果也不好（参数过多学不好或者是无用信息过多）。</p>
<p>初步选择以下参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首尾截断各150个词</span></span><br><span class="line">model3=fasttext.train_supervised(<span class="string">&#x27;train_summary_20w.csv&#x27;</span>,pretrainedVectors=<span class="string">&#x27;word_25w.vec&#x27;</span>,</span><br><span class="line">lr=<span class="number">0.8</span>,wordNgrams=<span class="number">3</span>,verbose=<span class="number">2</span>,minCount=<span class="number">1</span>,epoch=<span class="number">18</span>,loss=<span class="string">&quot;softmax&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最终分数f1=0.9421。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/11/27/读书笔记/学习笔记5：word2vec和fasttext/">https://zhxnlp.github.io/2021/11/27/读书笔记/学习笔记5：word2vec和fasttext/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/DNN/">DNN</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/"><i class="fa fa-chevron-left">  </i><span>学习笔记6：卷积神经网络CNN</span></a></div><div class="next-post pull-right"><a href="/2021/11/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN2/"><span>学习笔记4：深度学习DNN2</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>