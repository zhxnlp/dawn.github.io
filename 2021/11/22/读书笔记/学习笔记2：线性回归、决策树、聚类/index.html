<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="学习笔记2：线性回归、决策树、聚类"><meta name="keywords" content="机器学习,线性回归,决策树,聚类"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>学习笔记2：线性回归、决策树、聚类 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E8%81%9A%E7%B1%BB"><span class="toc-text">学习笔记2——线性回归、决策树、聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">一、.线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-sklearn-metrics"><span class="toc-text">3.sklearn.metrics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-PolynomialFeatures%E6%9E%84%E5%BB%BA%E7%89%B9%E5%BE%81"><span class="toc-text">4.PolynomialFeatures构建特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84random-state%E5%8F%82%E6%95%B0"><span class="toc-text">5.机器学习中的random_state参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Solver-lbfgs-supports-only-%E2%80%9Cl2%E2%80%9D-or-%E2%80%9Cnone%E2%80%9D-penalties-got-l1-penalty-%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="toc-text">5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-text">二、决策树和随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%80%A7%EF%BC%9A"><span class="toc-text">2.1随机森林的随机性：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-text">2.2优缺点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3%E8%B0%83%E5%8F%82"><span class="toc-text">2.3调参</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E8%81%9A%E7%B1%BB"><span class="toc-text">三、聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-kmeans"><span class="toc-text">3.2 kmeans</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-DBSCAN%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-text">3.3 DBSCAN聚类算法</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">48</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">学习笔记2：线性回归、决策树、聚类</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.2k</span><span class="post-meta__separator">|</span><span>阅读时长: 10 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="学习笔记2——线性回归、决策树、聚类"><a href="#学习笔记2——线性回归、决策树、聚类" class="headerlink" title="学习笔记2——线性回归、决策树、聚类"></a>学习笔记2——线性回归、决策树、聚类</h2><h2 id="一、-线性回归"><a href="#一、-线性回归" class="headerlink" title="一、.线性回归"></a>一、.线性回归</h2><pre><code>fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数。
normalize : 布尔型参数，若为True，则X在回归前进行归一化。可选参数。默认值为False。
copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为True。
n_jobs : 整型参数，表示用于计算的作业数量；若为-1，则用所有的CPU。可选参数。默认为1
positive=False#当设置为&#39;True&#39;时，强制系数为正。这选项仅支持密集阵列。

rint(model.coef_)#打印线性方程中的w
print(model.intercept_)#打印w0 就是线性方程中的截距b
</code></pre><span id="more"></span>
<h4 id="3-sklearn-metrics"><a href="#3-sklearn-metrics" class="headerlink" title="3.sklearn.metrics"></a>3.sklearn.metrics</h4><p>模块包括评分函数、性能指标和成对度量和距离计算。<br>F1-score:    2<em>(P</em>R)/(P+R)。<a target="_blank" rel="noopener" href="https://www.cnblogs.com/techengin/p/8962024.html">参考《sklearn中 F1-micro 与 F1-macro区别和计算原理》</a><br>导入：from sklearn import metrics<br>分类指标</p>
<pre><code>accuracy_score(y_true, y_pre)#精度

log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)交叉熵损失函数

auc(x, y, reorder=False)
ROC曲线下的面积;较大的AUC代表了较好的performance。
AUC：roc_auc_score(y_true, y_score, average=‘macro’, sample_weight=None)

f1_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) F1值

precision_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’,) 查准率

recall_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) 查全率
roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)
计算ROC曲线的横纵坐标值，TPR，FPR
TPR = TP/(TP+FN) = recall(真正例率，敏感度)
FPR = FP/(FP+TN)(假正例率，1-特异性)

classification_report(y_true, y_pred)#分类结果分析汇总
</code></pre><p>f1_score中关于参数average的用法描述:<br>‘micro’:Calculate metrics globally by counting the total true positives, false negatives and false positives.</p>
<p>‘micro’:通过先计算总体的TP，FN和FP的数量，再计算F1</p>
<p>‘macro’:Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p>
<p>‘macro’:分布计算每个类别的F1，然后做平均（各类别F1的权重相同）<br>回归指标</p>
<pre><code>explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)
回归方差(反应自变量与因变量之间的相关程度)
mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)
平均绝对误差MAE

mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #均方差MSE

median_absolute_error(y_true, y_pred)
中值绝对误差
r2_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #R平方值
</code></pre><h4 id="4-PolynomialFeatures构建特征"><a href="#4-PolynomialFeatures构建特征" class="headerlink" title="4.PolynomialFeatures构建特征"></a>4.PolynomialFeatures构建特征</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/tiange_xiao/article/details/79755793">https://blog.csdn.net/tiange_xiao/article/details/79755793</a><br>使用sklearn.preprocessing.PolynomialFeatures来进行特征的构造。<br>degree：控制多项式的度<br>interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2。<br>include_bias：默认为True。如果为True的话，那么就会有上面的 1那一项。</p>
<h4 id="5-机器学习中的random-state参数"><a href="#5-机器学习中的random-state参数" class="headerlink" title="5.机器学习中的random_state参数"></a>5.机器学习中的random_state参数</h4><p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/ytomc/article/details/113437926">https://blog.csdn.net/ytomc/article/details/113437926</a></p>
<pre><code>1、在构建模型时：
    forest = RandomForestClassifier(n_estimators=100, random_state=0)
    forest.fit(X_train, y_train)
2、在生成数据集时：
    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)

3、在拆分数据集为训练集、测试集时：
    X_train, X_test, y_train, y_test = train_test_split(
cancer.data, cancer.target, stratify=cancer.target, random_state=42)
    参数test_size：如果是浮点数，在0-1之间，表示test set的样本占比；如果是整数的话就表示test set样本数量。
</code></pre><p>例如1中，每次构建的模型是不同的。<br>例如2中，每次生成的数据集是不同的。<br>例如3中，每次拆分出的训练集、测试集是不同的。</p>
<h4 id="5-Solver-lbfgs-supports-only-“l2”-or-“none”-penalties-got-l1-penalty-解决办法"><a href="#5-Solver-lbfgs-supports-only-“l2”-or-“none”-penalties-got-l1-penalty-解决办法" class="headerlink" title="5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法"></a>5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法</h4><p>在用以下代码建立逻辑回归模型的时候<br>lr = LogisticRegression(C = c_param, penalty = ‘l1’)，正则化惩罚选择’L1’报错。<br>LogisticRegression的参数如下：</p>
<pre><code>LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, l1_ratio=None, max_iter=100,multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l1&#39;,random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0,warm_start=False)
</code></pre><p>我们看solver参数，这个参数定义的是分类器，‘newton-cg’，‘sag’和‘lbfgs’等solvers仅支持‘L2’regularization，‘liblinear’ solver同时支持‘L1’、‘L2’regularization，若dual=Ture，则仅支持L2 penalty。<br>决定惩罚项选择的有2个参数：dual和solver，如果要选L1范数，dual必须是False，solver必须是liblinear<br>因此，我们只需将solver=’liblinear’参数添加进去即可</p>
<pre><code>lr = LogisticRegression(C = c_param, penalty = ‘l1’,solver=‘liblinear’)
</code></pre><p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/kakak_/article/details/104923634">https://blog.csdn.net/kakak_/article/details/104923634</a></p>
<h2 id="二、决策树和随机森林"><a href="#二、决策树和随机森林" class="headerlink" title="二、决策树和随机森林"></a>二、决策树和随机森林</h2><p>随机森林算法详解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/139510947">https://zhuanlan.zhihu.com/p/139510947</a><br>&#8195;&#8195;GBDT、XGBOOST、LGBM都是以决策树为积木搭建出来的。<br>&#8195;&#8195;随机森林就是决策树们基于bagging集成学习思想搭建起来的。算法实现思路非常简单，只需要记住一句口诀：抽等量样本，选几个特征，构建多棵树。</p>
<h4 id="2-1随机森林的随机性："><a href="#2-1随机森林的随机性：" class="headerlink" title="2.1随机森林的随机性："></a>2.1随机森林的随机性：</h4><p>&#8195;&#8195;数据集的随机选取、每棵树所使用特征的随机选取。以上两个3.1随机性使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。<br>1）抽等量样本<br>&#8195;&#8195;抽样方式一般是有放回的抽样，也就是说，在训练某棵树的时候，这一次被抽到的样本会被放回数据集中，下一次还可能被抽到，因此，原训练集中有的样本会多次被抽出用来训练，而有的样本可能不会被使用到。<br>但是不用担心有的样本没有用到，只要训练的树的棵数足够多，大多数训练样本总会被取到的。有极少量的样本成为漏网之鱼也不用担心，后边我们会筛选他们出来用来测试模型。</p>
<p>2）选几个特征（“max_features”）<br>&#8195;&#8195;在训练某棵树的时候，会随机选择一部分特征用来训练。这样做的目的就是让不同的树重点关注不同的特征。</p>
<p>3）构建多棵树（“n_estimators”）<br>&#8195;&#8195;最终的结果由每棵决策树综合给出：如果是分类问题，所有树投票决定；如果是回归问题，各个树得到的结果加权平均。（每个树的结果是叶节点的均值，预测房价，就是样本输入模型分到某个节点，这个叶节点所以=有房子价格的均值。一棵树m个叶节点，就只有m个输出）所以随机森林做回归比较少。</p>
<h4 id="2-2优缺点："><a href="#2-2优缺点：" class="headerlink" title="2.2优缺点："></a>2.2优缺点：</h4><p>1）实现简单，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；<br>2）相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；<br>3）能直接特征很多的高维数据，因为在训练过程中依旧会从这些特征中随机选取部分特征用来训练；<br>4）相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；<br>5）训练完成后可以给出特征重要性。当然，这个优点主要来源于决策树。因为决策树在训练过程中会计算熵或者是基尼系数，越往树的根部，特征越重要。<br>2、缺点<br>1）在噪声过大的分类和处理回归问题时还是容易过拟合；<br>2）相比于单一决策树，它的随机性让我们难以对模型进行解释。</p>
<h4 id="2-3调参"><a href="#2-3调参" class="headerlink" title="2.3调参"></a>2.3调参</h4><p>1、用于调参的参数：<br>max_features（最大特征数）: 这个参数用来训练每棵树时需要考虑的最大特征个数，超过限制个数的特征都会被舍弃，默认为auto。可填入的值有：int值，float（特征总数目的百分比），“auto”/“sqrt”（总特征个数开平方取整）,“log2”（总特征个数取对数取整）。默认值为总特征个数开平方取整。值得一提的是，这个参数在决策树中也有但是不重要，因为其默认为None，即有多少特征用多少特征。为什么要设置这样一个参数呢？原因如下：考虑到训练模型会产生多棵树，如果在训练每棵树的时候都用到所有特征，以来导致运算量加大，二来每棵树训练出来也可能千篇一律，没有太多侧重，所以，设置这个参数，使训练每棵树的时候只随机用到部分特征，在减少整体运算的同时还可以让每棵树更注重自己选到的特征。</p>
<p>n_estimators：随机森林生成树的个数，默认为100。</p>
<p>2、控制样本抽样参数：<br>bootstrap：每次构建树是不是采用有放回样本的方式(bootstrap samples)抽取数据集。可选参数：True和False，默认为True。<br>oob_score：是否使用袋外数据来评估模型，默认为False。<br>boostrap和 oob_score两个参数一般要配合使用。如果boostrap是False，那么每次训练时都用整个数据集训练，如果boostrap是True，那么就会产生袋外数据。<br>选择criterion参数（决策树划分标准）<br>和决策树一样，这个参数只有两个参数 ‘entropy’（熵） 和 ‘gini’（基尼系数）可选，默认为gini</p>
<p>有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能被忽略，一般来说，每一次抽样，某个样本被抽到的概率是 1/n ，所以不被抽到的概率就是 1-1/n ,所以n个样本都不被抽到的概率就是：<br>用洛必达法则化简，可以得到这个概率收敛于(1/e)，约等于0.37。<br>因此，如果数据量足够大的时候，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。<br>为了这些数据不被浪费，我们也可以把他们用来作为集成算法的测试集。也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。</p>
<h3 id="三、聚类"><a href="#三、聚类" class="headerlink" title="三、聚类"></a>三、聚类</h3><p>见笔记<br>聚类是指试图将相似的数据点分组到人工确定的组或簇中</p>
<ol>
<li>聚类的基本思想：对于给定的M个样本的数据集，给定聚类（簇）的个数K（K&lt;M），初始化每个样本所属的类别，再根据一定的规则不断地迭代并重新划分数据集的类别（改变样本与簇的类别关系），使得每一次的划分都比上一次的划分要好。</li>
<li>聚类算法有很多种，主要分为划分聚类（KMeans）、密度聚类（DBSCAN）和谱聚类等三种聚类。<h4 id="3-2-kmeans"><a href="#3-2-kmeans" class="headerlink" title="3.2 kmeans"></a>3.2 kmeans</h4></li>
<li>KMeans算法的思想：对于给定的M个样本的数据集（无标签），给定聚类（簇）的个数K（K&lt;M），初始化每个样本所属的类别，再根据距离的不同，将每个样本分配到距离最近的中心点的簇中，然后再对迭代完成的每个簇更新中心点位置（改变样本与簇的类别关系），直到达到终止条件为止。</li>
<li>KMeans算法的终止条件：1）迭代次数    2）簇中心点变化率    3）最小平方误差值</li>
<li><p>KMeans算法的优点：理解简单容易，凸聚类的效果不错，效率高；对于服从高斯分布的数据集效果相当好。</p>
<ol>
<li>KMeans算法的缺点：</li>
</ol>
</li>
</ol>
<ul>
<li>对初始点敏感（局部最优）</li>
<li>被异常点影响（异常点影响聚类中心值，聚类边界点有可能被分到另一类，异常点很难清洗）</li>
<li>某些场景中心点缺乏物理意义（例如分类性别以0和1表示，中心点可能是小数）</li>
<li>数值问题（例如以人的身高体重做聚类，不同单位数值差别很大。例如身高以m为单位基本差别很小，以g为单位差别很大，此时基本由体重主导了，所以每一维要做归一化。可以让方差为1或者都除以最大值）</li>
<li>K值不好选择<br>K-means本身有很多问题，因为没有数据标注，是没有办法的办法</li>
</ul>
<p>伪代码：</p>
<p>初始化K个中心点（一般随机选择）：</p>
<pre><code>当中心点变化大于给定值或者小于迭代次数时：
    对于数据集中的每个点：
        对于每个中心点：
            计算数据点到中心点的距离
        将数据点分配到距离最近的簇中
    对于每个簇，计算簇中所有数据点的平均值，更新簇中心点的位置
返回最终的簇中心点和对应的簇
</code></pre><h4 id="3-3-DBSCAN聚类算法"><a href="#3-3-DBSCAN聚类算法" class="headerlink" title="3.3 DBSCAN聚类算法"></a>3.3 DBSCAN聚类算法</h4><p>DBSCAN的算法思想：用一个点附近的邻域内的数据点的个数来衡量该点所在空间的密度。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/11/22/读书笔记/学习笔记2：线性回归、决策树、聚类/">https://zhxnlp.github.io/2021/11/22/读书笔记/学习笔记2：线性回归、决策树、聚类/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a><a class="post-meta__tags" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a><a class="post-meta__tags" href="/tags/%E8%81%9A%E7%B1%BB/">聚类</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/23/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN/"><i class="fa fa-chevron-left">  </i><span>学习笔记3：深度学习DNN</span></a></div><div class="next-post pull-right"><a href="/2021/11/18/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9Ann.Module%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E3%80%81TensorBoard/"><span>PyTorch学习笔记2：nn.Module、优化器、模型的保存和加载、TensorBoard</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>