<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="学习笔记3：深度学习DNN"><meta name="keywords" content="深度学习,神经网络,DNN"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>学习笔记3：深度学习DNN | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">一、BP神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%B8%BA%E4%BD%95%E8%A6%81%E5%BC%95%E5%87%BABP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 为何要引出BP神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 BP神经网络基本原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 神经网络的多分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BD%BF%E7%94%A8softmax%E8%BF%98%E6%98%AFsigmoid%E5%A5%BD%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 二分类使用softmax还是sigmoid好？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 梯度下降和链式求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.6.</span> <span class="toc-text">1.6度量学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%9C%AF"><span class="toc-number">2.</span> <span class="toc-text">二、矩阵求导术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A0%87%E9%87%8F%E5%AF%B9%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 标量对向量求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%90%91%E9%87%8F%E5%AF%B9%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 向量对向量求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%A0%87%E9%87%8F%E5%AF%B9%E7%9F%A9%E9%98%B5%E7%9A%84%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 标量对矩阵的矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC%E5%8F%8A%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 向量求导及链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-BP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 BP反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0"><span class="toc-number">2.6.</span> <span class="toc-text">2.5 激活函数及其导数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E4%BC%98"><span class="toc-number">3.</span> <span class="toc-text">三、神经网络调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%BE%97%E9%80%89%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 激活函数得选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Relu激活函数及其变体</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E9%AB%98%E6%96%AF%E8%AF%AF%E5%B7%AE%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0gelu"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 高斯误差线性单元激活函数gelu</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-GELU%E6%A6%82%E8%BF%B0"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.3.1 GELU概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-GELU%E6%95%B0%E5%AD%A6%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.3.2 GELU数学表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Xavier%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 Xavier权重初始化</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">学习笔记3：深度学习DNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-11-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">5k</span><span class="post-meta__separator">|</span><span>Reading time: 18 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一、BP神经网络"><a href="#一、BP神经网络" class="headerlink" title="一、BP神经网络"></a>一、BP神经网络</h2><h3 id="1-1-为何要引出BP神经网络"><a href="#1-1-为何要引出BP神经网络" class="headerlink" title="1.1 为何要引出BP神经网络"></a>1.1 为何要引出BP神经网络</h3><ol>
<li>逻辑回归对于如今越来越复杂的任务效果越来越差，主要是难以处理线性不可分的数据，LR处理线性不可分，一般是特征变换和特征组合，将低维空间线性不可分的数据在高维空间中线性可分</li>
<li>改良方式有几种，本质上都是对原始输入特征做文章。但都是针对特定场景设计。如果实际场景中特征组合在设计之外，模型无能为力<ul>
<li>人工组合高维特征，将特征升维至高维空间。但是会耗费较多人力，而且需要对业务理解很深</li>
<li>自动交叉二阶特征，例如FM模型。缺点是只能进行二阶交叉</li>
<li>SVM+核方法：可以将特征投影到高维空间。缺点是核函数种类有限，升维具有局限性，运算量巨大。<span id="more"></span>
3.构建BP神经网络（也叫MLP多层感知器），用线性变换+非线性函数激活的方式进行特征变换。以分类为目的进行学习的时候，网络中的参数会以分类正确为目的自行调整，也就是自动提取合适的特征。&lt;/font &gt;（回归问题，去掉最后一层的激活函数就行。输出层激活函数只是为了结果有概率意义）神经网络最大的惊喜就是自动组合特征。<h3 id="1-2-BP神经网络基本原理"><a href="#1-2-BP神经网络基本原理" class="headerlink" title="1.2 BP神经网络基本原理"></a>1.2 BP神经网络基本原理</h3></li>
</ul>
</li>
</ol>
<ul>
<li>MLP网络中，每个节点都接收前一层所有节点的信号的加权和，累加后进行激活，再传入下一层的节点。这个过程和动物神经元细胞传递信号的过程类似，所以叫神经网络，各节点称为神经元。</li>
<li>每层神经元个数称为神经网络的宽度，层数称为神经网络的深度。所以多层神经网络称为深度神经网络DNN。</li>
<li>神经元数据过多会造成过拟合和运算量过大。层中神经元过多，相当于该层转换后的特征维度过高，会造成维数灾难。</li>
<li>DNN仍然使用交叉熵损失函数。<h3 id="1-3-神经网络的多分类"><a href="#1-3-神经网络的多分类" class="headerlink" title="1.3 神经网络的多分类"></a>1.3 神经网络的多分类</h3></li>
<li>机器学习模型相对简单，参数较少。可以通过训练N个二分类模型来完成多分类。而深度学习中，模型参数较多，训练多个模型不实际。而且多分类任务的前层特征变换是一致的，没必要训练多个模型。一般是输出层采用softmax函数来完成。</li>
<li>softmax做多分类只适用于类别互斥且和为1的情况。如果和不为1，可以加一个其它类。不互斥时不能保证分母能归一化。<h3 id="1-4-二分类使用softmax还是sigmoid好？"><a href="#1-4-二分类使用softmax还是sigmoid好？" class="headerlink" title="1.4 二分类使用softmax还是sigmoid好？"></a>1.4 二分类使用softmax还是sigmoid好？</h3></li>
</ul>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/120876730">《速通8-DNN神经网络学习笔记》</a></p>
<ul>
<li>softmax等于分别学习w1和w2，而sigmoid等于学这两个的差值就行了。sigmoid是softmax在二分类上的特例。二分类时sigmoid更好。因为我们只关注w1和w2的差值，但是不关心其具体的值。</li>
<li>softmax的运算量很大，因为要考虑别的概率值。一般只在神经网络最后一层用（多分类时）。中间层神经元各算各的，不需要考虑别的w数值，所以中间层不需要softmax函数。</li>
<li>softmax函数分子：通过指数函数，将实数输出映射到零到正无穷。softmax函数分母：将所有结果相加，进行归一化。</li>
<li>softmax和sigmoid一样有饱和区，x变化几乎不会引起softmax输出的变化，而且饱和区导数几乎为0，无法有效学习。所以需要合适的初始化，控制前层输出的值域。</li>
<li>两个函数的代码实现如下图<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">	 </span><br><span class="line">sigmoid_inputs = np.arange(-<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">sigmoid_outputs=sigmoid(sigmoid_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Input :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(sigmoid_inputs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Output :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(sigmoid_outputs))</span><br><span class="line"> </span><br><span class="line">plt.plot(sigmoid_inputs,sigmoid_outputs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sigmoid Inputs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sigmoid Outputs&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    orig_shape=x.shape</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(x.shape)&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="comment">#Matrix</span></span><br><span class="line">        <span class="comment">#shift max whithin each row</span></span><br><span class="line">        constant_shift=np.<span class="built_in">max</span>(x,axis=<span class="number">1</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        x-=constant_shift</span><br><span class="line">        x=np.exp(x)</span><br><span class="line">        normlize=np.<span class="built_in">sum</span>(x,axis=<span class="number">1</span>).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        x/=normlize</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#vector</span></span><br><span class="line">        constant_shift=np.<span class="built_in">max</span>(x)</span><br><span class="line">        x-=constant_shift</span><br><span class="line">        x=np.exp(x)</span><br><span class="line">        normlize=np.<span class="built_in">sum</span>(x)</span><br><span class="line">        x/=normlize</span><br><span class="line">    <span class="keyword">assert</span> x.shape==orig_shape</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line">softmax_inputs = np.arange(-<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">softmax_outputs=softmax(softmax_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Input :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(softmax_inputs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sigmoid Function Output :: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(softmax_outputs))</span><br><span class="line"><span class="comment"># 画图像</span></span><br><span class="line">plt.plot(softmax_inputs,softmax_outputs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Softmax Inputs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Softmax Outputs&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="1-5-梯度下降和链式求导"><a href="#1-5-梯度下降和链式求导" class="headerlink" title="1.5 梯度下降和链式求导"></a>1.5 梯度下降和链式求导</h3><h3 id="1-6度量学习"><a href="#1-6度量学习" class="headerlink" title="1.6度量学习"></a>1.6度量学习</h3><h2 id="二、矩阵求导术"><a href="#二、矩阵求导术" class="headerlink" title="二、矩阵求导术"></a>二、矩阵求导术</h2>线性代数参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16555103/article/details/84838943">《线性代数一（基本概念）》</a><h3 id="2-1-标量对向量求导"><a href="#2-1-标量对向量求导" class="headerlink" title="2.1 标量对向量求导"></a>2.1 标量对向量求导</h3>例如标量z对向量X求导，就是看X各元素变化对z的影响。所以结果是z对X各元素求导，结果也是一个同尺寸向量。<h3 id="2-2-向量对向量求导"><a href="#2-2-向量对向量求导" class="headerlink" title="2.2 向量对向量求导"></a>2.2 向量对向量求导</h3>列向量对行向量求导，结果是一个矩阵，方便后面进行链式求导中的导数连乘。&lt;/font&gt;（向量既可以写成列的形式，也可以写成行的形式。列向量可以直接对列向量求导，但是维数太多不便于操作，而且没法进行连乘，一般没人这么干。）</li>
<li>m×1维列向量$y$对n×1维列向量$x$求导，等于$y$的每个元素$y_{i}$分别对$x$求导，得到m×1维导数。而导数每个元素都是标量对向量求导，结果是n×1维列向量。所以最终结果是mn×1维的列向量。行向量亦然。</li>
<li>神经网络中一般都用列向量。，如果列向量$y$对列向量$x$求导，结果太长不便于计算。一般是$y$对$x$的转置求导，即$\frac{\partial y}{\partial x^{T}}$，最后结果是一个m×n的矩阵。<h3 id="2-3-标量对矩阵的矩阵"><a href="#2-3-标量对矩阵的矩阵" class="headerlink" title="2.3 标量对矩阵的矩阵"></a>2.3 标量对矩阵的矩阵</h3>向量可以看成某一维为1的矩阵（行为1或列为1），同理，标量对m×n维矩阵求导，是对矩阵中每个元素求导，结果也是一个m×n维的结果。<h3 id="2-4-向量求导及链式法则"><a href="#2-4-向量求导及链式法则" class="headerlink" title="2.4 向量求导及链式法则"></a>2.4 向量求导及链式法则</h3>对于<script type="math/tex">x_{3}=Wx_{2}</script><br>展开之后有：<script type="math/tex; mode=display">\begin{pmatrix}
x_{31}\\ 
x_{32}\\ 
...\\ 
x_{3m}\end{pmatrix}=\begin{pmatrix}
m_{11} & m_{12} &... &m _{1n} \\ 
m_{21} &m _{22} &...  &m_{2n} \\ 
...& ... & ... &... \\ 
m_{m1}&m_{m2}  &... & m_{mn} 
\end{pmatrix}\times \begin{pmatrix}
x_{21}\\ 
x_{22}\\ 
...\\ 
x_{2n}\end{pmatrix}</script>所以$\frac{\partial x<em>{3}}{\partial x</em>{2}^{T}}=W$，$\frac{\partial x<em>{3}}{\partial W}=x</em>{2}^{T}$</li>
<li>即之前列向量对列向量转置求导，推出结果是mn的矩阵，但是各元素的具体值不知道。这里直接求出，具体值为矩阵W</li>
<li>求导要一直盯着尺寸看。</li>
</ul>
<p>公式1——标量对向量求导链式法则：<br>向量$x<em>{1}…x</em>{n}$为神经网络各层的输入，最终输出结果是标量z。存在依赖关系：$x<em>{1}\rightarrow x</em>{2}\rightarrow x<em>{3}…\rightarrow x</em>{n}\rightarrow z$，则有：</p>
<script type="math/tex; mode=display">\frac{\partial z}{\partial x_{1}}=(\frac{\partial x_{n} }{\partial x_{n-1}^{T}}\cdot \frac{\partial x_{n-1} }{\partial x_{n-2}^{T}}...\frac{\partial x_{2} }{\partial x_{1}^{T}})^{T}\frac{\partial z}{\partial x_{n}}</script><ul>
<li>假如$x<em>{n}、x</em>{n-1}、x_{n-2}$分别是m、n、k维的列向量，则上式右边第一项分别是m×n和n×k的矩阵，这两个矩阵才有相乘的可能，结果是m×k的矩阵。所以必须是列向量对列向量的转置求导</li>
<li>假如$x<em>{1}$是h维列向量，则括号内最终结果是m×h维矩阵，$\frac{\partial z}{\partial x</em>{n}}$结果是m维列向量，无法直接相乘，所以括号内的矩阵必须转置。</li>
</ul>
<p>公式2——标量对矩阵求导链式法则：<br>W为矩阵，$x$和$y$是向量，有$y=Wx$。且标量$z=f(y)$，则：</p>
<script type="math/tex; mode=display">\frac{\partial z}{\partial W}=\frac{\partial z }{\partial y}\cdot x^{T}</script><p>参照上面写的：$\frac{\partial x<em>{3}}{\partial W}=x</em>{2}^{T}$</p>
<h3 id="2-5-BP反向传播"><a href="#2-5-BP反向传播" class="headerlink" title="2.5 BP反向传播"></a>2.5 BP反向传播</h3><p>Loss对任意层$W^k$求导有：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial W^{k}}=\frac{\partial Loss}{\partial d_{j}^{L}}\cdot \frac{\partial d_{j}^{L}}{\partial W^{k}}</script><p>对于一个L层的神经网络，j表示L层第j维数据（也就是第j个神经元）<br>第一项$\frac{\partial Loss}{\partial d<em>{j}^{L}}=y</em>{j}’-y_{j}$，是一个标量。(即loss对最后一层某个神经元导数为一个标量，推导见P119）<br>又因为：</p>
<script type="math/tex; mode=display">d^{k}=W^{k}\cdot a^{k-1}</script><script type="math/tex; mode=display">a^{k-1}=f(d^{l-1}+w_{0}^{k-1})</script><p>后一项是标量对矩阵求导，根据公式2有： </p>
<script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial W^{k}}= \frac{\partial d_{j}^{L}}{\partial d^{k}}\cdot (a^{k-1})^{T}</script><blockquote>
<p>将$d_{j}^{L}$看做是由向量$d^{L}$映射成标量</p>
</blockquote>
<p>上式右边第一项是标量对向量求导，根据公式1有：</p>
<script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial d^{k}}=\frac{\partial d_{j}^{L}}{\partial a^{L-1}}\cdot (\frac{\partial a^{L-1}}{\partial (d^{L-1})^{T}}\cdot \frac{\partial d^{L-1}}{\partial (a^{L-2})^{T}}...\frac{\partial d^{k+1}}{\partial (a^{k})^{T}}\cdot \frac{\partial a^{k}}{\partial (d^{k})^{T}})^T</script><p>依次求三项导数有：</p>
<script type="math/tex; mode=display">\frac{\partial d_{j}^{L}}{\partial a^{L-1}}=W^L</script><script type="math/tex; mode=display">\frac{\partial a^{m}}{\partial (d^{m})^{T}}=f'(d^m)</script><script type="math/tex; mode=display">\frac{\partial d^{m+1}}{\partial (a^{m})^{T}}=W^{m+1}</script><p>将以上结果联合起来就是：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial W^{k}}=\frac{\partial Loss}{\partial d_{j}^{L}}\cdot \frac{\partial d_{j}^{L}}{\partial W^{k}}=(y'_{j}-y_{j})\cdot (a^{k-1})^{T}\cdot W^{L}\cdot f'(d^{L-1})\cdot  W^{L-1}\cdot f'(d^{L-2})... W^{k+1}\cdot f'(d^{k})</script><p>第m层激活函数得导数有：</p>
<script type="math/tex; mode=display">a^{m}=f(d^{m}+w_{0}^{m})</script><p>展开后为：</p>
<script type="math/tex; mode=display">\begin{pmatrix}
a_{1}\\ 
a_{2}\\ 
...\\ 
a_{M}\end{pmatrix}=\begin{pmatrix}
f(d_{1}+w_{0}^{1})\\ 
f(d_{2}+w_{0}^{2})\\ 
...\\ 
f(d_{M})+w_{0}^{M}\end{pmatrix}</script><p>其实是省去了上标层数m，其中第m层共有M个神经元。相当于向量d数乘之后得到向量a，每个元素按位操作。$a<em>{1}=f(d</em>{1}+w<em>{0}^{1})$,$a</em>{1}$只和$d_{1}$有关，和向量d其它元素无关，对d其它分量结果为0。</p>
<script type="math/tex; mode=display">\frac{\partial a^{m}}{\partial (d^{m})^{T}}=f'(d^m)</script><p>列向量对行向量求导，结果是一个矩阵。所以有：</p>
<script type="math/tex; mode=display">f'(d^m)=\begin{pmatrix}
f'(d_{1}+w_{0}^{1}) &0  &...  &0 \\ 
 0& f'(d_{2}+w_{0}^{2}) & ... & 0\\ 
0 &...  & ... &... \\ 
0& 0 & ... & f'(d_{M}+w_{0}^{M})
\end{pmatrix}</script><ul>
<li>如果激活函数f的导数f’&gt;1，经过多次连乘，最后结果会非常大，即梯度爆炸。会产生震荡甚至溢出。</li>
<li>如果f’&lt;1，梯度消失，W几乎不会更新。<h3 id="2-5-激活函数及其导数"><a href="#2-5-激活函数及其导数" class="headerlink" title="2.5 激活函数及其导数"></a>2.5 激活函数及其导数</h3></li>
</ul>
<ol>
<li>sigmoid函数：$sigmoid(d)=\frac{1}{1+e^{-d}}$导数为：<br>$f’=f(1-f)=0.25-(f-0.5)^2$,后一项非负，当f=0.5时有最大值0.25。所以其值域为（0，0.25）</li>
<li><p>softmax函数及其导数，参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/cassiePython/article/details/80089760">《Softmax函数及其导数》</a><br><img src="https://img-blog.csdnimg.cn/7b1f7e77f53045e0bf79b29e00d463d7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
</li>
<li><p>relu函数：$f(x)=max(0,x)$。其导数w为：</p>
<script type="math/tex; mode=display">f'(x)=\left\{\begin{matrix}
0 ,x<0\\ 
1,x>0\end{matrix}\right.</script></li>
<li>Tanh函数<script type="math/tex; mode=display">f(d)=Tanh(d)=\frac{e^{d}-e^{-d}}{e^{d}+e^{-d}}=sigmoid(2d-1)</script>导数为：<script type="math/tex">\frac{\partial a}{\partial d}=1-f^{2}</script><br>Tanh值域（-1，1)，导数值域（0,1）。<h2 id="三、神经网络调优"><a href="#三、神经网络调优" class="headerlink" title="三、神经网络调优"></a>三、神经网络调优</h2>海量的数据和强大的算力为深度学习的发展提供了条件。但是也带来一些新的问题：</li>
</ol>
<ul>
<li>网络太深带来梯度消失和梯度爆炸</li>
<li>损失函数太复杂，有大量的极小点和鞍点，如果学习方法不合适，损失函数可能无法降低</li>
<li>参数过多造成过拟合<h3 id="3-1-激活函数得选型"><a href="#3-1-激活函数得选型" class="headerlink" title="3.1 激活函数得选型"></a>3.1 激活函数得选型</h3>合适的激活函数需要满足的条件</li>
</ul>
<ol>
<li>零均值输出：例如tanh，零均值输出，有正有负。W各个维度更新方向可以不同，避免单向更新学习慢的问题（各维度只能同增或者同减，走Z字路线。比如loss极小值在左下方，只能先左后下）这一点softmax、sigmoid、relu都不满足，都是非负的</li>
<li>适当的线性。激活函数对输入的变化不宜过于激烈，否则输入轻微变化造成输出剧烈变化，稳定性不好。softmax、sigmoid、relu、tanh都有近似线性的区域可以满足</li>
<li>导数不宜过大或过小，否则梯度消失或者爆炸。这也是relu成为神经网络首选的原因。tanh导数是0-1，比sigmoid好一点。</li>
<li>导数的单调性。避免导数有正有负造成学习震荡，例如三角函数</li>
<li>有值域的控制。避免多次激活后输出太大。例如$y=x^2$</li>
<li>没有超参数，计算简单。Maxout、PReLu函数有超参数，设置本身依赖经验。而且ReLu计算足够简单<h3 id="3-2-Relu激活函数及其变体"><a href="#3-2-Relu激活函数及其变体" class="headerlink" title="3.2 Relu激活函数及其变体"></a>3.2 Relu激活函数及其变体</h3>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/Sophia_11/article/details/103998468?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163769303916780357222745%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=163769303916780357222745&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-7-103998468.first_rank_v2_pc_rank_v29&amp;utm_term=gelu&amp;spm=1018.2226.3001.4187">《从ReLU到GELU，一文概览神经网络的激活函数》</a><br>线性整流函数Relu:<script type="math/tex">f(x)=max(0,x)</script></li>
</ol>
<ul>
<li>d≥0时，f(d)=d，导数为1，不存在梯度消失或者梯度爆炸，也不存在饱和区</li>
<li>d＜0时，f(d)=0，梯度为0 ，神经元死亡<br>梯度消失和激活函数饱和困扰业界多年，直到RELU的出现。</li>
</ul>
<p>神经元真死和假死：<br>真死：</p>
<ul>
<li>无论w和x各维度如何变化，恒有$d<em>{j}^{(l)}=w</em>{j}^{(l)}a^{(l-1)}$≤0，神经元死亡。（l表示神经网络层数，j表示某层中神经元节点j）。$a^{(l-1)}≥0$为上一层神经网络的输出。</li>
<li>所以是$w<em>{j}^{(l)}$各元素都是很大的负数时，恒有d≤0。这是参数初始化错误或者lr过大导致权重$w</em>{j}^{(l)}$更新过大造成的。因此用Relu做激活函数，lr不宜过大，权重初始化要合理</li>
</ul>
<p>假死：饱和形式之一</p>
<ul>
<li>恰巧造成$w<em>{j}^{(l)}a</em>{j}^{(l-1)}≤0$，重新训练时有可能会恢复正常。神经元大多数时是假死。</li>
<li>适当假死可以提升神经网络效果：<ul>
<li>如果没有神经元假死，$a_{j}^{(l-1)}≥0$，f(d)=d，Relu作为激活函数就是纯线性的，失去意义。</li>
<li>假死神经元就是对某些输入特征不进行输出，达到特征选择的作用（门控决策）</li>
</ul>
</li>
</ul>
<p>LRelu： d＜0时，f(d)=k，k为超参数，在0-1之间<br>PRelu： d＜0时，f(d)=k，k是可学习的参数。<br>Maxout：$w<em>{j}^{(l)}(1)$到$w</em>{j}^{(l)}(n)$有多个参数，分别和上一层输出相乘，即每个神经元节点j有多个输入，最终输出选最大值。(P133-134)</p>
<h3 id="3-3-高斯误差线性单元激活函数gelu"><a href="#3-3-高斯误差线性单元激活函数gelu" class="headerlink" title="3.3 高斯误差线性单元激活函数gelu"></a>3.3 高斯误差线性单元激活函数gelu</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/XqPKsM8yq8rbFFvvJKCtkQ">《超越ReLU却鲜为人知，3年后被挖掘》</a><br>参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.08415.pdf">《GELU 论文》</a></p>
</blockquote>
<h4 id="3-3-1-GELU概述"><a href="#3-3-1-GELU概述" class="headerlink" title="3.3.1 GELU概述"></a>3.3.1 GELU概述</h4><p>BERT、RoBERTa、ALBERT 等目前业内顶尖的 NLP 模型都使用了这种激活函数。另外，在 OpenAI 声名远播的无监督预训练模型 GPT-2 中，研究人员在所有编码器模块中都使用了 GELU 激活函数。在计算机视觉、自然语言处理和自动语音识别等任务上，使用 GELU 激活函数比使用ReLU 或 ELU 效果更好。</p>
<p>随着网络深度的不断增加，利用 Sigmoid 激活函数来训练被证实不如非平滑、低概率性的 ReLU 有效（Nair &amp; Hinton, 2010），因为 ReLU 基于输入信号做出门控决策。</p>
<p>深度学习中为了解决过拟合，会随机正则化（如在隐层中加入噪声）或采用 dropout 机制。这两个选择是和激活函数割裂的。非线性和 dropout 共同决定了神经元的输出，而随机正则化在执行时与输入无关。</p>
<p>由此提出高斯误差线性单元（Gaussian Error Linear Unit，GELU）。GELU 与随机正则化有关，因为它是自适应 Dropout 的修正预期（Ba &amp; Frey, 2013）&lt;/font &gt;。这表明神经元输出的概率性更高。</p>
<h4 id="3-3-2-GELU数学表示"><a href="#3-3-2-GELU数学表示" class="headerlink" title="3.3.2 GELU数学表示"></a>3.3.2 GELU数学表示</h4><p>Dropout、ReLU 等机制都希望将「不重要」的激活信息规整为零。即对于输入的值，我们根据它的情况乘上 1 或 0。&lt;/font &gt;或者说，对输入x乘上一个伯努利分布 Bernoulli(Φ(x))，其中Φ(x) = P(X ≤ x)。（x服从于标准正态分布 N(0, 1)）</p>
<p>对于一部分Φ(x)，它直接乘以输入 x，而对于另一部分 (1 − Φ(x))，它们需要归零。随着 x 的降低，它被归零的概率会升高。对于 ReLU 来说，这个界限就是 0。</p>
<p>我们经常希望神经网络具有确定性决策，这种想法催生了 GELU 激活函数的诞生。具体来说可以表示为：Φ(x) × Ix + (1 − Φ(x)) × 0x = xΦ(x)。可以理解为，不太严格地说，上面这个表达式可以按当前输入 x 比其它输入大多少来缩放 x。&lt;/font &gt;</p>
<p>高斯概率分布函数通常根据损失函数计算，因此研究者定义高斯误差线性单元（GELU）为：</p>
<script type="math/tex; mode=display">GELU(x)=xP(X\leqslant x)=x\Phi (x)</script><p>上面这个函数是无法直接计算的，因此可以通过另外的方法来逼近这样的激活函数，研究者得出来的表达式为：</p>
<script type="math/tex; mode=display">GELU(x)=0.5x(1+tanh[\sqrt{\frac{2}{\pi }}(x+0.044175x^{3})])</script><p>或：<script type="math/tex">GELU(x)=x\sigma (1.702x)</script><br>其中 σ() 是标准的 sigmoid 函数<br><img src="https://img-blog.csdnimg.cn/adbc2fffd2e04be5adf7ccc5702ecd40.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>当 x 大于 0 时，输出为 x；但 x=0 到 x=1 的区间除外，这时曲线更偏向于 y 轴。<br>没能找到该函数的导数，所以我使用了 WolframAlpha 来微分这个函数。结果如下：<br><img src="https://img-blog.csdnimg.cn/338d92546ede439195600449445c66a2.png" alt="在这里插入图片描述"><br>微分的GELU函数：<br><img src="https://img-blog.csdnimg.cn/0c8f7ea9572e4068946a53279375f94c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6K-75Lmm5LiN6KeJ5bey5pil5rex77yB,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>GELU 的近似实现方式有两种，借助 tanh() 和借助σ()。我们在 GPT-2 的官方代码中也发现，更多研究者采用了 tanh() 的实现方式尽管它看起来要比 xσ(1.702x) 复杂很多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GPT-2 的 GELU 实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0.5</span>*x*(<span class="number">1</span>+tf.tanh(np.sqrt(<span class="number">2</span>/np.pi)*(x+<span class="number">0.044715</span>*tf.<span class="built_in">pow</span>(x, <span class="number">3</span>))))</span><br></pre></td></tr></table></figure>
<h3 id="3-4-Xavier权重初始化"><a href="#3-4-Xavier权重初始化" class="headerlink" title="3.4 Xavier权重初始化"></a>3.4 Xavier权重初始化</h3><p>参考《苏神文章解析（6篇）》<br><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a><br><a target="_blank" rel="noopener" href="https://kexue.fm/archives/7180">《从几何视角来理解模型参数的初始化策略》</a></p>
<ul>
<li>如果两个神经元参数完全相同，则梯度也相同，更新幅度一致，最后输出也相同。看上去是两个神经元，但其实相当于就一个，是互相冗余的。所以 初始化W矩阵时要破坏其对称性，独立均匀分布的随机初始化就可以做到。（正态分布采样容易集中在均值附近，可能造成权重对称）均匀分布有两个参数$\mu/ \sigma$ ，均值和方差。</li>
<li>真实场景中，特征都是客观事实，一般都是＞0。如果W矩阵各元素都是＞0，则relu函数不起作用。都＜0，神经元都死了。所以 W矩阵各元素需要有正有负，所以一般均匀分布选择均值为0。$\sigma$ 越大，w采样到大值的可能性越大,即$W<em>{i,j}^{l}\propto \sigma </em>{i,j}^{l}$</li>
<li>从均值为0、方差为1/m的随机分布中独立重复采样，这就是Xavier初始化（无激活函数时）relu做激活函数时，方差为2/m</li>
<li>NTK参数化：均值为0、方差为1的随机分布来初始化，但是将输出结果除以$\sqrt{m}$。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的量级大致都是相同的O(1)级别，让我们更平等地处理每一个参数，于是我们可以设置较大的学习率。</li>
<li>对于$d<em>{j}^{(l)}=\sum</em>{i=1}^{M(l-1)}w<em>{i,j}^{(l)}a</em>{i}^{(l-1)}$，M(l-1)表示上一层神经元个数。$d<em>{j}^{(l)}$不宜过大,$W</em>{i,j}^{l}\propto \frac{1}{M^{l-1}},\sigma _{i,j}^{l}\propto \frac{1}{M^{l-1}}$&lt;/font &gt;。否则relu前向传播时，输出会层层放大而溢出。softmax、sigmoid、tanh进入饱和区，导数基本为0。<br>其它推导见P137</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/11/23/读书笔记/学习笔记3：深度学习DNN/">https://zhxnlp.github.io/2021/11/23/读书笔记/学习笔记3：深度学习DNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/DNN/">DNN</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/25/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0DNN2/"><i class="fa fa-chevron-left">  </i><span>学习笔记4：深度学习DNN2</span></a></div><div class="next-post pull-right"><a href="/2021/11/22/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E8%81%9A%E7%B1%BB/"><span>学习笔记2：线性回归、决策树、聚类</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>