<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="天池-新闻文本分类task0：任务简介"><meta name="keywords" content="nlp,文本分类"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>天池-新闻文本分类task0：任务简介 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%B5%9B%E4%BA%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">1.</span> <span class="toc-text">一、赛事说明</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%95%B0%E6%8D%AE%E6%A0%87%E7%AD%BE"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 数据标签</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">1.2.</span> <span class="toc-text">1.2  数据读取与数据分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81task1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">二、task1:机器学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-bagging"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-boosting"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 boosting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Adaboost%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 Adaboost算法原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-GBDT"><span class="toc-number">2.4.</span> <span class="toc-text">1.4 GBDT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-XGBoost"><span class="toc-number">2.5.</span> <span class="toc-text">1.5  XGBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-LightGBM"><span class="toc-number">2.6.</span> <span class="toc-text">1.6 LightGBM</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">天池-新闻文本分类task0：任务简介</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-01</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3k</span><span class="post-meta__separator">|</span><span>阅读时长: 10 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>task1:机器学习算法<br>用TF-IDF作为文档向量，分类器分别使用了岭回归、朴素贝叶斯、SVM、随机森林、Xgboost和lightGBM。<br>关于集成学习中的随机森林、Xgboost和lightGBM可以参考我在CSDN的帖子<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/122138831">《集成学习4：整理总结》</a></p>
<p>task2：fasttext<br>参考帖子：<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_64375823/article/details/121581268?spm=1001.2014.3001.5501">《学习笔记四：word2vec和fasttext》</a></p>
<p>task3：bert<br>参考帖子<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/120582114">《天池 入门赛-新闻文本分类-单个bert模型分数0.961》</a></p>
<h2 id="一、赛事说明"><a href="#一、赛事说明" class="headerlink" title="一、赛事说明"></a>一、赛事说明</h2><ul>
<li>比赛官方链接为：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">《零基础入门NLP - 新闻文本分类》</a>。</li>
<li>讨论区有大佬张帆、惊鹊和张贤等人的代码，值得大家仔细阅读。</li>
<li>最后我的模型参考了这些代码的一些config，比如bert.config，lr等等。然后大佬们的代码对我来说还是太复杂，pytorch功力不够，看的吃力。所以参考HF主页的教程，自己用huggingface实现了。<span id="more"></span>
<h3 id="1-1-数据标签"><a href="#1-1-数据标签" class="headerlink" title="1.1 数据标签"></a>1.1 数据标签</h3>处理后的赛题训练数据如下：</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/88963272/147489479-8f855ebe-2c98-4c03-aad7-9cf16d33de4f.png" alt="image"></p>
<p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p>
<p>评测指标为f1分数。</p>
<h3 id="1-2-数据读取与数据分析"><a href="#1-2-数据读取与数据分析" class="headerlink" title="1.2  数据读取与数据分析"></a>1.2  数据读取与数据分析</h3><ol>
<li><p>使用Pandas完成数据读取的操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;../data/train_set.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, nrows=<span class="number">100</span>)</span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/99a4bdd6a34d40db9a5e40bc383e2ae3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
</li>
<li><p>句子长度分析：<br>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度。统计并如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>] = train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line"><span class="built_in">print</span>(train_df[<span class="string">&#x27;text_len&#x27;</span>].describe())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Populating the interactive namespace <span class="keyword">from</span> numpy <span class="keyword">and</span> matplotlib</span><br><span class="line">count     <span class="number">100.000000</span></span><br><span class="line">mean      <span class="number">872.320000</span></span><br><span class="line">std       <span class="number">923.138191</span></span><br><span class="line"><span class="built_in">min</span>        <span class="number">64.000000</span></span><br><span class="line"><span class="number">25</span>%       <span class="number">359.500000</span></span><br><span class="line"><span class="number">50</span>%       <span class="number">598.000000</span></span><br><span class="line"><span class="number">75</span>%      <span class="number">1058.000000</span></span><br><span class="line"><span class="built_in">max</span>      <span class="number">7125.000000</span></span><br><span class="line">Name: text_len, dtype: float64</span><br></pre></td></tr></table></figure>
<p>对新闻句子的统计可以得出，本次赛题给定的文本比较长，每个句子平均由907个字符构成，最短的句子长度为2，最长的句子长度为57921。</p>
<p>下图将句子长度绘制了直方图，可见大部分句子的长度都几种在2000以内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = plt.hist(train_df[<span class="string">&#x27;text_len&#x27;</span>], bins=<span class="number">200</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Text char count&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Histogram of char count&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/0f307dec2202452ab7bd2aec624c34fb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<ol start="3">
<li>新闻类别分布<br>接下来可以对数据集的类别进行分布统计，具体统计每类新闻的样本个数</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;News class count&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/a6c82f64be1c456ca16dfcef27a638ae.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p>
<ol start="4">
<li>字符分布统计<br>接下来可以统计每个字符出现的次数，首先可以将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</li>
</ol>
<p>从统计结果中可以看出，在训练集中总共包括6869个字，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">all_lines = <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text&#x27;</span>]))</span><br><span class="line">word_count = Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count = <span class="built_in">sorted</span>(word_count.items(), key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>], reverse = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_count[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">2405</span></span><br><span class="line">(<span class="string">&#x27;3750&#x27;</span>, <span class="number">3702</span>)</span><br><span class="line">(<span class="string">&#x27;5034&#x27;</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">train_df[<span class="string">&#x27;text_unique&#x27;</span>] = train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(<span class="built_in">set</span>(x.split(<span class="string">&#x27; &#x27;</span>)))))</span><br><span class="line">all_lines = <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text_unique&#x27;</span>]))</span><br><span class="line">word_count = Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count = <span class="built_in">sorted</span>(word_count.items(), key=<span class="keyword">lambda</span> d:<span class="built_in">int</span>(d[<span class="number">1</span>]), reverse = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&#x27;900&#x27;</span>, <span class="number">99</span>)</span><br><span class="line">(<span class="string">&#x27;3750&#x27;</span>, <span class="number">99</span>)</span><br><span class="line">(<span class="string">&#x27;648&#x27;</span>, <span class="number">96</span>)</span><br></pre></td></tr></table></figure>
<p>数据分析的结论<br>通过上述分析我们可以得出以下结论：</p>
<ul>
<li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；</li>
<li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；</li>
<li>赛题总共包括7000-8000个字符；</li>
</ul>
<p>通过数据分析，我们还可以得出以下结论：</p>
<ul>
<li>每个新闻平均字符个数较多，可能需要截断；</li>
<li>由于类别不均衡，会严重影响模型的精度；<h2 id="二、task1-机器学习算法"><a href="#二、task1-机器学习算法" class="headerlink" title="二、task1:机器学习算法"></a>二、task1:机器学习算法</h2></li>
<li>用TF-IDF作为文档向量，分类器分别使用了岭回归、朴素贝叶斯、SVM、随机森林、Xgboost和lightGBM。</li>
<li>关于集成学习中的随机森林、Xgboost和lightGBM可以参考我在CSDN的帖子<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/122138831">《集成学习4：整理总结》</a><h3 id="1-1-bagging"><a href="#1-1-bagging" class="headerlink" title="1.1 bagging"></a>1.1 bagging</h3></li>
<li>Bagging：<font color='red'>通过Bootstrap 的方式对全样本数据集进行抽样得到抽样子集，对不同的子集使用同一种基本模型进行拟合，然后投票得出最终的预测。</font >Bagging主要通过降低方差的方式减少预测误差</li>
<li>Bagging的一个典型应用是随机森林。由许多“树”bagging组成的。<font color='red'>每个决策树训练的样本和构建决策树的特征都是通过随机采样得到的，随机森林的预测结果是多个决策树输出的组合（投票）</font ><h3 id="1-2-boosting"><a href="#1-2-boosting" class="headerlink" title="1.2 boosting"></a>1.2 boosting</h3></li>
<li>使用同一组数据集进行反复学习，得到一系列简单模型，然后组合这些模型构成一个预测性能十分强大的机器学习模型。</li>
<li>Boosting通过不断减少偏差的形式提高最终的预测效果，与Bagging有着本质的不同。<h3 id="1-3-Adaboost算法原理"><a href="#1-3-Adaboost算法原理" class="headerlink" title="1.3 Adaboost算法原理"></a>1.3 Adaboost算法原理</h3></li>
<li>不改变训练数据，而是改变其权值分布，使每一轮的基学习器学习不同权重分布的样本集，最后加权组合表决组合。</li>
<li>Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。</li>
<li>用前向分布算法，<font color='red'>从指数损失函数推导出分类错误率$e_{m}$、分类器$G_m(x)$的权重系数$\alpha_{m}$、样本权重更新公式。</font >每轮学习时提高那些被前一轮分类器错误分类的样本的权重，来改变数据的概率分布。</font ></li>
</ul>
<ol>
<li>分类误差率<br>$$e_{m}=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)$$</li>
<li>分类器$G_m(x)$的权重系数$\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$<br> $$\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$$</li>
<li>权重分布<br>$$\begin{array}{c}<br>D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right) \\<br>w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N,<br> \end{array}\  $$                    </li>
<li>基本分类器加权组合表决<br>$$<br> \begin{aligned}<br> G(x) &amp;=\operatorname{sign}(f(x)) \\<br> &amp;=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)<br> \end{aligned}<br> $$<br>$$sign(x)=\begin{cases}<br>1 &amp; \text{ if } x\ geqslant 0 \\ </li>
</ol>
<p>-1 &amp; \text{ if } x&lt; 0<br>\end{cases}$$      </p>
<h3 id="1-4-GBDT"><a href="#1-4-GBDT" class="headerlink" title="1.4 GBDT"></a>1.4 GBDT</h3><ul>
<li> GBDT 的全称是 <code>Gradient Boosting Decision Tree</code>，梯度提升树。GBDT使用的决策树是<code>CART回归树</code>。因为<code>GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树</code>。</li>
<li>回归问题没有分类错误率可言，<font color='deeppink'>用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题。</font ></li>
<li>GBDT和Adaboost区别：</li>
</ul>
<ol>
<li>拟合思路</li>
</ol>
<ul>
<li>Adaboost算法：使用了<code>分类错误率修正样本权重以及计算每个基本分类器的权重</code>。通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。</li>
<li>GBDT：<font color='deeppink'>拟合残差来学习基模型。残差定义为损失函数相对于前一轮组合树模型的负梯度方向的值作为残差的近似值</font >（损失函数的负梯度在当前模型的值）</li>
<li>除了均方差损失函数时，负梯度值等于残差。</li>
</ul>
<ol start="2">
<li><p>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而<code>对与分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本</code>。</p>
</li>
<li><p>Adaboost是分类树，GBDT是回归树，但是也可以做分类。</p>
</li>
<li><p> 损失函数不同。基于残差 GBDT 容易对异常值敏感，所以一般回归类的损失函数会用<code>绝对损失或者 Huber 损失函数来代替平方损失函数</code>。<br><img src="https://img-blog.csdnimg.cn/dd49dedb94d140e9892d79355a20606d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h3 id="1-5-XGBoost"><a href="#1-5-XGBoost" class="headerlink" title="1.5  XGBoost"></a>1.5  XGBoost</h3><p>XGBoost 是 Boosting 框架的一种实现结构， lightgbm 也是一种框架实现结构，而 GBDT 则是一种算法实现。XGBoost本质也是GBDT， 相比于 GBDT 的差别主要就是 XGBoost 做的优化。</p>
</li>
<li><p>构造目标函数为：<br><img src="https://user-images.githubusercontent.com/88963272/147495165-a123b594-2ac3-44fc-9e46-9dfb4d0d95db.png" alt="image"></p>
</li>
<li><p>使用泰勒级数<strong>近似</strong>目标函数：<br><img src="https://user-images.githubusercontent.com/88963272/147495189-99ed19b0-2e69-4eaa-9e78-00a725acfc98.png" alt="image"><br><img src="https://user-images.githubusercontent.com/88963272/147495210-a881a978-d20b-4093-b782-081e1f5807f9.png" alt="image"></p>
</li>
</ol>
<p>对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。并在代价函数里加入了正则项，用于控制模型的复杂度，使学习出来的模型更加简单，防止过拟合</p>
<ol start="3">
<li><p> 模型复杂度$\Omega\left(f_{K}\right)$，它可以<code>由叶子节点的个数以及节点函数值来构建</code>，则：$\Omega\left(f_{K}\right) = \gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$  。</p>
</li>
<li><p>分割节点的标准为$max\{\tilde{\mathcal{L}}^{(old)} - \tilde{\mathcal{L}}^{(new)} \}$，为了找到找到最优特征及最优切分点，有三种策略：</p>
<ul>
<li>精确贪心分裂算法：首先找到所有的候 选特征及所有的候选切分点, 求其 $\mathcal{L}<em>{\text {split }}$, 然后 <font color='red'>选择使$\mathcal{L}</em>{\mathrm{split}}$ 最大的特征及 对应切分点作为最优特征和最优切分点。节点分裂时只选择当前最优的分裂策略, 而非全局最优的分裂策略。</font ></li>
<li>精确贪心分裂算法缺点：当<font color='red'>数据无法一次载入内存或者在分布式情况下，计算时需要不断在内存与磁盘之间进行数据交换，非常耗时、效率很低</font></li>
<li>基于直方图的近似算法，用于高效地生成候选的分割点。分为全局策略和本地策略。<br>所以xgboost支持特征粒度上的并行。</li>
</ul>
</li>
<li><p>支持列抽样和缺失值处理。x列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向（稀疏感知算法）。</p>
</li>
</ol>
<p>XGBoost缺点：</p>
<p>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；<br>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存</p>
<h3 id="1-6-LightGBM"><a href="#1-6-LightGBM" class="headerlink" title="1.6 LightGBM"></a>1.6 LightGBM</h3><p>轻量级（Light）的梯度提升机（GBM），主要用于解决 GDBT 在海量数据中遇到的问题。其相对 XGBoost 具有训练速度快、内存占用低的特点。<br>LightGBM 为了解决XGBoost的问题提出了以下几点解决方案：</p>
<ol>
<li>单边梯度抽样算法；</li>
<li>直方图算法；</li>
<li>互斥特征捆绑算法；<br>。。。</li>
</ol>
<p>单边梯度抽样算法：</p>
<ul>
<li>梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好。单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）保留了梯度大的样本，并对梯度小的样本进行随机抽样，减少了大量梯度小的样本，极大的减少了计算量。(在接下来的计算锅中只需关注梯度高的样本)</li>
<li>为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/10/01/天池-新闻文本分类/README/">https://zhxnlp.github.io/2021/10/01/天池-新闻文本分类/README/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/10/04/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task1%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/"><i class="fa fa-chevron-left">  </i><span>天池-新闻文本分类task1: 机器学习模型</span></a></div><div class="next-post pull-right"><a href="/2021/09/27/10%E6%9C%88%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Pytorch/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E3%80%81%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/"><span>PyTorch学习笔记1——基本概念、模块简介、张量操作、自动微分</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>