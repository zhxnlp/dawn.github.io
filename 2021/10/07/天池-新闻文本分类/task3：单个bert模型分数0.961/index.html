<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="天池-新闻文本分类task3：bert模型"><meta name="keywords" content="nlp,文本分类,transformers"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>天池-新闻文本分类task3：bert模型 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E8%AF%B4%E6%98%8E"><span class="toc-text">一些说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%9C%80%E7%BB%88%E4%BB%A3%E7%A0%81%E5%8F%8A%E8%A7%A3%E6%9E%90"><span class="toc-text">三、最终代码及解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9E%84%E5%BB%BA%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">3.1 构建分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%A2%84%E8%AE%AD%E7%BB%83bert%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 预训练bert模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%EF%BC%9A"><span class="toc-text">3.3 分类任务微调：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%88%86%E8%AF%8Dtokenization"><span class="toc-text">零、分词tokenization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%88%86%E8%AF%8D%E8%A7%84%E5%88%99"><span class="toc-text">1.2 分词规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-character-based-tokenizer"><span class="toc-text">1.3 character-based-tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-Subword-tokenization"><span class="toc-text">1.4 Subword tokenization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-Byte-Pair-Encoding%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81-BPE"><span class="toc-text">1.5 Byte-Pair Encoding字节对编码 (BPE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E5%AD%97%E8%8A%82%E7%BA%A7-BPE%EF%BC%88Byte-level-BPE%EF%BC%89"><span class="toc-text">1.6 字节级 BPE（Byte-level BPE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-WordPiece"><span class="toc-text">1.7 WordPiece</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-Unigram"><span class="toc-text">1.8 Unigram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-9-SentencePiece"><span class="toc-text">1.9 SentencePiece</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">一、训练分词器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Using-tokenizers-from-%F0%9F%A4%97-Tokenizers"><span class="toc-text">1.1 Using tokenizers from 🤗 Tokenizers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Train-your-tokenizer"><span class="toc-text">1.2 Train your tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-%E4%BB%8E%E5%A4%B4%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">1.2.1 从头训练分词器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E4%BD%BF%E7%94%A8%E5%B7%B2%E6%9C%89%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%AD%E7%BB%83"><span class="toc-text">1.2.2 使用已有的分词器训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BA%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">1.2.3 从头构建分词器</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-2-WordPiece-model-like-BERT"><span class="toc-text">1.2.3.2 WordPiece model like BERT</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8WordPieceTrainer"><span class="toc-text">直接使用WordPieceTrainer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-3-BPE-model-like-GPT-2"><span class="toc-text">1.2.3.3 BPE model like GPT-2</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-4-Unigram-model-like-Albert"><span class="toc-text">1.2.3.4 Unigram model like Albert</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81HF%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F"><span class="toc-text">二、HF模型预训练方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-text">1.加载数据集：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83tokenizer"><span class="toc-text">2.训练tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">2.2 分词器的训练参数如下：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E5%88%86%E8%AF%8D%E5%99%A8%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-text">2.3 分词器保存和加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.从头开始训练语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 初始化模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-text">3.3 创建训练集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%88%9D%E5%A7%8B%E5%8C%96-Trainer%E5%B9%B6%E8%AE%AD%E7%BB%83"><span class="toc-text">3.4 初始化 Trainer并训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A3%80%E6%9F%A5%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">5. 检查训练好的模型</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">天池-新闻文本分类task3：bert模型</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-07</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">12.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 50 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h2><p>&#8195;&#8195;比赛官方链接为：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">《零基础入门NLP - 新闻文本分类》</a>。<br>&#8195;&#8195;讨论区有大佬张帆、惊鹊和张贤等人的代码，值得大家仔细阅读。<br>&#8195;&#8195;最后我的模型参考了这些代码的一些config，比如bert.config，lr等等。然后大佬们的代码对我来说还是太复杂，pytorch功力不够，看的吃力。所以自己用huggingface实现了。<br>&#8195;&#8195;第一步分词我就考虑了很久，没有像张帆他们那样用pytorch具体一步步写，而是参考HF主页的教程。所以一开始我是翻译了构建tokenizer的教程，如果对比赛代码中分词有疑问的可以参考。</p>
<h2 id="三、最终代码及解析"><a href="#三、最终代码及解析" class="headerlink" title="三、最终代码及解析"></a>三、最终代码及解析</h2><p>主要思路：</p>
<ol>
<li>构建分词器。参考HF教程<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">《How to train and use your very own tokenizer》</a>。3750、648、900这三个应该是标点符号（详见张帆task02的分析），直接把这三个替换成‘，’、‘.’和‘！’。主要是为了断句。在预分词器pre_tokenizers.BertPreTokenizer中，有根据标点进行断句的方法，直接将文本换成带标点的格式就行，预分词器会自动断句。<ul>
<li>和BERT 有关的 Tokenizer 主要写在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py">models/bert/tokenization_bert.py</a>中。这部分内容其实在<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT">nlp教程3.1</a>里面有写。<span id="more"></span></li>
<li>BasicTokenizer负责处理的第一步——按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。对于中文字符，通过预处理（加空格）来按字分割；同时可以通过never_split指定对某些词不进行分割；</li>
<li>分词器参考bert-baseChinese的分词器配置<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-chinese/blob/main/tokenizer.json">tokenizer.json</a>。具体的：<ul>
<li>“normalizer”:”BertNormalizer”。</li>
<li>“pre_tokenizer”:{“type”:”BertPreTokenizer”}</li>
<li>“post_processor”:{“type”:”TemplateProcessing”}</li>
<li>“decoder”:{“type”:”WordPiece”,”prefix”:”##”,”cleanup”:true}</li>
<li>“model”:{“type”:”WordPiece”,”unk_token”:”[UNK]”…}</li>
</ul>
</li>
<li>词表大小选的7000，我是看讨论区是6900+，这里还有点没想清楚。中文的wordpiece是也可以吧高频率的汉字拼成词语吧，用‘##’连接。如果这样，采用wordpiece，vocab size大一点，最后整词掩码感觉效果会更好。但是整词掩码我不知道怎么写，所以最后没有用。没有整词掩码，就没有wordpiece的必要了。所以我做的有点矛盾，最后是懒得改了，就这么写。</li>
</ul>
</li>
<li>预训练bert模型。参考nlp教程4.5 <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.5-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">《微调语言模型》</a>。用BertConfig配置模型参数，设置了一个小型的初始化bert进行mlm任务预训练。<ul>
<li>训练集选择train_set和test两个csv，因为test预训练时不需要分类标签，只作为掩码任务，不存在数据泄露问题。注意训练数据要把三个token换成标点。</li>
<li>最终我训练了8个epoch（第一次5个epoch，lr=4e-4，loss=1.695，batch_size=128。第二次3个epoch，lr=2e-4，结果一开始steps=3000时，loss=1.78，应该是第二次训练的lr还是太大，震荡了。第二个epoch快训练完才降到1.69，浪费了两个小时。最终loss是1.63）</li>
<li>我选择的是colab的tpu进行训练，每个epoch是13903steps，大概50-60分钟左右。如果是colab-GPU，大概31-35小时。colab tpu使用可以参考我的代码。如果选择tpu时提示无法分配，不用管，继续连接，第二次连接我都成功了。</li>
</ul>
</li>
<li>分类微调，加一个首尾截断。我是之前看文章说文章分类首尾截断效果更好（<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/WBK-XYzP-vIf6Ni6GO-diQ">论文解读】文本分类上分利器:Bert微调trick大全</a>）。trainer没有首尾截断的机制，在前面数据处理时用pandas实现。最终训练了6个epoch，用tpu大概88分钟（我也是跑了两次。中间colab断了…）</li>
<li>读取测试集，跟训练集一样处理，保存结果并提交。最终得分0.961。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span>(<span class="params">PreTrainedTokenizer</span>):</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case,</span><br><span class="line">                never_split=never_split,</span><br><span class="line">                tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">                strip_accents=strip_accents,</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">#BasicTokenizer中定义了标点分割的方法，不需要再去另外处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = <span class="built_in">list</span>(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[-<span class="number">1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-1-构建分词器"><a href="#3-1-构建分词器" class="headerlink" title="3.1 构建分词器"></a>3.1 构建分词器</h3><p>参考本文第二节，并查看了<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-chinese/tree/main">bert-base-chinese,josn</a>文件配置分词器。训练语言模型参考<a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">此教程</a>及<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.5-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">中文翻译</a>。<br>感受：最坑的是训练分词器，从头到尾选择decoders, models, pre_tokenizers, processors, trainers, Tokenizer有点麻烦。最后装进PreTrainedTokenizerFast之后还有些东西需要设置，看了好多次文档才试出来。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#从google云盘上加载数据</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&#x27;/content/drive/MyDrive/transformers/天池-入门NLP - 新闻文本分类&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#安装transformers=4.11.2</span></span><br><span class="line">!pip install transformers datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件读取</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep =<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">df=pd.concat((train_df,test_df))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将3750/648/900改成标点符号，删除原text列，新增words列重名为text列</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replacepunc</span>(<span class="params">x</span>):</span></span><br><span class="line">  x=re.sub(<span class="string">&#x27;3750&#x27;</span>,<span class="string">&quot;,&quot;</span>,x)</span><br><span class="line">  x=re.sub(<span class="string">&#x27;900&#x27;</span>,<span class="string">&quot;.&quot;</span>,x)</span><br><span class="line">  x=re.sub(<span class="string">&#x27;648&#x27;</span>,<span class="string">&quot;!&quot;</span>,x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;words&#x27;</span>]=df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: replacepunc(x))</span><br><span class="line">df.drop(<span class="string">&#x27;text&#x27;</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df.columns=[<span class="string">&#x27;label&#x27;</span>,<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据载入dataset，去除多余的列，只保留text列</span></span><br><span class="line">data=Dataset.from_pandas(df).remove_columns([<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;__index_level_0__&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构建数据批处理迭代器，这部分代码是参考HF主页教程</span></span><br><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iterator</span>():</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(data), batch_size):</span><br><span class="line">    <span class="keyword">yield</span> data[<span class="string">&#x27;text&#x27;</span>][i : i + batch_size]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置分词器并进行训练</span></span><br><span class="line"><span class="comment">#初始化分词器、预分词器</span></span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unl_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">7000</span>,min_frequency=<span class="number">2</span>,special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>])</span><br><span class="line">tokenizer.decoders = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#进行分词后处理</span></span><br><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">mask_token_id = tokenizer.token_to_id(<span class="string">&quot;[MASK]&quot;</span>)</span><br><span class="line">pad_token_id = tokenizer.token_to_id(<span class="string">&quot;[PAD]&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>,cls_token_id),(<span class="string">&quot;[SEP]&quot;</span>,sep_token_id),(<span class="string">&quot;[MASK]&quot;</span>,mask_token_id)],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">tokenizer.enable_truncation(max_length=<span class="number">512</span>)</span><br><span class="line">tokenizer.enable_padding(pad_token=<span class="string">&#x27;[PAD]&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试分词结果</span></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&#x27;2491 4109 1757 7539 648 3695 3038 4490 23 7019 3731 4109 3792 2465&#x27;</span>,<span class="string">&#x27; 2893 7212 5296 1667 3618 7044 1519 5413 1283 6122 4893 7495 2435 5510&#x27;</span>)</span><br><span class="line">encoding.tokens</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;保存模型并重新加载</span></span><br><span class="line"><span class="string">tokenizer已经完成，我们必须将它放在与我们要使用的模型相对应的标记器 fast 类。</span></span><br><span class="line"><span class="string">正在构建的分词器与 Transformers 中的任何类都不匹配(分词器非常特殊)，</span></span><br><span class="line"><span class="string">您可以将它包装在 PreTrainedTokenizerFast 中&quot;&quot;&quot;</span></span><br><span class="line">tokenizer.save(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast</span><br><span class="line">(tokenizer_file=<span class="string">&quot;tokenizer.json&quot;</span>,</span><br><span class="line">model_max_length=<span class="number">512</span>,mask_token=<span class="string">&#x27;[MASK]&#x27;</span>,pad_token=<span class="string">&#x27;[PAD]&#x27;</span>,</span><br><span class="line">unk_token=<span class="string">&#x27;[UNK]&#x27;</span>,cls_token=<span class="string">&#x27;[CLS]&#x27;</span>,sep_token=<span class="string">&#x27;[SEP]&#x27;</span>,</span><br><span class="line">padding_side=<span class="string">&#x27;right&#x27;</span>,return_special_tokens_mask=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#PreTrainedTokenizerFast中一定要设置mask_token，pad_token等，</span></span><br><span class="line"><span class="comment">#不然mlm报错没有设定mask_token以及分词器无法padding</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-预训练bert模型"><a href="#3-2-预训练bert模型" class="headerlink" title="3.2 预训练bert模型"></a>3.2 预训练bert模型</h3><p>参考nlp教程4.5<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#data_collator是一个函数，负责获取样本并将它们批处理成张量</span></span><br><span class="line"><span class="comment">#在data_collator中可以确保每次以新的方式完成随机掩蔽。</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForLanguageModeling</span><br><span class="line">data_collator = DataCollatorForLanguageModeling(tokenizer=fast_tokenizer,mlm=<span class="literal">True</span>,mlm_probability=<span class="number">0.15</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化bert模型，参数参考讨论区代码</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig</span><br><span class="line">config = BertConfig(</span><br><span class="line">    vocab_size=<span class="number">7000</span>,</span><br><span class="line">    hidden_size=<span class="number">512</span>,</span><br><span class="line">    intermediate_size=<span class="number">4</span>*<span class="number">512</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">512</span>,</span><br><span class="line">    num_hidden_layers=<span class="number">4</span>,</span><br><span class="line">    num_attention_heads=<span class="number">4</span>,</span><br><span class="line">    type_vocab_size=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line">model = BertForMaskedLM(config=config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#（掉线后）加载训练到一半的模型：</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">&#x27;/content/drive/MyDrive/transformers/天池-入门NLP - 新闻文本分类/test-clm/checkpoint-56000&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#数据进行分词预处理，删除‘text&#x27;列，否则后面拼接的时候会报错。</span></span><br><span class="line">tokenized_datasets=data.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;text&#x27;</span>]),batched=<span class="literal">True</span>).remove_columns(<span class="string">&quot;text&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼接所有文本，这一块解释可以看nlp 4.5教程</span></span><br><span class="line">block_size = <span class="number">128</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span></span><br><span class="line"></span><br><span class="line">  concatenated_examples = &#123;k: <span class="built_in">sum</span>(examples[k], []) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">  total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">  <span class="comment"># 我们将余数对应的部分去掉。但如果模型支持的话，可以添加padding，您可以根据需要定制此部件。</span></span><br><span class="line">  total_length = (total_length // block_size) * block_size</span><br><span class="line">  <span class="comment"># 通过max_len进行分割。</span></span><br><span class="line">  result = &#123;</span><br><span class="line">      k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">      <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">  &#125;</span><br><span class="line">  result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy()</span><br><span class="line">  <span class="keyword">return</span> result</span><br><span class="line"> </span><br><span class="line"> lm_datasets = tokenized_datasets.<span class="built_in">map</span>(</span><br><span class="line">    group_texts,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=<span class="number">1000</span>,</span><br><span class="line">    num_proc=<span class="number">4</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载和保存拼接后的文本，掉线的时候这么做</span></span><br><span class="line">lm_datasets.save_to_disk(<span class="string">&#x27;./lm&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_from_disk</span><br><span class="line">lm_datasets=load_from_disk(<span class="string">&#x27;./lm&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#解码分词器预处理的lm_datasets数据，里面有标点符号</span></span><br><span class="line">la=fast_tokenizer.decode(lm_datasets[<span class="number">0</span>][<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">la</span><br><span class="line"></span><br><span class="line">[CLS] <span class="number">2967</span> <span class="number">6758</span> <span class="number">339</span> <span class="number">2021</span> <span class="number">1854</span> <span class="number">3731</span> <span class="number">4109</span> <span class="number">3792</span> <span class="number">4149</span> <span class="number">1519</span> <span class="number">2058</span> <span class="number">3912</span> <span class="number">2465</span> <span class="number">2410</span> <span class="number">1219</span> <span class="number">6654</span> <span class="number">7539</span> <span class="number">264</span> <span class="number">2456</span> <span class="number">4811</span> <span class="number">1292</span> <span class="number">2109</span> <span class="number">6905</span> <span class="number">5520</span> <span class="number">7058</span> <span class="number">6045</span> <span class="number">3634</span> <span class="number">6591</span> <span class="number">3530</span> <span class="number">6508</span> <span class="number">2465</span> <span class="number">7044</span> <span class="number">1519</span> <span class="number">3659</span> <span class="number">2073</span>, <span class="number">3731</span> <span class="number">4109</span> <span class="number">3792</span> <span class="number">6831</span> <span class="number">2614</span> <span class="number">3370</span> <span class="number">4269</span> <span class="number">3370</span> <span class="number">486</span> <span class="number">5770</span> <span class="number">4109</span> <span class="number">4125</span>, <span class="number">5445</span> <span class="number">2466</span> <span class="number">6831</span> <span class="number">6758</span> <span class="number">3743</span> <span class="number">3630</span> <span class="number">1726</span> <span class="number">2313</span> <span class="number">5906</span> <span class="number">826</span> <span class="number">4516</span> <span class="number">657.</span> <span class="number">1871</span> <span class="number">7044</span>, <span class="number">2967</span> <span class="number">3731</span> <span class="number">1757</span> <span class="number">1939</span>! <span class="number">2828</span> <span class="number">4704</span> <span class="number">7039</span> <span class="number">3706</span>, <span class="number">965</span> <span class="number">2490</span> <span class="number">7399</span> <span class="number">3743</span> <span class="number">2145</span> <span class="number">2407</span> <span class="number">7451</span> <span class="number">3775</span> <span class="number">6017</span> <span class="number">5998</span> <span class="number">1641</span> <span class="number">299</span> <span class="number">4704</span> <span class="number">2621</span> <span class="number">7029</span> <span class="number">3056</span> <span class="number">6333</span> <span class="number">433</span>! <span class="number">1667</span> <span class="number">1099.</span> <span class="number">2289</span> <span class="number">1099</span>! <span class="number">5780</span> <span class="number">220</span> <span class="number">7044</span> <span class="number">1279</span> <span class="number">7426</span> <span class="number">4269</span>, <span class="number">2967</span> <span class="number">6758</span> <span class="number">6631</span> <span class="number">3099</span> <span class="number">2205</span> <span class="number">7305</span> <span class="number">2620</span> <span class="number">5977</span>, <span class="number">3329</span> <span class="number">1793</span> <span class="number">6666</span> <span class="number">2042</span> <span class="number">3193</span> <span class="number">4149</span> <span class="number">1519</span> <span class="number">7039</span> <span class="number">3706</span> <span class="number">2446</span> <span class="number">5399</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用GPU训练，运行这段代码</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>==GPU memory开始占用1GB，但是还没开始使用计算。==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#安装TPU依赖</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">assert</span> os.environ[<span class="string">&#x27;COLAB_TPU_ADDR&#x27;</span>], <span class="string">&#x27;Make sure to select TPU from Edit &gt; Notebook settings &gt; Hardware accelerator&#x27;</span></span><br><span class="line">!pip install cloud-tpu-client==<span class="number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="number">1.9</span>-cp37-cp37m-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line"><span class="comment">#将模型复制到TPU进行训练</span></span><br><span class="line"><span class="keyword">import</span> torch_xla.core.xla_model <span class="keyword">as</span> xm</span><br><span class="line">device = xm.xla_device()</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设定args和trainer准备训练.3000步看一次loss，9000步保存一次模型（怕掉线）</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;Test-Clm&quot;</span>,</span><br><span class="line">    logging_strategy=<span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    logging_steps=<span class="number">3000</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    save_steps=<span class="number">9000</span>,</span><br><span class="line">    num_train_epochs=<span class="number">2</span>,</span><br><span class="line">    learning_rate=<span class="number">3e-4</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">96</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=lm_datasets,</span><br><span class="line">    data_collator=data_collator)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练并保存模型</span></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line">trainer.save_model(<span class="string">&quot;./pre_Bert&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这段是当时batch_size太高，显存爆了，我找一下原因。可以忽略。<br>1%的数据试验</p>
<ol>
<li>第一次训练涨到5.9GB，5个epoch，540steps，batch_size=128。训练完后是显存1.2GB。logging_steps=100,可以选择多久看一次loss。</li>
<li>再次训练没有指定batch_size，显存是1.8GB。训练完1.5GB。算了一下默认batch_size=8。</li>
</ol>
<h3 id="3-3-分类任务微调："><a href="#3-3-分类任务微调：" class="headerlink" title="3.3 分类任务微调："></a>3.3 分类任务微调：</h3><ol>
<li>加载预训练好的模型，GPU或TPU训练<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;./pre_Bert&quot;</span>,num_labels=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用GPU训练</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将模型复制到TPU进行训练</span></span><br><span class="line"><span class="keyword">import</span> torch_xla.core.xla_model <span class="keyword">as</span> xm</span><br><span class="line">device = xm.xla_device()</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure></li>
<li>读取数据集，准备进行预处理</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#读取数据并shuffle</span></span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>).sample(frac=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将训练数据中三个token换成标点</span></span><br><span class="line">train_df[<span class="string">&#x27;texts&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x:replacepunc(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#准备将text文本首尾截断，各取255tokens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slipt2</span>(<span class="params">x</span>):</span></span><br><span class="line">	ls=x.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">	le=<span class="built_in">len</span>(ls)</span><br><span class="line">	<span class="keyword">if</span> le&lt;<span class="number">511</span>:</span><br><span class="line">	    <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">	    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(ls[:<span class="number">255</span>]+ls[-<span class="number">255</span>:])</span><br></pre></td></tr></table></figure>
<ol>
<li>划分训练集和测试集，比例0.1<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">val_df=train_df.iloc[:<span class="number">20000</span>, ]</span><br><span class="line">trains_df=train_df.iloc[<span class="number">20000</span>:, ]</span><br><span class="line"></span><br><span class="line"><span class="comment">#首尾截断</span></span><br><span class="line">val_df[<span class="string">&#x27;summary&#x27;</span>]=val_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">trains_df[<span class="string">&#x27;summary&#x27;</span>]=trains_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载到dataset并预处理</span></span><br><span class="line">trains_ds=Dataset.from_pandas(trains_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line">val_ds=Dataset.from_pandas(val_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenized_trains_ds=trains_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_val_ds=val_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>设置TrainingArguments和Trainer<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置acc评估方式</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line">metric = load_metric(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#进行任务微调</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments,Trainer</span><br><span class="line">args=TrainingArguments(</span><br><span class="line">  output_dir=<span class="string">&#x27;news-classification-2&#x27;</span>,</span><br><span class="line">  evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">  save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">  learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">  per_device_train_batch_size=<span class="number">96</span>,</span><br><span class="line">  per_device_eval_batch_size=<span class="number">96</span>,</span><br><span class="line">  num_train_epochs=<span class="number">6</span>,</span><br><span class="line">  weight_decay=<span class="number">0.01</span>,</span><br><span class="line">  load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">  metric_for_best_model=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer=Trainer(</span><br><span class="line">  model,</span><br><span class="line">  args,</span><br><span class="line">  train_dataset=tokenized_trains_ds,</span><br><span class="line">  eval_dataset=tokenized_val_ds,</span><br><span class="line">  tokenizer=fast_tokenizer,</span><br><span class="line">  compute_metrics=compute_metrics)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>==训练完GPU memory还是1.5GB==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.save_model(<span class="string">&quot;./finally_bert&quot;</span>)</span><br></pre></td></tr></table></figure><br>==一开始训练,GPU memory跳到15.8GB（batch_size=128）。爆了之后选择分类微调模型的batch_size=16，GPU memory为3.4GB==</p>
<ol>
<li>最后读取测试集，预测结果<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取测试集并预处理</span></span><br><span class="line"><span class="comment">#读取测试集</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将训练数据中三个token换成标点</span></span><br><span class="line">test_df[<span class="string">&#x27;texts&#x27;</span>]=test_df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x:replacepunc(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#首尾截断</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line">test_df[<span class="string">&#x27;summary&#x27;</span>]=test_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载到dataset并预处理</span></span><br><span class="line">test_ds=Dataset.from_pandas(test_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenized_test_ds=test_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用trainer预测结果并保存</span></span><br><span class="line">predictions,metrics,Loss=trainer.predict(tokenized_test_ds,metric_key_prefix=<span class="string">&quot;test&quot;</span>)</span><br><span class="line">pred=np.argmax(predictions,axis=<span class="number">1</span>)</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:pred&#125;).to_csv(<span class="string">&#x27;submit1022.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>3.4 赛事总结：</p>
<ol>
<li>一开始要搞懂baseline的基本框架和config作为参考，如果比较难读不懂，可以直接跑一遍或者debug（还没有跑，有人说内存爆了。。。）</li>
<li>最开始用少量数据跑，batch_size、学习率、数据集、epoch和时长确定好再跑一遍。之前就是嫌时间太长加了batch_size结果跑到模型微调时显存崩了，所有缓存数据都没了。</li>
<li>中间数据和训练中的模型要记得保存，一旦掉线或者崩了或者想修改参数可以继续加载再跑。因为一开始不知道如何保存和加载datasets数据、kaggle的notebook老是无法保存，结果总是白跑了模型，浪费时间。</li>
<li>想到再补</li>
</ol>
<h2 id="零、分词tokenization"><a href="#零、分词tokenization" class="headerlink" title="零、分词tokenization"></a>零、分词tokenization</h2><p>比赛数据脱敏，需要从头开始预训练。第一步就是建立词表，训练自己的分词器</p>
<p>参考资料：<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/tokenizer_summary.html">《Summary of the tokenizers》</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/jokerxsy/article/details/116998827">《[NLP]——BPE、WordPiece、Unigram and SentencePiece》</a></p>
<p>==wordpiece和BPE的差异在于合并时对token对的选择:BPE是选择出现次数最大的，wordpiece衡量的是token对和单独的两个token之间的概率差，选择概率差最大的进行合并。==</p>
<p>考虑token a和b，以及合并之后的token ab，概率差的公式如下:</p>
<script type="math/tex; mode=display">p(a,b)/(p(a)∗p(b))</script><p>==这可以近似理解为合并前后，整个语料的互信息。即，当前选择合并的token对能够让语料的熵最小化-&gt;确定性最大化-&gt;信息量最小化-&gt;在计算机中存储所需要的编码长度最短化。==</p>
<p>所以如果词表中字符a和b本身次数就很高，如果合并ab的概率就算不高（比如0.1）：</p>
<ul>
<li>对于bpe来说ab次数多，需要合并</li>
<li>对于wordpiece，ab合并概率低，不合并</li>
</ul>
<p>tokenizer可以将文本拆分为词或子词（即标记文本）。 🤗 Transformers 中使用的三种主要类型的分词器： Byte-Pair Encoding字节对编码 (BPE)、WordPiece 和 SentencePiece，下面展示哪个模型使用哪种分词器类型的示例。</p>
<blockquote>
<p>在每个模型页面上，您可以查看相关分词器的文档以了解预训练模型使用的分词器类型。 例如，如果我们查看 BertTokenizer，我们可以看到该模型使用 WordPiece</p>
</blockquote>
<h3 id="1-2-分词规则"><a href="#1-2-分词规则" class="headerlink" title="1.2 分词规则"></a>1.2 分词规则</h3><p>分词有多种方式，对于一个句子：<br>“Don’t you love 🤗 Transformers? We sure do.”</p>
<ul>
<li>可以按空格分词：<br>[“Don’t”, “you”, “love”, “🤗”, “Transformers?”, “We”, “sure”, “do.”]</li>
<li>区分标点：tokens和标点的各种组合会导致模型必须学习的表示数量激增，所以应该予以清理。标点处理后得到：<br>[“Don”, “‘“, “t”, “you”, “love”, “🤗”, “Transformers”, “?”, “We”, “sure”, “do”, “.”]</li>
<li>区分缩写：“Don’t”代表“do not”，因此最好将其标记为 [“Do”, “n’t”]。这就是事情开始变得复杂的地方，也是每个模型都有自己的标记器类型的部分原因</li>
</ul>
<p>根据我们应用于标记文本的规则，为相同的文本生成不同的标记输出。 预训练模型输入必须是，用于标记其训练数据的相同规则的标记输入，这样才能正常执行。</p>
<p>spaCy 和 Moses 是两种流行的基于规则的标记器。 将它们应用到我们的示例中，spaCy 和 Moses 将输出如下内容：<br>[“Do”, “n’t”, “you”, “love”, “🤗”, “Transformers”, “?”, “We”, “sure”, “do”, “.”]</p>
<p>这里使用了空格和标点符号化以及基于规则的标记化，其松散定义为将句子拆分为单词。这种标记化方法非常简单，但是可能会导致大量文本语料库出现问题，生成一个非常大的词汇表（使用的所有唯一单词和标记的集合）。例如，Transformer XL 使用空格和标点符号化，导致词汇量大小为 267,735！</p>
<p>如此大的词汇量迫使模型有一个巨大的嵌入矩阵作为输入和输出层，这会导致内存和时间复杂度的增加。一般来说，==transformers 模型的词汇量很少超过 50,000，尤其是当它们仅在一种语言上进行预训练时==。</p>
<p>那么如果简单的空格和标点符号化不能令人满意，为什么不简单地对字符char进行标记化呢？</p>
<h3 id="1-3-character-based-tokenizer"><a href="#1-3-character-based-tokenizer" class="headerlink" title="1.3 character-based-tokenizer"></a>1.3 character-based-tokenizer</h3><p>字符标记化往往伴随着性能的损失，使模型学习有意义的输入表示变得更加困难。例如。 学习字母“t”的有意义的上下文比学习单词“today”的上下文无关表示要困难得多。 因此，为了两全其美，transformers 模型使用了词级和字符级标记化之间的混合，称为子词标记化。</p>
<h3 id="1-4-Subword-tokenization"><a href="#1-4-Subword-tokenization" class="headerlink" title="1.4 Subword tokenization"></a>1.4 Subword tokenization</h3><p>原则：不应将常用词拆分为更小的子词，而应将稀有词分解为有意义的子词。以通过将子词串在一起来形成（几乎）任意长的复杂词。<br>例如，“annoyingly”可能被认为是一个罕见的词，可以分解为“annoying”和“ly”。 “annoying”和“ly”作为独立的子词出现的频率会更高，同时“annoyingly”的意思被“annoying”和“ly”的复合词所保持。</p>
<ul>
<li>子词标记化允许模型具有合理的词汇量</li>
<li>同时能够学习有意义的上下文无关表示</li>
<li>使模型能够通过将它们分解为已知的子词来处理它以前从未见过的词</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;I have a new GPU!&quot;</span>)</span><br><span class="line">[<span class="string">&quot;i&quot;</span>, <span class="string">&quot;have&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;new&quot;</span>, <span class="string">&quot;gp&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;!&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>我们可以看到单词 [“i”, “have”, “a”, “new”] 出现在分词器的词汇表中，但单词“gpu”却没有。 因此，分词器将“gpu”拆分为已知的子词：[“gp”和“##u”]。 “##”表示令牌的其余部分应附加到前一个，没有空格（用于解码或逆转令牌化）。</p>
<p>再举一个例子，XLNetTokenizer 将我们之前的示例文本分词如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizer</span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(<span class="string">&quot;xlnet-base-cased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;Don&#x27;t you love 🤗 Transformers? We sure do.&quot;</span>)</span><br><span class="line">[<span class="string">&quot;▁Don&quot;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="string">&quot;▁you&quot;</span>, <span class="string">&quot;▁love&quot;</span>, <span class="string">&quot;▁&quot;</span>, <span class="string">&quot;🤗&quot;</span>, <span class="string">&quot;▁&quot;</span>, <span class="string">&quot;Transform&quot;</span>, <span class="string">&quot;ers&quot;</span>, <span class="string">&quot;?&quot;</span>, <span class="string">&quot;▁We&quot;</span>, <span class="string">&quot;▁sure&quot;</span>, <span class="string">&quot;▁do&quot;</span>, <span class="string">&quot;.&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>SentencePiece：将罕见词”Transformers” 拆分成更常见的子词 “Transform” 和 “ers”.</p>
<p>现在让我们看看不同的子词标记化算法是如何工作的。</p>
<h3 id="1-5-Byte-Pair-Encoding字节对编码-BPE"><a href="#1-5-Byte-Pair-Encoding字节对编码-BPE" class="headerlink" title="1.5 Byte-Pair Encoding字节对编码 (BPE)"></a>1.5 Byte-Pair Encoding字节对编码 (BPE)</h3><ul>
<li>Byte-Pair Encoding (BPE) 是Neural Machine Translation引入的（具有罕见词的子词units）。依赖于将训练数据拆分为word的pre-tokenizer。预标记化可以像空格化（space tokenization）一样简单，就像GPT-2, Roberta。</li>
<li>更高级的预标记化包括基于规则的标记化，例如XLM、FlauBERT（在大多数语言中使用 Moses）或 GPT（使用 Spacy 和 ftfy）来计算训练语料库中每个单词的频率。</li>
<li>在预标记化之后：<ul>
<li>根据训练数据形成一系列唯一token及其出现频率</li>
<li>BPE 创建一个由唯一单词集中的所有符号组成的基本词汇表</li>
<li>学习合并规则以从基本词汇表的两个符号形成一个新符号。直到词汇量达到所需的词汇量大小。请注意，==所需的词汇量是在训练分词器之前定义的超参数==。</li>
</ul>
</li>
</ul>
<p>举个例子，让我们假设在预标记化之后，已经确定了以下一组单词，包括它们的频率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>基本词汇是 [“b”, “g”, “h”, “n”, “p”, “s”, “u”]。 将所有单词拆分为基本词汇表的符号，我们得到：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>BPE 计算每个可能的符号对的频率并选择出现频率最高的符号对。最频繁的符号对是“u”后跟“g”，总共出现 10 + 5 + 5 = 20 次，因此，分词器学习的第一个合并规则是将所有“u”符号和后跟“g”符号组合在一起。 接下来，将“ug”添加到词汇表中。 这组词然后变成：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>BPE 识别下一个最常见的符号对：“u”后跟“n”16 次。 “u”, “n” 合并到 “un” 并添加到词汇表中。 下一个最频繁的符号对是“h”后跟“ug”，出现 15 次。 这对再次合并，并且可以将“hug”添加到词汇表中。</li>
</ol>
<p>在这个阶段，词汇是 [“b”, “g”, “h”, “n”, “p”, “s”, “u”, “ug”, “un”, “hug”] 和我们的 一组独特的词表示为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>假设字节对编码训练将在此时停止，然后将学习到的合并规则应用于新单词。例如”bug”分词成 [“b”, “ug”]，但是”mug” 分词成 [“<unk>“, “ug”]。因为词汇表不包含：“m”。</li>
</ol>
<p>如前所述，词汇量大小，即基本词汇量大小 + 合并次数（base vocabulary size + the number of merges），是一个可供选择的超参数。 例如，GPT 的词汇量是 40,478，因为它们有 478 个基本字符，并且在 40,000 次合并后选择停止训练。</p>
<h3 id="1-6-字节级-BPE（Byte-level-BPE）"><a href="#1-6-字节级-BPE（Byte-level-BPE）" class="headerlink" title="1.6 字节级 BPE（Byte-level BPE）"></a>1.6 字节级 BPE（Byte-level BPE）</h3><p>GPT-2 使用字节作为基础词汇，这是一个巧妙的技巧，可以强制基础词汇的大小为 256，同时确保每个基础字符都包含在词汇中。再加上一些额外的标点符号处理规则，GPT-2的分词器不需要\<unk>符号。<br>GPT-2 的词汇量大小为 50,257，对应于 256 字节的基本标记、一个特殊的文本结束标记和通过 50,000 次合并学习的符号。</p>
<h3 id="1-7-WordPiece"><a href="#1-7-WordPiece" class="headerlink" title="1.7 WordPiece"></a>1.7 WordPiece</h3><p>WordPiece 是用于 BERT、DistilBERT 和 Electra 的子词标记化算法，与 BPE 非常相似。 WordPiece 首先初始化词汇表以包含训练数据中存在的每个字符，并逐步学习给定数量的合并规则。==与 BPE 相比，WordPiece 不选择最频繁的符号对，而是选择将训练数据添加到词汇表中的可能性最大化的符号对==。</p>
<p>那么这到底是什么意思呢？参考前面的例子，最大化训练数据的似然性相当于找到符号对，其概率除以其第一个符号后跟第二个符号的概率在所有符号对中最大。例如。只有当“ug”除以“u”、“g”的概率大于任何其他符号对时，“u”和“g”才会被合并。直观地说，WordPiece 与 BPE 略有不同，它通过合并两个符号来评估它的损失，以确保it’s worth it。</p>
<h3 id="1-8-Unigram"><a href="#1-8-Unigram" class="headerlink" title="1.8 Unigram"></a>1.8 Unigram</h3><ul>
<li>基本词汇表可以对应于所有预先标记的单词和最常见的子串</li>
<li>删除了损失增加最低的符号，重复这个过程，直到词汇量达到所需的大小。</li>
</ul>
<p>Unigram 是在 Subword 正则化：与 BPE 或 WordPiece 相比，==Unigram 将其基本词汇表初始化为大量符号，并逐步缩减每个符号以获得较小的词汇表。例如，基本词汇表可以对应于所有预先标记的单词和最常见的子串==。 Unigram 不直接用于transformers中的任何模型，但它与 SentencePiece 结合使用。</p>
<p>在每个训练步骤中，Unigram 算法在给定当前词汇和 unigram 语言模型的情况下定义训练数据的损失（通常定义为对数似然）。然后，==对于词汇表中的每个符号，算法计算如果要从词汇表中删除该符号，总体损失会增加多少。然后 Unigram 删除了损失增加最低的符号的 概率p（通常为 10% 或 20%），即那些对训练数据的整体损失影响最小的符号。重复这个过程，直到词汇量达到所需的大小。== Unigram 算法始终保留基本字符，以便可以对任何单词进行标记。</p>
<p>由于 Unigram 不基于合并规则（与 BPE 和 WordPiece 不同），因此该算法有多种方法可以在训练后对新文本进行标记。例如，如果经过训练的 Unigram 分词器展示词汇表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>, <span class="string">&quot;un&quot;</span>, <span class="string">&quot;hug&quot;</span>],</span><br></pre></td></tr></table></figure>
<p>“hug”可以标记为 [“hug”, “s”], [“h”, “ug”, “s”] 或 [“h”, “u”, “g”, “s”]。 那么该选择哪一个呢？ Unigram 在保存词汇的基础上还保存了训练语料库中每个标记的概率，以便在训练后计算每个可能的标记化的概率。 该算法在实践中只是简单地选择最可能的标记化，但也提供了根据概率对可能的标记化进行采样的可能性。</p>
<p>这些概率由分词器训练的损失定义。 假设训练数据由单词 x1,…,xN 组成，并且单词 xi 的所有可能标记的集合被定义为 S(xi)，那么总损失定义为：<br><img src="https://img-blog.csdnimg.cn/93a7af17c6b14b92ba23d3f6e59ceccc.png" alt="在这里插入图片描述"></p>
<h3 id="1-9-SentencePiece"><a href="#1-9-SentencePiece" class="headerlink" title="1.9 SentencePiece"></a>1.9 SentencePiece</h3><p>到目前为止描述的所有标记化算法都有相同的问题：==假设输入文本使用空格来分隔单词==。<br>但是，并非所有语言都使用空格来分隔单词，例如中文、日文和泰文。<br>一种可能的解决方案是使用特定于语言的预分词器，例如XLM 使用特定的预分词器。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.06226.pdf">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)</a> 将输入视为原始输入流，thus including the space in the set of characters to use.然后它使用 BPE 或 unigram 算法来构建适当的词汇表。</p>
<p>例如，XLNetTokenizer 中使用的 SentencePiece，解码非常容易，因为所有标记都可以连接起来，并且“-” 被空格替换。 SentencePiece 和unigram 结合使用，包括 ALBERT、XLNet、Marian 和 T5。</p>
<p>本文参考：<a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">how_to_train.ipynb</a></p>
<h2 id="一、训练分词器"><a href="#一、训练分词器" class="headerlink" title="一、训练分词器"></a>一、训练分词器</h2><h3 id="1-1-Using-tokenizers-from-🤗-Tokenizers"><a href="#1-1-Using-tokenizers-from-🤗-Tokenizers" class="headerlink" title="1.1 Using tokenizers from 🤗 Tokenizers"></a>1.1 Using tokenizers from 🤗 Tokenizers</h3><blockquote>
<p>参考文档：<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/fast_tokenizers.html">HF文档</a></p>
</blockquote>
<p>PreTrainedTokenizerFast 依赖于 tokenizers 库。 从 🤗 Tokenizers 库中获得的分词器可以非常简单地加载到 🤗 Transformers 中。<br>在详细介绍之前，让我们首先在几行中创建一个虚拟标记器：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tokenizers.models <span class="keyword">import</span> BPE</span><br><span class="line"><span class="keyword">from</span> tokenizers.trainers <span class="keyword">import</span> BpeTrainer</span><br><span class="line"><span class="keyword">from</span> tokenizers.pre_tokenizers <span class="keyword">import</span> Whitespace</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(BPE(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line">trainer = BpeTrainer(special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = Whitespace()</span><br><span class="line">files = [...]</span><br><span class="line">tokenizer.train(files, trainer)</span><br></pre></td></tr></table></figure><br>现在有了一个我们定义的标记器，可以继续使用，或者将它保存到一个 JSON 文件中以备将来重用。</p>
<ul>
<li>直接以tokenizer object使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<ul>
<li>json文件加载使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.save(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br><span class="line"><span class="comment">#我们保存此文件的路径可以使用 tokenizer_file 参数传递给 PreTrainedTokenizerFast 初始化方法：</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="string">&quot;tokenizer.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-2-Train-your-tokenizer"><a href="#1-2-Train-your-tokenizer" class="headerlink" title="1.2 Train your tokenizer"></a>1.2 Train your tokenizer</h3><p>Transformers Notebooks——<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">How to train and use your very own tokenizer</a></p>
<h4 id="1-2-1-从头训练分词器"><a href="#1-2-1-从头训练分词器" class="headerlink" title="1.2.1 从头训练分词器"></a>1.2.1 从头训练分词器</h4><p>给定语料库上训练分词器，进而从头训练transformer模型。 在<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/tokenizer_summary.html">tokenizers summary</a> 中可以查看子词分词算法之间的差异（也就是上一节内容）。</p>
<p>下面举例使用wikitext数据集（包含 4.5MB 的文本，所以我们的例子训练速度很快）训练分词器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;text&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">36718</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">dataset[:<span class="number">5</span>]</span><br><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: [<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27; = Valkyria Chronicles III = \n&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27; Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the &quot; Nameless &quot; , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit &quot; Calamaty Raven &quot; . \n&#x27;</span>,</span><br><span class="line">  <span class="string">&quot; The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game &#x27;s opening theme was sung by May &#x27;n . \n&quot;</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>训练我们的分词器的 API 将需要一批文本的迭代器，例如文本列表：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line">all_texts = [dataset[i : i + batch_size][<span class="string">&quot;text&quot;</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size)]</span><br></pre></td></tr></table></figure><br>为了避免将所有内容加载到内存中（因为 Datasets 库将元素保存在磁盘上并且仅在请求时将它们加载到内存中），我们定义了一个 Python 迭代器来进行批处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iterator</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + batch_size][<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>接下来有两种方法训练分词器：</p>
<ol>
<li>使用现有的分词器，一行代码就可以在给定数据集上训练新的分词器</li>
<li>逐块构建分词器，因此可以自定义每一步 ！</li>
</ol>
<h4 id="1-2-2-使用已有的分词器训练"><a href="#1-2-2-使用已有的分词器训练" class="headerlink" title="1.2.2 使用已有的分词器训练"></a>1.2.2 使用已有的分词器训练</h4><p>如果您想使用与现有算法完全相同的算法和参数来训练一个分词器，您可以只使用 train_new_from_iterator API。 例如，让我们使用相同的标记化算法在 Wikitext-2 上训练新版本的 GPT-2 tokenzier。</p>
<p>首先，我们需要加载我们想要用作模型的tokenizer：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>确保您选择的标记器是快速版本（由 🤗 Tokenizers 库支持），否则 notebook 的其余部分将无法运行：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.is_fast</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>然后我们将训练语料库（list of list或我们之前定义的迭代器）提供给 train_new_from_iterator 方法。 我们还必须指定要使用的词汇量大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=<span class="number">25000</span>)</span><br></pre></td></tr></table></figure>
<p>到此就完成了分词器的训练。由于使用了Rust 支持的 🤗 Tokenizers 库，训练进行得非常快。<br>您现在有一个新的标记器可以预处理您的数据并训练语言模型。 您可以像往常一样输入输入文本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer(dataset[:<span class="number">5</span>][<span class="string">&quot;text&quot;</span>])</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [[], [<span class="number">238</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">2987</span>, <span class="number">238</span>, <span class="number">252</span>], [], [<span class="number">4657</span>, <span class="number">74</span>, <span class="number">4762</span>, <span class="number">826</span>, <span class="number">8576</span>, <span class="number">428</span>, <span class="number">466</span>, <span class="number">609</span>, <span class="number">6881</span>, <span class="number">412</span>, <span class="number">204</span>, <span class="number">9441</span>, <span class="number">311</span>, <span class="number">2746</span>, <span class="number">466</span>, <span class="number">10816</span>, <span class="number">168</span>, <span class="number">99</span>, <span class="number">150</span>, <span class="number">192</span>, <span class="number">112</span>, <span class="number">14328</span>, <span class="number">3983</span>, <span class="number">112</span>, <span class="number">4446</span>, <span class="number">94</span>, <span class="number">18288</span>, <span class="number">4446</span>, <span class="number">193</span>, <span class="number">3983</span>, <span class="number">98</span>, <span class="number">3983</span>, <span class="number">22171</span>, <span class="number">95</span>, <span class="number">19</span>, <span class="number">201</span>, <span class="number">6374</span>, <span class="number">209</span>, <span class="number">8576</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">3455</span>, <span class="number">1972</span>, <span class="number">428</span>, <span class="number">310</span>, <span class="number">201</span>, <span class="number">5099</span>, <span class="number">3242</span>, <span class="number">227</span>, <span class="number">281</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">2987</span>, <span class="number">2553</span>, <span class="number">1759</span>, <span class="number">201</span>, <span class="number">301</span>, <span class="number">196</span>, <span class="number">13996</span>, <span class="number">1496</span>, <span class="number">277</span>, <span class="number">2330</span>, <span class="number">1464</span>, <span class="number">674</span>, <span class="number">1898</span>, <span class="number">307</span>, <span class="number">742</span>, <span class="number">3541</span>, <span class="number">225</span>, <span class="number">7514</span>, <span class="number">14</span>, <span class="number">54</span>, <span class="number">719</span>, <span class="number">274</span>, <span class="number">198</span>, <span class="number">4777</span>, <span class="number">15522</span>, <span class="number">209</span>, <span class="number">19895</span>, <span class="number">221</span>, <span class="number">1341</span>, <span class="number">1633</span>, <span class="number">221</span>, <span class="number">1759</span>, <span class="number">201</span>, <span class="number">322</span>, <span class="number">301</span>, <span class="number">198</span>, <span class="number">1368</span>, <span class="number">674</span>, <span class="number">221</span>, <span class="number">198</span>, <span class="number">8576</span>, <span class="number">843</span>, <span class="number">209</span>, <span class="number">2468</span>, <span class="number">1795</span>, <span class="number">223</span>, <span class="number">198</span>, <span class="number">1049</span>, <span class="number">9595</span>, <span class="number">218</span>, <span class="number">13996</span>, <span class="number">225</span>, <span class="number">1563</span>, <span class="number">277</span>, <span class="number">582</span>, <span class="number">6493</span>, <span class="number">281</span>, <span class="number">457</span>, <span class="number">14371</span>, <span class="number">201</span>, <span class="number">198</span>, <span class="number">1422</span>, <span class="number">3373</span>, <span class="number">7452</span>, <span class="number">227</span>, <span class="number">198</span>, <span class="number">455</span>, <span class="number">674</span>, <span class="number">225</span>, <span class="number">4687</span>, <span class="number">198</span>, <span class="number">239</span>, <span class="number">21976</span>, <span class="number">239</span>, <span class="number">201</span>, <span class="number">196</span>, <span class="number">21657</span>, <span class="number">1680</span>, <span class="number">3773</span>, <span class="number">5591</span>, <span class="number">198</span>, <span class="number">4196</span>, <span class="number">218</span>, <span class="number">4679</span>, <span class="number">427</span>, <span class="number">661</span>, <span class="number">198</span>, <span class="number">3518</span>, <span class="number">1288</span>, <span class="number">220</span>, <span class="number">1051</span>, <span class="number">516</span>, <span class="number">889</span>, <span class="number">3947</span>, <span class="number">1922</span>, <span class="number">2500</span>, <span class="number">225</span>, <span class="number">390</span>, <span class="number">2065</span>, <span class="number">744</span>, <span class="number">872</span>, <span class="number">198</span>, <span class="number">7592</span>, <span class="number">3773</span>, <span class="number">239</span>, <span class="number">1975</span>, <span class="number">251</span>, <span class="number">208</span>, <span class="number">89</span>, <span class="number">22351</span>, <span class="number">239</span>, <span class="number">209</span>, <span class="number">252</span>], [<span class="number">261</span>, <span class="number">674</span>, <span class="number">959</span>, <span class="number">1921</span>, <span class="number">221</span>, <span class="number">1462</span>, <span class="number">201</span>, <span class="number">7600</span>, <span class="number">547</span>, <span class="number">196</span>, <span class="number">1178</span>, <span class="number">4753</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">630</span>, <span class="number">3591</span>, <span class="number">263</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">1180</span>, <span class="number">209</span>, <span class="number">1831</span>, <span class="number">322</span>, <span class="number">7568</span>, <span class="number">198</span>, <span class="number">3621</span>, <span class="number">2240</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">843</span>, <span class="number">201</span>, <span class="number">322</span>, <span class="number">471</span>, <span class="number">9575</span>, <span class="number">5291</span>, <span class="number">16591</span>, <span class="number">967</span>, <span class="number">201</span>, <span class="number">781</span>, <span class="number">281</span>, <span class="number">1815</span>, <span class="number">198</span>, <span class="number">674</span>, <span class="number">604</span>, <span class="number">10344</span>, <span class="number">1252</span>, <span class="number">274</span>, <span class="number">843</span>, <span class="number">664</span>, <span class="number">3147</span>, <span class="number">320</span>, <span class="number">209</span>, <span class="number">13290</span>, <span class="number">8751</span>, <span class="number">8124</span>, <span class="number">2528</span>, <span class="number">6023</span>, <span class="number">74</span>, <span class="number">235</span>, <span class="number">225</span>, <span class="number">7445</span>, <span class="number">10040</span>, <span class="number">17384</span>, <span class="number">241</span>, <span class="number">11487</span>, <span class="number">8950</span>, <span class="number">857</span>, <span class="number">1835</span>, <span class="number">340</span>, <span class="number">1382</span>, <span class="number">22582</span>, <span class="number">201</span>, <span class="number">1008</span>, <span class="number">296</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">1180</span>, <span class="number">2436</span>, <span class="number">21134</span>, <span class="number">5337</span>, <span class="number">19463</span>, <span class="number">5161</span>, <span class="number">209</span>, <span class="number">240</span>, <span class="number">1178</span>, <span class="number">927</span>, <span class="number">218</span>, <span class="number">3776</span>, <span class="number">8650</span>, <span class="number">198</span>, <span class="number">3355</span>, <span class="number">209</span>, <span class="number">261</span>, <span class="number">674</span>, <span class="number">268</span>, <span class="number">83</span>, <span class="number">2511</span>, <span class="number">3472</span>, <span class="number">258</span>, <span class="number">8288</span>, <span class="number">307</span>, <span class="number">1010</span>, <span class="number">268</span>, <span class="number">78</span>, <span class="number">209</span>, <span class="number">252</span>]], <span class="string">&#x27;attention_mask&#x27;</span>: [[], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]&#125;</span><br></pre></td></tr></table></figure>
<p>保存模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer.save_pretrained(<span class="string">&quot;my-new-tokenizer&quot;</span>)</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;my-new-tokenizer/tokenizer_config.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/special_tokens_map.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/vocab.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/merges.txt&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/added_tokens.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/tokenizer.json&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>之后可以加载此分词器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tok = new_tokenizer.from_pretrained(<span class="string">&quot;my-new-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>或者推送到 Hugging Face Hub 以从任何地方使用这个新的 tokenzier，具体操作参考<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">此处</a>。</p>
<h4 id="1-2-3-从头构建分词器"><a href="#1-2-3-从头构建分词器" class="headerlink" title="1.2.3 从头构建分词器"></a>1.2.3 从头构建分词器</h4><p>如果你想创建和训练一个新的标记器，它看起来不像现有的任何东西，你需要使用 🤗 Tokenizers 库从头开始构建它.</p>
<p>要了解如何从头开始构建标记器，我们必须深入了解 🤗 Tokenizers 库和标记化管道。 此管道需要几个步骤：</p>
<ul>
<li>Normalization：对初始输入字符串执行所有初始转换。 例如，当您需要小写某些文本时，可能会将其剥离，甚至应用一种常见的 unicode 规范化过程，您将添加一个 Normalizer。</li>
<li>Pre-tokenization：负责分割初始输入字符串。 这是决定在何处以及如何对原始字符串进行预分段的组件。 最简单的例子是使用空格进行分割。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pre_tokenizers??</span><br><span class="line"></span><br><span class="line">BertPreTokenizer = pre_tokenizers.BertPreTokenizer</span><br><span class="line">ByteLevel = pre_tokenizers.ByteLevel</span><br><span class="line">CharDelimiterSplit = pre_tokenizers.CharDelimiterSplit</span><br><span class="line">Digits = pre_tokenizers.Digits</span><br><span class="line">Metaspace = pre_tokenizers.Metaspace</span><br><span class="line">Punctuation = pre_tokenizers.Punctuation</span><br><span class="line"><span class="type">Sequence</span> = pre_tokenizers.<span class="type">Sequence</span></span><br><span class="line">Split = pre_tokenizers.Split</span><br><span class="line">UnicodeScripts = pre_tokenizers.UnicodeScripts</span><br><span class="line">Whitespace = pre_tokenizers.Whitespace</span><br><span class="line">WhitespaceSplit = pre_tokenizers.WhitespaceSplit</span><br></pre></td></tr></table></figure>
<ul>
<li>model：处理所有sub-token的发现和生成，这是可训练且真正依赖于您的输入数据的部分。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">models??</span><br><span class="line"></span><br><span class="line">BPE = models.BPE</span><br><span class="line">Unigram = models.Unigram</span><br><span class="line">WordLevel = models.WordLevel</span><br><span class="line">WordPiece = models.WordPiece</span><br></pre></td></tr></table></figure>
<ul>
<li>后处理Post-Processing：提供与一些基于 Transformers 的 SoTA 模型兼容的高级构建功能。 例如，对于 BERT，它会将标记化的句子包裹在 [CLS] 和 [SEP] 标记周围。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">processors??</span><br><span class="line"></span><br><span class="line">BertProcessing = processors.BertProcessing</span><br><span class="line">ByteLevel = processors.ByteLevel</span><br><span class="line">RobertaProcessing = processors.RobertaProcessing</span><br><span class="line">TemplateProcessing = processors.TemplateProcessing</span><br></pre></td></tr></table></figure>
<ul>
<li>解码Decoding：负责将标记化的输入映射回原始字符串。 通常根据我们之前使用的 PreTokenizer 来选择解码器。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoders??</span><br><span class="line"></span><br><span class="line">ByteLevel = decoders.ByteLevel</span><br><span class="line">WordPiece = decoders.WordPiece</span><br><span class="line">Metaspace = decoders.Metaspace</span><br><span class="line">BPEDecoder = decoders.BPEDecoder</span><br></pre></td></tr></table></figure>
<p>对于模型的训练，🤗 Tokenizers 库提供了一个我们将使用的 Trainer 类。<br>trainers??</p>
<p>BPE = trainers.BPE<br>Unigram = trainers.Unigram<br>WordLevel = trainers.WordLevel<br>WordPiece = trainers.WordPiece</p>
<p>所有这些构建块都可以组合起来创建tokenization pipelines。 下面将展示三个完整的管道：GPT-2、BERT 和 T5（它将为你提供 BPE、WordPiece 和 Unigram 标记器的示例）。</p>
<h5 id="1-2-3-2-WordPiece-model-like-BERT"><a href="#1-2-3-2-WordPiece-model-like-BERT" class="headerlink" title="1.2.3.2 WordPiece model like BERT"></a>1.2.3.2 WordPiece model like BERT</h5><p>创建一个 WordPiece 标记器（like BERT）：</p>
<ol>
<li>创建一个带有空 WordPiece 模型的 Tokenizer：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unl_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure>
<ol>
<li>添加normalization（可选）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#如果你想自定义它，你可以使用现有的块并按顺序组合它们：例如，我们小写，应用 NFD 规范化并去除重音：</span></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ol>
<li>添加pre-tokenizer（分词）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#直接使用 BertPreTokenizer，它使用空格和标点符号预先标记：</span></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br></pre></td></tr></table></figure>
<p>与 normalizer 一样，我们可以在一个 Sequence 中组合多个 pre-tokenizer。 如果我们想快速了解它如何预处理输入，我们可以调用 pre_tokenize_str 方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)),</span><br><span class="line"> (<span class="string">&#x27;is&#x27;</span>, (<span class="number">5</span>, <span class="number">7</span>)),</span><br><span class="line"> (<span class="string">&#x27;an&#x27;</span>, (<span class="number">8</span>, <span class="number">10</span>)),</span><br><span class="line"> (<span class="string">&#x27;example&#x27;</span>, (<span class="number">11</span>, <span class="number">18</span>)),</span><br><span class="line"> (<span class="string">&#x27;!&#x27;</span>, (<span class="number">18</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>请注意，==pre-tokenizer 不仅将文本拆分为单词，还保留了偏移量==，即原始文本中每个单词的开头和开头。 这将使最终的分词器==能够将每个标记与它来自的文本部分进行匹配（我们用于问答或标记分类任务的功能）==。</p>
<ol>
<li>构建 post-processor，传递special tokens给trainer。<h1 id="直接使用WordPieceTrainer"><a href="#直接使用WordPieceTrainer" class="headerlink" title="直接使用WordPieceTrainer"></a>直接使用WordPieceTrainer</h1></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br></pre></td></tr></table></figure>
<ol>
<li>构建数据集（text files）或批处理工具（batches of texts）：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>分词器已经训练完毕，定义后处理器：开头添加 CLS 标记并在末尾添加 SEP 标记（对于单个句子）或几个 SEP 标记（对于句子对）。 可以使用 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing">TemplateProcessing</a> 来做到这一点。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取CLS 和SEP 的token id</span></span><br><span class="line"></span><br><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用TemplateProcessing构建后处理器</span></span><br><span class="line"><span class="comment">#在模板中指明如何用一个句子（$A）或两个句子（$A 和 $B）组织特殊标记。</span></span><br><span class="line"><span class="comment">#后跟一个数字表示要赋予每个部分的token type ID，也就是哪部分是第一句，哪部分是第二句。</span></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下面编码一个句子看看结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;This is one sentence.&quot;</span>, <span class="string">&quot;With this one we have a pair.&quot;</span>)</span><br><span class="line">encoding.tokens</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;one&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;sentence&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;[SEP]&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;with&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;one&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;we&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;have&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;pair&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line"></span><br><span class="line">encoding.type_ids</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ol>
<li>解码器：我们使用 WordPiece 解码器并指示特殊前缀 ##：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.decoder = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们的tokenizer已经完成，我们必须将它放在与我们要使用的模型相对应的标记器 fast 类中，这里是一个 BertTokenizerFast：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>和以前一样，我们可以将此分词器用作普通的 Transformers 分词器，并使用 save_pretrained 或 push_to_hub 方法。</p>
</li>
<li><p>如果您正在构建的分词器与 Transformers 中的任何类都不匹配(分词器非常特殊)，您可以将它包装在 PreTrainedTokenizerFast 中。</p>
</li>
</ul>
<h5 id="1-2-3-3-BPE-model-like-GPT-2"><a href="#1-2-3-3-BPE-model-like-GPT-2" class="headerlink" title="1.2.3.3 BPE model like GPT-2"></a>1.2.3.3 BPE model like GPT-2</h5><p>下面看看如何创建一个 BPE 标记器（like GPT-2 tokenizer）：</p>
<ol>
<li>创建一个带有初始 BPE model的 Tokenizer：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br></pre></td></tr></table></figure>
<ol>
<li>添加可选normalization（GPT2不使用）</li>
<li>指定pre-tokenizer（GPT2使用byte level pre-tokenizer）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>调用 pre_tokenize_str 方法，快速了解它如何预处理输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ġis&#x27;</span>, (<span class="number">4</span>, <span class="number">7</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ġan&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ġexample&#x27;</span>, (<span class="number">10</span>, <span class="number">18</span>)),</span><br><span class="line"> (<span class="string">&#x27;!&#x27;</span>, (<span class="number">18</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>我们对前缀空格使用 GPT-2的默认值，所以除了第一个单词之外，每个单词的开头都添加了一个首字母“Ġ”。</p>
<ol>
<li>使用 BpeTrainer训练分词器：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>添加后处理和解码器：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br></pre></td></tr></table></figure>
<ol>
<li>将此分词器包装在 Transformers tokenizer object中：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2TokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<h5 id="1-2-3-4-Unigram-model-like-Albert"><a href="#1-2-3-4-Unigram-model-like-Albert" class="headerlink" title="1.2.3.4 Unigram model like Albert"></a>1.2.3.4 Unigram model like Albert</h5><p>现在让我们看看如何创建一个 Unigram 分词器(类似 T5 的分词器）：</p>
<ol>
<li>创建 初始Unigram 模型的 Tokenizer：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br></pre></td></tr></table></figure>
<ol>
<li>添加normalization 和pre-tokenizer（Metaspace pre-tokenizer：它用一个特殊字符（默认为“▁” ）替换所有空格，然后在该字符上拆分。）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Lowercase()]</span><br><span class="line">)</span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br></pre></td></tr></table></figure>
<p>调用 pre_tokenize_str 方法，快速了解它如何预处理输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;▁This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)), (<span class="string">&#x27;▁is&#x27;</span>, (<span class="number">4</span>, <span class="number">7</span>)), (<span class="string">&#x27;▁an&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;▁example!&#x27;</span>, (<span class="number">10</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>每个单词都在开头添加了一个首字母“ ▁”，这是由 sentencepiece完成的。</p>
<ol>
<li>使用 UnigramTrainer训练分词器，并设置unknown token。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = trainers.UnigramTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>], unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>添加后处理和解码器（Metaspace，类似pre-tokenizer）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">tokenizer.decoder = decoders.Metaspace()</span><br></pre></td></tr></table></figure>
<ol>
<li>将此分词器包装在 Transformers tokenizer object中：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AlbertTokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<p>现在可以用新的tokenizer训练模型了。</p>
<ul>
<li>使用新的分析器在notebook上从头训练模型</li>
<li>在l<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling">anguage modeling scripts</a> 上使用tokenizer_name参数来从头训练模型。<h2 id="二、HF模型预训练方式"><a href="#二、HF模型预训练方式" class="headerlink" title="二、HF模型预训练方式"></a>二、HF模型预训练方式</h2>使用HF主页的tokenizer和MLM包，进行trainer训练<h3 id="1-加载数据集："><a href="#1-加载数据集：" class="headerlink" title="1.加载数据集："></a>1.加载数据集：</h3>选择多语言多语料数据集<a target="_blank" rel="noopener" href="https://traces1.inria.fr/oscar/">OSCAR corpus</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in this notebook we&#x27;ll only get one of the files (the Oscar one) for the sake of simplicity and performance</span></span><br><span class="line">!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt</span><br></pre></td></tr></table></figure>
<h3 id="2-训练tokenizer"><a href="#2-训练tokenizer" class="headerlink" title="2.训练tokenizer"></a>2.训练tokenizer</h3>选择字节级别byte-level BPE分词器（类似GPT2使用的），比BERT的WordPiece（字符级别BPE分词器，切分成子词）好处是几乎不会有未登录词”\<unk> tokens”。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装transformers和tokenizers</span></span><br><span class="line">!pip install git+https://github.com/huggingface/transformers</span><br><span class="line">!pip <span class="built_in">list</span> | grep -E <span class="string">&#x27;transformers|tokenizers&#x27;</span></span><br><span class="line"><span class="comment"># transformers version at notebook update --- 2.11.0</span></span><br><span class="line"><span class="comment"># tokenizers version at notebook update --- 0.8.0rc1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time </span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> ByteLevelBPETokenizer</span><br><span class="line">paths = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> Path(<span class="string">&quot;.&quot;</span>).glob(<span class="string">&quot;**/*.txt&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer初始化</span></span><br><span class="line">tokenizer = ByteLevelBPETokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Customize training</span></span><br><span class="line">tokenizer.train(files=paths, vocab_size=<span class="number">52_000</span>, min_frequency=<span class="number">2</span>, special_tokens=[</span><br><span class="line">    <span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="2-2-分词器的训练参数如下："><a href="#2-2-分词器的训练参数如下：" class="headerlink" title="2.2 分词器的训练参数如下："></a>2.2 分词器的训练参数如下：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#BPE的分词器</span></span><br><span class="line">classtokenizers.trainers.BpeTrainer(self, vocab_size=<span class="number">30000</span>, min_frequency=<span class="number">0</span>, show_progress=<span class="literal">True</span>, special_tokens=[], limit_alphabet=<span class="literal">None</span>, initial_alphabet=[], continuing_subword_prefix=<span class="literal">None</span>, end_of_word_suffix=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>vocab_size (int, optional) – 最终词汇的大小，包括所有标记和字母表。</li>
<li>min_frequency (int, optional) – 为了合并，一对应该具有的最小频率。</li>
<li>show_progress (bool, optional) – 训练时是否显示进度条。</li>
<li>special_tokens (List[Union[str,AddedToken]], optional) – 模型应该知道的特殊标记列表。</li>
<li>limit_alphabet (int, optional) – 字母表中保留的最大不同字符数。</li>
<li>initial_alphabet (List[str], optional) – 包含在初始字母表中的字符列表，即使在训练数据集中没有出现。 如果字符串包含多个字符，则仅保留第一个字符。</li>
<li>continue_subword_prefix (str, optional) — 用于每个不是词开头的子词的前缀。</li>
<li>end_of_word_suffix (str, optional) – 用于每个词尾的子词的后缀。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#WordPiece分词器，参数和上一个相同</span></span><br><span class="line">classtokenizers.trainers.WordPieceTrainer(self, vocab_size=<span class="number">30000</span>, min_frequency=<span class="number">0</span>, show_progress=<span class="literal">True</span>, special_tokens=[], limit_alphabet=<span class="literal">None</span>, initial_alphabet=[], continuing_subword_prefix=<span class="string">&#x27;##&#x27;</span>, end_of_word_suffix=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>vocab_size (int, optional) – 最终词汇的大小，包括所有标记和字母表。</li>
<li>min_frequency (int, optional) – 为了合并，一对应该具有的最小频率。</li>
<li>show_progress (bool, optional) – 训练时是否显示进度条。</li>
<li>special_tokens (List[Union[str,AddedToken]], optional) – 模型应该知道的特殊标记列表。</li>
<li>limit_alphabet (int, optional) – 字母表中保留的最大不同字符数。</li>
<li>initial_alphabet (List[str], optional) – 包含在初始字母表中的字符列表，即使在训练数据集中没有出现。 如果字符串包含多个字符，则仅保留第一个字符。</li>
<li>continue_subword_prefix (str, optional) — 用于每个不是词开头的子词的前缀。</li>
<li>end_of_word_suffix (str, optional) – 用于每个词尾的子词的后缀。</li>
</ul>
<h4 id="2-3-分词器保存和加载"><a href="#2-3-分词器保存和加载" class="headerlink" title="2.3 分词器保存和加载"></a>2.3 分词器保存和加载</h4><p>将训练好的分词器保存在EsperBERTo文件夹：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!mkdir EsperBERTo</span><br><span class="line">tokenizer.save_model(<span class="string">&quot;EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最终得到两个分词器文件：</p>
<ul>
<li>EsperBERTo/vocab.json：vocab.json，按频率排列的常见token的列表</li>
<li>EsperBERTo/merges.txt’： merges.txt，merges列表</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123; <span class="string">&quot;&lt;s&gt;&quot;</span>: <span class="number">0</span>,<span class="string">&quot;&lt;pad&gt;&quot;</span>: <span class="number">1</span>,<span class="string">&quot;&lt;/s&gt;&quot;</span>: <span class="number">2</span>,<span class="string">&quot;&lt;unk&gt;&quot;</span>: <span class="number">3</span>, <span class="string">&quot;&lt;mask&gt;&quot;</span>: <span class="number">4</span>,<span class="string">&quot;!&quot;</span>: <span class="number">5</span>,<span class="string">&quot;\&quot;&quot;</span>: <span class="number">6</span>,<span class="string">&quot;#&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;$&quot;</span>: <span class="number">8</span>,<span class="string">&quot;%&quot;</span>: <span class="number">9</span>,<span class="string">&quot;&amp;&quot;</span>: <span class="number">10</span>,<span class="string">&quot;&#x27;&quot;</span>: <span class="number">11</span>,<span class="string">&quot;(&quot;</span>: <span class="number">12</span>,<span class="string">&quot;)&quot;</span>: <span class="number">13</span>, <span class="comment"># ...&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># merges.txt</span></span><br><span class="line">l a</span><br><span class="line">Ġ k</span><br><span class="line">o n</span><br><span class="line">Ġ la</span><br><span class="line">t a</span><br><span class="line">Ġ e</span><br><span class="line">Ġ d</span><br><span class="line">Ġ p</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>tokenizer针对Esperanto进行了优化，更多单词是a single, unsplit token表示。==我们还以更有效的方式表示序列。 在这个语料库中，编码序列的平均长度比使用预训练的 GPT-2 标记器时小约 30%。==</p>
<p>加载分词器，处理 RoBERTa 特殊标记：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers.implementations <span class="keyword">import</span> ByteLevelBPETokenizer</span><br><span class="line"><span class="keyword">from</span> tokenizers.processors <span class="keyword">import</span> BertProcessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = ByteLevelBPETokenizer(</span><br><span class="line">    <span class="string">&quot;./EsperBERTo/vocab.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;./EsperBERTo/merges.txt&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer._tokenizer.post_processor = BertProcessing(</span><br><span class="line">    (<span class="string">&quot;&lt;/s&gt;&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;&lt;/s&gt;&quot;</span>)),</span><br><span class="line">    (<span class="string">&quot;&lt;s&gt;&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;&lt;s&gt;&quot;</span>)),</span><br><span class="line">)</span><br><span class="line">tokenizer.enable_truncation(max_length=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<p>token_to_id：将给定的token转换为其对应的 id<br>BertProcessing参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classtokenizers.processors.BertProcessing(self, sep, cls)</span><br></pre></td></tr></table></figure>
<p>这个后处理器负责添加 Bert 模型所需的特殊标记：</p>
<ul>
<li>sep (Tuple[str, int]) – 带有 SEP 令牌的字符串表示及其 id 的元组</li>
<li>cls (Tuple[str, int]) – 一个带有 CLS 标记的字符串表示的元组，以及它的 id</li>
</ul>
<p>测试效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&quot;Mi estas Julien.&quot;</span>)</span><br><span class="line">Encoding(num_tokens=<span class="number">7</span>, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&quot;Mi estas Julien.&quot;</span>).tokens</span><br><span class="line">[<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;Mi&#x27;</span>, <span class="string">&#x27;Ġestas&#x27;</span>, <span class="string">&#x27;ĠJuli&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;&lt;/s&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="3-从头开始训练语言模型"><a href="#3-从头开始训练语言模型" class="headerlink" title="3.从头开始训练语言模型"></a>3.从头开始训练语言模型</h3><p>参考<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py">run_language_modeling.py</a> 文件。直接设置 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py">Trainer</a> 选择训练方法。下面以训练类似 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a> 的模型来举例：（相比bert采用动态掩码、舍弃NSP任务，以及更大的训练）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#定义模型参数</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaConfig</span><br><span class="line"></span><br><span class="line">config = RobertaConfig(</span><br><span class="line">    vocab_size=<span class="number">52_000</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">514</span>,</span><br><span class="line">    num_attention_heads=<span class="number">12</span>,</span><br><span class="line">    num_hidden_layers=<span class="number">6</span>,</span><br><span class="line">    type_vocab_size=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重新创建tokenizer</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaTokenizerFast</span><br><span class="line">tokenizer = RobertaTokenizerFast.from_pretrained(<span class="string">&quot;./EsperBERTo&quot;</span>, max_len=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-2-初始化模型"><a href="#3-2-初始化模型" class="headerlink" title="3.2 初始化模型"></a>3.2 初始化模型</h4><p>由于我们是从头开始训练，因此我们仅从配置进行初始化，而不是从现有的预训练模型或检查点进行初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaForMaskedLM</span><br><span class="line">model = RobertaForMaskedLM(config=config)</span><br><span class="line"></span><br><span class="line">model.num_parameters()</span><br><span class="line"></span><br><span class="line"><span class="number">84095008</span><span class="comment"># =&gt; 84 million parameters</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-创建训练集"><a href="#3-3-创建训练集" class="headerlink" title="3.3 创建训练集"></a>3.3 创建训练集</h4><p>由于只有一个text文件，不需要自定义数据集。直接使用LineByLineDataset加载之后用tokenizer预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> LineByLineTextDataset</span><br><span class="line"></span><br><span class="line">dataset = LineByLineTextDataset(</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    file_path=<span class="string">&quot;./oscar.eo.txt&quot;</span>,</span><br><span class="line">    block_size=<span class="number">128</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CPU times: user 4<span class="built_in">min</span> 54s, sys: <span class="number">2.98</span> s, total: 4<span class="built_in">min</span> 57s</span><br><span class="line">Wall time: 1<span class="built_in">min</span> 37s</span><br></pre></td></tr></table></figure>
<p>定义data_collator：帮助我们将数据集样本进行批处理的数据整理器。 如果输入的长度不同，则输入会动态填充到批次的最大长度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForLanguageModeling</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForLanguageModeling(</span><br><span class="line">    tokenizer=tokenizer, mlm=<span class="literal">True</span>, mlm_probability=<span class="number">0.15</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">transformers</span>.<span class="title">data</span>.<span class="title">data_collator</span>.<span class="title">DataCollatorForLanguageModeling</span>(<span class="params">tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, mlm: <span class="built_in">bool</span> = <span class="literal">True</span>, mlm_probability: <span class="built_in">float</span> = <span class="number">0.15</span>, pad_to_multiple_of: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>, tf_experimental_compile: <span class="built_in">bool</span> = <span class="literal">False</span>, return_tensors: <span class="built_in">str</span> = <span class="string">&#x27;pt&#x27;</span></span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>tokenizer（PreTrainedTokenizer 或 PreTrainedTokenizerFast）——用于编码数据的标记器。</li>
<li>mlm (bool, optional, defaults to True) – 是否使用掩码语言建模。 如果设置为 False，则标签与忽略填充标记的输入相同（通过将它们设置为 -100）。 否则，non-masked tokens 的label和masked token的预测值为 -100。（If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.）</li>
<li>mlm_probability（浮点数，可选，默认为 0.15）– 当 mlm 设置为 True 时（随机）屏蔽输入中的标记的概率。</li>
<li>pad_to_multiple_of (int, optional) – 如果设置，则将序列填充为所提供值的倍数。</li>
</ul>
<h3 id="3-4-初始化-Trainer并训练"><a href="#3-4-初始化-Trainer并训练" class="headerlink" title="3.4 初始化 Trainer并训练"></a>3.4 初始化 Trainer并训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./EsperBERTo&quot;</span>,</span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,</span><br><span class="line">    per_gpu_train_batch_size=<span class="number">64</span>,</span><br><span class="line">    save_steps=<span class="number">10_000</span>,</span><br><span class="line">    save_total_limit=<span class="number">2</span>,</span><br><span class="line">    prediction_loss_only=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>开始训练<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line">CPU times: user 1h 43<span class="built_in">min</span> 36s, sys: 1h 3<span class="built_in">min</span> 28s, total: 2h 47<span class="built_in">min</span> 4s</span><br><span class="line">Wall time: 2h 46<span class="built_in">min</span> 46s</span><br><span class="line">TrainOutput(global_step=<span class="number">15228</span>, training_loss=<span class="number">5.762423221226405</span>)</span><br></pre></td></tr></table></figure><br>保存模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.save_model(<span class="string">&quot;./EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-检查训练好的模型"><a href="#5-检查训练好的模型" class="headerlink" title="5. 检查训练好的模型"></a>5. 检查训练好的模型</h3><p>除了查看训练和评估损失下降之外，可以通过FillMaskPipeline加载模型进行预测</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">fill_mask = pipeline(<span class="string">&quot;fill-mask&quot;</span>,model=<span class="string">&quot;./EsperBERTo&quot;</span>,tokenizer=<span class="string">&quot;./EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The sun &lt;mask&gt;.</span></span><br><span class="line"><span class="comment"># =&gt;</span></span><br><span class="line"></span><br><span class="line">fill_mask(<span class="string">&quot;La suno &lt;mask&gt;.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.02119220793247223</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno estas.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">316</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.012403824366629124</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno situas.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">2340</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.011061107739806175</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno estis.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">394</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.008284995332360268</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno de.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">274</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.006471084896475077</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno akvo.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">1833</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>最后，当你有一个不错的模型时，请考虑与社区分享：</p>
<p>使用 CLI 上传您的模型：transformers-cli upload<br>写一个 README.md 模型卡并将其添加到 model_cards/ 下的存储库中。 理想情况下，您的模型卡应包括：</p>
<ul>
<li>模型描述</li>
<li>训练参数（数据集、预处理、超参数）</li>
<li>评估结果</li>
<li>预期用途和限制</li>
<li>其它有用信息 🤓</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/10/07/天池-新闻文本分类/task3：单个bert模型分数0.961/">https://zhxnlp.github.io/2021/10/07/天池-新闻文本分类/task3：单个bert模型分数0.961/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a><a class="post-meta__tags" href="/tags/transformers/">transformers</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/10/08/huggingface/hugging%20face%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94datasets%E3%80%81optimizer/"><i class="fa fa-chevron-left">  </i><span>Hugging Face官方文档——datasets、optimizer</span></a></div><div class="next-post pull-right"><a href="/2021/10/05/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task2%20%EF%BC%9Afasttext/"><span>天池-新闻文本分类task2：fasttext模型</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>