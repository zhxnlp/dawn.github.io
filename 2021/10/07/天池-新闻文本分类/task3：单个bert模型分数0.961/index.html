<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»task3ï¼šbertæ¨¡å‹"><meta name="keywords" content="nlp,æ–‡æœ¬åˆ†ç±»,transformers"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»task3ï¼šbertæ¨¡å‹ | zhxnlpã®Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"æœç´¢æ–‡ç« ","hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹:${query}","hits_stats":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼Œç”¨æ—¶ ${time} æ¯«ç§’"}},
  localSearch: undefined,
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpã®Blog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="åˆ‡æ¢æ–‡ç« è¯¦æƒ…">åˆ‡æ¢ç«™ç‚¹æ¦‚è§ˆ</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">ç›®å½•</div><div class="sidebar-toc__progress"><span class="progress-notice">ä½ å·²ç»è¯»äº†</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E8%AF%B4%E6%98%8E"><span class="toc-text">ä¸€äº›è¯´æ˜</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%9C%80%E7%BB%88%E4%BB%A3%E7%A0%81%E5%8F%8A%E8%A7%A3%E6%9E%90"><span class="toc-text">ä¸‰ã€æœ€ç»ˆä»£ç åŠè§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9E%84%E5%BB%BA%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">3.1 æ„å»ºåˆ†è¯å™¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%A2%84%E8%AE%AD%E7%BB%83bert%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 é¢„è®­ç»ƒbertæ¨¡å‹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%EF%BC%9A"><span class="toc-text">3.3 åˆ†ç±»ä»»åŠ¡å¾®è°ƒï¼š</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%88%86%E8%AF%8Dtokenization"><span class="toc-text">é›¶ã€åˆ†è¯tokenization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%88%86%E8%AF%8D%E8%A7%84%E5%88%99"><span class="toc-text">1.2 åˆ†è¯è§„åˆ™</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-character-based-tokenizer"><span class="toc-text">1.3 character-based-tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-Subword-tokenization"><span class="toc-text">1.4 Subword tokenization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-Byte-Pair-Encoding%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81-BPE"><span class="toc-text">1.5 Byte-Pair Encodingå­—èŠ‚å¯¹ç¼–ç  (BPE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E5%AD%97%E8%8A%82%E7%BA%A7-BPE%EF%BC%88Byte-level-BPE%EF%BC%89"><span class="toc-text">1.6 å­—èŠ‚çº§ BPEï¼ˆByte-level BPEï¼‰</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-WordPiece"><span class="toc-text">1.7 WordPiece</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-Unigram"><span class="toc-text">1.8 Unigram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-9-SentencePiece"><span class="toc-text">1.9 SentencePiece</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">ä¸€ã€è®­ç»ƒåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Using-tokenizers-from-%F0%9F%A4%97-Tokenizers"><span class="toc-text">1.1 Using tokenizers from ğŸ¤— Tokenizers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Train-your-tokenizer"><span class="toc-text">1.2 Train your tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-%E4%BB%8E%E5%A4%B4%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">1.2.1 ä»å¤´è®­ç»ƒåˆ†è¯å™¨</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E4%BD%BF%E7%94%A8%E5%B7%B2%E6%9C%89%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%AD%E7%BB%83"><span class="toc-text">1.2.2 ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨è®­ç»ƒ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BA%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">1.2.3 ä»å¤´æ„å»ºåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-2-WordPiece-model-like-BERT"><span class="toc-text">1.2.3.2 WordPiece model like BERT</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8WordPieceTrainer"><span class="toc-text">ç›´æ¥ä½¿ç”¨WordPieceTrainer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-3-BPE-model-like-GPT-2"><span class="toc-text">1.2.3.3 BPE model like GPT-2</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-3-4-Unigram-model-like-Albert"><span class="toc-text">1.2.3.4 Unigram model like Albert</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81HF%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F"><span class="toc-text">äºŒã€HFæ¨¡å‹é¢„è®­ç»ƒæ–¹å¼</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-text">1.åŠ è½½æ•°æ®é›†ï¼š</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83tokenizer"><span class="toc-text">2.è®­ç»ƒtokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">2.2 åˆ†è¯å™¨çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E5%88%86%E8%AF%8D%E5%99%A8%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-text">2.3 åˆ†è¯å™¨ä¿å­˜å’ŒåŠ è½½</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.ä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 åˆå§‹åŒ–æ¨¡å‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-text">3.3 åˆ›å»ºè®­ç»ƒé›†</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%88%9D%E5%A7%8B%E5%8C%96-Trainer%E5%B9%B6%E8%AE%AD%E7%BB%83"><span class="toc-text">3.4 åˆå§‹åŒ– Trainerå¹¶è®­ç»ƒ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A3%80%E6%9F%A5%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">5. æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">æ–‡ç« </span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">æ ‡ç­¾</span><span class="pull-right">50</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">åˆ†ç±»</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpã®Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> æœç´¢</span></a></span></div><div id="post-info"><div id="post-title">å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»task3ï¼šbertæ¨¡å‹</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-07</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»</a><div class="post-meta-wordcount"><span>å­—æ•°æ€»è®¡: </span><span class="word-count">12.1k</span><span class="post-meta__separator">|</span><span>é˜…è¯»æ—¶é•¿: 50 åˆ†é’Ÿ</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="ä¸€äº›è¯´æ˜"><a href="#ä¸€äº›è¯´æ˜" class="headerlink" title="ä¸€äº›è¯´æ˜"></a>ä¸€äº›è¯´æ˜</h2><p>&#8195;&#8195;æ¯”èµ›å®˜æ–¹é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">ã€Šé›¶åŸºç¡€å…¥é—¨NLP - æ–°é—»æ–‡æœ¬åˆ†ç±»ã€‹</a>ã€‚<br>&#8195;&#8195;è®¨è®ºåŒºæœ‰å¤§ä½¬å¼ å¸†ã€æƒŠé¹Šå’Œå¼ è´¤ç­‰äººçš„ä»£ç ï¼Œå€¼å¾—å¤§å®¶ä»”ç»†é˜…è¯»ã€‚<br>&#8195;&#8195;æœ€åæˆ‘çš„æ¨¡å‹å‚è€ƒäº†è¿™äº›ä»£ç çš„ä¸€äº›configï¼Œæ¯”å¦‚bert.configï¼Œlrç­‰ç­‰ã€‚ç„¶åå¤§ä½¬ä»¬çš„ä»£ç å¯¹æˆ‘æ¥è¯´è¿˜æ˜¯å¤ªå¤æ‚ï¼ŒpytorchåŠŸåŠ›ä¸å¤Ÿï¼Œçœ‹çš„åƒåŠ›ã€‚æ‰€ä»¥è‡ªå·±ç”¨huggingfaceå®ç°äº†ã€‚<br>&#8195;&#8195;ç¬¬ä¸€æ­¥åˆ†è¯æˆ‘å°±è€ƒè™‘äº†å¾ˆä¹…ï¼Œæ²¡æœ‰åƒå¼ å¸†ä»–ä»¬é‚£æ ·ç”¨pytorchå…·ä½“ä¸€æ­¥æ­¥å†™ï¼Œè€Œæ˜¯å‚è€ƒHFä¸»é¡µçš„æ•™ç¨‹ã€‚æ‰€ä»¥ä¸€å¼€å§‹æˆ‘æ˜¯ç¿»è¯‘äº†æ„å»ºtokenizerçš„æ•™ç¨‹ï¼Œå¦‚æœå¯¹æ¯”èµ›ä»£ç ä¸­åˆ†è¯æœ‰ç–‘é—®çš„å¯ä»¥å‚è€ƒã€‚</p>
<h2 id="ä¸‰ã€æœ€ç»ˆä»£ç åŠè§£æ"><a href="#ä¸‰ã€æœ€ç»ˆä»£ç åŠè§£æ" class="headerlink" title="ä¸‰ã€æœ€ç»ˆä»£ç åŠè§£æ"></a>ä¸‰ã€æœ€ç»ˆä»£ç åŠè§£æ</h2><p>ä¸»è¦æ€è·¯ï¼š</p>
<ol>
<li>æ„å»ºåˆ†è¯å™¨ã€‚å‚è€ƒHFæ•™ç¨‹<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">ã€ŠHow to train and use your very own tokenizerã€‹</a>ã€‚3750ã€648ã€900è¿™ä¸‰ä¸ªåº”è¯¥æ˜¯æ ‡ç‚¹ç¬¦å·ï¼ˆè¯¦è§å¼ å¸†task02çš„åˆ†æï¼‰ï¼Œç›´æ¥æŠŠè¿™ä¸‰ä¸ªæ›¿æ¢æˆâ€˜ï¼Œâ€™ã€â€˜.â€™å’Œâ€˜ï¼â€™ã€‚ä¸»è¦æ˜¯ä¸ºäº†æ–­å¥ã€‚åœ¨é¢„åˆ†è¯å™¨pre_tokenizers.BertPreTokenizerä¸­ï¼Œæœ‰æ ¹æ®æ ‡ç‚¹è¿›è¡Œæ–­å¥çš„æ–¹æ³•ï¼Œç›´æ¥å°†æ–‡æœ¬æ¢æˆå¸¦æ ‡ç‚¹çš„æ ¼å¼å°±è¡Œï¼Œé¢„åˆ†è¯å™¨ä¼šè‡ªåŠ¨æ–­å¥ã€‚<ul>
<li>å’ŒBERT æœ‰å…³çš„ Tokenizer ä¸»è¦å†™åœ¨<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py">models/bert/tokenization_bert.py</a>ä¸­ã€‚è¿™éƒ¨åˆ†å†…å®¹å…¶å®åœ¨<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT">nlpæ•™ç¨‹3.1</a>é‡Œé¢æœ‰å†™ã€‚<span id="more"></span></li>
<li>BasicTokenizerè´Ÿè´£å¤„ç†çš„ç¬¬ä¸€æ­¥â€”â€”æŒ‰æ ‡ç‚¹ã€ç©ºæ ¼ç­‰åˆ†å‰²å¥å­ï¼Œå¹¶å¤„ç†æ˜¯å¦ç»Ÿä¸€å°å†™ï¼Œä»¥åŠæ¸…ç†éæ³•å­—ç¬¦ã€‚å¯¹äºä¸­æ–‡å­—ç¬¦ï¼Œé€šè¿‡é¢„å¤„ç†ï¼ˆåŠ ç©ºæ ¼ï¼‰æ¥æŒ‰å­—åˆ†å‰²ï¼›åŒæ—¶å¯ä»¥é€šè¿‡never_splitæŒ‡å®šå¯¹æŸäº›è¯ä¸è¿›è¡Œåˆ†å‰²ï¼›</li>
<li>åˆ†è¯å™¨å‚è€ƒbert-baseChineseçš„åˆ†è¯å™¨é…ç½®<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-chinese/blob/main/tokenizer.json">tokenizer.json</a>ã€‚å…·ä½“çš„ï¼š<ul>
<li>â€œnormalizerâ€:â€BertNormalizerâ€ã€‚</li>
<li>â€œpre_tokenizerâ€:{â€œtypeâ€:â€BertPreTokenizerâ€}</li>
<li>â€œpost_processorâ€:{â€œtypeâ€:â€TemplateProcessingâ€}</li>
<li>â€œdecoderâ€:{â€œtypeâ€:â€WordPieceâ€,â€prefixâ€:â€##â€,â€cleanupâ€:true}</li>
<li>â€œmodelâ€:{â€œtypeâ€:â€WordPieceâ€,â€unk_tokenâ€:â€[UNK]â€â€¦}</li>
</ul>
</li>
<li>è¯è¡¨å¤§å°é€‰çš„7000ï¼Œæˆ‘æ˜¯çœ‹è®¨è®ºåŒºæ˜¯6900+ï¼Œè¿™é‡Œè¿˜æœ‰ç‚¹æ²¡æƒ³æ¸…æ¥šã€‚ä¸­æ–‡çš„wordpieceæ˜¯ä¹Ÿå¯ä»¥å§é«˜é¢‘ç‡çš„æ±‰å­—æ‹¼æˆè¯è¯­å§ï¼Œç”¨â€˜##â€™è¿æ¥ã€‚å¦‚æœè¿™æ ·ï¼Œé‡‡ç”¨wordpieceï¼Œvocab sizeå¤§ä¸€ç‚¹ï¼Œæœ€åæ•´è¯æ©ç æ„Ÿè§‰æ•ˆæœä¼šæ›´å¥½ã€‚ä½†æ˜¯æ•´è¯æ©ç æˆ‘ä¸çŸ¥é“æ€ä¹ˆå†™ï¼Œæ‰€ä»¥æœ€åæ²¡æœ‰ç”¨ã€‚æ²¡æœ‰æ•´è¯æ©ç ï¼Œå°±æ²¡æœ‰wordpieceçš„å¿…è¦äº†ã€‚æ‰€ä»¥æˆ‘åšçš„æœ‰ç‚¹çŸ›ç›¾ï¼Œæœ€åæ˜¯æ‡’å¾—æ”¹äº†ï¼Œå°±è¿™ä¹ˆå†™ã€‚</li>
</ul>
</li>
<li>é¢„è®­ç»ƒbertæ¨¡å‹ã€‚å‚è€ƒnlpæ•™ç¨‹4.5 <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.5-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">ã€Šå¾®è°ƒè¯­è¨€æ¨¡å‹ã€‹</a>ã€‚ç”¨BertConfigé…ç½®æ¨¡å‹å‚æ•°ï¼Œè®¾ç½®äº†ä¸€ä¸ªå°å‹çš„åˆå§‹åŒ–bertè¿›è¡Œmlmä»»åŠ¡é¢„è®­ç»ƒã€‚<ul>
<li>è®­ç»ƒé›†é€‰æ‹©train_setå’Œtestä¸¤ä¸ªcsvï¼Œå› ä¸ºtesté¢„è®­ç»ƒæ—¶ä¸éœ€è¦åˆ†ç±»æ ‡ç­¾ï¼Œåªä½œä¸ºæ©ç ä»»åŠ¡ï¼Œä¸å­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜ã€‚æ³¨æ„è®­ç»ƒæ•°æ®è¦æŠŠä¸‰ä¸ªtokenæ¢æˆæ ‡ç‚¹ã€‚</li>
<li>æœ€ç»ˆæˆ‘è®­ç»ƒäº†8ä¸ªepochï¼ˆç¬¬ä¸€æ¬¡5ä¸ªepochï¼Œlr=4e-4ï¼Œloss=1.695ï¼Œbatch_size=128ã€‚ç¬¬äºŒæ¬¡3ä¸ªepochï¼Œlr=2e-4ï¼Œç»“æœä¸€å¼€å§‹steps=3000æ—¶ï¼Œloss=1.78ï¼Œåº”è¯¥æ˜¯ç¬¬äºŒæ¬¡è®­ç»ƒçš„lrè¿˜æ˜¯å¤ªå¤§ï¼Œéœ‡è¡äº†ã€‚ç¬¬äºŒä¸ªepochå¿«è®­ç»ƒå®Œæ‰é™åˆ°1.69ï¼Œæµªè´¹äº†ä¸¤ä¸ªå°æ—¶ã€‚æœ€ç»ˆlossæ˜¯1.63ï¼‰</li>
<li>æˆ‘é€‰æ‹©çš„æ˜¯colabçš„tpuè¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªepochæ˜¯13903stepsï¼Œå¤§æ¦‚50-60åˆ†é’Ÿå·¦å³ã€‚å¦‚æœæ˜¯colab-GPUï¼Œå¤§æ¦‚31-35å°æ—¶ã€‚colab tpuä½¿ç”¨å¯ä»¥å‚è€ƒæˆ‘çš„ä»£ç ã€‚å¦‚æœé€‰æ‹©tpuæ—¶æç¤ºæ— æ³•åˆ†é…ï¼Œä¸ç”¨ç®¡ï¼Œç»§ç»­è¿æ¥ï¼Œç¬¬äºŒæ¬¡è¿æ¥æˆ‘éƒ½æˆåŠŸäº†ã€‚</li>
</ul>
</li>
<li>åˆ†ç±»å¾®è°ƒï¼ŒåŠ ä¸€ä¸ªé¦–å°¾æˆªæ–­ã€‚æˆ‘æ˜¯ä¹‹å‰çœ‹æ–‡ç« è¯´æ–‡ç« åˆ†ç±»é¦–å°¾æˆªæ–­æ•ˆæœæ›´å¥½ï¼ˆ<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/WBK-XYzP-vIf6Ni6GO-diQ">è®ºæ–‡è§£è¯»ã€‘æ–‡æœ¬åˆ†ç±»ä¸Šåˆ†åˆ©å™¨:Bertå¾®è°ƒtrickå¤§å…¨</a>ï¼‰ã€‚traineræ²¡æœ‰é¦–å°¾æˆªæ–­çš„æœºåˆ¶ï¼Œåœ¨å‰é¢æ•°æ®å¤„ç†æ—¶ç”¨pandaså®ç°ã€‚æœ€ç»ˆè®­ç»ƒäº†6ä¸ªepochï¼Œç”¨tpuå¤§æ¦‚88åˆ†é’Ÿï¼ˆæˆ‘ä¹Ÿæ˜¯è·‘äº†ä¸¤æ¬¡ã€‚ä¸­é—´colabæ–­äº†â€¦ï¼‰</li>
<li>è¯»å–æµ‹è¯•é›†ï¼Œè·Ÿè®­ç»ƒé›†ä¸€æ ·å¤„ç†ï¼Œä¿å­˜ç»“æœå¹¶æäº¤ã€‚æœ€ç»ˆå¾—åˆ†0.961ã€‚<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span>(<span class="params">PreTrainedTokenizer</span>):</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case,</span><br><span class="line">                never_split=never_split,</span><br><span class="line">                tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">                strip_accents=strip_accents,</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">#BasicTokenizerä¸­å®šä¹‰äº†æ ‡ç‚¹åˆ†å‰²çš„æ–¹æ³•ï¼Œä¸éœ€è¦å†å»å¦å¤–å¤„ç†</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = <span class="built_in">list</span>(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[-<span class="number">1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-1-æ„å»ºåˆ†è¯å™¨"><a href="#3-1-æ„å»ºåˆ†è¯å™¨" class="headerlink" title="3.1 æ„å»ºåˆ†è¯å™¨"></a>3.1 æ„å»ºåˆ†è¯å™¨</h3><p>å‚è€ƒæœ¬æ–‡ç¬¬äºŒèŠ‚ï¼Œå¹¶æŸ¥çœ‹äº†<a target="_blank" rel="noopener" href="https://huggingface.co/bert-base-chinese/tree/main">bert-base-chinese,josn</a>æ–‡ä»¶é…ç½®åˆ†è¯å™¨ã€‚è®­ç»ƒè¯­è¨€æ¨¡å‹å‚è€ƒ<a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">æ­¤æ•™ç¨‹</a>åŠ<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.5-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">ä¸­æ–‡ç¿»è¯‘</a>ã€‚<br>æ„Ÿå—ï¼šæœ€å‘çš„æ˜¯è®­ç»ƒåˆ†è¯å™¨ï¼Œä»å¤´åˆ°å°¾é€‰æ‹©decoders, models, pre_tokenizers, processors, trainers, Tokenizeræœ‰ç‚¹éº»çƒ¦ã€‚æœ€åè£…è¿›PreTrainedTokenizerFastä¹‹åè¿˜æœ‰äº›ä¸œè¥¿éœ€è¦è®¾ç½®ï¼Œçœ‹äº†å¥½å¤šæ¬¡æ–‡æ¡£æ‰è¯•å‡ºæ¥ã€‚<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ä»googleäº‘ç›˜ä¸ŠåŠ è½½æ•°æ®</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&#x27;/content/drive/MyDrive/transformers/å¤©æ± -å…¥é—¨NLP - æ–°é—»æ–‡æœ¬åˆ†ç±»&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#å®‰è£…transformers=4.11.2</span></span><br><span class="line">!pip install transformers datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ–‡ä»¶è¯»å–</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>, sep =<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">df=pd.concat((train_df,test_df))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#å°†3750/648/900æ”¹æˆæ ‡ç‚¹ç¬¦å·ï¼Œåˆ é™¤åŸtextåˆ—ï¼Œæ–°å¢wordsåˆ—é‡åä¸ºtextåˆ—</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replacepunc</span>(<span class="params">x</span>):</span></span><br><span class="line">  x=re.sub(<span class="string">&#x27;3750&#x27;</span>,<span class="string">&quot;,&quot;</span>,x)</span><br><span class="line">  x=re.sub(<span class="string">&#x27;900&#x27;</span>,<span class="string">&quot;.&quot;</span>,x)</span><br><span class="line">  x=re.sub(<span class="string">&#x27;648&#x27;</span>,<span class="string">&quot;!&quot;</span>,x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;words&#x27;</span>]=df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: replacepunc(x))</span><br><span class="line">df.drop(<span class="string">&#x27;text&#x27;</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df.columns=[<span class="string">&#x27;label&#x27;</span>,<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#æ•°æ®è½½å…¥datasetï¼Œå»é™¤å¤šä½™çš„åˆ—ï¼Œåªä¿ç•™textåˆ—</span></span><br><span class="line">data=Dataset.from_pandas(df).remove_columns([<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;__index_level_0__&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#æ„å»ºæ•°æ®æ‰¹å¤„ç†è¿­ä»£å™¨ï¼Œè¿™éƒ¨åˆ†ä»£ç æ˜¯å‚è€ƒHFä¸»é¡µæ•™ç¨‹</span></span><br><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iterator</span>():</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(data), batch_size):</span><br><span class="line">    <span class="keyword">yield</span> data[<span class="string">&#x27;text&#x27;</span>][i : i + batch_size]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è®¾ç½®åˆ†è¯å™¨å¹¶è¿›è¡Œè®­ç»ƒ</span></span><br><span class="line"><span class="comment">#åˆå§‹åŒ–åˆ†è¯å™¨ã€é¢„åˆ†è¯å™¨</span></span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unl_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">7000</span>,min_frequency=<span class="number">2</span>,special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>])</span><br><span class="line">tokenizer.decoders = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#å¼€å§‹è®­ç»ƒ</span></span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è¿›è¡Œåˆ†è¯åå¤„ç†</span></span><br><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">mask_token_id = tokenizer.token_to_id(<span class="string">&quot;[MASK]&quot;</span>)</span><br><span class="line">pad_token_id = tokenizer.token_to_id(<span class="string">&quot;[PAD]&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>,cls_token_id),(<span class="string">&quot;[SEP]&quot;</span>,sep_token_id),(<span class="string">&quot;[MASK]&quot;</span>,mask_token_id)],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">tokenizer.enable_truncation(max_length=<span class="number">512</span>)</span><br><span class="line">tokenizer.enable_padding(pad_token=<span class="string">&#x27;[PAD]&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#æµ‹è¯•åˆ†è¯ç»“æœ</span></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&#x27;2491 4109 1757 7539 648 3695 3038 4490 23 7019 3731 4109 3792 2465&#x27;</span>,<span class="string">&#x27; 2893 7212 5296 1667 3618 7044 1519 5413 1283 6122 4893 7495 2435 5510&#x27;</span>)</span><br><span class="line">encoding.tokens</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;ä¿å­˜æ¨¡å‹å¹¶é‡æ–°åŠ è½½</span></span><br><span class="line"><span class="string">tokenizerå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¿…é¡»å°†å®ƒæ”¾åœ¨ä¸æˆ‘ä»¬è¦ä½¿ç”¨çš„æ¨¡å‹ç›¸å¯¹åº”çš„æ ‡è®°å™¨ fast ç±»ã€‚</span></span><br><span class="line"><span class="string">æ­£åœ¨æ„å»ºçš„åˆ†è¯å™¨ä¸ Transformers ä¸­çš„ä»»ä½•ç±»éƒ½ä¸åŒ¹é…(åˆ†è¯å™¨éå¸¸ç‰¹æ®Š)ï¼Œ</span></span><br><span class="line"><span class="string">æ‚¨å¯ä»¥å°†å®ƒåŒ…è£…åœ¨ PreTrainedTokenizerFast ä¸­&quot;&quot;&quot;</span></span><br><span class="line">tokenizer.save(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast</span><br><span class="line">(tokenizer_file=<span class="string">&quot;tokenizer.json&quot;</span>,</span><br><span class="line">model_max_length=<span class="number">512</span>,mask_token=<span class="string">&#x27;[MASK]&#x27;</span>,pad_token=<span class="string">&#x27;[PAD]&#x27;</span>,</span><br><span class="line">unk_token=<span class="string">&#x27;[UNK]&#x27;</span>,cls_token=<span class="string">&#x27;[CLS]&#x27;</span>,sep_token=<span class="string">&#x27;[SEP]&#x27;</span>,</span><br><span class="line">padding_side=<span class="string">&#x27;right&#x27;</span>,return_special_tokens_mask=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#PreTrainedTokenizerFastä¸­ä¸€å®šè¦è®¾ç½®mask_tokenï¼Œpad_tokenç­‰ï¼Œ</span></span><br><span class="line"><span class="comment">#ä¸ç„¶mlmæŠ¥é”™æ²¡æœ‰è®¾å®šmask_tokenä»¥åŠåˆ†è¯å™¨æ— æ³•padding</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-é¢„è®­ç»ƒbertæ¨¡å‹"><a href="#3-2-é¢„è®­ç»ƒbertæ¨¡å‹" class="headerlink" title="3.2 é¢„è®­ç»ƒbertæ¨¡å‹"></a>3.2 é¢„è®­ç»ƒbertæ¨¡å‹</h3><p>å‚è€ƒnlpæ•™ç¨‹4.5<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#data_collatoræ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡</span></span><br><span class="line"><span class="comment">#åœ¨data_collatorä¸­å¯ä»¥ç¡®ä¿æ¯æ¬¡ä»¥æ–°çš„æ–¹å¼å®Œæˆéšæœºæ©è”½ã€‚</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForLanguageModeling</span><br><span class="line">data_collator = DataCollatorForLanguageModeling(tokenizer=fast_tokenizer,mlm=<span class="literal">True</span>,mlm_probability=<span class="number">0.15</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#åˆå§‹åŒ–bertæ¨¡å‹ï¼Œå‚æ•°å‚è€ƒè®¨è®ºåŒºä»£ç </span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig</span><br><span class="line">config = BertConfig(</span><br><span class="line">    vocab_size=<span class="number">7000</span>,</span><br><span class="line">    hidden_size=<span class="number">512</span>,</span><br><span class="line">    intermediate_size=<span class="number">4</span>*<span class="number">512</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">512</span>,</span><br><span class="line">    num_hidden_layers=<span class="number">4</span>,</span><br><span class="line">    num_attention_heads=<span class="number">4</span>,</span><br><span class="line">    type_vocab_size=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line">model = BertForMaskedLM(config=config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ï¼ˆæ‰çº¿åï¼‰åŠ è½½è®­ç»ƒåˆ°ä¸€åŠçš„æ¨¡å‹ï¼š</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">&#x27;/content/drive/MyDrive/transformers/å¤©æ± -å…¥é—¨NLP - æ–°é—»æ–‡æœ¬åˆ†ç±»/test-clm/checkpoint-56000&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#æ•°æ®è¿›è¡Œåˆ†è¯é¢„å¤„ç†ï¼Œåˆ é™¤â€˜text&#x27;åˆ—ï¼Œå¦åˆ™åé¢æ‹¼æ¥çš„æ—¶å€™ä¼šæŠ¥é”™ã€‚</span></span><br><span class="line">tokenized_datasets=data.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;text&#x27;</span>]),batched=<span class="literal">True</span>).remove_columns(<span class="string">&quot;text&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬ï¼Œè¿™ä¸€å—è§£é‡Šå¯ä»¥çœ‹nlp 4.5æ•™ç¨‹</span></span><br><span class="line">block_size = <span class="number">128</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span></span><br><span class="line"></span><br><span class="line">  concatenated_examples = &#123;k: <span class="built_in">sum</span>(examples[k], []) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">  total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">  <span class="comment"># æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚</span></span><br><span class="line">  total_length = (total_length // block_size) * block_size</span><br><span class="line">  <span class="comment"># é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚</span></span><br><span class="line">  result = &#123;</span><br><span class="line">      k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">      <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">  &#125;</span><br><span class="line">  result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy()</span><br><span class="line">  <span class="keyword">return</span> result</span><br><span class="line"> </span><br><span class="line"> lm_datasets = tokenized_datasets.<span class="built_in">map</span>(</span><br><span class="line">    group_texts,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=<span class="number">1000</span>,</span><br><span class="line">    num_proc=<span class="number">4</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#åŠ è½½å’Œä¿å­˜æ‹¼æ¥åçš„æ–‡æœ¬ï¼Œæ‰çº¿çš„æ—¶å€™è¿™ä¹ˆåš</span></span><br><span class="line">lm_datasets.save_to_disk(<span class="string">&#x27;./lm&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_from_disk</span><br><span class="line">lm_datasets=load_from_disk(<span class="string">&#x27;./lm&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è§£ç åˆ†è¯å™¨é¢„å¤„ç†çš„lm_datasetsæ•°æ®ï¼Œé‡Œé¢æœ‰æ ‡ç‚¹ç¬¦å·</span></span><br><span class="line">la=fast_tokenizer.decode(lm_datasets[<span class="number">0</span>][<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">la</span><br><span class="line"></span><br><span class="line">[CLS] <span class="number">2967</span> <span class="number">6758</span> <span class="number">339</span> <span class="number">2021</span> <span class="number">1854</span> <span class="number">3731</span> <span class="number">4109</span> <span class="number">3792</span> <span class="number">4149</span> <span class="number">1519</span> <span class="number">2058</span> <span class="number">3912</span> <span class="number">2465</span> <span class="number">2410</span> <span class="number">1219</span> <span class="number">6654</span> <span class="number">7539</span> <span class="number">264</span> <span class="number">2456</span> <span class="number">4811</span> <span class="number">1292</span> <span class="number">2109</span> <span class="number">6905</span> <span class="number">5520</span> <span class="number">7058</span> <span class="number">6045</span> <span class="number">3634</span> <span class="number">6591</span> <span class="number">3530</span> <span class="number">6508</span> <span class="number">2465</span> <span class="number">7044</span> <span class="number">1519</span> <span class="number">3659</span> <span class="number">2073</span>, <span class="number">3731</span> <span class="number">4109</span> <span class="number">3792</span> <span class="number">6831</span> <span class="number">2614</span> <span class="number">3370</span> <span class="number">4269</span> <span class="number">3370</span> <span class="number">486</span> <span class="number">5770</span> <span class="number">4109</span> <span class="number">4125</span>, <span class="number">5445</span> <span class="number">2466</span> <span class="number">6831</span> <span class="number">6758</span> <span class="number">3743</span> <span class="number">3630</span> <span class="number">1726</span> <span class="number">2313</span> <span class="number">5906</span> <span class="number">826</span> <span class="number">4516</span> <span class="number">657.</span> <span class="number">1871</span> <span class="number">7044</span>, <span class="number">2967</span> <span class="number">3731</span> <span class="number">1757</span> <span class="number">1939</span>! <span class="number">2828</span> <span class="number">4704</span> <span class="number">7039</span> <span class="number">3706</span>, <span class="number">965</span> <span class="number">2490</span> <span class="number">7399</span> <span class="number">3743</span> <span class="number">2145</span> <span class="number">2407</span> <span class="number">7451</span> <span class="number">3775</span> <span class="number">6017</span> <span class="number">5998</span> <span class="number">1641</span> <span class="number">299</span> <span class="number">4704</span> <span class="number">2621</span> <span class="number">7029</span> <span class="number">3056</span> <span class="number">6333</span> <span class="number">433</span>! <span class="number">1667</span> <span class="number">1099.</span> <span class="number">2289</span> <span class="number">1099</span>! <span class="number">5780</span> <span class="number">220</span> <span class="number">7044</span> <span class="number">1279</span> <span class="number">7426</span> <span class="number">4269</span>, <span class="number">2967</span> <span class="number">6758</span> <span class="number">6631</span> <span class="number">3099</span> <span class="number">2205</span> <span class="number">7305</span> <span class="number">2620</span> <span class="number">5977</span>, <span class="number">3329</span> <span class="number">1793</span> <span class="number">6666</span> <span class="number">2042</span> <span class="number">3193</span> <span class="number">4149</span> <span class="number">1519</span> <span class="number">7039</span> <span class="number">3706</span> <span class="number">2446</span> <span class="number">5399</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ä½¿ç”¨GPUè®­ç»ƒï¼Œè¿è¡Œè¿™æ®µä»£ç </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>==GPU memoryå¼€å§‹å ç”¨1GBï¼Œä½†æ˜¯è¿˜æ²¡å¼€å§‹ä½¿ç”¨è®¡ç®—ã€‚==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#å®‰è£…TPUä¾èµ–</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">assert</span> os.environ[<span class="string">&#x27;COLAB_TPU_ADDR&#x27;</span>], <span class="string">&#x27;Make sure to select TPU from Edit &gt; Notebook settings &gt; Hardware accelerator&#x27;</span></span><br><span class="line">!pip install cloud-tpu-client==<span class="number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="number">1.9</span>-cp37-cp37m-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line"><span class="comment">#å°†æ¨¡å‹å¤åˆ¶åˆ°TPUè¿›è¡Œè®­ç»ƒ</span></span><br><span class="line"><span class="keyword">import</span> torch_xla.core.xla_model <span class="keyword">as</span> xm</span><br><span class="line">device = xm.xla_device()</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è®¾å®šargså’Œtrainerå‡†å¤‡è®­ç»ƒ.3000æ­¥çœ‹ä¸€æ¬¡lossï¼Œ9000æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼ˆæ€•æ‰çº¿ï¼‰</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;Test-Clm&quot;</span>,</span><br><span class="line">    logging_strategy=<span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    logging_steps=<span class="number">3000</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    save_steps=<span class="number">9000</span>,</span><br><span class="line">    num_train_epochs=<span class="number">2</span>,</span><br><span class="line">    learning_rate=<span class="number">3e-4</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">96</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=lm_datasets,</span><br><span class="line">    data_collator=data_collator)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹</span></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line">trainer.save_model(<span class="string">&quot;./pre_Bert&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>è¿™æ®µæ˜¯å½“æ—¶batch_sizeå¤ªé«˜ï¼Œæ˜¾å­˜çˆ†äº†ï¼Œæˆ‘æ‰¾ä¸€ä¸‹åŸå› ã€‚å¯ä»¥å¿½ç•¥ã€‚<br>1%çš„æ•°æ®è¯•éªŒ</p>
<ol>
<li>ç¬¬ä¸€æ¬¡è®­ç»ƒæ¶¨åˆ°5.9GBï¼Œ5ä¸ªepochï¼Œ540stepsï¼Œbatch_size=128ã€‚è®­ç»ƒå®Œåæ˜¯æ˜¾å­˜1.2GBã€‚logging_steps=100,å¯ä»¥é€‰æ‹©å¤šä¹…çœ‹ä¸€æ¬¡lossã€‚</li>
<li>å†æ¬¡è®­ç»ƒæ²¡æœ‰æŒ‡å®šbatch_sizeï¼Œæ˜¾å­˜æ˜¯1.8GBã€‚è®­ç»ƒå®Œ1.5GBã€‚ç®—äº†ä¸€ä¸‹é»˜è®¤batch_size=8ã€‚</li>
</ol>
<h3 id="3-3-åˆ†ç±»ä»»åŠ¡å¾®è°ƒï¼š"><a href="#3-3-åˆ†ç±»ä»»åŠ¡å¾®è°ƒï¼š" class="headerlink" title="3.3 åˆ†ç±»ä»»åŠ¡å¾®è°ƒï¼š"></a>3.3 åˆ†ç±»ä»»åŠ¡å¾®è°ƒï¼š</h3><ol>
<li>åŠ è½½é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒGPUæˆ–TPUè®­ç»ƒ<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;./pre_Bert&quot;</span>,num_labels=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ä½¿ç”¨GPUè®­ç»ƒ</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#å°†æ¨¡å‹å¤åˆ¶åˆ°TPUè¿›è¡Œè®­ç»ƒ</span></span><br><span class="line"><span class="keyword">import</span> torch_xla.core.xla_model <span class="keyword">as</span> xm</span><br><span class="line">device = xm.xla_device()</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure></li>
<li>è¯»å–æ•°æ®é›†ï¼Œå‡†å¤‡è¿›è¡Œé¢„å¤„ç†</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#è¯»å–æ•°æ®å¹¶shuffle</span></span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;./train_set.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>).sample(frac=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#å°†è®­ç»ƒæ•°æ®ä¸­ä¸‰ä¸ªtokenæ¢æˆæ ‡ç‚¹</span></span><br><span class="line">train_df[<span class="string">&#x27;texts&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x:replacepunc(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#å‡†å¤‡å°†textæ–‡æœ¬é¦–å°¾æˆªæ–­ï¼Œå„å–255tokens</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slipt2</span>(<span class="params">x</span>):</span></span><br><span class="line">	ls=x.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">	le=<span class="built_in">len</span>(ls)</span><br><span class="line">	<span class="keyword">if</span> le&lt;<span class="number">511</span>:</span><br><span class="line">	    <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">	    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(ls[:<span class="number">255</span>]+ls[-<span class="number">255</span>:])</span><br></pre></td></tr></table></figure>
<ol>
<li>åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹0.1<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">val_df=train_df.iloc[:<span class="number">20000</span>, ]</span><br><span class="line">trains_df=train_df.iloc[<span class="number">20000</span>:, ]</span><br><span class="line"></span><br><span class="line"><span class="comment">#é¦–å°¾æˆªæ–­</span></span><br><span class="line">val_df[<span class="string">&#x27;summary&#x27;</span>]=val_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line">trains_df[<span class="string">&#x27;summary&#x27;</span>]=trains_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#åŠ è½½åˆ°datasetå¹¶é¢„å¤„ç†</span></span><br><span class="line">trains_ds=Dataset.from_pandas(trains_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line">val_ds=Dataset.from_pandas(val_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenized_trains_ds=trains_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_val_ds=val_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>è®¾ç½®TrainingArgumentså’ŒTrainer<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è®¾ç½®accè¯„ä¼°æ–¹å¼</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line">metric = load_metric(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#è¿›è¡Œä»»åŠ¡å¾®è°ƒ</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments,Trainer</span><br><span class="line">args=TrainingArguments(</span><br><span class="line">  output_dir=<span class="string">&#x27;news-classification-2&#x27;</span>,</span><br><span class="line">  evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">  save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">  learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">  per_device_train_batch_size=<span class="number">96</span>,</span><br><span class="line">  per_device_eval_batch_size=<span class="number">96</span>,</span><br><span class="line">  num_train_epochs=<span class="number">6</span>,</span><br><span class="line">  weight_decay=<span class="number">0.01</span>,</span><br><span class="line">  load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">  metric_for_best_model=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer=Trainer(</span><br><span class="line">  model,</span><br><span class="line">  args,</span><br><span class="line">  train_dataset=tokenized_trains_ds,</span><br><span class="line">  eval_dataset=tokenized_val_ds,</span><br><span class="line">  tokenizer=fast_tokenizer,</span><br><span class="line">  compute_metrics=compute_metrics)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>==è®­ç»ƒå®ŒGPU memoryè¿˜æ˜¯1.5GB==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.save_model(<span class="string">&quot;./finally_bert&quot;</span>)</span><br></pre></td></tr></table></figure><br>==ä¸€å¼€å§‹è®­ç»ƒ,GPU memoryè·³åˆ°15.8GBï¼ˆbatch_size=128ï¼‰ã€‚çˆ†äº†ä¹‹åé€‰æ‹©åˆ†ç±»å¾®è°ƒæ¨¡å‹çš„batch_size=16ï¼ŒGPU memoryä¸º3.4GB==</p>
<ol>
<li>æœ€åè¯»å–æµ‹è¯•é›†ï¼Œé¢„æµ‹ç»“æœ<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è¯»å–æµ‹è¯•é›†å¹¶é¢„å¤„ç†</span></span><br><span class="line"><span class="comment">#è¯»å–æµ‹è¯•é›†</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">test_df=pd.read_csv(<span class="string">&#x27;./test_a.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#å°†è®­ç»ƒæ•°æ®ä¸­ä¸‰ä¸ªtokenæ¢æˆæ ‡ç‚¹</span></span><br><span class="line">test_df[<span class="string">&#x27;texts&#x27;</span>]=test_df[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x:replacepunc(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#é¦–å°¾æˆªæ–­</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line">test_df[<span class="string">&#x27;summary&#x27;</span>]=test_df[<span class="string">&#x27;texts&#x27;</span>].apply(<span class="keyword">lambda</span> x:slipt2(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#åŠ è½½åˆ°datasetå¹¶é¢„å¤„ç†</span></span><br><span class="line">test_ds=Dataset.from_pandas(test_df).remove_columns([<span class="string">&quot;texts&quot;</span>,<span class="string">&quot;text&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenized_test_ds=test_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> examples:fast_tokenizer(examples[<span class="string">&#x27;summary&#x27;</span>],truncation=<span class="literal">True</span>,padding=<span class="literal">True</span>),batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ç”¨traineré¢„æµ‹ç»“æœå¹¶ä¿å­˜</span></span><br><span class="line">predictions,metrics,Loss=trainer.predict(tokenized_test_ds,metric_key_prefix=<span class="string">&quot;test&quot;</span>)</span><br><span class="line">pred=np.argmax(predictions,axis=<span class="number">1</span>)</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>:pred&#125;).to_csv(<span class="string">&#x27;submit1022.csv&#x27;</span>,index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>3.4 èµ›äº‹æ€»ç»“ï¼š</p>
<ol>
<li>ä¸€å¼€å§‹è¦ææ‡‚baselineçš„åŸºæœ¬æ¡†æ¶å’Œconfigä½œä¸ºå‚è€ƒï¼Œå¦‚æœæ¯”è¾ƒéš¾è¯»ä¸æ‡‚ï¼Œå¯ä»¥ç›´æ¥è·‘ä¸€éæˆ–è€…debugï¼ˆè¿˜æ²¡æœ‰è·‘ï¼Œæœ‰äººè¯´å†…å­˜çˆ†äº†ã€‚ã€‚ã€‚ï¼‰</li>
<li>æœ€å¼€å§‹ç”¨å°‘é‡æ•°æ®è·‘ï¼Œbatch_sizeã€å­¦ä¹ ç‡ã€æ•°æ®é›†ã€epochå’Œæ—¶é•¿ç¡®å®šå¥½å†è·‘ä¸€éã€‚ä¹‹å‰å°±æ˜¯å«Œæ—¶é—´å¤ªé•¿åŠ äº†batch_sizeç»“æœè·‘åˆ°æ¨¡å‹å¾®è°ƒæ—¶æ˜¾å­˜å´©äº†ï¼Œæ‰€æœ‰ç¼“å­˜æ•°æ®éƒ½æ²¡äº†ã€‚</li>
<li>ä¸­é—´æ•°æ®å’Œè®­ç»ƒä¸­çš„æ¨¡å‹è¦è®°å¾—ä¿å­˜ï¼Œä¸€æ—¦æ‰çº¿æˆ–è€…å´©äº†æˆ–è€…æƒ³ä¿®æ”¹å‚æ•°å¯ä»¥ç»§ç»­åŠ è½½å†è·‘ã€‚å› ä¸ºä¸€å¼€å§‹ä¸çŸ¥é“å¦‚ä½•ä¿å­˜å’ŒåŠ è½½datasetsæ•°æ®ã€kaggleçš„notebookè€æ˜¯æ— æ³•ä¿å­˜ï¼Œç»“æœæ€»æ˜¯ç™½è·‘äº†æ¨¡å‹ï¼Œæµªè´¹æ—¶é—´ã€‚</li>
<li>æƒ³åˆ°å†è¡¥</li>
</ol>
<h2 id="é›¶ã€åˆ†è¯tokenization"><a href="#é›¶ã€åˆ†è¯tokenization" class="headerlink" title="é›¶ã€åˆ†è¯tokenization"></a>é›¶ã€åˆ†è¯tokenization</h2><p>æ¯”èµ›æ•°æ®è„±æ•ï¼Œéœ€è¦ä»å¤´å¼€å§‹é¢„è®­ç»ƒã€‚ç¬¬ä¸€æ­¥å°±æ˜¯å»ºç«‹è¯è¡¨ï¼Œè®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨</p>
<p>å‚è€ƒèµ„æ–™ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/tokenizer_summary.html">ã€ŠSummary of the tokenizersã€‹</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/jokerxsy/article/details/116998827">ã€Š[NLP]â€”â€”BPEã€WordPieceã€Unigram and SentencePieceã€‹</a></p>
<p>==wordpieceå’ŒBPEçš„å·®å¼‚åœ¨äºåˆå¹¶æ—¶å¯¹tokenå¯¹çš„é€‰æ‹©:BPEæ˜¯é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤§çš„ï¼Œwordpieceè¡¡é‡çš„æ˜¯tokenå¯¹å’Œå•ç‹¬çš„ä¸¤ä¸ªtokenä¹‹é—´çš„æ¦‚ç‡å·®ï¼Œé€‰æ‹©æ¦‚ç‡å·®æœ€å¤§çš„è¿›è¡Œåˆå¹¶ã€‚==</p>
<p>è€ƒè™‘token aå’Œbï¼Œä»¥åŠåˆå¹¶ä¹‹åçš„token abï¼Œæ¦‚ç‡å·®çš„å…¬å¼å¦‚ä¸‹:</p>
<script type="math/tex; mode=display">p(a,b)/(p(a)âˆ—p(b))</script><p>==è¿™å¯ä»¥è¿‘ä¼¼ç†è§£ä¸ºåˆå¹¶å‰åï¼Œæ•´ä¸ªè¯­æ–™çš„äº’ä¿¡æ¯ã€‚å³ï¼Œå½“å‰é€‰æ‹©åˆå¹¶çš„tokenå¯¹èƒ½å¤Ÿè®©è¯­æ–™çš„ç†µæœ€å°åŒ–-&gt;ç¡®å®šæ€§æœ€å¤§åŒ–-&gt;ä¿¡æ¯é‡æœ€å°åŒ–-&gt;åœ¨è®¡ç®—æœºä¸­å­˜å‚¨æ‰€éœ€è¦çš„ç¼–ç é•¿åº¦æœ€çŸ­åŒ–ã€‚==</p>
<p>æ‰€ä»¥å¦‚æœè¯è¡¨ä¸­å­—ç¬¦aå’Œbæœ¬èº«æ¬¡æ•°å°±å¾ˆé«˜ï¼Œå¦‚æœåˆå¹¶abçš„æ¦‚ç‡å°±ç®—ä¸é«˜ï¼ˆæ¯”å¦‚0.1ï¼‰ï¼š</p>
<ul>
<li>å¯¹äºbpeæ¥è¯´abæ¬¡æ•°å¤šï¼Œéœ€è¦åˆå¹¶</li>
<li>å¯¹äºwordpieceï¼Œabåˆå¹¶æ¦‚ç‡ä½ï¼Œä¸åˆå¹¶</li>
</ul>
<p>tokenizerå¯ä»¥å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯æˆ–å­è¯ï¼ˆå³æ ‡è®°æ–‡æœ¬ï¼‰ã€‚ ğŸ¤— Transformers ä¸­ä½¿ç”¨çš„ä¸‰ç§ä¸»è¦ç±»å‹çš„åˆ†è¯å™¨ï¼š Byte-Pair Encodingå­—èŠ‚å¯¹ç¼–ç  (BPE)ã€WordPiece å’Œ SentencePieceï¼Œä¸‹é¢å±•ç¤ºå“ªä¸ªæ¨¡å‹ä½¿ç”¨å“ªç§åˆ†è¯å™¨ç±»å‹çš„ç¤ºä¾‹ã€‚</p>
<blockquote>
<p>åœ¨æ¯ä¸ªæ¨¡å‹é¡µé¢ä¸Šï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ç›¸å…³åˆ†è¯å™¨çš„æ–‡æ¡£ä»¥äº†è§£é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„åˆ†è¯å™¨ç±»å‹ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ BertTokenizerï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹ä½¿ç”¨ WordPiece</p>
</blockquote>
<h3 id="1-2-åˆ†è¯è§„åˆ™"><a href="#1-2-åˆ†è¯è§„åˆ™" class="headerlink" title="1.2 åˆ†è¯è§„åˆ™"></a>1.2 åˆ†è¯è§„åˆ™</h3><p>åˆ†è¯æœ‰å¤šç§æ–¹å¼ï¼Œå¯¹äºä¸€ä¸ªå¥å­ï¼š<br>â€œDonâ€™t you love ğŸ¤— Transformers? We sure do.â€</p>
<ul>
<li>å¯ä»¥æŒ‰ç©ºæ ¼åˆ†è¯ï¼š<br>[â€œDonâ€™tâ€, â€œyouâ€, â€œloveâ€, â€œğŸ¤—â€, â€œTransformers?â€, â€œWeâ€, â€œsureâ€, â€œdo.â€]</li>
<li>åŒºåˆ†æ ‡ç‚¹ï¼štokenså’Œæ ‡ç‚¹çš„å„ç§ç»„åˆä¼šå¯¼è‡´æ¨¡å‹å¿…é¡»å­¦ä¹ çš„è¡¨ç¤ºæ•°é‡æ¿€å¢ï¼Œæ‰€ä»¥åº”è¯¥äºˆä»¥æ¸…ç†ã€‚æ ‡ç‚¹å¤„ç†åå¾—åˆ°ï¼š<br>[â€œDonâ€, â€œâ€˜â€œ, â€œtâ€, â€œyouâ€, â€œloveâ€, â€œğŸ¤—â€, â€œTransformersâ€, â€œ?â€, â€œWeâ€, â€œsureâ€, â€œdoâ€, â€œ.â€]</li>
<li>åŒºåˆ†ç¼©å†™ï¼šâ€œDonâ€™tâ€ä»£è¡¨â€œdo notâ€ï¼Œå› æ­¤æœ€å¥½å°†å…¶æ ‡è®°ä¸º [â€œDoâ€, â€œnâ€™tâ€]ã€‚è¿™å°±æ˜¯äº‹æƒ…å¼€å§‹å˜å¾—å¤æ‚çš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„æ ‡è®°å™¨ç±»å‹çš„éƒ¨åˆ†åŸå› </li>
</ul>
<p>æ ¹æ®æˆ‘ä»¬åº”ç”¨äºæ ‡è®°æ–‡æœ¬çš„è§„åˆ™ï¼Œä¸ºç›¸åŒçš„æ–‡æœ¬ç”Ÿæˆä¸åŒçš„æ ‡è®°è¾“å‡ºã€‚ é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥å¿…é¡»æ˜¯ï¼Œç”¨äºæ ‡è®°å…¶è®­ç»ƒæ•°æ®çš„ç›¸åŒè§„åˆ™çš„æ ‡è®°è¾“å…¥ï¼Œè¿™æ ·æ‰èƒ½æ­£å¸¸æ‰§è¡Œã€‚</p>
<p>spaCy å’Œ Moses æ˜¯ä¸¤ç§æµè¡Œçš„åŸºäºè§„åˆ™çš„æ ‡è®°å™¨ã€‚ å°†å®ƒä»¬åº”ç”¨åˆ°æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼ŒspaCy å’Œ Moses å°†è¾“å‡ºå¦‚ä¸‹å†…å®¹ï¼š<br>[â€œDoâ€, â€œnâ€™tâ€, â€œyouâ€, â€œloveâ€, â€œğŸ¤—â€, â€œTransformersâ€, â€œ?â€, â€œWeâ€, â€œsureâ€, â€œdoâ€, â€œ.â€]</p>
<p>è¿™é‡Œä½¿ç”¨äº†ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åŒ–ä»¥åŠåŸºäºè§„åˆ™çš„æ ‡è®°åŒ–ï¼Œå…¶æ¾æ•£å®šä¹‰ä¸ºå°†å¥å­æ‹†åˆ†ä¸ºå•è¯ã€‚è¿™ç§æ ‡è®°åŒ–æ–¹æ³•éå¸¸ç®€å•ï¼Œä½†æ˜¯å¯èƒ½ä¼šå¯¼è‡´å¤§é‡æ–‡æœ¬è¯­æ–™åº“å‡ºç°é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªéå¸¸å¤§çš„è¯æ±‡è¡¨ï¼ˆä½¿ç”¨çš„æ‰€æœ‰å”¯ä¸€å•è¯å’Œæ ‡è®°çš„é›†åˆï¼‰ã€‚ä¾‹å¦‚ï¼ŒTransformer XL ä½¿ç”¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åŒ–ï¼Œå¯¼è‡´è¯æ±‡é‡å¤§å°ä¸º 267,735ï¼</p>
<p>å¦‚æ­¤å¤§çš„è¯æ±‡é‡è¿«ä½¿æ¨¡å‹æœ‰ä¸€ä¸ªå·¨å¤§çš„åµŒå…¥çŸ©é˜µä½œä¸ºè¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œè¿™ä¼šå¯¼è‡´å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦çš„å¢åŠ ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ==transformers æ¨¡å‹çš„è¯æ±‡é‡å¾ˆå°‘è¶…è¿‡ 50,000ï¼Œå°¤å…¶æ˜¯å½“å®ƒä»¬ä»…åœ¨ä¸€ç§è¯­è¨€ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶==ã€‚</p>
<p>é‚£ä¹ˆå¦‚æœç®€å•çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åŒ–ä¸èƒ½ä»¤äººæ»¡æ„ï¼Œä¸ºä»€ä¹ˆä¸ç®€å•åœ°å¯¹å­—ç¬¦charè¿›è¡Œæ ‡è®°åŒ–å‘¢ï¼Ÿ</p>
<h3 id="1-3-character-based-tokenizer"><a href="#1-3-character-based-tokenizer" class="headerlink" title="1.3 character-based-tokenizer"></a>1.3 character-based-tokenizer</h3><p>å­—ç¬¦æ ‡è®°åŒ–å¾€å¾€ä¼´éšç€æ€§èƒ½çš„æŸå¤±ï¼Œä½¿æ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰çš„è¾“å…¥è¡¨ç¤ºå˜å¾—æ›´åŠ å›°éš¾ã€‚ä¾‹å¦‚ã€‚ å­¦ä¹ å­—æ¯â€œtâ€çš„æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡æ¯”å­¦ä¹ å•è¯â€œtodayâ€çš„ä¸Šä¸‹æ–‡æ— å…³è¡¨ç¤ºè¦å›°éš¾å¾—å¤šã€‚ å› æ­¤ï¼Œä¸ºäº†ä¸¤å…¨å…¶ç¾ï¼Œtransformers æ¨¡å‹ä½¿ç”¨äº†è¯çº§å’Œå­—ç¬¦çº§æ ‡è®°åŒ–ä¹‹é—´çš„æ··åˆï¼Œç§°ä¸ºå­è¯æ ‡è®°åŒ–ã€‚</p>
<h3 id="1-4-Subword-tokenization"><a href="#1-4-Subword-tokenization" class="headerlink" title="1.4 Subword tokenization"></a>1.4 Subword tokenization</h3><p>åŸåˆ™ï¼šä¸åº”å°†å¸¸ç”¨è¯æ‹†åˆ†ä¸ºæ›´å°çš„å­è¯ï¼Œè€Œåº”å°†ç¨€æœ‰è¯åˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„å­è¯ã€‚ä»¥é€šè¿‡å°†å­è¯ä¸²åœ¨ä¸€èµ·æ¥å½¢æˆï¼ˆå‡ ä¹ï¼‰ä»»æ„é•¿çš„å¤æ‚è¯ã€‚<br>ä¾‹å¦‚ï¼Œâ€œannoyinglyâ€å¯èƒ½è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªç½•è§çš„è¯ï¼Œå¯ä»¥åˆ†è§£ä¸ºâ€œannoyingâ€å’Œâ€œlyâ€ã€‚ â€œannoyingâ€å’Œâ€œlyâ€ä½œä¸ºç‹¬ç«‹çš„å­è¯å‡ºç°çš„é¢‘ç‡ä¼šæ›´é«˜ï¼ŒåŒæ—¶â€œannoyinglyâ€çš„æ„æ€è¢«â€œannoyingâ€å’Œâ€œlyâ€çš„å¤åˆè¯æ‰€ä¿æŒã€‚</p>
<ul>
<li>å­è¯æ ‡è®°åŒ–å…è®¸æ¨¡å‹å…·æœ‰åˆç†çš„è¯æ±‡é‡</li>
<li>åŒæ—¶èƒ½å¤Ÿå­¦ä¹ æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡æ— å…³è¡¨ç¤º</li>
<li>ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å°†å®ƒä»¬åˆ†è§£ä¸ºå·²çŸ¥çš„å­è¯æ¥å¤„ç†å®ƒä»¥å‰ä»æœªè§è¿‡çš„è¯</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;I have a new GPU!&quot;</span>)</span><br><span class="line">[<span class="string">&quot;i&quot;</span>, <span class="string">&quot;have&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;new&quot;</span>, <span class="string">&quot;gp&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;!&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å•è¯ [â€œiâ€, â€œhaveâ€, â€œaâ€, â€œnewâ€] å‡ºç°åœ¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¸­ï¼Œä½†å•è¯â€œgpuâ€å´æ²¡æœ‰ã€‚ å› æ­¤ï¼Œåˆ†è¯å™¨å°†â€œgpuâ€æ‹†åˆ†ä¸ºå·²çŸ¥çš„å­è¯ï¼š[â€œgpâ€å’Œâ€œ##uâ€]ã€‚ â€œ##â€è¡¨ç¤ºä»¤ç‰Œçš„å…¶ä½™éƒ¨åˆ†åº”é™„åŠ åˆ°å‰ä¸€ä¸ªï¼Œæ²¡æœ‰ç©ºæ ¼ï¼ˆç”¨äºè§£ç æˆ–é€†è½¬ä»¤ç‰ŒåŒ–ï¼‰ã€‚</p>
<p>å†ä¸¾ä¸€ä¸ªä¾‹å­ï¼ŒXLNetTokenizer å°†æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹æ–‡æœ¬åˆ†è¯å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizer</span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(<span class="string">&quot;xlnet-base-cased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;Don&#x27;t you love ğŸ¤— Transformers? We sure do.&quot;</span>)</span><br><span class="line">[<span class="string">&quot;â–Don&quot;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="string">&quot;â–you&quot;</span>, <span class="string">&quot;â–love&quot;</span>, <span class="string">&quot;â–&quot;</span>, <span class="string">&quot;ğŸ¤—&quot;</span>, <span class="string">&quot;â–&quot;</span>, <span class="string">&quot;Transform&quot;</span>, <span class="string">&quot;ers&quot;</span>, <span class="string">&quot;?&quot;</span>, <span class="string">&quot;â–We&quot;</span>, <span class="string">&quot;â–sure&quot;</span>, <span class="string">&quot;â–do&quot;</span>, <span class="string">&quot;.&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>SentencePieceï¼šå°†ç½•è§è¯â€Transformersâ€ æ‹†åˆ†æˆæ›´å¸¸è§çš„å­è¯ â€œTransformâ€ å’Œ â€œersâ€.</p>
<p>ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸åŒçš„å­è¯æ ‡è®°åŒ–ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚</p>
<h3 id="1-5-Byte-Pair-Encodingå­—èŠ‚å¯¹ç¼–ç -BPE"><a href="#1-5-Byte-Pair-Encodingå­—èŠ‚å¯¹ç¼–ç -BPE" class="headerlink" title="1.5 Byte-Pair Encodingå­—èŠ‚å¯¹ç¼–ç  (BPE)"></a>1.5 Byte-Pair Encodingå­—èŠ‚å¯¹ç¼–ç  (BPE)</h3><ul>
<li>Byte-Pair Encoding (BPE) æ˜¯Neural Machine Translationå¼•å…¥çš„ï¼ˆå…·æœ‰ç½•è§è¯çš„å­è¯unitsï¼‰ã€‚ä¾èµ–äºå°†è®­ç»ƒæ•°æ®æ‹†åˆ†ä¸ºwordçš„pre-tokenizerã€‚é¢„æ ‡è®°åŒ–å¯ä»¥åƒç©ºæ ¼åŒ–ï¼ˆspace tokenizationï¼‰ä¸€æ ·ç®€å•ï¼Œå°±åƒGPT-2, Robertaã€‚</li>
<li>æ›´é«˜çº§çš„é¢„æ ‡è®°åŒ–åŒ…æ‹¬åŸºäºè§„åˆ™çš„æ ‡è®°åŒ–ï¼Œä¾‹å¦‚XLMã€FlauBERTï¼ˆåœ¨å¤§å¤šæ•°è¯­è¨€ä¸­ä½¿ç”¨ Mosesï¼‰æˆ– GPTï¼ˆä½¿ç”¨ Spacy å’Œ ftfyï¼‰æ¥è®¡ç®—è®­ç»ƒè¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ã€‚</li>
<li>åœ¨é¢„æ ‡è®°åŒ–ä¹‹åï¼š<ul>
<li>æ ¹æ®è®­ç»ƒæ•°æ®å½¢æˆä¸€ç³»åˆ—å”¯ä¸€tokenåŠå…¶å‡ºç°é¢‘ç‡</li>
<li>BPE åˆ›å»ºä¸€ä¸ªç”±å”¯ä¸€å•è¯é›†ä¸­çš„æ‰€æœ‰ç¬¦å·ç»„æˆçš„åŸºæœ¬è¯æ±‡è¡¨</li>
<li>å­¦ä¹ åˆå¹¶è§„åˆ™ä»¥ä»åŸºæœ¬è¯æ±‡è¡¨çš„ä¸¤ä¸ªç¬¦å·å½¢æˆä¸€ä¸ªæ–°ç¬¦å·ã€‚ç›´åˆ°è¯æ±‡é‡è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡é‡å¤§å°ã€‚è¯·æ³¨æ„ï¼Œ==æ‰€éœ€çš„è¯æ±‡é‡æ˜¯åœ¨è®­ç»ƒåˆ†è¯å™¨ä¹‹å‰å®šä¹‰çš„è¶…å‚æ•°==ã€‚</li>
</ul>
</li>
</ul>
<p>ä¸¾ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬å‡è®¾åœ¨é¢„æ ‡è®°åŒ–ä¹‹åï¼Œå·²ç»ç¡®å®šäº†ä»¥ä¸‹ä¸€ç»„å•è¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„é¢‘ç‡ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>åŸºæœ¬è¯æ±‡æ˜¯ [â€œbâ€, â€œgâ€, â€œhâ€, â€œnâ€, â€œpâ€, â€œsâ€, â€œuâ€]ã€‚ å°†æ‰€æœ‰å•è¯æ‹†åˆ†ä¸ºåŸºæœ¬è¯æ±‡è¡¨çš„ç¬¦å·ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>BPE è®¡ç®—æ¯ä¸ªå¯èƒ½çš„ç¬¦å·å¯¹çš„é¢‘ç‡å¹¶é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„ç¬¦å·å¯¹ã€‚æœ€é¢‘ç¹çš„ç¬¦å·å¯¹æ˜¯â€œuâ€åè·Ÿâ€œgâ€ï¼Œæ€»å…±å‡ºç° 10 + 5 + 5 = 20 æ¬¡ï¼Œå› æ­¤ï¼Œåˆ†è¯å™¨å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯å°†æ‰€æœ‰â€œuâ€ç¬¦å·å’Œåè·Ÿâ€œgâ€ç¬¦å·ç»„åˆåœ¨ä¸€èµ·ã€‚ æ¥ä¸‹æ¥ï¼Œå°†â€œugâ€æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚ è¿™ç»„è¯ç„¶åå˜æˆï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>BPE è¯†åˆ«ä¸‹ä¸€ä¸ªæœ€å¸¸è§çš„ç¬¦å·å¯¹ï¼šâ€œuâ€åè·Ÿâ€œnâ€16 æ¬¡ã€‚ â€œuâ€, â€œnâ€ åˆå¹¶åˆ° â€œunâ€ å¹¶æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚ ä¸‹ä¸€ä¸ªæœ€é¢‘ç¹çš„ç¬¦å·å¯¹æ˜¯â€œhâ€åè·Ÿâ€œugâ€ï¼Œå‡ºç° 15 æ¬¡ã€‚ è¿™å¯¹å†æ¬¡åˆå¹¶ï¼Œå¹¶ä¸”å¯ä»¥å°†â€œhugâ€æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚</li>
</ol>
<p>åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè¯æ±‡æ˜¯ [â€œbâ€, â€œgâ€, â€œhâ€, â€œnâ€, â€œpâ€, â€œsâ€, â€œuâ€, â€œugâ€, â€œunâ€, â€œhugâ€] å’Œæˆ‘ä»¬çš„ ä¸€ç»„ç‹¬ç‰¹çš„è¯è¡¨ç¤ºä¸ºï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>å‡è®¾å­—èŠ‚å¯¹ç¼–ç è®­ç»ƒå°†åœ¨æ­¤æ—¶åœæ­¢ï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™åº”ç”¨äºæ–°å•è¯ã€‚ä¾‹å¦‚â€bugâ€åˆ†è¯æˆ [â€œbâ€, â€œugâ€]ï¼Œä½†æ˜¯â€mugâ€ åˆ†è¯æˆ [â€œ<unk>â€œ, â€œugâ€]ã€‚å› ä¸ºè¯æ±‡è¡¨ä¸åŒ…å«ï¼šâ€œmâ€ã€‚</li>
</ol>
<p>å¦‚å‰æ‰€è¿°ï¼Œè¯æ±‡é‡å¤§å°ï¼Œå³åŸºæœ¬è¯æ±‡é‡å¤§å° + åˆå¹¶æ¬¡æ•°ï¼ˆbase vocabulary size + the number of mergesï¼‰ï¼Œæ˜¯ä¸€ä¸ªå¯ä¾›é€‰æ‹©çš„è¶…å‚æ•°ã€‚ ä¾‹å¦‚ï¼ŒGPT çš„è¯æ±‡é‡æ˜¯ 40,478ï¼Œå› ä¸ºå®ƒä»¬æœ‰ 478 ä¸ªåŸºæœ¬å­—ç¬¦ï¼Œå¹¶ä¸”åœ¨ 40,000 æ¬¡åˆå¹¶åé€‰æ‹©åœæ­¢è®­ç»ƒã€‚</p>
<h3 id="1-6-å­—èŠ‚çº§-BPEï¼ˆByte-level-BPEï¼‰"><a href="#1-6-å­—èŠ‚çº§-BPEï¼ˆByte-level-BPEï¼‰" class="headerlink" title="1.6 å­—èŠ‚çº§ BPEï¼ˆByte-level BPEï¼‰"></a>1.6 å­—èŠ‚çº§ BPEï¼ˆByte-level BPEï¼‰</h3><p>GPT-2 ä½¿ç”¨å­—èŠ‚ä½œä¸ºåŸºç¡€è¯æ±‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå·§å¦™çš„æŠ€å·§ï¼Œå¯ä»¥å¼ºåˆ¶åŸºç¡€è¯æ±‡çš„å¤§å°ä¸º 256ï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªåŸºç¡€å­—ç¬¦éƒ½åŒ…å«åœ¨è¯æ±‡ä¸­ã€‚å†åŠ ä¸Šä¸€äº›é¢å¤–çš„æ ‡ç‚¹ç¬¦å·å¤„ç†è§„åˆ™ï¼ŒGPT-2çš„åˆ†è¯å™¨ä¸éœ€è¦\<unk>ç¬¦å·ã€‚<br>GPT-2 çš„è¯æ±‡é‡å¤§å°ä¸º 50,257ï¼Œå¯¹åº”äº 256 å­—èŠ‚çš„åŸºæœ¬æ ‡è®°ã€ä¸€ä¸ªç‰¹æ®Šçš„æ–‡æœ¬ç»“æŸæ ‡è®°å’Œé€šè¿‡ 50,000 æ¬¡åˆå¹¶å­¦ä¹ çš„ç¬¦å·ã€‚</p>
<h3 id="1-7-WordPiece"><a href="#1-7-WordPiece" class="headerlink" title="1.7 WordPiece"></a>1.7 WordPiece</h3><p>WordPiece æ˜¯ç”¨äº BERTã€DistilBERT å’Œ Electra çš„å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼Œä¸ BPE éå¸¸ç›¸ä¼¼ã€‚ WordPiece é¦–å…ˆåˆå§‹åŒ–è¯æ±‡è¡¨ä»¥åŒ…å«è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„æ¯ä¸ªå­—ç¬¦ï¼Œå¹¶é€æ­¥å­¦ä¹ ç»™å®šæ•°é‡çš„åˆå¹¶è§„åˆ™ã€‚==ä¸ BPE ç›¸æ¯”ï¼ŒWordPiece ä¸é€‰æ‹©æœ€é¢‘ç¹çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©å°†è®­ç»ƒæ•°æ®æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„å¯èƒ½æ€§æœ€å¤§åŒ–çš„ç¬¦å·å¯¹==ã€‚</p>
<p>é‚£ä¹ˆè¿™åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå‚è€ƒå‰é¢çš„ä¾‹å­ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶æ€§ç›¸å½“äºæ‰¾åˆ°ç¬¦å·å¯¹ï¼Œå…¶æ¦‚ç‡é™¤ä»¥å…¶ç¬¬ä¸€ä¸ªç¬¦å·åè·Ÿç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡åœ¨æ‰€æœ‰ç¬¦å·å¯¹ä¸­æœ€å¤§ã€‚ä¾‹å¦‚ã€‚åªæœ‰å½“â€œugâ€é™¤ä»¥â€œuâ€ã€â€œgâ€çš„æ¦‚ç‡å¤§äºä»»ä½•å…¶ä»–ç¬¦å·å¯¹æ—¶ï¼Œâ€œuâ€å’Œâ€œgâ€æ‰ä¼šè¢«åˆå¹¶ã€‚ç›´è§‚åœ°è¯´ï¼ŒWordPiece ä¸ BPE ç•¥æœ‰ä¸åŒï¼Œå®ƒé€šè¿‡åˆå¹¶ä¸¤ä¸ªç¬¦å·æ¥è¯„ä¼°å®ƒçš„æŸå¤±ï¼Œä»¥ç¡®ä¿itâ€™s worth itã€‚</p>
<h3 id="1-8-Unigram"><a href="#1-8-Unigram" class="headerlink" title="1.8 Unigram"></a>1.8 Unigram</h3><ul>
<li>åŸºæœ¬è¯æ±‡è¡¨å¯ä»¥å¯¹åº”äºæ‰€æœ‰é¢„å…ˆæ ‡è®°çš„å•è¯å’Œæœ€å¸¸è§çš„å­ä¸²</li>
<li>åˆ é™¤äº†æŸå¤±å¢åŠ æœ€ä½çš„ç¬¦å·ï¼Œé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¯æ±‡é‡è¾¾åˆ°æ‰€éœ€çš„å¤§å°ã€‚</li>
</ul>
<p>Unigram æ˜¯åœ¨ Subword æ­£åˆ™åŒ–ï¼šä¸ BPE æˆ– WordPiece ç›¸æ¯”ï¼Œ==Unigram å°†å…¶åŸºæœ¬è¯æ±‡è¡¨åˆå§‹åŒ–ä¸ºå¤§é‡ç¬¦å·ï¼Œå¹¶é€æ­¥ç¼©å‡æ¯ä¸ªç¬¦å·ä»¥è·å¾—è¾ƒå°çš„è¯æ±‡è¡¨ã€‚ä¾‹å¦‚ï¼ŒåŸºæœ¬è¯æ±‡è¡¨å¯ä»¥å¯¹åº”äºæ‰€æœ‰é¢„å…ˆæ ‡è®°çš„å•è¯å’Œæœ€å¸¸è§çš„å­ä¸²==ã€‚ Unigram ä¸ç›´æ¥ç”¨äºtransformersä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œä½†å®ƒä¸ SentencePiece ç»“åˆä½¿ç”¨ã€‚</p>
<p>åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼ŒUnigram ç®—æ³•åœ¨ç»™å®šå½“å‰è¯æ±‡å’Œ unigram è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹å®šä¹‰è®­ç»ƒæ•°æ®çš„æŸå¤±ï¼ˆé€šå¸¸å®šä¹‰ä¸ºå¯¹æ•°ä¼¼ç„¶ï¼‰ã€‚ç„¶åï¼Œ==å¯¹äºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·ï¼Œç®—æ³•è®¡ç®—å¦‚æœè¦ä»è¯æ±‡è¡¨ä¸­åˆ é™¤è¯¥ç¬¦å·ï¼Œæ€»ä½“æŸå¤±ä¼šå¢åŠ å¤šå°‘ã€‚ç„¶å Unigram åˆ é™¤äº†æŸå¤±å¢åŠ æœ€ä½çš„ç¬¦å·çš„ æ¦‚ç‡pï¼ˆé€šå¸¸ä¸º 10% æˆ– 20%ï¼‰ï¼Œå³é‚£äº›å¯¹è®­ç»ƒæ•°æ®çš„æ•´ä½“æŸå¤±å½±å“æœ€å°çš„ç¬¦å·ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¯æ±‡é‡è¾¾åˆ°æ‰€éœ€çš„å¤§å°ã€‚== Unigram ç®—æ³•å§‹ç»ˆä¿ç•™åŸºæœ¬å­—ç¬¦ï¼Œä»¥ä¾¿å¯ä»¥å¯¹ä»»ä½•å•è¯è¿›è¡Œæ ‡è®°ã€‚</p>
<p>ç”±äº Unigram ä¸åŸºäºåˆå¹¶è§„åˆ™ï¼ˆä¸ BPE å’Œ WordPiece ä¸åŒï¼‰ï¼Œå› æ­¤è¯¥ç®—æ³•æœ‰å¤šç§æ–¹æ³•å¯ä»¥åœ¨è®­ç»ƒåå¯¹æ–°æ–‡æœ¬è¿›è¡Œæ ‡è®°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç»è¿‡è®­ç»ƒçš„ Unigram åˆ†è¯å™¨å±•ç¤ºè¯æ±‡è¡¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>, <span class="string">&quot;un&quot;</span>, <span class="string">&quot;hug&quot;</span>],</span><br></pre></td></tr></table></figure>
<p>â€œhugâ€å¯ä»¥æ ‡è®°ä¸º [â€œhugâ€, â€œsâ€], [â€œhâ€, â€œugâ€, â€œsâ€] æˆ– [â€œhâ€, â€œuâ€, â€œgâ€, â€œsâ€]ã€‚ é‚£ä¹ˆè¯¥é€‰æ‹©å“ªä¸€ä¸ªå‘¢ï¼Ÿ Unigram åœ¨ä¿å­˜è¯æ±‡çš„åŸºç¡€ä¸Šè¿˜ä¿å­˜äº†è®­ç»ƒè¯­æ–™åº“ä¸­æ¯ä¸ªæ ‡è®°çš„æ¦‚ç‡ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒåè®¡ç®—æ¯ä¸ªå¯èƒ½çš„æ ‡è®°åŒ–çš„æ¦‚ç‡ã€‚ è¯¥ç®—æ³•åœ¨å®è·µä¸­åªæ˜¯ç®€å•åœ°é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°åŒ–ï¼Œä½†ä¹Ÿæä¾›äº†æ ¹æ®æ¦‚ç‡å¯¹å¯èƒ½çš„æ ‡è®°åŒ–è¿›è¡Œé‡‡æ ·çš„å¯èƒ½æ€§ã€‚</p>
<p>è¿™äº›æ¦‚ç‡ç”±åˆ†è¯å™¨è®­ç»ƒçš„æŸå¤±å®šä¹‰ã€‚ å‡è®¾è®­ç»ƒæ•°æ®ç”±å•è¯ x1,â€¦,xN ç»„æˆï¼Œå¹¶ä¸”å•è¯ xi çš„æ‰€æœ‰å¯èƒ½æ ‡è®°çš„é›†åˆè¢«å®šä¹‰ä¸º S(xi)ï¼Œé‚£ä¹ˆæ€»æŸå¤±å®šä¹‰ä¸ºï¼š<br><img src="https://img-blog.csdnimg.cn/93a7af17c6b14b92ba23d3f6e59ceccc.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h3 id="1-9-SentencePiece"><a href="#1-9-SentencePiece" class="headerlink" title="1.9 SentencePiece"></a>1.9 SentencePiece</h3><p>åˆ°ç›®å‰ä¸ºæ­¢æè¿°çš„æ‰€æœ‰æ ‡è®°åŒ–ç®—æ³•éƒ½æœ‰ç›¸åŒçš„é—®é¢˜ï¼š==å‡è®¾è¾“å…¥æ–‡æœ¬ä½¿ç”¨ç©ºæ ¼æ¥åˆ†éš”å•è¯==ã€‚<br>ä½†æ˜¯ï¼Œå¹¶éæ‰€æœ‰è¯­è¨€éƒ½ä½¿ç”¨ç©ºæ ¼æ¥åˆ†éš”å•è¯ï¼Œä¾‹å¦‚ä¸­æ–‡ã€æ—¥æ–‡å’Œæ³°æ–‡ã€‚<br>ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ç‰¹å®šäºè¯­è¨€çš„é¢„åˆ†è¯å™¨ï¼Œä¾‹å¦‚XLM ä½¿ç”¨ç‰¹å®šçš„é¢„åˆ†è¯å™¨ã€‚<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.06226.pdf">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)</a> å°†è¾“å…¥è§†ä¸ºåŸå§‹è¾“å…¥æµï¼Œthus including the space in the set of characters to use.ç„¶åå®ƒä½¿ç”¨ BPE æˆ– unigram ç®—æ³•æ¥æ„å»ºé€‚å½“çš„è¯æ±‡è¡¨ã€‚</p>
<p>ä¾‹å¦‚ï¼ŒXLNetTokenizer ä¸­ä½¿ç”¨çš„ SentencePieceï¼Œè§£ç éå¸¸å®¹æ˜“ï¼Œå› ä¸ºæ‰€æœ‰æ ‡è®°éƒ½å¯ä»¥è¿æ¥èµ·æ¥ï¼Œå¹¶ä¸”â€œ-â€ è¢«ç©ºæ ¼æ›¿æ¢ã€‚ SentencePiece å’Œunigram ç»“åˆä½¿ç”¨ï¼ŒåŒ…æ‹¬ ALBERTã€XLNetã€Marian å’Œ T5ã€‚</p>
<p>æœ¬æ–‡å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">how_to_train.ipynb</a></p>
<h2 id="ä¸€ã€è®­ç»ƒåˆ†è¯å™¨"><a href="#ä¸€ã€è®­ç»ƒåˆ†è¯å™¨" class="headerlink" title="ä¸€ã€è®­ç»ƒåˆ†è¯å™¨"></a>ä¸€ã€è®­ç»ƒåˆ†è¯å™¨</h2><h3 id="1-1-Using-tokenizers-from-ğŸ¤—-Tokenizers"><a href="#1-1-Using-tokenizers-from-ğŸ¤—-Tokenizers" class="headerlink" title="1.1 Using tokenizers from ğŸ¤— Tokenizers"></a>1.1 Using tokenizers from ğŸ¤— Tokenizers</h3><blockquote>
<p>å‚è€ƒæ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/fast_tokenizers.html">HFæ–‡æ¡£</a></p>
</blockquote>
<p>PreTrainedTokenizerFast ä¾èµ–äº tokenizers åº“ã€‚ ä» ğŸ¤— Tokenizers åº“ä¸­è·å¾—çš„åˆ†è¯å™¨å¯ä»¥éå¸¸ç®€å•åœ°åŠ è½½åˆ° ğŸ¤— Transformers ä¸­ã€‚<br>åœ¨è¯¦ç»†ä»‹ç»ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆåœ¨å‡ è¡Œä¸­åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ ‡è®°å™¨ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tokenizers.models <span class="keyword">import</span> BPE</span><br><span class="line"><span class="keyword">from</span> tokenizers.trainers <span class="keyword">import</span> BpeTrainer</span><br><span class="line"><span class="keyword">from</span> tokenizers.pre_tokenizers <span class="keyword">import</span> Whitespace</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(BPE(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line">trainer = BpeTrainer(special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>])</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = Whitespace()</span><br><span class="line">files = [...]</span><br><span class="line">tokenizer.train(files, trainer)</span><br></pre></td></tr></table></figure><br>ç°åœ¨æœ‰äº†ä¸€ä¸ªæˆ‘ä»¬å®šä¹‰çš„æ ‡è®°å™¨ï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨ï¼Œæˆ–è€…å°†å®ƒä¿å­˜åˆ°ä¸€ä¸ª JSON æ–‡ä»¶ä¸­ä»¥å¤‡å°†æ¥é‡ç”¨ã€‚</p>
<ul>
<li>ç›´æ¥ä»¥tokenizer objectä½¿ç”¨</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<ul>
<li>jsonæ–‡ä»¶åŠ è½½ä½¿ç”¨</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.save(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br><span class="line"><span class="comment">#æˆ‘ä»¬ä¿å­˜æ­¤æ–‡ä»¶çš„è·¯å¾„å¯ä»¥ä½¿ç”¨ tokenizer_file å‚æ•°ä¼ é€’ç»™ PreTrainedTokenizerFast åˆå§‹åŒ–æ–¹æ³•ï¼š</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="string">&quot;tokenizer.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-2-Train-your-tokenizer"><a href="#1-2-Train-your-tokenizer" class="headerlink" title="1.2 Train your tokenizer"></a>1.2 Train your tokenizer</h3><p>Transformers Notebooksâ€”â€”<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">How to train and use your very own tokenizer</a></p>
<h4 id="1-2-1-ä»å¤´è®­ç»ƒåˆ†è¯å™¨"><a href="#1-2-1-ä»å¤´è®­ç»ƒåˆ†è¯å™¨" class="headerlink" title="1.2.1 ä»å¤´è®­ç»ƒåˆ†è¯å™¨"></a>1.2.1 ä»å¤´è®­ç»ƒåˆ†è¯å™¨</h4><p>ç»™å®šè¯­æ–™åº“ä¸Šè®­ç»ƒåˆ†è¯å™¨ï¼Œè¿›è€Œä»å¤´è®­ç»ƒtransformeræ¨¡å‹ã€‚ åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/tokenizer_summary.html">tokenizers summary</a> ä¸­å¯ä»¥æŸ¥çœ‹å­è¯åˆ†è¯ç®—æ³•ä¹‹é—´çš„å·®å¼‚ï¼ˆä¹Ÿå°±æ˜¯ä¸Šä¸€èŠ‚å†…å®¹ï¼‰ã€‚</p>
<p>ä¸‹é¢ä¸¾ä¾‹ä½¿ç”¨wikitextæ•°æ®é›†ï¼ˆåŒ…å« 4.5MB çš„æ–‡æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ä¾‹å­è®­ç»ƒé€Ÿåº¦å¾ˆå¿«ï¼‰è®­ç»ƒåˆ†è¯å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;text&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">36718</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">dataset[:<span class="number">5</span>]</span><br><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: [<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27; = Valkyria Chronicles III = \n&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27; SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the &quot; Nameless &quot; , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit &quot; Calamaty Raven &quot; . \n&#x27;</span>,</span><br><span class="line">  <span class="string">&quot; The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game &#x27;s opening theme was sung by May &#x27;n . \n&quot;</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>è®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨çš„ API å°†éœ€è¦ä¸€æ‰¹æ–‡æœ¬çš„è¿­ä»£å™¨ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ—è¡¨ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line">all_texts = [dataset[i : i + batch_size][<span class="string">&quot;text&quot;</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size)]</span><br></pre></td></tr></table></figure><br>ä¸ºäº†é¿å…å°†æ‰€æœ‰å†…å®¹åŠ è½½åˆ°å†…å­˜ä¸­ï¼ˆå› ä¸º Datasets åº“å°†å…ƒç´ ä¿å­˜åœ¨ç£ç›˜ä¸Šå¹¶ä¸”ä»…åœ¨è¯·æ±‚æ—¶å°†å®ƒä»¬åŠ è½½åˆ°å†…å­˜ä¸­ï¼‰ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª Python è¿­ä»£å™¨æ¥è¿›è¡Œæ‰¹å¤„ç†ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_iterator</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + batch_size][<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>æ¥ä¸‹æ¥æœ‰ä¸¤ç§æ–¹æ³•è®­ç»ƒåˆ†è¯å™¨ï¼š</p>
<ol>
<li>ä½¿ç”¨ç°æœ‰çš„åˆ†è¯å™¨ï¼Œä¸€è¡Œä»£ç å°±å¯ä»¥åœ¨ç»™å®šæ•°æ®é›†ä¸Šè®­ç»ƒæ–°çš„åˆ†è¯å™¨</li>
<li>é€å—æ„å»ºåˆ†è¯å™¨ï¼Œå› æ­¤å¯ä»¥è‡ªå®šä¹‰æ¯ä¸€æ­¥ ï¼</li>
</ol>
<h4 id="1-2-2-ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨è®­ç»ƒ"><a href="#1-2-2-ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨è®­ç»ƒ" class="headerlink" title="1.2.2 ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨è®­ç»ƒ"></a>1.2.2 ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨è®­ç»ƒ</h4><p>å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸ç°æœ‰ç®—æ³•å®Œå…¨ç›¸åŒçš„ç®—æ³•å’Œå‚æ•°æ¥è®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨ï¼Œæ‚¨å¯ä»¥åªä½¿ç”¨ train_new_from_iterator APIã€‚ ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ ‡è®°åŒ–ç®—æ³•åœ¨ Wikitext-2 ä¸Šè®­ç»ƒæ–°ç‰ˆæœ¬çš„ GPT-2 tokenzierã€‚</p>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬æƒ³è¦ç”¨ä½œæ¨¡å‹çš„tokenizerï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>ç¡®ä¿æ‚¨é€‰æ‹©çš„æ ‡è®°å™¨æ˜¯å¿«é€Ÿç‰ˆæœ¬ï¼ˆç”± ğŸ¤— Tokenizers åº“æ”¯æŒï¼‰ï¼Œå¦åˆ™ notebook çš„å…¶ä½™éƒ¨åˆ†å°†æ— æ³•è¿è¡Œï¼š</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.is_fast</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>ç„¶åæˆ‘ä»¬å°†è®­ç»ƒè¯­æ–™åº“ï¼ˆlist of listæˆ–æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è¿­ä»£å™¨ï¼‰æä¾›ç»™ train_new_from_iterator æ–¹æ³•ã€‚ æˆ‘ä»¬è¿˜å¿…é¡»æŒ‡å®šè¦ä½¿ç”¨çš„è¯æ±‡é‡å¤§å°ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=<span class="number">25000</span>)</span><br></pre></td></tr></table></figure>
<p>åˆ°æ­¤å°±å®Œæˆäº†åˆ†è¯å™¨çš„è®­ç»ƒã€‚ç”±äºä½¿ç”¨äº†Rust æ”¯æŒçš„ ğŸ¤— Tokenizers åº“ï¼Œè®­ç»ƒè¿›è¡Œå¾—éå¸¸å¿«ã€‚<br>æ‚¨ç°åœ¨æœ‰ä¸€ä¸ªæ–°çš„æ ‡è®°å™¨å¯ä»¥é¢„å¤„ç†æ‚¨çš„æ•°æ®å¹¶è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ æ‚¨å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è¾“å…¥è¾“å…¥æ–‡æœ¬ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer(dataset[:<span class="number">5</span>][<span class="string">&quot;text&quot;</span>])</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [[], [<span class="number">238</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">2987</span>, <span class="number">238</span>, <span class="number">252</span>], [], [<span class="number">4657</span>, <span class="number">74</span>, <span class="number">4762</span>, <span class="number">826</span>, <span class="number">8576</span>, <span class="number">428</span>, <span class="number">466</span>, <span class="number">609</span>, <span class="number">6881</span>, <span class="number">412</span>, <span class="number">204</span>, <span class="number">9441</span>, <span class="number">311</span>, <span class="number">2746</span>, <span class="number">466</span>, <span class="number">10816</span>, <span class="number">168</span>, <span class="number">99</span>, <span class="number">150</span>, <span class="number">192</span>, <span class="number">112</span>, <span class="number">14328</span>, <span class="number">3983</span>, <span class="number">112</span>, <span class="number">4446</span>, <span class="number">94</span>, <span class="number">18288</span>, <span class="number">4446</span>, <span class="number">193</span>, <span class="number">3983</span>, <span class="number">98</span>, <span class="number">3983</span>, <span class="number">22171</span>, <span class="number">95</span>, <span class="number">19</span>, <span class="number">201</span>, <span class="number">6374</span>, <span class="number">209</span>, <span class="number">8576</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">3455</span>, <span class="number">1972</span>, <span class="number">428</span>, <span class="number">310</span>, <span class="number">201</span>, <span class="number">5099</span>, <span class="number">3242</span>, <span class="number">227</span>, <span class="number">281</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">2987</span>, <span class="number">2553</span>, <span class="number">1759</span>, <span class="number">201</span>, <span class="number">301</span>, <span class="number">196</span>, <span class="number">13996</span>, <span class="number">1496</span>, <span class="number">277</span>, <span class="number">2330</span>, <span class="number">1464</span>, <span class="number">674</span>, <span class="number">1898</span>, <span class="number">307</span>, <span class="number">742</span>, <span class="number">3541</span>, <span class="number">225</span>, <span class="number">7514</span>, <span class="number">14</span>, <span class="number">54</span>, <span class="number">719</span>, <span class="number">274</span>, <span class="number">198</span>, <span class="number">4777</span>, <span class="number">15522</span>, <span class="number">209</span>, <span class="number">19895</span>, <span class="number">221</span>, <span class="number">1341</span>, <span class="number">1633</span>, <span class="number">221</span>, <span class="number">1759</span>, <span class="number">201</span>, <span class="number">322</span>, <span class="number">301</span>, <span class="number">198</span>, <span class="number">1368</span>, <span class="number">674</span>, <span class="number">221</span>, <span class="number">198</span>, <span class="number">8576</span>, <span class="number">843</span>, <span class="number">209</span>, <span class="number">2468</span>, <span class="number">1795</span>, <span class="number">223</span>, <span class="number">198</span>, <span class="number">1049</span>, <span class="number">9595</span>, <span class="number">218</span>, <span class="number">13996</span>, <span class="number">225</span>, <span class="number">1563</span>, <span class="number">277</span>, <span class="number">582</span>, <span class="number">6493</span>, <span class="number">281</span>, <span class="number">457</span>, <span class="number">14371</span>, <span class="number">201</span>, <span class="number">198</span>, <span class="number">1422</span>, <span class="number">3373</span>, <span class="number">7452</span>, <span class="number">227</span>, <span class="number">198</span>, <span class="number">455</span>, <span class="number">674</span>, <span class="number">225</span>, <span class="number">4687</span>, <span class="number">198</span>, <span class="number">239</span>, <span class="number">21976</span>, <span class="number">239</span>, <span class="number">201</span>, <span class="number">196</span>, <span class="number">21657</span>, <span class="number">1680</span>, <span class="number">3773</span>, <span class="number">5591</span>, <span class="number">198</span>, <span class="number">4196</span>, <span class="number">218</span>, <span class="number">4679</span>, <span class="number">427</span>, <span class="number">661</span>, <span class="number">198</span>, <span class="number">3518</span>, <span class="number">1288</span>, <span class="number">220</span>, <span class="number">1051</span>, <span class="number">516</span>, <span class="number">889</span>, <span class="number">3947</span>, <span class="number">1922</span>, <span class="number">2500</span>, <span class="number">225</span>, <span class="number">390</span>, <span class="number">2065</span>, <span class="number">744</span>, <span class="number">872</span>, <span class="number">198</span>, <span class="number">7592</span>, <span class="number">3773</span>, <span class="number">239</span>, <span class="number">1975</span>, <span class="number">251</span>, <span class="number">208</span>, <span class="number">89</span>, <span class="number">22351</span>, <span class="number">239</span>, <span class="number">209</span>, <span class="number">252</span>], [<span class="number">261</span>, <span class="number">674</span>, <span class="number">959</span>, <span class="number">1921</span>, <span class="number">221</span>, <span class="number">1462</span>, <span class="number">201</span>, <span class="number">7600</span>, <span class="number">547</span>, <span class="number">196</span>, <span class="number">1178</span>, <span class="number">4753</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">630</span>, <span class="number">3591</span>, <span class="number">263</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">1180</span>, <span class="number">209</span>, <span class="number">1831</span>, <span class="number">322</span>, <span class="number">7568</span>, <span class="number">198</span>, <span class="number">3621</span>, <span class="number">2240</span>, <span class="number">218</span>, <span class="number">198</span>, <span class="number">843</span>, <span class="number">201</span>, <span class="number">322</span>, <span class="number">471</span>, <span class="number">9575</span>, <span class="number">5291</span>, <span class="number">16591</span>, <span class="number">967</span>, <span class="number">201</span>, <span class="number">781</span>, <span class="number">281</span>, <span class="number">1815</span>, <span class="number">198</span>, <span class="number">674</span>, <span class="number">604</span>, <span class="number">10344</span>, <span class="number">1252</span>, <span class="number">274</span>, <span class="number">843</span>, <span class="number">664</span>, <span class="number">3147</span>, <span class="number">320</span>, <span class="number">209</span>, <span class="number">13290</span>, <span class="number">8751</span>, <span class="number">8124</span>, <span class="number">2528</span>, <span class="number">6023</span>, <span class="number">74</span>, <span class="number">235</span>, <span class="number">225</span>, <span class="number">7445</span>, <span class="number">10040</span>, <span class="number">17384</span>, <span class="number">241</span>, <span class="number">11487</span>, <span class="number">8950</span>, <span class="number">857</span>, <span class="number">1835</span>, <span class="number">340</span>, <span class="number">1382</span>, <span class="number">22582</span>, <span class="number">201</span>, <span class="number">1008</span>, <span class="number">296</span>, <span class="number">8576</span>, <span class="number">9441</span>, <span class="number">1180</span>, <span class="number">2436</span>, <span class="number">21134</span>, <span class="number">5337</span>, <span class="number">19463</span>, <span class="number">5161</span>, <span class="number">209</span>, <span class="number">240</span>, <span class="number">1178</span>, <span class="number">927</span>, <span class="number">218</span>, <span class="number">3776</span>, <span class="number">8650</span>, <span class="number">198</span>, <span class="number">3355</span>, <span class="number">209</span>, <span class="number">261</span>, <span class="number">674</span>, <span class="number">268</span>, <span class="number">83</span>, <span class="number">2511</span>, <span class="number">3472</span>, <span class="number">258</span>, <span class="number">8288</span>, <span class="number">307</span>, <span class="number">1010</span>, <span class="number">268</span>, <span class="number">78</span>, <span class="number">209</span>, <span class="number">252</span>]], <span class="string">&#x27;attention_mask&#x27;</span>: [[], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]&#125;</span><br></pre></td></tr></table></figure>
<p>ä¿å­˜æ¨¡å‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_tokenizer.save_pretrained(<span class="string">&quot;my-new-tokenizer&quot;</span>)</span><br><span class="line"></span><br><span class="line">(<span class="string">&#x27;my-new-tokenizer/tokenizer_config.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/special_tokens_map.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/vocab.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/merges.txt&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/added_tokens.json&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;my-new-tokenizer/tokenizer.json&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>ä¹‹åå¯ä»¥åŠ è½½æ­¤åˆ†è¯å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tok = new_tokenizer.from_pretrained(<span class="string">&quot;my-new-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>æˆ–è€…æ¨é€åˆ° Hugging Face Hub ä»¥ä»ä»»ä½•åœ°æ–¹ä½¿ç”¨è¿™ä¸ªæ–°çš„ tokenzierï¼Œå…·ä½“æ“ä½œå‚è€ƒ<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb">æ­¤å¤„</a>ã€‚</p>
<h4 id="1-2-3-ä»å¤´æ„å»ºåˆ†è¯å™¨"><a href="#1-2-3-ä»å¤´æ„å»ºåˆ†è¯å™¨" class="headerlink" title="1.2.3 ä»å¤´æ„å»ºåˆ†è¯å™¨"></a>1.2.3 ä»å¤´æ„å»ºåˆ†è¯å™¨</h4><p>å¦‚æœä½ æƒ³åˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªæ–°çš„æ ‡è®°å™¨ï¼Œå®ƒçœ‹èµ·æ¥ä¸åƒç°æœ‰çš„ä»»ä½•ä¸œè¥¿ï¼Œä½ éœ€è¦ä½¿ç”¨ ğŸ¤— Tokenizers åº“ä»å¤´å¼€å§‹æ„å»ºå®ƒ.</p>
<p>è¦äº†è§£å¦‚ä½•ä»å¤´å¼€å§‹æ„å»ºæ ‡è®°å™¨ï¼Œæˆ‘ä»¬å¿…é¡»æ·±å…¥äº†è§£ ğŸ¤— Tokenizers åº“å’Œæ ‡è®°åŒ–ç®¡é“ã€‚ æ­¤ç®¡é“éœ€è¦å‡ ä¸ªæ­¥éª¤ï¼š</p>
<ul>
<li>Normalizationï¼šå¯¹åˆå§‹è¾“å…¥å­—ç¬¦ä¸²æ‰§è¡Œæ‰€æœ‰åˆå§‹è½¬æ¢ã€‚ ä¾‹å¦‚ï¼Œå½“æ‚¨éœ€è¦å°å†™æŸäº›æ–‡æœ¬æ—¶ï¼Œå¯èƒ½ä¼šå°†å…¶å‰¥ç¦»ï¼Œç”šè‡³åº”ç”¨ä¸€ç§å¸¸è§çš„ unicode è§„èŒƒåŒ–è¿‡ç¨‹ï¼Œæ‚¨å°†æ·»åŠ ä¸€ä¸ª Normalizerã€‚</li>
<li>Pre-tokenizationï¼šè´Ÿè´£åˆ†å‰²åˆå§‹è¾“å…¥å­—ç¬¦ä¸²ã€‚ è¿™æ˜¯å†³å®šåœ¨ä½•å¤„ä»¥åŠå¦‚ä½•å¯¹åŸå§‹å­—ç¬¦ä¸²è¿›è¡Œé¢„åˆ†æ®µçš„ç»„ä»¶ã€‚ æœ€ç®€å•çš„ä¾‹å­æ˜¯ä½¿ç”¨ç©ºæ ¼è¿›è¡Œåˆ†å‰²ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pre_tokenizers??</span><br><span class="line"></span><br><span class="line">BertPreTokenizer = pre_tokenizers.BertPreTokenizer</span><br><span class="line">ByteLevel = pre_tokenizers.ByteLevel</span><br><span class="line">CharDelimiterSplit = pre_tokenizers.CharDelimiterSplit</span><br><span class="line">Digits = pre_tokenizers.Digits</span><br><span class="line">Metaspace = pre_tokenizers.Metaspace</span><br><span class="line">Punctuation = pre_tokenizers.Punctuation</span><br><span class="line"><span class="type">Sequence</span> = pre_tokenizers.<span class="type">Sequence</span></span><br><span class="line">Split = pre_tokenizers.Split</span><br><span class="line">UnicodeScripts = pre_tokenizers.UnicodeScripts</span><br><span class="line">Whitespace = pre_tokenizers.Whitespace</span><br><span class="line">WhitespaceSplit = pre_tokenizers.WhitespaceSplit</span><br></pre></td></tr></table></figure>
<ul>
<li>modelï¼šå¤„ç†æ‰€æœ‰sub-tokençš„å‘ç°å’Œç”Ÿæˆï¼Œè¿™æ˜¯å¯è®­ç»ƒä¸”çœŸæ­£ä¾èµ–äºæ‚¨çš„è¾“å…¥æ•°æ®çš„éƒ¨åˆ†ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">models??</span><br><span class="line"></span><br><span class="line">BPE = models.BPE</span><br><span class="line">Unigram = models.Unigram</span><br><span class="line">WordLevel = models.WordLevel</span><br><span class="line">WordPiece = models.WordPiece</span><br></pre></td></tr></table></figure>
<ul>
<li>åå¤„ç†Post-Processingï¼šæä¾›ä¸ä¸€äº›åŸºäº Transformers çš„ SoTA æ¨¡å‹å…¼å®¹çš„é«˜çº§æ„å»ºåŠŸèƒ½ã€‚ ä¾‹å¦‚ï¼Œå¯¹äº BERTï¼Œå®ƒä¼šå°†æ ‡è®°åŒ–çš„å¥å­åŒ…è£¹åœ¨ [CLS] å’Œ [SEP] æ ‡è®°å‘¨å›´ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">processors??</span><br><span class="line"></span><br><span class="line">BertProcessing = processors.BertProcessing</span><br><span class="line">ByteLevel = processors.ByteLevel</span><br><span class="line">RobertaProcessing = processors.RobertaProcessing</span><br><span class="line">TemplateProcessing = processors.TemplateProcessing</span><br></pre></td></tr></table></figure>
<ul>
<li>è§£ç Decodingï¼šè´Ÿè´£å°†æ ‡è®°åŒ–çš„è¾“å…¥æ˜ å°„å›åŸå§‹å­—ç¬¦ä¸²ã€‚ é€šå¸¸æ ¹æ®æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„ PreTokenizer æ¥é€‰æ‹©è§£ç å™¨ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoders??</span><br><span class="line"></span><br><span class="line">ByteLevel = decoders.ByteLevel</span><br><span class="line">WordPiece = decoders.WordPiece</span><br><span class="line">Metaspace = decoders.Metaspace</span><br><span class="line">BPEDecoder = decoders.BPEDecoder</span><br></pre></td></tr></table></figure>
<p>å¯¹äºæ¨¡å‹çš„è®­ç»ƒï¼ŒğŸ¤— Tokenizers åº“æä¾›äº†ä¸€ä¸ªæˆ‘ä»¬å°†ä½¿ç”¨çš„ Trainer ç±»ã€‚<br>trainers??</p>
<p>BPE = trainers.BPE<br>Unigram = trainers.Unigram<br>WordLevel = trainers.WordLevel<br>WordPiece = trainers.WordPiece</p>
<p>æ‰€æœ‰è¿™äº›æ„å»ºå—éƒ½å¯ä»¥ç»„åˆèµ·æ¥åˆ›å»ºtokenization pipelinesã€‚ ä¸‹é¢å°†å±•ç¤ºä¸‰ä¸ªå®Œæ•´çš„ç®¡é“ï¼šGPT-2ã€BERT å’Œ T5ï¼ˆå®ƒå°†ä¸ºä½ æä¾› BPEã€WordPiece å’Œ Unigram æ ‡è®°å™¨çš„ç¤ºä¾‹ï¼‰ã€‚</p>
<h5 id="1-2-3-2-WordPiece-model-like-BERT"><a href="#1-2-3-2-WordPiece-model-like-BERT" class="headerlink" title="1.2.3.2 WordPiece model like BERT"></a>1.2.3.2 WordPiece model like BERT</h5><p>åˆ›å»ºä¸€ä¸ª WordPiece æ ‡è®°å™¨ï¼ˆlike BERTï¼‰ï¼š</p>
<ol>
<li>åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç©º WordPiece æ¨¡å‹çš„ Tokenizerï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unl_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ normalizationï¼ˆå¯é€‰ï¼‰</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#å¦‚æœä½ æƒ³è‡ªå®šä¹‰å®ƒï¼Œä½ å¯ä»¥ä½¿ç”¨ç°æœ‰çš„å—å¹¶æŒ‰é¡ºåºç»„åˆå®ƒä»¬ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬å°å†™ï¼Œåº”ç”¨ NFD è§„èŒƒåŒ–å¹¶å»é™¤é‡éŸ³ï¼š</span></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ pre-tokenizerï¼ˆåˆ†è¯ï¼‰</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ç›´æ¥ä½¿ç”¨ BertPreTokenizerï¼Œå®ƒä½¿ç”¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·é¢„å…ˆæ ‡è®°ï¼š</span></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br></pre></td></tr></table></figure>
<p>ä¸ normalizer ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ª Sequence ä¸­ç»„åˆå¤šä¸ª pre-tokenizerã€‚ å¦‚æœæˆ‘ä»¬æƒ³å¿«é€Ÿäº†è§£å®ƒå¦‚ä½•é¢„å¤„ç†è¾“å…¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ pre_tokenize_str æ–¹æ³•ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)),</span><br><span class="line"> (<span class="string">&#x27;is&#x27;</span>, (<span class="number">5</span>, <span class="number">7</span>)),</span><br><span class="line"> (<span class="string">&#x27;an&#x27;</span>, (<span class="number">8</span>, <span class="number">10</span>)),</span><br><span class="line"> (<span class="string">&#x27;example&#x27;</span>, (<span class="number">11</span>, <span class="number">18</span>)),</span><br><span class="line"> (<span class="string">&#x27;!&#x27;</span>, (<span class="number">18</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>è¯·æ³¨æ„ï¼Œ==pre-tokenizer ä¸ä»…å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ï¼Œè¿˜ä¿ç•™äº†åç§»é‡==ï¼Œå³åŸå§‹æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„å¼€å¤´å’Œå¼€å¤´ã€‚ è¿™å°†ä½¿æœ€ç»ˆçš„åˆ†è¯å™¨==èƒ½å¤Ÿå°†æ¯ä¸ªæ ‡è®°ä¸å®ƒæ¥è‡ªçš„æ–‡æœ¬éƒ¨åˆ†è¿›è¡ŒåŒ¹é…ï¼ˆæˆ‘ä»¬ç”¨äºé—®ç­”æˆ–æ ‡è®°åˆ†ç±»ä»»åŠ¡çš„åŠŸèƒ½ï¼‰==ã€‚</p>
<ol>
<li>æ„å»º post-processorï¼Œä¼ é€’special tokensç»™trainerã€‚<h1 id="ç›´æ¥ä½¿ç”¨WordPieceTrainer"><a href="#ç›´æ¥ä½¿ç”¨WordPieceTrainer" class="headerlink" title="ç›´æ¥ä½¿ç”¨WordPieceTrainer"></a>ç›´æ¥ä½¿ç”¨WordPieceTrainer</h1></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br></pre></td></tr></table></figure>
<ol>
<li>æ„å»ºæ•°æ®é›†ï¼ˆtext filesï¼‰æˆ–æ‰¹å¤„ç†å·¥å…·ï¼ˆbatches of textsï¼‰ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>åˆ†è¯å™¨å·²ç»è®­ç»ƒå®Œæ¯•ï¼Œå®šä¹‰åå¤„ç†å™¨ï¼šå¼€å¤´æ·»åŠ  CLS æ ‡è®°å¹¶åœ¨æœ«å°¾æ·»åŠ  SEP æ ‡è®°ï¼ˆå¯¹äºå•ä¸ªå¥å­ï¼‰æˆ–å‡ ä¸ª SEP æ ‡è®°ï¼ˆå¯¹äºå¥å­å¯¹ï¼‰ã€‚ å¯ä»¥ä½¿ç”¨ <a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing">TemplateProcessing</a> æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è·å–CLS å’ŒSEP çš„token id</span></span><br><span class="line"></span><br><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ä½¿ç”¨TemplateProcessingæ„å»ºåå¤„ç†å™¨</span></span><br><span class="line"><span class="comment">#åœ¨æ¨¡æ¿ä¸­æŒ‡æ˜å¦‚ä½•ç”¨ä¸€ä¸ªå¥å­ï¼ˆ$Aï¼‰æˆ–ä¸¤ä¸ªå¥å­ï¼ˆ$A å’Œ $Bï¼‰ç»„ç»‡ç‰¹æ®Šæ ‡è®°ã€‚</span></span><br><span class="line"><span class="comment">#åè·Ÿä¸€ä¸ªæ•°å­—è¡¨ç¤ºè¦èµ‹äºˆæ¯ä¸ªéƒ¨åˆ†çš„token type IDï¼Œä¹Ÿå°±æ˜¯å“ªéƒ¨åˆ†æ˜¯ç¬¬ä¸€å¥ï¼Œå“ªéƒ¨åˆ†æ˜¯ç¬¬äºŒå¥ã€‚</span></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢ç¼–ç ä¸€ä¸ªå¥å­çœ‹çœ‹ç»“æœï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;This is one sentence.&quot;</span>, <span class="string">&quot;With this one we have a pair.&quot;</span>)</span><br><span class="line">encoding.tokens</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;one&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;sentence&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;[SEP]&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;with&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;one&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;we&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;have&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;pair&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line"></span><br><span class="line">encoding.type_ids</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ol>
<li>è§£ç å™¨ï¼šæˆ‘ä»¬ä½¿ç”¨ WordPiece è§£ç å™¨å¹¶æŒ‡ç¤ºç‰¹æ®Šå‰ç¼€ ##ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.decoder = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨æˆ‘ä»¬çš„tokenizerå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¿…é¡»å°†å®ƒæ”¾åœ¨ä¸æˆ‘ä»¬è¦ä½¿ç”¨çš„æ¨¡å‹ç›¸å¯¹åº”çš„æ ‡è®°å™¨ fast ç±»ä¸­ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ª BertTokenizerFastï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤åˆ†è¯å™¨ç”¨ä½œæ™®é€šçš„ Transformers åˆ†è¯å™¨ï¼Œå¹¶ä½¿ç”¨ save_pretrained æˆ– push_to_hub æ–¹æ³•ã€‚</p>
</li>
<li><p>å¦‚æœæ‚¨æ­£åœ¨æ„å»ºçš„åˆ†è¯å™¨ä¸ Transformers ä¸­çš„ä»»ä½•ç±»éƒ½ä¸åŒ¹é…(åˆ†è¯å™¨éå¸¸ç‰¹æ®Š)ï¼Œæ‚¨å¯ä»¥å°†å®ƒåŒ…è£…åœ¨ PreTrainedTokenizerFast ä¸­ã€‚</p>
</li>
</ul>
<h5 id="1-2-3-3-BPE-model-like-GPT-2"><a href="#1-2-3-3-BPE-model-like-GPT-2" class="headerlink" title="1.2.3.3 BPE model like GPT-2"></a>1.2.3.3 BPE model like GPT-2</h5><p>ä¸‹é¢çœ‹çœ‹å¦‚ä½•åˆ›å»ºä¸€ä¸ª BPE æ ‡è®°å™¨ï¼ˆlike GPT-2 tokenizerï¼‰ï¼š</p>
<ol>
<li>åˆ›å»ºä¸€ä¸ªå¸¦æœ‰åˆå§‹ BPE modelçš„ Tokenizerï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ å¯é€‰normalizationï¼ˆGPT2ä¸ä½¿ç”¨ï¼‰</li>
<li>æŒ‡å®špre-tokenizerï¼ˆGPT2ä½¿ç”¨byte level pre-tokenizerï¼‰</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>è°ƒç”¨ pre_tokenize_str æ–¹æ³•ï¼Œå¿«é€Ÿäº†è§£å®ƒå¦‚ä½•é¢„å¤„ç†è¾“å…¥ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ä is&#x27;</span>, (<span class="number">4</span>, <span class="number">7</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ä an&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)),</span><br><span class="line"> (<span class="string">&#x27;Ä example&#x27;</span>, (<span class="number">10</span>, <span class="number">18</span>)),</span><br><span class="line"> (<span class="string">&#x27;!&#x27;</span>, (<span class="number">18</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯¹å‰ç¼€ç©ºæ ¼ä½¿ç”¨ GPT-2çš„é»˜è®¤å€¼ï¼Œæ‰€ä»¥é™¤äº†ç¬¬ä¸€ä¸ªå•è¯ä¹‹å¤–ï¼Œæ¯ä¸ªå•è¯çš„å¼€å¤´éƒ½æ·»åŠ äº†ä¸€ä¸ªé¦–å­—æ¯â€œÄ â€ã€‚</p>
<ol>
<li>ä½¿ç”¨ BpeTrainerè®­ç»ƒåˆ†è¯å™¨ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ åå¤„ç†å’Œè§£ç å™¨ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br></pre></td></tr></table></figure>
<ol>
<li>å°†æ­¤åˆ†è¯å™¨åŒ…è£…åœ¨ Transformers tokenizer objectä¸­ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2TokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<h5 id="1-2-3-4-Unigram-model-like-Albert"><a href="#1-2-3-4-Unigram-model-like-Albert" class="headerlink" title="1.2.3.4 Unigram model like Albert"></a>1.2.3.4 Unigram model like Albert</h5><p>ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ›å»ºä¸€ä¸ª Unigram åˆ†è¯å™¨(ç±»ä¼¼ T5 çš„åˆ†è¯å™¨ï¼‰ï¼š</p>
<ol>
<li>åˆ›å»º åˆå§‹Unigram æ¨¡å‹çš„ Tokenizerï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ normalization å’Œpre-tokenizerï¼ˆMetaspace pre-tokenizerï¼šå®ƒç”¨ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ï¼ˆé»˜è®¤ä¸ºâ€œâ–â€ ï¼‰æ›¿æ¢æ‰€æœ‰ç©ºæ ¼ï¼Œç„¶ååœ¨è¯¥å­—ç¬¦ä¸Šæ‹†åˆ†ã€‚ï¼‰</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Lowercase()]</span><br><span class="line">)</span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br></pre></td></tr></table></figure>
<p>è°ƒç”¨ pre_tokenize_str æ–¹æ³•ï¼Œå¿«é€Ÿäº†è§£å®ƒå¦‚ä½•é¢„å¤„ç†è¾“å…¥ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line">[(<span class="string">&#x27;â–This&#x27;</span>, (<span class="number">0</span>, <span class="number">4</span>)), (<span class="string">&#x27;â–is&#x27;</span>, (<span class="number">4</span>, <span class="number">7</span>)), (<span class="string">&#x27;â–an&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;â–example!&#x27;</span>, (<span class="number">10</span>, <span class="number">19</span>))]</span><br></pre></td></tr></table></figure>
<p>æ¯ä¸ªå•è¯éƒ½åœ¨å¼€å¤´æ·»åŠ äº†ä¸€ä¸ªé¦–å­—æ¯â€œ â–â€ï¼Œè¿™æ˜¯ç”± sentencepieceå®Œæˆçš„ã€‚</p>
<ol>
<li>ä½¿ç”¨ UnigramTrainerè®­ç»ƒåˆ†è¯å™¨ï¼Œå¹¶è®¾ç½®unknown tokenã€‚</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer = trainers.UnigramTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>], unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<ol>
<li>æ·»åŠ åå¤„ç†å’Œè§£ç å™¨ï¼ˆMetaspaceï¼Œç±»ä¼¼pre-tokenizerï¼‰</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">tokenizer.decoder = decoders.Metaspace()</span><br></pre></td></tr></table></figure>
<ol>
<li>å°†æ­¤åˆ†è¯å™¨åŒ…è£…åœ¨ Transformers tokenizer objectä¸­ï¼š</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AlbertTokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨å¯ä»¥ç”¨æ–°çš„tokenizerè®­ç»ƒæ¨¡å‹äº†ã€‚</p>
<ul>
<li>ä½¿ç”¨æ–°çš„åˆ†æå™¨åœ¨notebookä¸Šä»å¤´è®­ç»ƒæ¨¡å‹</li>
<li>åœ¨l<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling">anguage modeling scripts</a> ä¸Šä½¿ç”¨tokenizer_nameå‚æ•°æ¥ä»å¤´è®­ç»ƒæ¨¡å‹ã€‚<h2 id="äºŒã€HFæ¨¡å‹é¢„è®­ç»ƒæ–¹å¼"><a href="#äºŒã€HFæ¨¡å‹é¢„è®­ç»ƒæ–¹å¼" class="headerlink" title="äºŒã€HFæ¨¡å‹é¢„è®­ç»ƒæ–¹å¼"></a>äºŒã€HFæ¨¡å‹é¢„è®­ç»ƒæ–¹å¼</h2>ä½¿ç”¨HFä¸»é¡µçš„tokenizerå’ŒMLMåŒ…ï¼Œè¿›è¡Œtrainerè®­ç»ƒ<h3 id="1-åŠ è½½æ•°æ®é›†ï¼š"><a href="#1-åŠ è½½æ•°æ®é›†ï¼š" class="headerlink" title="1.åŠ è½½æ•°æ®é›†ï¼š"></a>1.åŠ è½½æ•°æ®é›†ï¼š</h3>é€‰æ‹©å¤šè¯­è¨€å¤šè¯­æ–™æ•°æ®é›†<a target="_blank" rel="noopener" href="https://traces1.inria.fr/oscar/">OSCAR corpus</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in this notebook we&#x27;ll only get one of the files (the Oscar one) for the sake of simplicity and performance</span></span><br><span class="line">!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt</span><br></pre></td></tr></table></figure>
<h3 id="2-è®­ç»ƒtokenizer"><a href="#2-è®­ç»ƒtokenizer" class="headerlink" title="2.è®­ç»ƒtokenizer"></a>2.è®­ç»ƒtokenizer</h3>é€‰æ‹©å­—èŠ‚çº§åˆ«byte-level BPEåˆ†è¯å™¨ï¼ˆç±»ä¼¼GPT2ä½¿ç”¨çš„ï¼‰ï¼Œæ¯”BERTçš„WordPieceï¼ˆå­—ç¬¦çº§åˆ«BPEåˆ†è¯å™¨ï¼Œåˆ‡åˆ†æˆå­è¯ï¼‰å¥½å¤„æ˜¯å‡ ä¹ä¸ä¼šæœ‰æœªç™»å½•è¯â€\<unk> tokensâ€ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å®‰è£…transformerså’Œtokenizers</span></span><br><span class="line">!pip install git+https://github.com/huggingface/transformers</span><br><span class="line">!pip <span class="built_in">list</span> | grep -E <span class="string">&#x27;transformers|tokenizers&#x27;</span></span><br><span class="line"><span class="comment"># transformers version at notebook update --- 2.11.0</span></span><br><span class="line"><span class="comment"># tokenizers version at notebook update --- 0.8.0rc1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time </span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> ByteLevelBPETokenizer</span><br><span class="line">paths = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> Path(<span class="string">&quot;.&quot;</span>).glob(<span class="string">&quot;**/*.txt&quot;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizeråˆå§‹åŒ–</span></span><br><span class="line">tokenizer = ByteLevelBPETokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Customize training</span></span><br><span class="line">tokenizer.train(files=paths, vocab_size=<span class="number">52_000</span>, min_frequency=<span class="number">2</span>, special_tokens=[</span><br><span class="line">    <span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="2-2-åˆ†è¯å™¨çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š"><a href="#2-2-åˆ†è¯å™¨çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š" class="headerlink" title="2.2 åˆ†è¯å™¨çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š"></a>2.2 åˆ†è¯å™¨çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#BPEçš„åˆ†è¯å™¨</span></span><br><span class="line">classtokenizers.trainers.BpeTrainer(self, vocab_size=<span class="number">30000</span>, min_frequency=<span class="number">0</span>, show_progress=<span class="literal">True</span>, special_tokens=[], limit_alphabet=<span class="literal">None</span>, initial_alphabet=[], continuing_subword_prefix=<span class="literal">None</span>, end_of_word_suffix=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>vocab_size (int, optional) â€“ æœ€ç»ˆè¯æ±‡çš„å¤§å°ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ‡è®°å’Œå­—æ¯è¡¨ã€‚</li>
<li>min_frequency (int, optional) â€“ ä¸ºäº†åˆå¹¶ï¼Œä¸€å¯¹åº”è¯¥å…·æœ‰çš„æœ€å°é¢‘ç‡ã€‚</li>
<li>show_progress (bool, optional) â€“ è®­ç»ƒæ—¶æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ã€‚</li>
<li>special_tokens (List[Union[str,AddedToken]], optional) â€“ æ¨¡å‹åº”è¯¥çŸ¥é“çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚</li>
<li>limit_alphabet (int, optional) â€“ å­—æ¯è¡¨ä¸­ä¿ç•™çš„æœ€å¤§ä¸åŒå­—ç¬¦æ•°ã€‚</li>
<li>initial_alphabet (List[str], optional) â€“ åŒ…å«åœ¨åˆå§‹å­—æ¯è¡¨ä¸­çš„å­—ç¬¦åˆ—è¡¨ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ²¡æœ‰å‡ºç°ã€‚ å¦‚æœå­—ç¬¦ä¸²åŒ…å«å¤šä¸ªå­—ç¬¦ï¼Œåˆ™ä»…ä¿ç•™ç¬¬ä¸€ä¸ªå­—ç¬¦ã€‚</li>
<li>continue_subword_prefix (str, optional) â€” ç”¨äºæ¯ä¸ªä¸æ˜¯è¯å¼€å¤´çš„å­è¯çš„å‰ç¼€ã€‚</li>
<li>end_of_word_suffix (str, optional) â€“ ç”¨äºæ¯ä¸ªè¯å°¾çš„å­è¯çš„åç¼€ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#WordPieceåˆ†è¯å™¨ï¼Œå‚æ•°å’Œä¸Šä¸€ä¸ªç›¸åŒ</span></span><br><span class="line">classtokenizers.trainers.WordPieceTrainer(self, vocab_size=<span class="number">30000</span>, min_frequency=<span class="number">0</span>, show_progress=<span class="literal">True</span>, special_tokens=[], limit_alphabet=<span class="literal">None</span>, initial_alphabet=[], continuing_subword_prefix=<span class="string">&#x27;##&#x27;</span>, end_of_word_suffix=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>vocab_size (int, optional) â€“ æœ€ç»ˆè¯æ±‡çš„å¤§å°ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ‡è®°å’Œå­—æ¯è¡¨ã€‚</li>
<li>min_frequency (int, optional) â€“ ä¸ºäº†åˆå¹¶ï¼Œä¸€å¯¹åº”è¯¥å…·æœ‰çš„æœ€å°é¢‘ç‡ã€‚</li>
<li>show_progress (bool, optional) â€“ è®­ç»ƒæ—¶æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ã€‚</li>
<li>special_tokens (List[Union[str,AddedToken]], optional) â€“ æ¨¡å‹åº”è¯¥çŸ¥é“çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚</li>
<li>limit_alphabet (int, optional) â€“ å­—æ¯è¡¨ä¸­ä¿ç•™çš„æœ€å¤§ä¸åŒå­—ç¬¦æ•°ã€‚</li>
<li>initial_alphabet (List[str], optional) â€“ åŒ…å«åœ¨åˆå§‹å­—æ¯è¡¨ä¸­çš„å­—ç¬¦åˆ—è¡¨ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ²¡æœ‰å‡ºç°ã€‚ å¦‚æœå­—ç¬¦ä¸²åŒ…å«å¤šä¸ªå­—ç¬¦ï¼Œåˆ™ä»…ä¿ç•™ç¬¬ä¸€ä¸ªå­—ç¬¦ã€‚</li>
<li>continue_subword_prefix (str, optional) â€” ç”¨äºæ¯ä¸ªä¸æ˜¯è¯å¼€å¤´çš„å­è¯çš„å‰ç¼€ã€‚</li>
<li>end_of_word_suffix (str, optional) â€“ ç”¨äºæ¯ä¸ªè¯å°¾çš„å­è¯çš„åç¼€ã€‚</li>
</ul>
<h4 id="2-3-åˆ†è¯å™¨ä¿å­˜å’ŒåŠ è½½"><a href="#2-3-åˆ†è¯å™¨ä¿å­˜å’ŒåŠ è½½" class="headerlink" title="2.3 åˆ†è¯å™¨ä¿å­˜å’ŒåŠ è½½"></a>2.3 åˆ†è¯å™¨ä¿å­˜å’ŒåŠ è½½</h4><p>å°†è®­ç»ƒå¥½çš„åˆ†è¯å™¨ä¿å­˜åœ¨EsperBERToæ–‡ä»¶å¤¹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!mkdir EsperBERTo</span><br><span class="line">tokenizer.save_model(<span class="string">&quot;EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>æœ€ç»ˆå¾—åˆ°ä¸¤ä¸ªåˆ†è¯å™¨æ–‡ä»¶ï¼š</p>
<ul>
<li>EsperBERTo/vocab.jsonï¼švocab.jsonï¼ŒæŒ‰é¢‘ç‡æ’åˆ—çš„å¸¸è§tokençš„åˆ—è¡¨</li>
<li>EsperBERTo/merges.txtâ€™ï¼š merges.txtï¼Œmergesåˆ—è¡¨</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123; <span class="string">&quot;&lt;s&gt;&quot;</span>: <span class="number">0</span>,<span class="string">&quot;&lt;pad&gt;&quot;</span>: <span class="number">1</span>,<span class="string">&quot;&lt;/s&gt;&quot;</span>: <span class="number">2</span>,<span class="string">&quot;&lt;unk&gt;&quot;</span>: <span class="number">3</span>, <span class="string">&quot;&lt;mask&gt;&quot;</span>: <span class="number">4</span>,<span class="string">&quot;!&quot;</span>: <span class="number">5</span>,<span class="string">&quot;\&quot;&quot;</span>: <span class="number">6</span>,<span class="string">&quot;#&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;$&quot;</span>: <span class="number">8</span>,<span class="string">&quot;%&quot;</span>: <span class="number">9</span>,<span class="string">&quot;&amp;&quot;</span>: <span class="number">10</span>,<span class="string">&quot;&#x27;&quot;</span>: <span class="number">11</span>,<span class="string">&quot;(&quot;</span>: <span class="number">12</span>,<span class="string">&quot;)&quot;</span>: <span class="number">13</span>, <span class="comment"># ...&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># merges.txt</span></span><br><span class="line">l a</span><br><span class="line">Ä  k</span><br><span class="line">o n</span><br><span class="line">Ä  la</span><br><span class="line">t a</span><br><span class="line">Ä  e</span><br><span class="line">Ä  d</span><br><span class="line">Ä  p</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>tokenizeré’ˆå¯¹Esperantoè¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ›´å¤šå•è¯æ˜¯a single, unsplit tokenè¡¨ç¤ºã€‚==æˆ‘ä»¬è¿˜ä»¥æ›´æœ‰æ•ˆçš„æ–¹å¼è¡¨ç¤ºåºåˆ—ã€‚ åœ¨è¿™ä¸ªè¯­æ–™åº“ä¸­ï¼Œç¼–ç åºåˆ—çš„å¹³å‡é•¿åº¦æ¯”ä½¿ç”¨é¢„è®­ç»ƒçš„ GPT-2 æ ‡è®°å™¨æ—¶å°çº¦ 30%ã€‚==</p>
<p>åŠ è½½åˆ†è¯å™¨ï¼Œå¤„ç† RoBERTa ç‰¹æ®Šæ ‡è®°ï¼š<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers.implementations <span class="keyword">import</span> ByteLevelBPETokenizer</span><br><span class="line"><span class="keyword">from</span> tokenizers.processors <span class="keyword">import</span> BertProcessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = ByteLevelBPETokenizer(</span><br><span class="line">    <span class="string">&quot;./EsperBERTo/vocab.json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;./EsperBERTo/merges.txt&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer._tokenizer.post_processor = BertProcessing(</span><br><span class="line">    (<span class="string">&quot;&lt;/s&gt;&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;&lt;/s&gt;&quot;</span>)),</span><br><span class="line">    (<span class="string">&quot;&lt;s&gt;&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;&lt;s&gt;&quot;</span>)),</span><br><span class="line">)</span><br><span class="line">tokenizer.enable_truncation(max_length=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<p>token_to_idï¼šå°†ç»™å®šçš„tokenè½¬æ¢ä¸ºå…¶å¯¹åº”çš„ id<br>BertProcessingå‚æ•°ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classtokenizers.processors.BertProcessing(self, sep, cls)</span><br></pre></td></tr></table></figure>
<p>è¿™ä¸ªåå¤„ç†å™¨è´Ÿè´£æ·»åŠ  Bert æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ï¼š</p>
<ul>
<li>sep (Tuple[str, int]) â€“ å¸¦æœ‰ SEP ä»¤ç‰Œçš„å­—ç¬¦ä¸²è¡¨ç¤ºåŠå…¶ id çš„å…ƒç»„</li>
<li>cls (Tuple[str, int]) â€“ ä¸€ä¸ªå¸¦æœ‰ CLS æ ‡è®°çš„å­—ç¬¦ä¸²è¡¨ç¤ºçš„å…ƒç»„ï¼Œä»¥åŠå®ƒçš„ id</li>
</ul>
<p>æµ‹è¯•æ•ˆæœï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&quot;Mi estas Julien.&quot;</span>)</span><br><span class="line">Encoding(num_tokens=<span class="number">7</span>, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&quot;Mi estas Julien.&quot;</span>).tokens</span><br><span class="line">[<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;Mi&#x27;</span>, <span class="string">&#x27;Ä estas&#x27;</span>, <span class="string">&#x27;Ä Juli&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;&lt;/s&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="3-ä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹"><a href="#3-ä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹" class="headerlink" title="3.ä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹"></a>3.ä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹</h3><p>å‚è€ƒ<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py">run_language_modeling.py</a> æ–‡ä»¶ã€‚ç›´æ¥è®¾ç½® <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py">Trainer</a> é€‰æ‹©è®­ç»ƒæ–¹æ³•ã€‚ä¸‹é¢ä»¥è®­ç»ƒç±»ä¼¼ <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a> çš„æ¨¡å‹æ¥ä¸¾ä¾‹ï¼šï¼ˆç›¸æ¯”berté‡‡ç”¨åŠ¨æ€æ©ç ã€èˆå¼ƒNSPä»»åŠ¡ï¼Œä»¥åŠæ›´å¤§çš„è®­ç»ƒï¼‰</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#å®šä¹‰æ¨¡å‹å‚æ•°</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaConfig</span><br><span class="line"></span><br><span class="line">config = RobertaConfig(</span><br><span class="line">    vocab_size=<span class="number">52_000</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">514</span>,</span><br><span class="line">    num_attention_heads=<span class="number">12</span>,</span><br><span class="line">    num_hidden_layers=<span class="number">6</span>,</span><br><span class="line">    type_vocab_size=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#é‡æ–°åˆ›å»ºtokenizer</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaTokenizerFast</span><br><span class="line">tokenizer = RobertaTokenizerFast.from_pretrained(<span class="string">&quot;./EsperBERTo&quot;</span>, max_len=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-2-åˆå§‹åŒ–æ¨¡å‹"><a href="#3-2-åˆå§‹åŒ–æ¨¡å‹" class="headerlink" title="3.2 åˆå§‹åŒ–æ¨¡å‹"></a>3.2 åˆå§‹åŒ–æ¨¡å‹</h4><p>ç”±äºæˆ‘ä»¬æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå› æ­¤æˆ‘ä»¬ä»…ä»é…ç½®è¿›è¡Œåˆå§‹åŒ–ï¼Œè€Œä¸æ˜¯ä»ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥ç‚¹è¿›è¡Œåˆå§‹åŒ–ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> RobertaForMaskedLM</span><br><span class="line">model = RobertaForMaskedLM(config=config)</span><br><span class="line"></span><br><span class="line">model.num_parameters()</span><br><span class="line"></span><br><span class="line"><span class="number">84095008</span><span class="comment"># =&gt; 84 million parameters</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-åˆ›å»ºè®­ç»ƒé›†"><a href="#3-3-åˆ›å»ºè®­ç»ƒé›†" class="headerlink" title="3.3 åˆ›å»ºè®­ç»ƒé›†"></a>3.3 åˆ›å»ºè®­ç»ƒé›†</h4><p>ç”±äºåªæœ‰ä¸€ä¸ªtextæ–‡ä»¶ï¼Œä¸éœ€è¦è‡ªå®šä¹‰æ•°æ®é›†ã€‚ç›´æ¥ä½¿ç”¨LineByLineDatasetåŠ è½½ä¹‹åç”¨tokenizeré¢„å¤„ç†ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> LineByLineTextDataset</span><br><span class="line"></span><br><span class="line">dataset = LineByLineTextDataset(</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    file_path=<span class="string">&quot;./oscar.eo.txt&quot;</span>,</span><br><span class="line">    block_size=<span class="number">128</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CPU times: user 4<span class="built_in">min</span> 54s, sys: <span class="number">2.98</span> s, total: 4<span class="built_in">min</span> 57s</span><br><span class="line">Wall time: 1<span class="built_in">min</span> 37s</span><br></pre></td></tr></table></figure>
<p>å®šä¹‰data_collatorï¼šå¸®åŠ©æˆ‘ä»¬å°†æ•°æ®é›†æ ·æœ¬è¿›è¡Œæ‰¹å¤„ç†çš„æ•°æ®æ•´ç†å™¨ã€‚ å¦‚æœè¾“å…¥çš„é•¿åº¦ä¸åŒï¼Œåˆ™è¾“å…¥ä¼šåŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForLanguageModeling</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForLanguageModeling(</span><br><span class="line">    tokenizer=tokenizer, mlm=<span class="literal">True</span>, mlm_probability=<span class="number">0.15</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">transformers</span>.<span class="title">data</span>.<span class="title">data_collator</span>.<span class="title">DataCollatorForLanguageModeling</span>(<span class="params">tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, mlm: <span class="built_in">bool</span> = <span class="literal">True</span>, mlm_probability: <span class="built_in">float</span> = <span class="number">0.15</span>, pad_to_multiple_of: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>, tf_experimental_compile: <span class="built_in">bool</span> = <span class="literal">False</span>, return_tensors: <span class="built_in">str</span> = <span class="string">&#x27;pt&#x27;</span></span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>tokenizerï¼ˆPreTrainedTokenizer æˆ– PreTrainedTokenizerFastï¼‰â€”â€”ç”¨äºç¼–ç æ•°æ®çš„æ ‡è®°å™¨ã€‚</li>
<li>mlm (bool, optional, defaults to True) â€“ æ˜¯å¦ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡ã€‚ å¦‚æœè®¾ç½®ä¸º Falseï¼Œåˆ™æ ‡ç­¾ä¸å¿½ç•¥å¡«å……æ ‡è®°çš„è¾“å…¥ç›¸åŒï¼ˆé€šè¿‡å°†å®ƒä»¬è®¾ç½®ä¸º -100ï¼‰ã€‚ å¦åˆ™ï¼Œnon-masked tokens çš„labelå’Œmasked tokençš„é¢„æµ‹å€¼ä¸º -100ã€‚ï¼ˆIf set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.ï¼‰</li>
<li>mlm_probabilityï¼ˆæµ®ç‚¹æ•°ï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º 0.15ï¼‰â€“ å½“ mlm è®¾ç½®ä¸º True æ—¶ï¼ˆéšæœºï¼‰å±è”½è¾“å…¥ä¸­çš„æ ‡è®°çš„æ¦‚ç‡ã€‚</li>
<li>pad_to_multiple_of (int, optional) â€“ å¦‚æœè®¾ç½®ï¼Œåˆ™å°†åºåˆ—å¡«å……ä¸ºæ‰€æä¾›å€¼çš„å€æ•°ã€‚</li>
</ul>
<h3 id="3-4-åˆå§‹åŒ–-Trainerå¹¶è®­ç»ƒ"><a href="#3-4-åˆå§‹åŒ–-Trainerå¹¶è®­ç»ƒ" class="headerlink" title="3.4 åˆå§‹åŒ– Trainerå¹¶è®­ç»ƒ"></a>3.4 åˆå§‹åŒ– Trainerå¹¶è®­ç»ƒ</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./EsperBERTo&quot;</span>,</span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,</span><br><span class="line">    per_gpu_train_batch_size=<span class="number">64</span>,</span><br><span class="line">    save_steps=<span class="number">10_000</span>,</span><br><span class="line">    save_total_limit=<span class="number">2</span>,</span><br><span class="line">    prediction_loss_only=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>å¼€å§‹è®­ç»ƒ<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%time</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line">CPU times: user 1h 43<span class="built_in">min</span> 36s, sys: 1h 3<span class="built_in">min</span> 28s, total: 2h 47<span class="built_in">min</span> 4s</span><br><span class="line">Wall time: 2h 46<span class="built_in">min</span> 46s</span><br><span class="line">TrainOutput(global_step=<span class="number">15228</span>, training_loss=<span class="number">5.762423221226405</span>)</span><br></pre></td></tr></table></figure><br>ä¿å­˜æ¨¡å‹</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.save_model(<span class="string">&quot;./EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹"><a href="#5-æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹" class="headerlink" title="5. æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹"></a>5. æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹</h3><p>é™¤äº†æŸ¥çœ‹è®­ç»ƒå’Œè¯„ä¼°æŸå¤±ä¸‹é™ä¹‹å¤–ï¼Œå¯ä»¥é€šè¿‡FillMaskPipelineåŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">fill_mask = pipeline(<span class="string">&quot;fill-mask&quot;</span>,model=<span class="string">&quot;./EsperBERTo&quot;</span>,tokenizer=<span class="string">&quot;./EsperBERTo&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The sun &lt;mask&gt;.</span></span><br><span class="line"><span class="comment"># =&gt;</span></span><br><span class="line"></span><br><span class="line">fill_mask(<span class="string">&quot;La suno &lt;mask&gt;.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.02119220793247223</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno estas.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">316</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.012403824366629124</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno situas.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">2340</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.011061107739806175</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno estis.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">394</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.008284995332360268</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno de.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">274</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.006471084896475077</span>,</span><br><span class="line">  <span class="string">&#x27;sequence&#x27;</span>: <span class="string">&#x27;&lt;s&gt; La suno akvo.&lt;/s&gt;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;token&#x27;</span>: <span class="number">1833</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>æœ€åï¼Œå½“ä½ æœ‰ä¸€ä¸ªä¸é”™çš„æ¨¡å‹æ—¶ï¼Œè¯·è€ƒè™‘ä¸ç¤¾åŒºåˆ†äº«ï¼š</p>
<p>ä½¿ç”¨ CLI ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼štransformers-cli upload<br>å†™ä¸€ä¸ª README.md æ¨¡å‹å¡å¹¶å°†å…¶æ·»åŠ åˆ° model_cards/ ä¸‹çš„å­˜å‚¨åº“ä¸­ã€‚ ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨çš„æ¨¡å‹å¡åº”åŒ…æ‹¬ï¼š</p>
<ul>
<li>æ¨¡å‹æè¿°</li>
<li>è®­ç»ƒå‚æ•°ï¼ˆæ•°æ®é›†ã€é¢„å¤„ç†ã€è¶…å‚æ•°ï¼‰</li>
<li>è¯„ä¼°ç»“æœ</li>
<li>é¢„æœŸç”¨é€”å’Œé™åˆ¶</li>
<li>å…¶å®ƒæœ‰ç”¨ä¿¡æ¯ ğŸ¤“</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/10/07/å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»/task3ï¼šå•ä¸ªbertæ¨¡å‹åˆ†æ•°0.961/">https://zhxnlp.github.io/2021/10/07/å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»/task3ï¼šå•ä¸ªbertæ¨¡å‹åˆ†æ•°0.961/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://zhxnlp.github.io">zhxnlpã®Blog</a>ï¼</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">æ–‡æœ¬åˆ†ç±»</a><a class="post-meta__tags" href="/tags/transformers/">transformers</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/10/08/huggingface/hugging%20face%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94datasets%E3%80%81optimizer/"><i class="fa fa-chevron-left">  </i><span>Hugging Faceå®˜æ–¹æ–‡æ¡£â€”â€”datasetsã€optimizer</span></a></div><div class="next-post pull-right"><a href="/2021/10/05/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/task2%20%EF%BC%9Afasttext/"><span>å¤©æ± -æ–°é—»æ–‡æœ¬åˆ†ç±»task2ï¼šfasttextæ¨¡å‹</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By zhxnlp</div><div class="framework-info"><span>é©±åŠ¨ - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>ä¸»é¢˜ - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>