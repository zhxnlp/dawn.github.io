<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="张贤：textcnn做天池-新闻文本分类"><meta name="keywords" content="nlp,文本分类"><meta name="author" content="zhxnlp"><meta name="copyright" content="zhxnlp"><title>张贤：textcnn做天池-新闻文本分类 | zhxnlpのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JU6VFRRZVF","apiKey":"ca17f8e20c4dc91dfe1214d03173a8be","indexName":"github-io","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="zhxnlpのBlog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%86%99%E7%AF%87%E6%96%87%E7%AB%A0"><span class="toc-number">2.</span> <span class="toc-text">1. 为什么写篇文章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Datawhale-%E6%8F%90%E4%BE%9B%E7%9A%84%E4%BB%A3%E7%A0%81%E6%9C%89%E5%93%AA%E4%BA%9B%E9%9C%80%E8%A6%81%E6%94%B9%E8%BF%9B%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 Datawhale 提供的代码有哪些需要改进？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%88%91%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88%E6%94%B9%E8%BF%9B%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 我做了什么改进？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">2. 数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E6%8B%86%E5%88%86%E4%B8%BA-10-%E4%BB%BD"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 数据拆分为 10 份</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%AE%9A%E4%B9%89%E5%B9%B6%E5%88%9B%E5%BB%BA-Vacab"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 定义并创建 Vacab</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">3. 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%8A%8A%E6%96%87%E7%AB%A0%E5%88%86%E5%89%B2%E4%B8%BA%E5%8F%A5%E5%AD%90"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 把文章分割为句子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E7%94%9F%E6%88%90%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 生成训练数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E7%BD%91%E7%BB%9C%E9%83%A8%E5%88%86"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 网络部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-WordCNNEncoder"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.3.1 WordCNNEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Embedding"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">1. Embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-CNN"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">2. CNN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-shape-%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.3.2 shape 转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-SentEncoder"><span class="toc-number">4.3.3.</span> <span class="toc-text">3.3.3 SentEncoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-Attention"><span class="toc-number">4.3.4.</span> <span class="toc-text">3.3.4 Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-5-FC"><span class="toc-number">4.3.5.</span> <span class="toc-text">3.3.5 FC</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-%E6%B3%A8%E9%87%8A"><span class="toc-number">5.</span> <span class="toc-text">4. 完整代码+注释</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E6%8A%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%88%90-10-%E4%BB%BD"><span class="toc-number">5.1.1.</span> <span class="toc-text">4.1.1 把数据分成  10 份</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-%E6%8B%86%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%8C%E8%AF%BB%E5%8F%96%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">5.1.2.</span> <span class="toc-text">4.1.2 拆分训练集、验证集，读取测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3%E5%88%9B%E5%BB%BA-Vocab"><span class="toc-number">5.1.3.</span> <span class="toc-text">4.1.3创建 Vocab</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E5%AE%9A%E4%B9%89-Attention"><span class="toc-number">5.2.1.</span> <span class="toc-text">4.2.1 定义 Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E5%AE%9A%E4%B9%89-WordCNNEncoder"><span class="toc-number">5.2.2.</span> <span class="toc-text">4.2.2 定义 WordCNNEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Embedding-1"><span class="toc-number">5.2.2.1.</span> <span class="toc-text">1. Embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-CNN-1"><span class="toc-number">5.2.2.2.</span> <span class="toc-text">2. CNN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-%E5%AE%9A%E4%B9%89-SentEncoder"><span class="toc-number">5.2.3.</span> <span class="toc-text">4.2.3 定义 SentEncoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E5%AE%9A%E4%B9%89%E6%95%B4%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.2.4.</span> <span class="toc-text">4.2.4 定义整个模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5-%E5%AE%9A%E4%B9%89-Optimizer"><span class="toc-number">5.2.5.</span> <span class="toc-text">4.2.5 定义 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-6%E5%AE%9A%E4%B9%89-sentence-split%EF%BC%8C%E6%8A%8A%E6%96%87%E7%AB%A0%E5%88%92%E5%88%86%E4%B8%BA%E5%8F%A5%E5%AD%90"><span class="toc-number">5.2.6.</span> <span class="toc-text">4.2.6定义 sentence_split，把文章划分为句子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-7-%E5%AE%9A%E4%B9%89-get-examples"><span class="toc-number">5.2.7.</span> <span class="toc-text">4.2.7 定义 get_examples</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-8-%E5%AE%9A%E4%B9%89-batch-slice"><span class="toc-number">5.2.8.</span> <span class="toc-text">4.2.8 定义 batch_slice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-9-%E5%AE%9A%E4%B9%89-data-iter"><span class="toc-number">5.2.9.</span> <span class="toc-text">4.2.9 定义 data_iter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-10-%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E8%AE%A1%E7%AE%97"><span class="toc-number">5.2.10.</span> <span class="toc-text">4.2.10 定义指标计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-11-%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.11.</span> <span class="toc-text">4.2.11 定义训练和测试的方法</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img-blog.csdnimg.cn/92202ef591744a55a05a1fc7e108f4d6.png"></div><div class="author-info__name text-center">zhxnlp</div><div class="author-info__description text-center">nlp</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/zhxnlp">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">49</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://relph1119.github.io/my-team-learning">relph</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://ifwind.github.io/">ifwind</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://chuxiaoyu.cn/nlp-transformer-summary/">chuxiaoyu</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">zhxnlpのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">张贤：textcnn做天池-新闻文本分类</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A9%E6%B1%A0-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">天池-新闻文本分类</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">8k</span><span class="post-meta__separator">|</span><span>阅读时长: 35 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇文章用于记录阿里天池 NLP 入门赛，详细讲解了整个数据处理流程，以及如何从零构建一个模型，适合新手入门。</p>
<p>赛题以新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。实质上是一个 14 分类问题。</p>
<p>赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。</p>
<p>比赛地址：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">https://tianchi.aliyun.com/competition/entrance/531810/introduction</a><br><span id="more"></span><br>数据可以通过上面的链接下载。</p>
<p>其中还用到了训练好的词向量文件。</p>
<p>词向量下载链接: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1ewlck3zwXVQuAzraZ26Euw">https://pan.baidu.com/s/1ewlck3zwXVQuAzraZ26Euw</a> 提取码: qbpr</p>
<p>这篇文章中使用的模型主要是<strong>CNN + LSTM + Attention</strong>，主要学习的是<strong>数据处理的完整流程，以及模型构建的完整流程</strong>。虽然还没有使用 Bert 等方案，不过如果看完了这篇文章，理解了整个流程之后，即使你想要使用其他模型来处理，也能更快实现。</p>
<!--more-->
<h1 id="1-为什么写篇文章"><a href="#1-为什么写篇文章" class="headerlink" title="1. 为什么写篇文章"></a>1. 为什么写篇文章</h1><p>首先，这篇文章的代码全部都来源于 Datawhale 提供的开源代码，我添加了自己的笔记，帮助新手更好地理解这个代码。</p>
<h2 id="1-1-Datawhale-提供的代码有哪些需要改进？"><a href="#1-1-Datawhale-提供的代码有哪些需要改进？" class="headerlink" title="1.1 Datawhale 提供的代码有哪些需要改进？"></a>1.1 Datawhale 提供的代码有哪些需要改进？</h2><p>Datawhale 提供的代码里包含了数据处理，以及从 0 到 1模型建立的完整流程。但是和前面提供的 basesline 的都不太一样，它包含了非常多数据处理的细节，模型也是由 3 个部分构成，所以看起来难度陡然上升。</p>
<p>其次，代码里的注释非常少，也没有讲解整个数据处理和网络的整体流程。这些对于<strong>新手</strong>来说，增加了理解的门槛。<br>在数据竞赛方面，我也是一个新人，花了一天的时间，仔细研究数据在一种每一个步骤的转化，对于一些难以理解的代码，在群里询问之后，也得到了 Datawhale 成员的热心解答。最终才明白了全部的代码。</p>
<h2 id="1-2-我做了什么改进？"><a href="#1-2-我做了什么改进？" class="headerlink" title="1.2 我做了什么改进？"></a>1.2 我做了什么改进？</h2><p>所以，为了减少对于新手的阅读难度，我添加了一些内容。</p>
<ol>
<li><p>首先，梳理了整个流程，包括两大部分：<strong>数据处理</strong>和<strong>模型</strong>。</p>
<p>因为代码<strong>不是从上到下顺序阅读</strong>的。因此，更容易让人理解的做法是：先从整体上给出宏观的数据转换流程图，其中要包括数据在每一步的 shape，以及包含的转换步骤，让读者心中有一个框架图，再带着这个框架图去看细节，会更加了然于胸。</p>
</li>
<li><p>其次，除了了解了整体流程，在真正的代码细节里，读者可能还是会看不懂某一段小逻辑。因此，我在原有代码的基础之上增添了许多<strong>注释</strong>，以降低代码的理解门槛。</p>
</li>
</ol>
<h1 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h1><h2 id="2-1-数据拆分为-10-份"><a href="#2-1-数据拆分为-10-份" class="headerlink" title="2.1 数据拆分为 10 份"></a>2.1 数据拆分为 10 份</h2><ol>
<li><p>数据首先会经过<code>all_data2fold</code>函数，这个函数的作用是把原始的 DataFrame 数据，转换为一个<code>list</code>，有 10 个元素，表示交叉验证里的 10 份，每个元素是 <code>dict</code>，每个<code>dict</code>包括 <code>label</code> 和 <code>text</code>。</p>
<p>首先根据 <code>label</code> 来划分数据行所在 <code>index</code>, 生成 <code>label2id</code>。</p>
<p><code>label2id</code> 是一个 <code>dict</code>，<code>key</code> 为 <code>label</code>，<code>value</code> 是一个 <code>list</code>，存储的是该类对应的 <code>index</code>。</p>
<p><div align="center"><img src="https://image.zhangxiann.com/20200814084354.png"/></div><br><br>然后根据<code>label2id</code>，把每一类别的数据，划分到 10 份数据中。</p>
<p><div align="center"><img src="https://image.zhangxiann.com/数据处理.gif"/></div><br></p>
<p>最终得到的数据<code>fold_data</code>是一个<code>list</code>，有 10 个元素，每个元素是 <code>dict</code>，包括 <code>label</code> 和 <code>text</code>的列表：<code>[&#123;labels:textx&#125;, &#123;labels:textx&#125;. . .]</code>。</p>
</li>
<li>最后，把前 9 份数据作为训练集<code>train_data</code>，最后一份数据作为验证集<code>dev_data</code>，并读取测试集<code>test_data</code>。</li>
</ol>
<h2 id="2-2-定义并创建-Vacab"><a href="#2-2-定义并创建-Vacab" class="headerlink" title="2.2 定义并创建 Vacab"></a>2.2 定义并创建 Vacab</h2><p>Vocab 的作用是：</p>
<ul>
<li>创建 词 和 <code>index</code> 对应的字典，这里包括 2 份字典，分别是：<code>_id2word</code> 和 <code>_id2extword</code>。</li>
<li>其中 <code>_id2word</code> 是从新闻得到的， 把词频小于 5 的词替换为了 <code>UNK</code>。对应到模型输入的 <code>batch_inputs1</code>。</li>
<li><code>_id2extword</code> 是从 <code>word2vec.txt</code> 中得到的，有 5976 个词。对应到模型输入的 <code>batch_inputs2</code>。</li>
<li>后面会有两个 <code>embedding</code> 层，其中 <code>_id2word</code> 对应的 <code>embedding</code> 是可学习的，<code>_id2extword</code> 对应的 <code>embedding</code> 是从文件中加载的，是固定的。</li>
<li>创建 label 和 index 对应的字典。</li>
<li>上面这些字典，都是基于<code>train_data</code>创建的。</li>
</ul>
<h1 id="3-模型"><a href="#3-模型" class="headerlink" title="3. 模型"></a>3. 模型</h1><h2 id="3-1-把文章分割为句子"><a href="#3-1-把文章分割为句子" class="headerlink" title="3.1 把文章分割为句子"></a>3.1 把文章分割为句子</h2><ol>
<li><p>上上一步得到的 3 个数据，都是一个<code>list</code>，<code>list</code>里的每个元素是 dict，每个 dict 包括 <code>label</code> 和 <code>text</code>。这 3 个数据会经过 <code>get_examples</code>函数。 <code>get_examples</code>函数里，会调用<code>sentence_split</code>函数，把每一篇文章分割成为句子。</p>
<p>然后，根据<code>vocab</code>，把 word 转换为对应的索引，这里使用了 2 个字典，转换为 2 份索引，分别是：<code>word_ids</code>和<code>extword_ids</code>。最后返回的数据是一个 list，每个元素是一个 tuple: <code>(label, 句子数量，doc)</code>。其中<code>doc</code>又是一个 list，每个 元素是一个 tuple: <code>(句子长度，word_ids, extword_ids)</code>。</p>
<p><div align="center"><img src="https://image.zhangxiann.com/20200814105531.png"/></div><br></p>
</li>
</ol>
<ol>
<li>在迭代训练时，调用<code>data_iter</code>函数，生成每一批的<code>batch_data</code>。在<code>data_iter</code>函数里，会调用<code>batch_slice</code>函数生成每一个<code>batch</code>。拿到<code>batch_data</code>后，每个数据的格式仍然是上图中所示的格式，下面，调用<code>batch2tensor</code>函数。</li>
</ol>
<h2 id="3-2-生成训练数据"><a href="#3-2-生成训练数据" class="headerlink" title="3.2 生成训练数据"></a>3.2 生成训练数据</h2><p><code>batch2tensor</code>函数最后返回的数据是：<code>(batch_inputs1, batch_inputs2, batch_masks), batch_labels</code>。形状都是<code>(batch_size, doc_len, sent_len)</code>。<code>doc_len</code>表示每篇新闻有几句话，<code>sent_len</code>表示每句话有多少个单词。</p>
<p><code>batch_masks</code>在有单词的位置，值为1，其他地方为 0，用于后面计算 Attention，把那些没有单词的位置的 attention 改为 0。</p>
<p><code>batch_inputs1, batch_inputs2, batch_masks</code>，形状是<code>(batch_size, doc_len, sent_len)</code>，转换为<code>(batch_size * doc_len, sent_len)</code>。</p>
<h2 id="3-3-网络部分"><a href="#3-3-网络部分" class="headerlink" title="3.3 网络部分"></a>3.3 网络部分</h2><p>下面，终于来到网络部分。模型结构图如下：</p>
<p><div align="center"><img src="https://image.zhangxiann.com/20200814132004.png"/></div><br></p>
<h3 id="3-3-1-WordCNNEncoder"><a href="#3-3-1-WordCNNEncoder" class="headerlink" title="3.3.1 WordCNNEncoder"></a>3.3.1 WordCNNEncoder</h3><p>WordCNNEncoder 网络结构示意图如下：</p>
<p><div align="center"><img src="https://image.zhangxiann.com/20200814132200.png"/></div><br></p>
<h4 id="1-Embedding"><a href="#1-Embedding" class="headerlink" title="1. Embedding"></a>1. Embedding</h4><p><code>batch_inputs1, batch_inputs2</code>都输入到<code>WordCNNEncoder</code>。<code>WordCNNEncoder</code>包括两个<code>embedding</code>层，分别对应<code>batch_inputs1</code>，embedding 层是可学习的，得到<code>word_embed</code>；<code>batch_inputs2</code>，读取的是外部训练好的词向量，因此是不可学习的，得到<code>extword_embed</code>。所以会分别得到两个词向量，将 2 个词向量相加，得到最终的词向量<code>batch_embed</code>，形状是<code>(batch_size * doc_len, sent_len, 100)</code>，然后添加一个维度，变为<code>(batch_size * doc_len, 1, sent_len, 100)</code>，对应 Pytorch 里图像的<code>(B, C, H, W)</code>。</p>
<h4 id="2-CNN"><a href="#2-CNN" class="headerlink" title="2. CNN"></a>2. CNN</h4><p>然后，分别定义 3 个卷积核，output channel 都是 100 维。</p>
<p>第一个卷积核大小为<code>[2,100]</code>，得到的输出是<code>(batch_size * doc_len, 100， sent_len-2+1, 1)</code>，定义一个池化层大小为<code>[sent_len-2+1, 1]</code>，最终得到输出经过<code>squeeze()</code>的形状是<code>(batch_size * doc_len, 100)</code>。</p>
<p>同理，第 2 个卷积核大小为<code>[3,100]</code>，第 3 个卷积核大小为<code>[4,100]</code>。卷积+池化得到的输出形状也是<code>(batch_size * doc_len, 100)</code>。</p>
<p>最后，将这 3 个向量在第 2 个维度上做拼接，得到输出的形状是<code>(batch_size * doc_len, 300)</code>。</p>
<h3 id="3-3-2-shape-转换"><a href="#3-3-2-shape-转换" class="headerlink" title="3.3.2 shape 转换"></a>3.3.2 shape 转换</h3><p>把上一步得到的数据的形状，转换为<code>(batch_size , doc_len, 300)</code>名字是<code>sent_reps</code>。然后，对<code>mask</code>进行处理。</p>
<p><code>batch_masks</code>的形状是<code>(batch_size , doc_len, 300)</code>，表示单词的 mask，经过<code>sent_masks = batch_masks.bool().any(2).float()</code>得到句子的 mask。含义是：在最后一个维度，判断是否有单词，只要有 1 个单词，那么整句话的 mask 就是 1，<code>sent_masks</code>的维度是：<code>(batch_size , doc_len)</code>。</p>
<h3 id="3-3-3-SentEncoder"><a href="#3-3-3-SentEncoder" class="headerlink" title="3.3.3 SentEncoder"></a>3.3.3 SentEncoder</h3><p>SentEncoder 网络结构示意图如下：</p>
<p><div align="center"><img src="https://image.zhangxiann.com/20200814145134.png"/></div><br><br><code>SentEncoder</code>包含了 2 层的双向 LSTM，输入数据<code>sent_reps</code>的形状是<code>(batch_size , doc_len, 300)</code>，LSTM 的 hidden_size 为 256，由于是双向的，经过 LSTM  后的数据维度是<code>(batch_size , doc_len, 512)</code>，然后和 mask 按位置相乘，把没有单词的句子的位置改为 0，最后输出的数据<code>sent_hiddens</code>，维度依然是<code>(batch_size , doc_len, 512)</code>。</p>
<h3 id="3-3-4-Attention"><a href="#3-3-4-Attention" class="headerlink" title="3.3.4 Attention"></a>3.3.4 Attention</h3><p>接着，经过<code>Attention</code>。<code>Attention</code>的输入是<code>sent_hiddens</code>和<code>sent_masks</code>。在<code>Attention</code>里，<code>sent_hiddens</code>首先经过线性变化得到<code>key</code>，维度不变，依然是<code>(batch_size , doc_len, 512)</code>。</p>
<p>然后<code>key</code>和<code>query</code>相乘，得到<code>outputs</code>。<code>query</code>的维度是<code>512</code>，因此<code>output</code>的维度是<code>(batch_size , doc_len)</code>，这个就是我们需要的<code>attention</code>，表示分配到每个句子的权重。下一步需要对这个<code>attetion</code>做<code>softmax</code>，并使用<code>sent_masks</code>，把没有单词的句子的权重置为<code>-1e32</code>，得到<code>masked_attn_scores</code>。</p>
<p>最后把<code>masked_attn_scores</code>和<code>key</code>相乘，得到<code>batch_outputs</code>，形状是<code>(batch_size, 512)</code>。</p>
<h3 id="3-3-5-FC"><a href="#3-3-5-FC" class="headerlink" title="3.3.5 FC"></a>3.3.5 FC</h3><p>最后经过<code>FC</code>层，得到分类概率的向量。</p>
<h1 id="4-完整代码-注释"><a href="#4-完整代码-注释" class="headerlink" title="4. 完整代码+注释"></a>4. 完整代码+注释</h1><h2 id="4-1-数据处理"><a href="#4-1-数据处理" class="headerlink" title="4.1 数据处理"></a>4.1 数据处理</h2><p>导入包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.INFO, <span class="built_in">format</span>=<span class="string">&#x27;%(asctime)-15s %(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set seed </span></span><br><span class="line">seed = <span class="number">666</span></span><br><span class="line">random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.cuda.manual_seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set cuda</span></span><br><span class="line">gpu = <span class="number">0</span></span><br><span class="line">use_cuda = gpu &gt;= <span class="number">0</span> <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> use_cuda:</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>, gpu)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">logging.info(<span class="string">&quot;Use cuda: %s, gpu id: %d.&quot;</span>, use_cuda, gpu)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-13 17:12:16,510 INFO: Use cuda: False, gpu id: 0.
</code></pre><h3 id="4-1-1-把数据分成-10-份"><a href="#4-1-1-把数据分成-10-份" class="headerlink" title="4.1.1 把数据分成  10 份"></a>4.1.1 把数据分成  10 份</h3><p>数据会经过<code>all_data2fold</code>函数，这个函数的作用是把原始的 DataFrame 数据，转换为一个<code>list</code>，有 10 个元素，每个元素是 <code>dict</code>，包括 <code>label</code> 和 <code>text</code>的列表：<code>[&#123;labels:textx&#125;, &#123;labels:textx&#125;. . .]</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split data to 10 fold</span></span><br><span class="line">fold_num = <span class="number">10</span></span><br><span class="line">data_file = <span class="string">&#x27;train_set.csv&#x27;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_data2fold</span>(<span class="params">fold_num, num=<span class="number">10000</span></span>):</span></span><br><span class="line">    fold_data = []</span><br><span class="line">    f = pd.read_csv(data_file, sep=<span class="string">&#x27;\t&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    texts = f[<span class="string">&#x27;text&#x27;</span>].tolist()[:num]</span><br><span class="line">    labels = f[<span class="string">&#x27;label&#x27;</span>].tolist()[:num]</span><br><span class="line"></span><br><span class="line">    total = <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line">    index = <span class="built_in">list</span>(<span class="built_in">range</span>(total))</span><br><span class="line">    <span class="comment"># 打乱数据</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># all_texts 和 all_labels 都是 shuffle 之后的数据</span></span><br><span class="line">    all_texts = []</span><br><span class="line">    all_labels = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        all_texts.append(texts[i])</span><br><span class="line">        all_labels.append(labels[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造一个 dict，key 为 label，value 是一个 list，存储的是该类对应的 index</span></span><br><span class="line">    label2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total):</span><br><span class="line">        label = <span class="built_in">str</span>(all_labels[i])</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label2id:</span><br><span class="line">            label2id[label] = [i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label2id[label].append(i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># all_index 是一个 list，里面包括 10 个 list，称为 10 个 fold，存储 10 个 fold 对应的 index</span></span><br><span class="line">    all_index = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(fold_num)]</span><br><span class="line">    <span class="keyword">for</span> label, data <span class="keyword">in</span> label2id.items():</span><br><span class="line">        <span class="comment"># print(label, len(data))</span></span><br><span class="line">        batch_size = <span class="built_in">int</span>(<span class="built_in">len</span>(data) / fold_num)</span><br><span class="line">        <span class="comment"># other 表示多出来的数据，other 的数据量是小于 fold_num 的</span></span><br><span class="line">        other = <span class="built_in">len</span>(data) - batch_size * fold_num</span><br><span class="line">        <span class="comment"># 把每一类对应的 index，添加到每个 fold 里面去</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(fold_num):</span><br><span class="line">            <span class="comment"># 如果 i &lt; other，那么将一个数据添加到这一轮 batch 的数据中</span></span><br><span class="line">            cur_batch_size = batch_size + <span class="number">1</span> <span class="keyword">if</span> i &lt; other <span class="keyword">else</span> batch_size</span><br><span class="line">            <span class="comment"># print(cur_batch_size)</span></span><br><span class="line">            <span class="comment"># batch_data 是该轮 batch 对应的索引</span></span><br><span class="line">            batch_data = [data[i * batch_size + b] <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(cur_batch_size)]</span><br><span class="line">            all_index[i].extend(batch_data)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="built_in">int</span>(total / fold_num)</span><br><span class="line">    other_texts = []</span><br><span class="line">    other_labels = []</span><br><span class="line">    other_num = <span class="number">0</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 由于上面在分 batch 的过程中，每个 batch 的数据量不一样，这里是把数据平均到每个 batch</span></span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(fold_num):</span><br><span class="line">        num = <span class="built_in">len</span>(all_index[fold])</span><br><span class="line">        texts = [all_texts[i] <span class="keyword">for</span> i <span class="keyword">in</span> all_index[fold]]</span><br><span class="line">        labels = [all_labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> all_index[fold]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num &gt; batch_size: <span class="comment"># 如果大于 batch_size 那么就取 batch_size 大小的数据</span></span><br><span class="line">            fold_texts = texts[:batch_size]</span><br><span class="line">            other_texts.extend(texts[batch_size:])</span><br><span class="line">            fold_labels = labels[:batch_size]</span><br><span class="line">            other_labels.extend(labels[batch_size:])</span><br><span class="line">            other_num += num - batch_size</span><br><span class="line">        <span class="keyword">elif</span> num &lt; batch_size: <span class="comment"># 如果小于 batch_size，那么就补全到 batch_size 的大小</span></span><br><span class="line">            end = start + batch_size - num</span><br><span class="line">            fold_texts = texts + other_texts[start: end]</span><br><span class="line">            fold_labels = labels + other_labels[start: end]</span><br><span class="line">            start = end</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fold_texts = texts</span><br><span class="line">            fold_labels = labels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> batch_size == <span class="built_in">len</span>(fold_labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shuffle</span></span><br><span class="line">        index = <span class="built_in">list</span>(<span class="built_in">range</span>(batch_size))</span><br><span class="line">        np.random.shuffle(index)</span><br><span class="line">        <span class="comment"># 这里是为了打乱数据</span></span><br><span class="line">        shuffle_fold_texts = []</span><br><span class="line">        shuffle_fold_labels = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">            shuffle_fold_texts.append(fold_texts[i])</span><br><span class="line">            shuffle_fold_labels.append(fold_labels[i])</span><br><span class="line"></span><br><span class="line">        data = &#123;<span class="string">&#x27;label&#x27;</span>: shuffle_fold_labels, <span class="string">&#x27;text&#x27;</span>: shuffle_fold_texts&#125;</span><br><span class="line">        fold_data.append(data)</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">&quot;Fold lens %s&quot;</span>, <span class="built_in">str</span>([<span class="built_in">len</span>(data[<span class="string">&#x27;label&#x27;</span>]) <span class="keyword">for</span> data <span class="keyword">in</span> fold_data]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fold_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># fold_data 是一个 list，有 10 个元素，每个元素是 dict，包括 label 和 text</span></span><br><span class="line">fold_data = all_data2fold(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>2020-08-13 17:12:45,012 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]
</code></pre><h3 id="4-1-2-拆分训练集、验证集，读取测试集"><a href="#4-1-2-拆分训练集、验证集，读取测试集" class="headerlink" title="4.1.2 拆分训练集、验证集，读取测试集"></a>4.1.2 拆分训练集、验证集，读取测试集</h3><p>把前 9 份数据作为训练集<code>train_data</code>，最后一份数据作为验证集<code>dev_data</code>，并读取测试集<code>test_data</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build train, dev, test data</span></span><br><span class="line">fold_id = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dev</span></span><br><span class="line">dev_data = fold_data[fold_id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># train 取出前 9 个 fold 的数据</span></span><br><span class="line">train_texts = []</span><br><span class="line">train_labels = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, fold_id):</span><br><span class="line">    data = fold_data[i]</span><br><span class="line">    train_texts.extend(data[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">    train_labels.extend(data[<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train_data = &#123;<span class="string">&#x27;label&#x27;</span>: train_labels, <span class="string">&#x27;text&#x27;</span>: train_texts&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># test 读取测试集数据</span></span><br><span class="line">test_data_file = <span class="string">&#x27;test_a.csv&#x27;</span></span><br><span class="line">f = pd.read_csv(test_data_file, sep=<span class="string">&#x27;\t&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">texts = f[<span class="string">&#x27;text&#x27;</span>].tolist()</span><br><span class="line">test_data = &#123;<span class="string">&#x27;label&#x27;</span>: [<span class="number">0</span>] * <span class="built_in">len</span>(texts), <span class="string">&#x27;text&#x27;</span>: texts&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-1-3创建-Vocab"><a href="#4-1-3创建-Vocab" class="headerlink" title="4.1.3创建 Vocab"></a>4.1.3创建 Vocab</h3><p>Vocab 的作用是：</p>
<ul>
<li>创建 词 和 <code>index</code> 对应的字典，这里包括 2 份字典，分别是：<code>_id2word</code> 和 <code>_id2extword</code>。</li>
<li>其中 <code>_id2word</code> 是从新闻得到的， 把词频小于 5 的词替换为了 <code>UNK</code>。对应到模型输入的 <code>batch_inputs1</code>。</li>
<li><code>_id2extword</code> 是从 <code>word2vec.txt</code> 中得到的，有 5976 个词。对应到模型输入的 <code>batch_inputs2</code>。</li>
<li>后面会有两个 <code>embedding</code> 层，其中 <code>_id2word</code> 对应的 <code>embedding</code> 是可学习的，<code>_id2extword</code> 对应的 <code>embedding</code> 是从文件中加载的，是固定的。</li>
<li>创建 label 和 index 对应的字典。</li>
<li>上面这些字典，都是基于<code>train_data</code>创建的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build vocab</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BasicTokenizer</span><br><span class="line"></span><br><span class="line">basic_tokenizer = BasicTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Vocab 的作用是：</span></span><br><span class="line"><span class="comment"># 1. 创建 词 和 index 对应的字典，这里包括 2 份字典，分别是：_id2word 和 _id2extword</span></span><br><span class="line"><span class="comment"># 其中 _id2word 是从新闻得到的， 把词频小于 5 的词替换为了 UNK。对应到模型输入的 batch_inputs1。</span></span><br><span class="line"><span class="comment"># _id2extword 是从 word2vec.txt 中得到的，有 5976 个词。对应到模型输入的 batch_inputs2。</span></span><br><span class="line"><span class="comment"># 后面会有两个 embedding 层，其中 _id2word 对应的 embedding 是可学习的，_id2extword 对应的 embedding 是从文件中加载的，是固定的</span></span><br><span class="line"><span class="comment"># 2.创建 label 和 index 对应的字典</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        self.min_count = <span class="number">5</span></span><br><span class="line">        self.pad = <span class="number">0</span></span><br><span class="line">        self.unk = <span class="number">1</span></span><br><span class="line">        self._id2word = [<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line">        self._id2extword = [<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self._id2label = []</span><br><span class="line">        self.target_names = []</span><br><span class="line"></span><br><span class="line">        self.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: <span class="built_in">dict</span>(<span class="built_in">zip</span>(x, <span class="built_in">range</span>(<span class="built_in">len</span>(x))))</span><br><span class="line">        <span class="comment">#创建词和 index 对应的字典</span></span><br><span class="line">        self._word2id = reverse(self._id2word)</span><br><span class="line">        <span class="comment">#创建 label 和 index 对应的字典</span></span><br><span class="line">        self._label2id = reverse(self._id2label)</span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">&quot;Build vocab: words %d, labels %d.&quot;</span> % (self.word_size, self.label_size))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建词典</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        self.word_counter = Counter()</span><br><span class="line">        <span class="comment">#计算每个词出现的次数</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> data[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">            words = text.split()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                self.word_counter[word] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 去掉频次小于 min_count = 5 的词，把词存到 _id2word</span></span><br><span class="line">        <span class="keyword">for</span> word, count <span class="keyword">in</span> self.word_counter.most_common():</span><br><span class="line">            <span class="keyword">if</span> count &gt;= self.min_count:</span><br><span class="line">                self._id2word.append(word)</span><br><span class="line"></span><br><span class="line">        label2name = &#123;<span class="number">0</span>: <span class="string">&#x27;科技&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;股票&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;体育&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;娱乐&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;时政&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;社会&#x27;</span>, <span class="number">6</span>: <span class="string">&#x27;教育&#x27;</span>, <span class="number">7</span>: <span class="string">&#x27;财经&#x27;</span>,</span><br><span class="line">                      <span class="number">8</span>: <span class="string">&#x27;家居&#x27;</span>, <span class="number">9</span>: <span class="string">&#x27;游戏&#x27;</span>, <span class="number">10</span>: <span class="string">&#x27;房产&#x27;</span>, <span class="number">11</span>: <span class="string">&#x27;时尚&#x27;</span>, <span class="number">12</span>: <span class="string">&#x27;彩票&#x27;</span>, <span class="number">13</span>: <span class="string">&#x27;星座&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">        self.label_counter = Counter(data[<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.label_counter)):</span><br><span class="line">            count = self.label_counter[label] <span class="comment"># 取出 label 对应的次数</span></span><br><span class="line">            self._id2label.append(label) </span><br><span class="line">            self.target_names.append(label2name[label]) <span class="comment"># 根据label数字取出对应的名字</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_pretrained_embs</span>(<span class="params">self, embfile</span>):</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(embfile, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">            items = lines[<span class="number">0</span>].split()</span><br><span class="line">            <span class="comment"># 第一行分别是单词数量、词向量维度</span></span><br><span class="line">            word_count, embedding_dim = <span class="built_in">int</span>(items[<span class="number">0</span>]), <span class="built_in">int</span>(items[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        index = <span class="built_in">len</span>(self._id2extword)</span><br><span class="line">        embeddings = np.zeros((word_count + index, embedding_dim))</span><br><span class="line">        <span class="comment"># 下面的代码和 word2vec.txt 的结构有关</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">            values = line.split()</span><br><span class="line">            self._id2extword.append(values[<span class="number">0</span>]) <span class="comment"># 首先添加第一列的单词</span></span><br><span class="line">            vector = np.array(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float64&#x27;</span>) <span class="comment"># 然后添加后面 100 列的词向量</span></span><br><span class="line">            embeddings[self.unk] += vector</span><br><span class="line">            embeddings[index] = vector</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># unk 的词向量是所有词的平均</span></span><br><span class="line">        embeddings[self.unk] = embeddings[self.unk] / word_count</span><br><span class="line">        <span class="comment"># 除以标准差干嘛？</span></span><br><span class="line">        embeddings = embeddings / np.std(embeddings)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: <span class="built_in">dict</span>(<span class="built_in">zip</span>(x, <span class="built_in">range</span>(<span class="built_in">len</span>(x))))</span><br><span class="line">        self._extword2id = reverse(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="built_in">set</span>(self._id2extword)) == <span class="built_in">len</span>(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据单词得到 id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._word2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._word2id.get(xs, self.unk)</span><br><span class="line">    <span class="comment"># 根据单词得到 ext id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._extword2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._extword2id.get(xs, self.unk)</span><br><span class="line">    <span class="comment"># 根据 label 得到 id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._label2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._label2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2word)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2extword)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2label)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(train_data)</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, 0, 0, 2, 0, 6, 2, 1, 4]
</code></pre><h2 id="4-2-模型"><a href="#4-2-模型" class="headerlink" title="4.2 模型"></a>4.2 模型</h2><h3 id="4-2-1-定义-Attention"><a href="#4-2-1-定义-Attention" class="headerlink" title="4.2.1 定义 Attention"></a>4.2.1 定义 Attention</h3><p><code>Attention</code>的输入是<code>sent_hiddens</code>和<code>sent_masks</code>。在<code>Attention</code>里，<code>sent_hiddens</code>首先经过线性变化得到<code>key</code>，维度不变，依然是<code>(batch_size , doc_len, 512)</code>。</p>
<p>然后<code>key</code>和<code>query</code>相乘，得到<code>outputs</code>。<code>query</code>的维度是<code>512</code>，因此<code>output</code>的维度是<code>(batch_size , doc_len)</code>，这个就是我们需要的<code>attention</code>，表示分配到每个句子的权重。下一步需要对这个<code>attetion</code>做<code>softmax</code>，并使用<code>sent_masks</code>，把没有单词的句子的权重置为<code>-1e32</code>，得到<code>masked_attn_scores</code>。</p>
<p>最后把<code>masked_attn_scores</code>和<code>key</code>相乘，得到<code>batch_outputs</code>，形状是<code>(batch_size, 512)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build module</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))</span><br><span class="line">        self.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        b = np.zeros(hidden_size, dtype=np.float32)</span><br><span class="line">        self.bias.data.copy_(torch.from_numpy(b))</span><br><span class="line"></span><br><span class="line">        self.query = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        self.query.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch_hidden, batch_masks</span>):</span></span><br><span class="line">        <span class="comment"># batch_hidden: b * doc_len * hidden_size (2 * hidden_size of lstm)</span></span><br><span class="line">        <span class="comment"># batch_masks:  b x doc_len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        <span class="comment"># key： b * doc_len * hidden</span></span><br><span class="line">        key = torch.matmul(batch_hidden, self.weight) + self.bias </span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute attention</span></span><br><span class="line">        <span class="comment"># matmul 会进行广播</span></span><br><span class="line">        <span class="comment">#outputs: b * doc_len</span></span><br><span class="line">        outputs = torch.matmul(key, self.query)  </span><br><span class="line">        <span class="comment"># 1 - batch_masks 就是取反，把没有单词的句子置为 0</span></span><br><span class="line">        <span class="comment"># masked_fill 的作用是 在 为 1 的地方替换为 value: float(-1e32)</span></span><br><span class="line">        masked_outputs = outputs.masked_fill((<span class="number">1</span> - batch_masks).<span class="built_in">bool</span>(), <span class="built_in">float</span>(-<span class="number">1e32</span>))</span><br><span class="line">        <span class="comment">#attn_scores：b * doc_len</span></span><br><span class="line">        attn_scores = F.softmax(masked_outputs, dim=<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0</span></span><br><span class="line">        masked_attn_scores = attn_scores.masked_fill((<span class="number">1</span> - batch_masks).<span class="built_in">bool</span>(), <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sum weighted sources</span></span><br><span class="line">        <span class="comment"># masked_attn_scores.unsqueeze(1)：# b * 1 * doc_len</span></span><br><span class="line">        <span class="comment"># key：b * doc_len * hidden</span></span><br><span class="line">        <span class="comment"># batch_outputs：b * hidden</span></span><br><span class="line">        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(<span class="number">1</span>), key).squeeze(<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs, attn_scores</span><br></pre></td></tr></table></figure>
<h3 id="4-2-2-定义-WordCNNEncoder"><a href="#4-2-2-定义-WordCNNEncoder" class="headerlink" title="4.2.2 定义 WordCNNEncoder"></a>4.2.2 定义 WordCNNEncoder</h3><h4 id="1-Embedding-1"><a href="#1-Embedding-1" class="headerlink" title="1. Embedding"></a>1. Embedding</h4><p><code>batch_inputs1, batch_inputs2</code>都输入到<code>WordCNNEncoder</code>。<code>WordCNNEncoder</code>包括两个<code>embedding</code>层，分别对应<code>batch_inputs1</code>，embedding 层是可学习的，得到<code>word_embed</code>；<code>batch_inputs2</code>，读取的是外部训练好的词向，因此是不可学习的，得到<code>extword_embed</code>。所以会分别得到两个词向量，将 2 个词向量相加，得到最终的词向量<code>batch_embed</code>，形状是<code>(batch_size * doc_len, sent_len, 100)</code>，然后添加一个维度，变为<code>(batch_size * doc_len, 1, sent_len, 100)</code>，对应 Pytorch 里图像的<code>(B, C, H, W)</code>。</p>
<h4 id="2-CNN-1"><a href="#2-CNN-1" class="headerlink" title="2. CNN"></a>2. CNN</h4><p>然后，分别定义 3 个卷积核，output channel 都是 100 维。</p>
<p>第一个卷积核大小为<code>[2,100]</code>，得到的输出是<code>(batch_size * doc_len, 100， sent_len-2+1, 1)</code>，定义一个池化层大小为<code>[sent_len-2+1, 1]</code>，最终得到输出经过<code>squeeze()</code>的形状是<code>(batch_size * doc_len, 100)</code>。</p>
<p>同理，第 2 个卷积核大小为<code>[3,100]</code>，第 3 个卷积核大小为<code>[4,100]</code>。卷积+池化得到的输出形状也是<code>(batch_size * doc_len, 100)</code>。</p>
<p>最后，将这 3 个向量在第 2 个维度上做拼接，得到输出的形状是<code>(batch_size * doc_len, 300)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取训练好的词向量文件</span></span><br><span class="line">word2vec_path = <span class="string">&#x27;../emb/word2vec.txt&#x27;</span></span><br><span class="line">dropout = <span class="number">0.15</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入是：</span></span><br><span class="line"><span class="comment"># 输出是：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCNNEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(WordCNNEncoder, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.word_dims = <span class="number">100</span> <span class="comment"># 词向量的长度是 100 维</span></span><br><span class="line">        <span class="comment"># padding_idx 表示当取第 0 个词时，向量全为 0</span></span><br><span class="line">        <span class="comment"># 这个 Embedding 层是可学习的</span></span><br><span class="line">        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        extword_embed = vocab.load_pretrained_embs(word2vec_path)</span><br><span class="line">        extword_size, word_dims = extword_embed.shape</span><br><span class="line">        logging.info(<span class="string">&quot;Load extword embed: words %d, dims %d.&quot;</span> % (extword_size, word_dims))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # 这个 Embedding 层是不可学习的</span></span><br><span class="line">        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))</span><br><span class="line">        self.extword_embed.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        input_size = self.word_dims</span><br><span class="line"></span><br><span class="line">        self.filter_sizes = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  <span class="comment"># n-gram window</span></span><br><span class="line">        self.out_channel = <span class="number">100</span></span><br><span class="line">        <span class="comment"># 3 个卷积层，卷积核大小分别为 [2,100], [3,100], [4,100]</span></span><br><span class="line">        self.convs = nn.ModuleList([nn.Conv2d(<span class="number">1</span>, self.out_channel, (filter_size, input_size), bias=<span class="literal">True</span>)</span><br><span class="line">                                    <span class="keyword">for</span> filter_size <span class="keyword">in</span> self.filter_sizes])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word_ids, extword_ids</span>):</span></span><br><span class="line">        <span class="comment"># word_ids: sentence_num * sentence_len</span></span><br><span class="line">        <span class="comment"># extword_ids: sentence_num * sentence_len</span></span><br><span class="line">        <span class="comment"># batch_masks: sentence_num * sentence_len</span></span><br><span class="line">        sen_num, sent_len = word_ids.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># word_embed: sentence_num * sentence_len * 100</span></span><br><span class="line">        <span class="comment"># 根据 index 取出词向量</span></span><br><span class="line">        word_embed = self.word_embed(word_ids)</span><br><span class="line">        extword_embed = self.extword_embed(extword_ids)</span><br><span class="line">        batch_embed = word_embed + extword_embed</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_embed = self.dropout(batch_embed)</span><br><span class="line">        <span class="comment"># batch_embed: sentence_num x 1 x sentence_len x 100</span></span><br><span class="line">        <span class="comment"># squeeze 是为了添加一个 channel 的维度，成为 B * C * H * W</span></span><br><span class="line">        <span class="comment"># 方便下面做 卷积</span></span><br><span class="line">        batch_embed.unsqueeze_(<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">        pooled_outputs = []</span><br><span class="line">        <span class="comment"># 通过 3 个卷积核做 3 次卷积核池化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.filter_sizes)):</span><br><span class="line">            <span class="comment"># 通过池化公式计算池化后的高度: o = (i-k)/s+1</span></span><br><span class="line">            <span class="comment"># 其中 o 表示输出的长度</span></span><br><span class="line">            <span class="comment"># k 表示卷积核大小</span></span><br><span class="line">            <span class="comment"># s 表示步长，这里为 1</span></span><br><span class="line">            filter_height = sent_len - self.filter_sizes[i] + <span class="number">1</span></span><br><span class="line">            <span class="comment"># conv：sentence_num * out_channel * filter_height * 1</span></span><br><span class="line">            conv = self.convs[i](batch_embed)</span><br><span class="line">            hidden = F.relu(conv)  </span><br><span class="line">            <span class="comment"># 定义池化层</span></span><br><span class="line">            mp = nn.MaxPool2d((filter_height, <span class="number">1</span>))  <span class="comment"># (filter_height, filter_width)</span></span><br><span class="line">            <span class="comment"># pooled：sentence_num * out_channel * 1 * 1 -&gt; sen_num * out_channel</span></span><br><span class="line">            <span class="comment"># 也可以通过 squeeze 来删除无用的维度</span></span><br><span class="line">            pooled = mp(hidden).reshape(sen_num,</span><br><span class="line">                                        self.out_channel) </span><br><span class="line">            </span><br><span class="line">            pooled_outputs.append(pooled)</span><br><span class="line">        <span class="comment"># 拼接 3 个池化后的向量</span></span><br><span class="line">        <span class="comment"># reps: sen_num * (3*out_channel)</span></span><br><span class="line">        reps = torch.cat(pooled_outputs, dim=<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            reps = self.dropout(reps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reps</span><br></pre></td></tr></table></figure>
<h3 id="4-2-3-定义-SentEncoder"><a href="#4-2-3-定义-SentEncoder" class="headerlink" title="4.2.3 定义 SentEncoder"></a>4.2.3 定义 SentEncoder</h3><p><code>SentEncoder</code>包含了 2 层的双向 LSTM，输入数据<code>sent_reps</code>的形状是<code>(batch_size , doc_len, 300)</code>，LSTM 的 hidden_size 为 256，由于是双向的，经过 LSTM 后的数据维度是<code>(batch_size , doc_len, 512)</code>，然后和 mask 按位置相乘，把没有单词的句子的位置改为 0，最后输出的数据<code>sent_hiddens</code>，维度依然是<code>(batch_size , doc_len, 512)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build sent encoder</span></span><br><span class="line">sent_hidden_size = <span class="number">256</span></span><br><span class="line">sent_num_layers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sent_rep_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SentEncoder, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.sent_lstm = nn.LSTM(</span><br><span class="line">            input_size=sent_rep_size, <span class="comment"># 每个句子经过 CNN 后得到 300 维向量</span></span><br><span class="line">            hidden_size=sent_hidden_size,<span class="comment"># 输出的维度</span></span><br><span class="line">            num_layers=sent_num_layers,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sent_reps, sent_masks</span>):</span></span><br><span class="line">        <span class="comment"># sent_reps:  b * doc_len * sent_rep_size</span></span><br><span class="line">        <span class="comment"># sent_masks: b * doc_len</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  b * doc_len * hidden*2</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  batch, seq_len, num_directions * hidden_size</span></span><br><span class="line">        sent_hiddens, _ = self.sent_lstm(sent_reps)  </span><br><span class="line">        <span class="comment"># 对应相乘，用到广播，是为了只保留有句子的位置的数值</span></span><br><span class="line">        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            sent_hiddens = self.dropout(sent_hiddens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sent_hiddens</span><br></pre></td></tr></table></figure>
<h3 id="4-2-4-定义整个模型"><a href="#4-2-4-定义整个模型" class="headerlink" title="4.2.4 定义整个模型"></a>4.2.4 定义整个模型</h3><p>把 WordCNNEncoder、SentEncoder、Attention、FC 全部连接起来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.sent_rep_size = <span class="number">300</span> <span class="comment"># 经过 CNN 后得到的 300 维向量</span></span><br><span class="line">        self.doc_rep_size = sent_hidden_size * <span class="number">2</span> <span class="comment"># lstm 最后输出的向量长度</span></span><br><span class="line">        self.all_parameters = &#123;&#125;</span><br><span class="line">        parameters = []</span><br><span class="line">        self.word_encoder = WordCNNEncoder(vocab)</span><br><span class="line">        </span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.word_encoder.parameters())))</span><br><span class="line"></span><br><span class="line">        self.sent_encoder = SentEncoder(self.sent_rep_size)</span><br><span class="line">        self.sent_attention = Attention(self.doc_rep_size)</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_encoder.parameters())))</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_attention.parameters())))</span><br><span class="line">        <span class="comment"># doc_rep_size</span></span><br><span class="line">        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=<span class="literal">True</span>)</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.out.parameters())))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            self.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parameters) &gt; <span class="number">0</span>:</span><br><span class="line">            self.all_parameters[<span class="string">&quot;basic_parameters&quot;</span>] = parameters</span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">&#x27;Build model with cnn word encoder, lstm sent encoder.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        para_num = <span class="built_in">sum</span>([np.prod(<span class="built_in">list</span>(p.size())) <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters()])</span><br><span class="line">        logging.info(<span class="string">&#x27;Model param num: %.2f M.&#x27;</span> % (para_num / <span class="number">1e6</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch_inputs</span>):</span></span><br><span class="line">        <span class="comment"># batch_inputs(batch_inputs1, batch_inputs2): b * doc_len * sentence_len</span></span><br><span class="line">        <span class="comment"># batch_masks : b * doc_len * sentence_len</span></span><br><span class="line">        batch_inputs1, batch_inputs2, batch_masks = batch_inputs</span><br><span class="line">        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[<span class="number">0</span>], batch_inputs1.shape[<span class="number">1</span>], batch_inputs1.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># batch_inputs1: sentence_num * sentence_len</span></span><br><span class="line">        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  </span><br><span class="line">        <span class="comment"># batch_inputs2: sentence_num * sentence_len</span></span><br><span class="line">        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)</span><br><span class="line">        <span class="comment"># batch_masks: sentence_num * sentence_len </span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  </span><br><span class="line">        <span class="comment"># sent_reps: sentence_num * sentence_rep_size</span></span><br><span class="line">        <span class="comment"># sen_num * (3*out_channel) =  sen_num * 300</span></span><br><span class="line">        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2) </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># sent_reps：b * doc_len * sent_rep_size</span></span><br><span class="line">        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  </span><br><span class="line">        <span class="comment"># batch_masks：b * doc_len * max_sent_len</span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  </span><br><span class="line">        <span class="comment"># sent_masks：b * doc_len any(2) 表示在 第二个维度上判断</span></span><br><span class="line">        <span class="comment"># 表示如果如果一个句子中有词 true，那么这个句子就是 true，用于给 lstm 过滤</span></span><br><span class="line">        sent_masks = batch_masks.<span class="built_in">bool</span>().<span class="built_in">any</span>(<span class="number">2</span>).<span class="built_in">float</span>()  <span class="comment"># b x doc_len</span></span><br><span class="line">        <span class="comment"># sent_hiddens: b * doc_len * num_directions * hidden_size</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  batch, seq_len, 2 * hidden_size</span></span><br><span class="line">        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># doc_reps: b * (2 * hidden_size)</span></span><br><span class="line">        <span class="comment"># atten_scores: b * doc_len</span></span><br><span class="line">        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># b * num_labels</span></span><br><span class="line">        batch_outputs = self.out(doc_reps)  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model(vocab)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-5-定义-Optimizer"><a href="#4-2-5-定义-Optimizer" class="headerlink" title="4.2.5 定义 Optimizer"></a>4.2.5 定义 Optimizer</h3><p>这部分比较容易理解，就是把所有的参数都添加到<code>Optimizer</code>里，定义一些辅助函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build optimizer</span></span><br><span class="line">learning_rate = <span class="number">2e-4</span></span><br><span class="line">decay = <span class="number">.75</span></span><br><span class="line">decay_step = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_parameters</span>):</span></span><br><span class="line">        self.all_params = []</span><br><span class="line">        self.optims = []</span><br><span class="line">        self.schedulers = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, parameters <span class="keyword">in</span> model_parameters.items():</span><br><span class="line">            <span class="keyword">if</span> name.startswith(<span class="string">&quot;basic&quot;</span>):</span><br><span class="line">                optim = torch.optim.Adam(parameters, lr=learning_rate)</span><br><span class="line">                self.optims.append(optim)</span><br><span class="line"></span><br><span class="line">                l = <span class="keyword">lambda</span> step: decay ** (step // decay_step)</span><br><span class="line">                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)</span><br><span class="line">                self.schedulers.append(scheduler)</span><br><span class="line">                self.all_params.extend(parameters)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                Exception(<span class="string">&quot;no nameed parameters.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.num = <span class="built_in">len</span>(self.optims)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> optim, scheduler <span class="keyword">in</span> <span class="built_in">zip</span>(self.optims, self.schedulers):</span><br><span class="line">            optim.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> optim <span class="keyword">in</span> self.optims:</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        lrs = <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.get_lr()[-<span class="number">1</span>], self.schedulers))</span><br><span class="line">        lr = <span class="string">&#x27; %.5f&#x27;</span> * self.num</span><br><span class="line">        res = lr % lrs</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="4-2-6定义-sentence-split，把文章划分为句子"><a href="#4-2-6定义-sentence-split，把文章划分为句子" class="headerlink" title="4.2.6定义 sentence_split，把文章划分为句子"></a>4.2.6定义 sentence_split，把文章划分为句子</h3><p>输入的<code>text</code>表示一篇新闻，最后返回的 segments 是一个list，其中每个元素是 tuple：(句子长度，句子本身)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 作用是：根据一篇文章，把这篇文章分割成多个句子</span></span><br><span class="line"><span class="comment"># text 是一个新闻的文章</span></span><br><span class="line"><span class="comment"># vocab 是词典</span></span><br><span class="line"><span class="comment"># max_sent_len 表示每句话的长度</span></span><br><span class="line"><span class="comment"># max_segment 表示最多有几句话</span></span><br><span class="line"><span class="comment"># 最后返回的 segments 是一个list，其中每个元素是 tuple：(句子长度，句子本身)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_split</span>(<span class="params">text, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">16</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    words = text.strip().split()</span><br><span class="line">    document_len = <span class="built_in">len</span>(words)</span><br><span class="line">    <span class="comment"># 划分句子的索引，句子长度为 max_sent_len</span></span><br><span class="line">    index = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, document_len, max_sent_len))</span><br><span class="line">    index.append(document_len)</span><br><span class="line"></span><br><span class="line">    segments = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(index) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 根据索引划分句子</span></span><br><span class="line">        segment = words[index[i]: index[i + <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(segment) &gt; <span class="number">0</span></span><br><span class="line">        <span class="comment"># 把出现太少的词替换为 UNK</span></span><br><span class="line">        segment = [word <span class="keyword">if</span> word <span class="keyword">in</span> vocab._id2word <span class="keyword">else</span> <span class="string">&#x27;&lt;UNK&gt;&#x27;</span> <span class="keyword">for</span> word <span class="keyword">in</span> segment]</span><br><span class="line">        <span class="comment"># 添加 tuple:(句子长度，句子本身)</span></span><br><span class="line">        segments.append([<span class="built_in">len</span>(segment), segment])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(segments) &gt; <span class="number">0</span></span><br><span class="line">    <span class="comment"># 如果大于 max_segment 句话，则局数减少一半，返回一半的句子</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(segments) &gt; max_segment:</span><br><span class="line">        segment_ = <span class="built_in">int</span>(max_segment / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> segments[:segment_] + segments[-segment_:]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 否则返回全部句子</span></span><br><span class="line">        <span class="keyword">return</span> segments</span><br></pre></td></tr></table></figure>
<h3 id="4-2-7-定义-get-examples"><a href="#4-2-7-定义-get-examples" class="headerlink" title="4.2.7 定义 get_examples"></a>4.2.7 定义 get_examples</h3><p>遍历每一篇新闻，对每篇新闻都调用<code>sentence_split</code>来分割句子。</p>
<p>最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)。其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_examples</span>(<span class="params">data, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">8</span></span>):</span></span><br><span class="line">    label2id = vocab.label2id</span><br><span class="line">    examples = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text, label <span class="keyword">in</span> <span class="built_in">zip</span>(data[<span class="string">&#x27;text&#x27;</span>], data[<span class="string">&#x27;label&#x27;</span>]):</span><br><span class="line">        <span class="comment"># label</span></span><br><span class="line">        <span class="built_in">id</span> = label2id(label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sents_words: 是一个list，其中每个元素是 tuple：(句子长度，句子本身)</span></span><br><span class="line">        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)</span><br><span class="line">        doc = []</span><br><span class="line">        <span class="keyword">for</span> sent_len, sent_words <span class="keyword">in</span> sents_words:</span><br><span class="line">            <span class="comment"># 把 word 转为 id</span></span><br><span class="line">            word_ids = vocab.word2id(sent_words)</span><br><span class="line">            <span class="comment"># 把 word 转为 ext id</span></span><br><span class="line">            extword_ids = vocab.extword2id(sent_words)</span><br><span class="line">            doc.append([sent_len, word_ids, extword_ids])</span><br><span class="line">        examples.append([<span class="built_in">id</span>, <span class="built_in">len</span>(doc), doc])</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">&#x27;Total %d docs.&#x27;</span> % <span class="built_in">len</span>(examples))</span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h3 id="4-2-8-定义-batch-slice"><a href="#4-2-8-定义-batch-slice" class="headerlink" title="4.2.8 定义 batch_slice"></a>4.2.8 定义 batch_slice</h3><p>把数据分割为多个 batch，组成一个 list 并返回</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build loader</span></span><br><span class="line"><span class="comment"># data 参数就是 get_examples() 得到的</span></span><br><span class="line"><span class="comment"># data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_slice</span>(<span class="params">data, batch_size</span>):</span></span><br><span class="line">    batch_num = <span class="built_in">int</span>(np.ceil(<span class="built_in">len</span>(data) / <span class="built_in">float</span>(batch_size)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_num):</span><br><span class="line">        <span class="comment"># 如果 i &lt; batch_num - 1，那么大小为 batch_size，否则就是最后一批数据</span></span><br><span class="line">        cur_batch_size = batch_size <span class="keyword">if</span> i &lt; batch_num - <span class="number">1</span> <span class="keyword">else</span> <span class="built_in">len</span>(data) - batch_size * i</span><br><span class="line">        docs = [data[i * batch_size + b] <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(cur_batch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> docs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="4-2-9-定义-data-iter"><a href="#4-2-9-定义-data-iter" class="headerlink" title="4.2.9 定义 data_iter"></a>4.2.9 定义 data_iter</h3><p>在迭代训练时，调用<code>data_iter</code>函数，生成每一批的<code>batch_data</code>。而<code>data_iter</code>函数里面会调用<code>batch_slice</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data 参数就是 get_examples() 得到的</span></span><br><span class="line"><span class="comment"># data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">data, batch_size, shuffle=<span class="literal">True</span>, noise=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    randomly permute data, then sort by source length, and partition into batches</span></span><br><span class="line"><span class="string">    ensure that the length of  sentences in each batch</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    batched_data = []</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 这里是打乱所有数据</span></span><br><span class="line">        np.random.shuffle(data)</span><br><span class="line">        <span class="comment"># lengths 表示的是 每篇文章的句子数量</span></span><br><span class="line">        lengths = [example[<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> data] </span><br><span class="line">        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) <span class="keyword">for</span> l <span class="keyword">in</span> lengths]</span><br><span class="line">        sorted_indices = np.argsort(noisy_lengths).tolist()</span><br><span class="line">        sorted_data = [data[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sorted_data = data</span><br><span class="line">    <span class="comment"># 把 batch 的数据放进一个 list    </span></span><br><span class="line">    batched_data.extend(<span class="built_in">list</span>(batch_slice(sorted_data, batch_size)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 打乱 多个 batch</span></span><br><span class="line">        np.random.shuffle(batched_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> batched_data:</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure>
<h3 id="4-2-10-定义指标计算"><a href="#4-2-10-定义指标计算" class="headerlink" title="4.2.10 定义指标计算"></a>4.2.10 定义指标计算</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># some function</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span>(<span class="params">y_ture, y_pred</span>):</span></span><br><span class="line">    y_ture = np.array(y_ture)</span><br><span class="line">    y_pred = np.array(y_pred)</span><br><span class="line">    f1 = f1_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line">    p = precision_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line">    r = recall_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>((reformat(p, <span class="number">2</span>), reformat(r, <span class="number">2</span>), reformat(f1, <span class="number">2</span>))), reformat(f1, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留 n 位小数点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat</span>(<span class="params">num, n</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(<span class="built_in">format</span>(num, <span class="string">&#x27;0.&#x27;</span> + <span class="built_in">str</span>(n) + <span class="string">&#x27;f&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="4-2-11-定义训练和测试的方法"><a href="#4-2-11-定义训练和测试的方法" class="headerlink" title="4.2.11 定义训练和测试的方法"></a>4.2.11 定义训练和测试的方法</h3><p>比较难看懂的是<code>batch2tensor</code>函数。<code>batch2tensor</code>函数最后返回的数据是：<code>(batch_inputs1, batch_inputs2, batch_masks), batch_labels</code>。形状都是<code>(batch_size, doc_len, sent_len)</code>。<code>doc_len</code>表示每篇新闻有几乎话，<code>sent_len</code>表示每句话有多少个单词。</p>
<p><code>batch_masks</code>在有单词的位置，值为1，其他地方为 0，用于后面计算 Attention，把那些没有单词的位置的 attention 改为 0。</p>
<p><code>batch_inputs1, batch_inputs2, batch_masks</code>，形状是<code>(batch_size, doc_len, sent_len)</code>，转换为<code>(batch_size * doc_len, sent_len)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build trainer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">clip = <span class="number">5.0</span></span><br><span class="line">epochs = <span class="number">1</span></span><br><span class="line">early_stops = <span class="number">3</span></span><br><span class="line">log_interval = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">test_batch_size = <span class="number">128</span></span><br><span class="line">train_batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">save_model = <span class="string">&#x27;./cnn.bin&#x27;</span></span><br><span class="line">save_test = <span class="string">&#x27;./cnn.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model, vocab</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.report = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get_examples() 返回的结果是 一个 list</span></span><br><span class="line">        <span class="comment"># 每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">        <span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">        self.train_data = get_examples(train_data, vocab)</span><br><span class="line">        self.batch_num = <span class="built_in">int</span>(np.ceil(<span class="built_in">len</span>(self.train_data) / <span class="built_in">float</span>(train_batch_size)))</span><br><span class="line">        self.dev_data = get_examples(dev_data, vocab)</span><br><span class="line">        self.test_data = get_examples(test_data, vocab)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># criterion</span></span><br><span class="line">        self.criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label name</span></span><br><span class="line">        self.target_names = vocab.target_names</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        self.optimizer = Optimizer(model.all_parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># count</span></span><br><span class="line">        self.step = <span class="number">0</span></span><br><span class="line">        self.early_stop = -<span class="number">1</span></span><br><span class="line">        self.best_train_f1, self.best_dev_f1 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        self.last_epoch = epochs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        logging.info(<span class="string">&#x27;Start training...&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">            train_f1 = self._train(epoch)</span><br><span class="line"></span><br><span class="line">            dev_f1 = self._<span class="built_in">eval</span>(epoch)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.best_dev_f1 &lt;= dev_f1:</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">&quot;Exceed history dev = %.2f, current dev = %.2f&quot;</span> % (self.best_dev_f1, dev_f1))</span><br><span class="line">                torch.save(self.model.state_dict(), save_model)</span><br><span class="line"></span><br><span class="line">                self.best_train_f1 = train_f1</span><br><span class="line">                self.best_dev_f1 = dev_f1</span><br><span class="line">                self.early_stop = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.early_stop += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> self.early_stop == early_stops:</span><br><span class="line">                    logging.info(</span><br><span class="line">                        <span class="string">&quot;Eearly stop in epoch %d, best train: %.2f, dev: %.2f&quot;</span> % (</span><br><span class="line">                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))</span><br><span class="line">                    self.last_epoch = epoch</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model.load_state_dict(torch.load(save_model))</span><br><span class="line">        self._<span class="built_in">eval</span>(self.last_epoch + <span class="number">1</span>, test=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train</span>(<span class="params">self, epoch</span>):</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        self.model.train()</span><br><span class="line"></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        epoch_start_time = time.time()</span><br><span class="line">        overall_losses = <span class="number">0</span></span><br><span class="line">        losses = <span class="number">0</span></span><br><span class="line">        batch_idx = <span class="number">1</span></span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(self.train_data, train_batch_size, shuffle=<span class="literal">True</span>):</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line">            <span class="comment"># batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)</span></span><br><span class="line">            <span class="comment"># 形状都是：batch_size * doc_len * sent_len</span></span><br><span class="line">            <span class="comment"># batch_labels: batch_size</span></span><br><span class="line">            batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">            <span class="comment"># batch_outputs：b * num_labels</span></span><br><span class="line">            batch_outputs = self.model(batch_inputs)</span><br><span class="line">            <span class="comment"># criterion 是 CrossEntropyLoss，真实标签的形状是：N</span></span><br><span class="line">            <span class="comment"># 预测标签的形状是：(N,C)</span></span><br><span class="line">            loss = self.criterion(batch_outputs, batch_labels)</span><br><span class="line">            </span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            loss_value = loss.detach().cpu().item()</span><br><span class="line">            losses += loss_value</span><br><span class="line">            overall_losses += loss_value</span><br><span class="line">            <span class="comment"># 把预测值转换为一维，方便下面做 classification_report，计算 f1</span></span><br><span class="line">            y_pred.extend(torch.<span class="built_in">max</span>(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">            y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)</span><br><span class="line">            <span class="keyword">for</span> optimizer, scheduler <span class="keyword">in</span> <span class="built_in">zip</span>(self.optimizer.optims, self.optimizer.schedulers):</span><br><span class="line">                optimizer.step()</span><br><span class="line">                scheduler.step()</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            self.step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">                elapsed = time.time() - start_time</span><br><span class="line">                </span><br><span class="line">                lrs = self.optimizer.get_lr()</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">&#x27;| epoch &#123;:3d&#125; | step &#123;:3d&#125; | batch &#123;:3d&#125;/&#123;:3d&#125; | lr&#123;&#125; | loss &#123;:.4f&#125; | s/batch &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        epoch, self.step, batch_idx, self.batch_num, lrs,</span><br><span class="line">                        losses / log_interval,</span><br><span class="line">                        elapsed / log_interval))</span><br><span class="line">                </span><br><span class="line">                losses = <span class="number">0</span></span><br><span class="line">                start_time = time.time()</span><br><span class="line">                </span><br><span class="line">            batch_idx += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        overall_losses /= self.batch_num</span><br><span class="line">        during_time = time.time() - epoch_start_time</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reformat 保留 4 位数字</span></span><br><span class="line">        overall_losses = reformat(overall_losses, <span class="number">4</span>)</span><br><span class="line">        score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">        logging.info(</span><br><span class="line">            <span class="string">&#x27;| epoch &#123;:3d&#125; | score &#123;&#125; | f1 &#123;&#125; | loss &#123;:.4f&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, score, f1,</span><br><span class="line">                                                                                  overall_losses, during_time))</span><br><span class="line">        <span class="comment"># 如果预测和真实的标签都包含相同的类别数目，才能调用 classification_report                                                                        </span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">set</span>(y_true) == <span class="built_in">set</span>(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">            report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">            logging.info(<span class="string">&#x27;\n&#x27;</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里验证集、测试集都使用这个函数，通过 test 来区分使用哪个数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_eval</span>(<span class="params">self, epoch, test=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        data = self.test_data <span class="keyword">if</span> test <span class="keyword">else</span> self.dev_data</span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(data, test_batch_size, shuffle=<span class="literal">False</span>):</span><br><span class="line">                torch.cuda.empty_cache()</span><br><span class="line">                            <span class="comment"># batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)</span></span><br><span class="line">            <span class="comment"># 形状都是：batch_size * doc_len * sent_len</span></span><br><span class="line">            <span class="comment"># batch_labels: batch_size                                                                  </span></span><br><span class="line">                batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">                <span class="comment"># batch_outputs：b * num_labels                                                                  </span></span><br><span class="line">                batch_outputs = self.model(batch_inputs)</span><br><span class="line">                <span class="comment"># 把预测值转换为一维，方便下面做 classification_report，计算 f1                                                                  </span></span><br><span class="line">                y_pred.extend(torch.<span class="built_in">max</span>(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">                y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">            score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">            during_time = time.time() - start_time</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> test:</span><br><span class="line">                df = pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>: y_pred&#125;)</span><br><span class="line">                df.to_csv(save_test, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logging.info(</span><br><span class="line">                    <span class="string">&#x27;| epoch &#123;:3d&#125; | dev | score &#123;&#125; | f1 &#123;&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, score, f1,</span><br><span class="line">                                                                              during_time))</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">set</span>(y_true) == <span class="built_in">set</span>(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">                    report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">                    logging.info(<span class="string">&#x27;\n&#x27;</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># data 参数就是 get_examples() 得到的，经过了分 batch</span></span><br><span class="line">    <span class="comment"># batch_data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">    <span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch2tensor</span>(<span class="params">self, batch_data</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        batch_size = <span class="built_in">len</span>(batch_data)</span><br><span class="line">        doc_labels = []</span><br><span class="line">        doc_lens = []</span><br><span class="line">        doc_max_sent_len = []</span><br><span class="line">        <span class="keyword">for</span> doc_data <span class="keyword">in</span> batch_data:</span><br><span class="line">            <span class="comment"># doc_data 代表一篇新闻，是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">            <span class="comment"># doc_data[0] 是 label</span></span><br><span class="line">            doc_labels.append(doc_data[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># doc_data[1] 是 这篇文章的句子数量</span></span><br><span class="line">            doc_lens.append(doc_data[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># doc_data[2] 是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">            <span class="comment"># 所以 sent_data[0] 表示每个句子的长度（单词个数）</span></span><br><span class="line">            sent_lens = [sent_data[<span class="number">0</span>] <span class="keyword">for</span> sent_data <span class="keyword">in</span> doc_data[<span class="number">2</span>]]</span><br><span class="line">            <span class="comment"># 取出这篇新闻中最长的句子长度（单词个数）</span></span><br><span class="line">            max_sent_len = <span class="built_in">max</span>(sent_lens)</span><br><span class="line">            doc_max_sent_len.append(max_sent_len)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 取出最长的句子数量</span></span><br><span class="line">        max_doc_len = <span class="built_in">max</span>(doc_lens)</span><br><span class="line">        <span class="comment"># 取出这批 batch 数据中最长的句子长度（单词个数）</span></span><br><span class="line">        max_sent_len = <span class="built_in">max</span>(doc_max_sent_len)</span><br><span class="line">        <span class="comment"># 创建 数据</span></span><br><span class="line">        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)</span><br><span class="line">        batch_labels = torch.LongTensor(doc_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            <span class="keyword">for</span> sent_idx <span class="keyword">in</span> <span class="built_in">range</span>(doc_lens[b]):</span><br><span class="line">                <span class="comment"># batch_data[b][2] 表示一个 list，是一篇文章中的句子</span></span><br><span class="line">                sent_data = batch_data[b][<span class="number">2</span>][sent_idx] <span class="comment">#sent_data 表示一个句子</span></span><br><span class="line">                <span class="keyword">for</span> word_idx <span class="keyword">in</span> <span class="built_in">range</span>(sent_data[<span class="number">0</span>]): <span class="comment"># sent_data[0] 是句子长度(单词数量)</span></span><br><span class="line">                    <span class="comment"># sent_data[1] 表示 word_ids</span></span><br><span class="line">                    batch_inputs1[b, sent_idx, word_idx] = sent_data[<span class="number">1</span>][word_idx]</span><br><span class="line">                    <span class="comment"># # sent_data[2] 表示 extword_ids</span></span><br><span class="line">                    batch_inputs2[b, sent_idx, word_idx] = sent_data[<span class="number">2</span>][word_idx]</span><br><span class="line">                    <span class="comment"># mask 表示 哪个位置是有词，后面计算 attention 时，没有词的地方会被置为 0                                               </span></span><br><span class="line">                    batch_masks[b, sent_idx, word_idx] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            batch_inputs1 = batch_inputs1.to(device)</span><br><span class="line">            batch_inputs2 = batch_inputs2.to(device)</span><br><span class="line">            batch_masks = batch_masks.to(device)</span><br><span class="line">            batch_labels = batch_labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (batch_inputs1, batch_inputs2, batch_masks), batch_labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line">trainer = Trainer(model, vocab)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">trainer.test()</span><br></pre></td></tr></table></figure>
<p>至此，整个流程就讲解完了。希望对你有所帮助。</p>
<p><br></p>
<p>如果你觉得这篇文章对你有帮助，不妨点个赞，让我有更多动力写出好文章。<br><br></p>
<p>我的文章会首发在公众号上，欢迎扫码关注我的公众号<strong>张贤同学</strong>。</p>
<p><div align="center"><img src="https://image.zhangxiann.com/QRcode_8cm.jpg"/></div><br></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zhxnlp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhxnlp.github.io/2021/10/09/天池-新闻文本分类/textcnn/阿里天池 NLP 入门赛 TextCNN 方案代码详细注释和流程讲解/">https://zhxnlp.github.io/2021/10/09/天池-新闻文本分类/textcnn/阿里天池 NLP 入门赛 TextCNN 方案代码详细注释和流程讲解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhxnlp.github.io">zhxnlpのBlog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/11/09/CLUENER%20%E7%BB%86%E7%B2%92%E5%BA%A6%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/README/"><i class="fa fa-chevron-left">  </i><span>CLUENER 命名实体识别任务介绍</span></a></div><div class="next-post pull-right"><a href="/2021/10/08/huggingface/hugging%20face%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E2%80%94%E2%80%94datasets%E3%80%81optimizer/"><span>Hugging Face官方文档——datasets、optimizer</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://img-blog.csdnimg.cn/ffe1f736be4548dc9101388503288e4d.png)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By zhxnlp</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>